<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://USTCcoder.github.io/"/>
  <updated>2020-08-30T01:48:21.927Z</updated>
  <id>https://USTCcoder.github.io/</id>
  
  <author>
    <name>USTCcoder</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>C++容器(Set)</title>
    <link href="https://USTCcoder.github.io/2020/09/03/C++_stl_set/"/>
    <id>https://USTCcoder.github.io/2020/09/03/C++_stl_set/</id>
    <published>2020-09-02T16:11:00.000Z</published>
    <updated>2020-08-30T01:48:21.927Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++73.png" alt="2"></p><h1 id="C-容器-Set"><a href="#C-容器-Set" class="headerlink" title="C++容器(Set)"></a><font size="5" color="red">C++容器(Set)</font></h1><p>  容器是C++非常方便的功能，今天给小伙伴们介绍set库，set库是C++中的哈希表，也可以称其为集合，其特点是每个元素只出现一次，可以查找某个元素是否在集合中出现，在判重或者记忆化等题目中有很好的使用场景，是小伙伴们必须掌握的一种数据结构。<br><a id="more"></a></p><h1 id="set容器"><a href="#set容器" class="headerlink" title="set容器"></a><font size="5">set容器</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;set&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">// 为了方便显示，在这里将打印操作定义为一个函数</span><br><span class="line">// 注意，set集合中插入元素时会自动进行排序，因此传入其他类型时，要加载比较函数。</span><br><span class="line">void printSet(set&lt;int&gt; s) {</span><br><span class="line">cout &lt;&lt; "s：";</span><br><span class="line">for (set&lt;int&gt;::iterator it = s.begin(); it != s.end(); it++) {</span><br><span class="line">cout &lt;&lt; *it &lt;&lt; " ";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">// 创建一个set容器，同vector相同，可以存放任意类型的数据，但是需要指定数据类型。</span><br><span class="line">// 而且建立以后只能存放该类型的数据。</span><br><span class="line">set&lt;int&gt; s = { 6, 2, 4 };</span><br><span class="line">printSet(s);</span><br><span class="line"></span><br><span class="line">// empty判断容器是否为空，为空则返回true，否则返回false</span><br><span class="line">cout &lt;&lt; "s是否为空：" &lt;&lt; s.empty() &lt;&lt; endl;</span><br><span class="line">// set对象没有capacity方法，只有size方法，和vector相同，可以获取容器中的元素个数。</span><br><span class="line">cout &lt;&lt; "s的尺寸为：" &lt;&lt; s.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// insert是set中最最重要的一个方法，用于向容器中添加元素，</span><br><span class="line">// 虽然其他容器也有insert方法，但是我并没有介绍，因为要传入一个迭代器对象，而且操作并不是很方便。</span><br><span class="line">// 但是set集合的insert非常方便，也是唯一一个能够增加元素的方法。</span><br><span class="line">s.insert(3);</span><br><span class="line">s.insert(1);</span><br><span class="line">s.insert(5);</span><br><span class="line">s.insert(1);</span><br><span class="line">printSet(s);</span><br><span class="line">cout &lt;&lt; "s是否为空：" &lt;&lt; s.empty() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "s的尺寸为：" &lt;&lt; s.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// erase方法和insert相反，将set集合中的元素移除。</span><br><span class="line">// 在其他容器中也有erase方法，但是也要传入迭代器对象，因此也没有过多介绍。</span><br><span class="line">s.erase(2);</span><br><span class="line">printSet(s);</span><br><span class="line">cout &lt;&lt; "s是否为空：" &lt;&lt; s.empty() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "s的尺寸为：" &lt;&lt; s.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// count方法也是set集合非常重要的方法，一般使用set集合，就是为了判断某个元素是否存在于集合之中。</span><br><span class="line">// count值为0说明元素不在其中，count元素为1说明在集合中。</span><br><span class="line">// 为什么不返回布尔类型呢，小伙伴们可以参考multiset数据类型，是一种可插入重复元素的集合。</span><br><span class="line">cout &lt;&lt; "1的个数为：" &lt;&lt; s.count(1) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "2的个数为：" &lt;&lt; s.count(2) &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// clear清空容器中的所有内容</span><br><span class="line">s.clear();</span><br><span class="line">cout &lt;&lt; "s是否为空：" &lt;&lt; s.empty() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "s的尺寸为：" &lt;&lt; s.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++74.png" alt="1"></p><h1 id="unordered-set容器"><a href="#unordered-set容器" class="headerlink" title="unordered_set容器"></a><font size="5">unordered_set容器</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;unordered_set&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">// 为了方便显示，在这里将打印操作定义为一个函数</span><br><span class="line">// 注意，unordered_set集合中插入元素时不会自动进行排序，其顺序为创建时和插入的顺序。因此可以传入任意类型。</span><br><span class="line">void printUnorderedSet(unordered_set&lt;int&gt; s) {</span><br><span class="line">cout &lt;&lt; "s：";</span><br><span class="line">for (unordered_set&lt;int&gt;::iterator it = s.begin(); it != s.end(); it++) {</span><br><span class="line">cout &lt;&lt; *it &lt;&lt; " ";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">// 创建一个unordered_set容器，创建方法与set相同</span><br><span class="line">// 而且建立以后只能存放该类型的数据。</span><br><span class="line">unordered_set&lt;int&gt; s = { 6, 2, 4 };</span><br><span class="line">printUnorderedSet(s);</span><br><span class="line"></span><br><span class="line">// 其他方法和set容器几乎完全相同，可以参考上面的内容，在这也不做过多赘述。</span><br><span class="line">cout &lt;&lt; "s是否为空：" &lt;&lt; s.empty() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "s的尺寸为：" &lt;&lt; s.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">s.insert(3);</span><br><span class="line">s.insert(1);</span><br><span class="line">s.insert(5);</span><br><span class="line">s.insert(1);</span><br><span class="line">printSet(s);</span><br><span class="line">cout &lt;&lt; "s是否为空：" &lt;&lt; s.empty() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "s的尺寸为：" &lt;&lt; s.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">s.erase(2);</span><br><span class="line">printSet(s);</span><br><span class="line">cout &lt;&lt; "s是否为空：" &lt;&lt; s.empty() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "s的尺寸为：" &lt;&lt; s.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "1的个数为：" &lt;&lt; s.count(1) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "2的个数为：" &lt;&lt; s.count(2) &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">s.clear();</span><br><span class="line">cout &lt;&lt; "s是否为空：" &lt;&lt; s.empty() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "s的尺寸为：" &lt;&lt; s.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++79.png" alt="1"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  哈希表没有front和back方法，也无法通过索引进行遍历。使用哈希表时，只要记住4个重要的函数即可，insert，erase，count，size。常常在BFS或者DFS或者数组中出现，如搜索时，该点已经搜索完成，则可以将其加入哈希表，下次查到该点时则可以不再进行查找，节约大量的时间，因此小伙伴们一定要掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++容器(Set)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++容器(List)</title>
    <link href="https://USTCcoder.github.io/2020/08/31/C++_stl_list/"/>
    <id>https://USTCcoder.github.io/2020/08/31/C++_stl_list/</id>
    <published>2020-08-31T15:51:40.000Z</published>
    <updated>2020-08-25T16:10:17.437Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++77.png" alt="2"></p><h1 id="C-容器-List"><a href="#C-容器-List" class="headerlink" title="C++容器(List)"></a><font size="5" color="red">C++容器(List)</font></h1><p>  容器是C++非常方便的功能，今天给小伙伴们介绍List库，List库是C++中的链表容器，其优点是不会造成内存的浪费和溢出，这和Vector相反，而且插入元素非常方便。但是缺点也很明显，需要一个额外的内存空间存放指针，而且因为其不按照内存进行存放，因此遍历花费时间较长。<br><a id="more"></a></p><h1 id="List容器"><a href="#List容器" class="headerlink" title="List容器"></a><font size="5">List容器</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;list&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">// 为了方便显示，在这里将打印操作定义为一个函数</span><br><span class="line">void printList(list&lt;int&gt; l) {</span><br><span class="line">cout &lt;&lt; "l：";</span><br><span class="line">for (list&lt;int&gt;::iterator it = l.begin(); it != l.end(); it++) {</span><br><span class="line">cout &lt;&lt; *it &lt;&lt; " ";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">// 创建一个list容器，同vector相同，可以存放任意类型的数据，但是需要指定数据类型。</span><br><span class="line">// 而且建立以后只能存放该类型的数据。</span><br><span class="line">list&lt;int&gt; l = { 2, 3, 4 };</span><br><span class="line"></span><br><span class="line">// empty判断容器是否为空，为空则返回true，否则返回false</span><br><span class="line">cout &lt;&lt; "l是否为空：" &lt;&lt; l.empty() &lt;&lt; endl;</span><br><span class="line">// list对象没有capacity方法，只有size方法，和vector相同，可以获取容器中的元素个数。</span><br><span class="line">cout &lt;&lt; "l的尺寸为：" &lt;&lt; l.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// resize是将list调整大小，如果大于原始数组，则填充数值，达到resize的大小，否则会删除超出的部分。</span><br><span class="line">l.resize(10);</span><br><span class="line">printList(l);</span><br><span class="line">cout &lt;&lt; "l的尺寸为：" &lt;&lt; l.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// push_front是list最经常用的操作，在头部添加元素。</span><br><span class="line">// push_back在尾部添加元素。</span><br><span class="line">l.push_front(1);</span><br><span class="line">l.push_back(4);</span><br><span class="line">printList(l);</span><br><span class="line">cout &lt;&lt; "l的尺寸为：" &lt;&lt; l.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// 和push_front相反，pop_front时在头部移除元素。</span><br><span class="line">//pop_back在尾部移除元素。</span><br><span class="line">l.pop_front();</span><br><span class="line">l.pop_back();</span><br><span class="line">printList(l);</span><br><span class="line">cout &lt;&lt; "l的尺寸为：" &lt;&lt; l.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// front获取list第一个元素，但是不能使用l[0]，因为list是不连续的内存空间，因此无法通过索引直接访问。</span><br><span class="line">cout &lt;&lt; "第一个元素为：" &lt;&lt; l.front() &lt;&lt; endl;</span><br><span class="line">// back获取list最后一个元素。</span><br><span class="line">cout &lt;&lt; "最后一个元素为：" &lt;&lt; l.back() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// remove移除所有值为目标的节点</span><br><span class="line">l.remove(0);</span><br><span class="line">printList(l);</span><br><span class="line">cout &lt;&lt; "l的尺寸为：" &lt;&lt; l.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// clear清空容器中的所有内容</span><br><span class="line">l.clear();</span><br><span class="line">printList(l);</span><br><span class="line">cout &lt;&lt; "l的尺寸为：" &lt;&lt; l.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++78.png" alt="1"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  list容器使用并不是很频繁，一般都是使用vector和deque就可以完成大部分的应用场景，而且list不方便的地方在于无法通过下表对元素进行索引，因此小伙伴们作为了解即可。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++容器(List)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>最长上升子序列(Leetcode 300)</title>
    <link href="https://USTCcoder.github.io/2020/08/30/program%20Leetcode300/"/>
    <id>https://USTCcoder.github.io/2020/08/30/program Leetcode300/</id>
    <published>2020-08-30T15:26:14.000Z</published>
    <updated>2020-09-04T16:03:25.762Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode300.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>  <strong>最长上升子序列(Longest  Increasing Subsequence, LIS)</strong>:这是非常有趣的一个题目，也是笔试面试常常出现的一道数组题，这道题目小伙伴们必须要掌握动态规划的解法，但是能否想到更加巧妙的方法呢？</p><a id="more"></a><h1 id="DP"><a href="#DP" class="headerlink" title="DP"></a><font size="5" color="red">DP</font></h1><p>我们使用动态规划进行求解，<strong>dp[i]表示右端点选择第i个数可得的最长上升子序列</strong>，可以得到状态转移方程</p><script type="math/tex; mode=display">dp[i] = \max_{j = 1}^{i - 1} dp[j] + 1 if nums[i] > nums[j]</script><p><strong>当考虑到所有情况后，就可以得到右端点为任何情况下的最大值，然后求dp数组的最大值即可</strong>。DP的<strong>时间复杂度为$O(n^2)$，空间复杂度为$O(n)$</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def lengthOfLIS(self, nums):</span><br><span class="line">        lens = len(nums)</span><br><span class="line">        dp = [1] * lens </span><br><span class="line">        for i in range(1, lens):</span><br><span class="line">            for j in range(i):</span><br><span class="line">                if nums[i] &gt; nums[j]:</span><br><span class="line">                    dp[i] = max(dp[i], dp[j] + 1)</span><br><span class="line">        return max(dp) if lens else 0</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="贪心-二分查找"><a href="#贪心-二分查找" class="headerlink" title="贪心+二分查找"></a><font size="5" color="red">贪心+二分查找</font></h1><p>我们思考一些可能出现的情况，假设前k个元素的最长上升子序列为[1, 3, 5, 7, 9]。</p><ol><li><strong>如果第k + 1个元素大于9，假设为11，则最长上升子序列变为[1, 3, 5, 7, 9, 11]</strong>。</li><li><strong>如果第k + 1个元素小于9，假设为6，最长上升子序列长度不变，但是会将大于6的最小值变为6，则此时最长上升子序列变为[1, 3, 5, 6, 9]，当以后再出现7时，最长上升子序列的长度仍然不变，但是子序列就已经更新为更好的一组值了[1, 3, 5, 6, 7]</strong>。<br>也就是说<strong>当后续出现更小的值时，不会立刻改变最长子序列的长度，而是从中间的某个地方开始逐渐替换，当此时出现更大的数字时，仍然按照替换之前的子序列继续追加。如果此时出现较小的数字时，则继续替换。当替换到最后一个数字时，说明子序列已经出现了更优解，以后按照更优解进行计算</strong>。<br><strong>从上升子序列中寻找大于某个值的最小值时，可以使用二分查找法，这样可以将第二次遍历的时间复杂度大大缩小</strong>。算法的<strong>时间复杂度为O(nlog(n))，空间复杂度为O(n)</strong>。<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def lengthOfLIS(self, nums):</span><br><span class="line">        res = []</span><br><span class="line">        for x in nums:</span><br><span class="line">            if not res or res[-1] &lt; x:</span><br><span class="line">                res.append(x)</span><br><span class="line">            else:</span><br><span class="line">                left, right = 0, len(res)</span><br><span class="line">                while left &lt; right:</span><br><span class="line">                    mid = (left + right) // 2</span><br><span class="line">                    if res[mid] &gt;= x:</span><br><span class="line">                        right = mid</span><br><span class="line">                    else:</span><br><span class="line">                        left = mid + 1</span><br><span class="line">                res[left] = x</span><br><span class="line">        return len(res)</span><br></pre></td></tr></tbody></table></figure></li></ol><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  这个题目虽然动态规划是必须要掌握的，但是贪心+二分的思路也是非常非常重要的，因为一旦笔试面试中出现这个问题，往往不是考察动态规划的知识点。笔试中会给很大的数据量，面试时面试官也会问有没有更好的解法，所以小伙伴们一定要学会它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 300&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>C++容器(Stack &amp; Queue)</title>
    <link href="https://USTCcoder.github.io/2020/08/29/C++_stl_stack_queue/"/>
    <id>https://USTCcoder.github.io/2020/08/29/C++_stl_stack_queue/</id>
    <published>2020-08-29T15:40:33.000Z</published>
    <updated>2020-08-24T15:43:33.817Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++70.png" alt="2"></p><h1 id="C-容器-Stack-amp-Queue"><a href="#C-容器-Stack-amp-Queue" class="headerlink" title="C++容器(Stack &amp; Queue)"></a><font size="5" color="red">C++容器(Stack &amp; Queue)</font></h1><p>  容器是C++非常方便的功能，今天给小伙伴们介绍stack和queue库，一个是栈，一个是队列，在数据结构中这两个内容也是必须掌握的重点内容，在刷题时也常常使用它们，因此在一篇博客中将它们放在一起介绍。<br><a id="more"></a></p><h1 id="stack容器"><a href="#stack容器" class="headerlink" title="stack容器"></a><font size="5">stack容器</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;stack&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">// 创建栈的时候，只有拷贝构造和空构造方法，不能像vector一样传入一个数组。</span><br><span class="line">stack&lt;int&gt; s;</span><br><span class="line"></span><br><span class="line">// empty判断容器是否为空，为空则返回true，否则返回false</span><br><span class="line">cout &lt;&lt; "s是否为空：" &lt;&lt; s.empty() &lt;&lt; endl;</span><br><span class="line">// stack对象没有capacity方法，只有size方法，和vector相同，可以获取容器中的元素个数。</span><br><span class="line">cout &lt;&lt; "s的大小为：" &lt;&lt; s.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// 堆栈中最最重要的一个函数，push，这要简单介绍一下堆栈的特点，堆栈是一个先进后出的数据结构。</span><br><span class="line">// 生活中类似于洗盘子，进电梯，先洗的盘子都是放在最下面，最先进电梯的人是最后出来的。</span><br><span class="line">// 向堆栈中放入元素的方法之有一个，就是push，向栈顶放入元素，因此非常重要。</span><br><span class="line">s.push(1);</span><br><span class="line">s.push(2);</span><br><span class="line">s.push(3);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "s是否为空：" &lt;&lt; s.empty() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "s的大小为：" &lt;&lt; s.size() &lt;&lt; endl;</span><br><span class="line">// top可以查看堆栈顶部的元素，因为堆栈中只能看到顶部元素，因此不可以随机访问，也不能够遍历。</span><br><span class="line">// 因为3是最后入栈的，因此栈顶元素尾3</span><br><span class="line">cout &lt;&lt; "栈顶元素为：" &lt;&lt; s.top() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// 堆栈中和push相反的操作，和push的地位相同，pop弹出栈顶元素。</span><br><span class="line">s.pop();</span><br><span class="line">// 因为栈顶元素3被弹出，因此现在栈顶元素为2</span><br><span class="line">cout &lt;&lt; "栈顶元素为：" &lt;&lt; s.top() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "s是否为空：" &lt;&lt; s.empty() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "s的大小为：" &lt;&lt; s.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++71.png" alt="1"></p><h1 id="queue容器"><a href="#queue容器" class="headerlink" title="queue容器"></a><font size="5">queue容器</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;queue&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">// 创建队列的时候，和栈相同，只有拷贝构造和空构造方法。</span><br><span class="line">queue&lt;int&gt; q;</span><br><span class="line"></span><br><span class="line">// empty判断容器是否为空，为空则返回true，否则返回false</span><br><span class="line">cout &lt;&lt; "q是否为空：" &lt;&lt; q.empty() &lt;&lt; endl;</span><br><span class="line">// size方法，获取容器中的元素个数。</span><br><span class="line">cout &lt;&lt; "q的大小为：" &lt;&lt; q.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// 队列中最最重要的一个函数，push，这要简单介绍一下队列的特点，队列是一个先进先出的数据结构。</span><br><span class="line">// 生活中类似于排队购物，先排的人可以最先获得服务。</span><br><span class="line">// 向队列中放入元素的方法之有一个，就是push，向队列尾增加元素，因此非常重要。</span><br><span class="line">q.push(1);</span><br><span class="line">q.push(2);</span><br><span class="line">q.push(3);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "q是否为空：" &lt;&lt; q.empty() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "q的大小为：" &lt;&lt; q.size() &lt;&lt; endl;</span><br><span class="line">// 和栈不同的是，栈只能获取栈顶元素，而队列可以通过front获取队头元素，也可以通过back获取队尾元素。</span><br><span class="line">// 因为队列中只能看到队头和队尾元素，因此不可以随机访问，也不能够遍历。</span><br><span class="line">// 因为1先入队，3最后入队，所以队列头元素为1，队列尾元素为3</span><br><span class="line">cout &lt;&lt; "队列头元素为：" &lt;&lt; q.front() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "队列尾元素为：" &lt;&lt; q.back() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// 队列中和push相反的操作，和push的地位相同，pop弹出队列头部元素。</span><br><span class="line">q.pop();</span><br><span class="line">// 当弹出队列头元素时，2就变成了新的队列头元素，队列尾元素仍为3。</span><br><span class="line">cout &lt;&lt; "队列头元素为：" &lt;&lt; q.front() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "队列尾元素为：" &lt;&lt; q.back() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "q是否为空：" &lt;&lt; q.empty() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "q的大小为：" &lt;&lt; q.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++72.png" alt="1"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  栈和队列非常重要，相比较而言栈会更加常用一些，有一个经典的括号匹配的问题，其就是利用堆栈的思想，小伙伴可以去尝试一下呦。细心的小伙伴也可以发现vector能够看成一个堆栈，deque能够看成一个队列，而且功能也比stack和queue要强大，所以一般可以使用vector和deque完成相应的操作。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++容器(Stack &amp; Queue)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>最长公共子序列(Leetcode 1143)</title>
    <link href="https://USTCcoder.github.io/2020/08/28/program%20Leetcode1143/"/>
    <id>https://USTCcoder.github.io/2020/08/28/program Leetcode1143/</id>
    <published>2020-08-28T01:31:15.000Z</published>
    <updated>2020-09-03T02:05:46.795Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode1143.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   最长公共子序列(Longest Common Subsequence, LCS):是一个经典的问题，也是在某厂的笔试中遇到了这个问题，小伙伴们一定要会做呀。</p><a id="more"></a><h1 id="DP"><a href="#DP" class="headerlink" title="DP"></a><font size="5" color="red">DP</font></h1><p>我们使用动态规划进行求解，dp[i][j]表示text1前i个字符和text2前j个字符的最长公共子序列，<strong>因为空序列和任何序列匹配都是0，初始状态i = 0或j = 0时，dp[i][j] = 0</strong>。可以得到状态转移方程。</p><script type="math/tex; mode=display">dp[i][j] =  \begin{case} dp[i - 1][j - 1] + 1 & text[i - 1] = text[j - 1] \\ max(dp[i - 1][j], dp[i][j - 1]) & text[i - 1] \ne text[j - 1] \end{cases}</script><p>DP的<strong>时间复杂度为$O(n^2)$，空间复杂度为$O(n^2)$</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def longestCommonSubsequence(self, text1, text2):</span><br><span class="line">        len1, len2 = len(text1), len(text2)</span><br><span class="line">        dp = [[0 for _ in range(len2 + 1)] for _ in range(len1 + 1)]</span><br><span class="line">        for i in range(1, len1 + 1):</span><br><span class="line">            for j in range(1, len2 + 1):</span><br><span class="line">                if text1[i - 1] == text2[j - 1]:</span><br><span class="line">                    dp[i][j] = dp[i - 1][j - 1] + 1</span><br><span class="line">                else:</span><br><span class="line">                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])</span><br><span class="line">        return dp[-1][-1]</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="优化DP"><a href="#优化DP" class="headerlink" title="优化DP"></a><font size="5" color="red">优化DP</font></h1><p><strong>动态规划一般都可以进行优化，优化的方法大多是通过空间复杂度进行优化，因为DP中保存着大量的数据，可能这些数据我们在后面的计算中并不会用到</strong>。<br>如此题中第i次循环，只会用到第i次循环和第i - 1次循环中的内容，之前的计算结果并不会用到，因此可以只保留第i次的结果和第i - 1次的结果，这样优化DP的<strong>时间复杂度为$O(n^2)$，空间复杂度为$O(n)$</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def longestCommonSubsequence(self, text1, text2):</span><br><span class="line">        if len(text1) &lt; len(text2):</span><br><span class="line">            text1, text2 = text2, text1</span><br><span class="line">        len1, len2 = len(text1), len(text2)</span><br><span class="line">        dp_pre, dp_cur = [0] * (len2 + 1), [0] * (len2 + 1)</span><br><span class="line">        for i in range(1, len1 + 1):</span><br><span class="line">            for j in range(1, len2 + 1):</span><br><span class="line">                if text1[i - 1] == text2[j - 1]:</span><br><span class="line">                    dp_cur[j] = dp_pre[j - 1] + 1</span><br><span class="line">                else:</span><br><span class="line">                    dp_cur[j] = max(dp_pre[j], dp_cur[j - 1])</span><br><span class="line">            dp_cur, dp_pre = [0] * (len2 + 1), dp_cur</span><br><span class="line">        return dp_pre[-1]</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  LCS问题太经典了，不需要过多介绍，干就完事了。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 1143&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>C++容器(Deque)</title>
    <link href="https://USTCcoder.github.io/2020/08/27/C++_stl_deque/"/>
    <id>https://USTCcoder.github.io/2020/08/27/C++_stl_deque/</id>
    <published>2020-08-27T14:32:13.000Z</published>
    <updated>2020-08-25T16:33:37.455Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++68.png" alt="2"></p><h1 id="C-容器-Deque"><a href="#C-容器-Deque" class="headerlink" title="C++容器(Deque)"></a><font size="5" color="red">C++容器(Deque)</font></h1><p>  容器是C++非常方便的功能，今天给小伙伴们介绍deque库，vector库是C++专门用于处理动态双端数组的库，里面内置了许多增删改查的算法，在刷题时常常使用它。<br><a id="more"></a></p><h1 id="deque容器"><a href="#deque容器" class="headerlink" title="deque容器"></a><font size="5">deque容器</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;deque&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">// 为了方便显示，在这里将打印操作定义为一个函数</span><br><span class="line">void printDeque(deque&lt;int&gt; d) {</span><br><span class="line">cout &lt;&lt; "d：";</span><br><span class="line">for (deque&lt;int&gt;::iterator it = d.begin(); it &lt; d.end(); it++) {</span><br><span class="line">cout &lt;&lt; *it &lt;&lt; " ";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">// 创建一个deque容器，同vector相同，可以存放任意类型的数据，但是需要指定数据类型。</span><br><span class="line">// 而且建立以后只能存放该类型的数据。</span><br><span class="line">deque&lt;int&gt; d = { 2, 3, 4 };</span><br><span class="line">printDeque(d);</span><br><span class="line"></span><br><span class="line">// empty判断容器是否为空，为空则返回true，否则返回false</span><br><span class="line">cout &lt;&lt; "d是否为空：" &lt;&lt; d.empty() &lt;&lt; endl;</span><br><span class="line">// deque对象没有capacity方法，只有size方法，和vector相同，可以获取容器中的元素个数。</span><br><span class="line">cout &lt;&lt; "d的尺寸为：" &lt;&lt; d.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// resize是将deque调整大小，如果大于原始数组，则填充数值，达到resize的大小，否则会删除超出的部分。</span><br><span class="line">d.resize(10);</span><br><span class="line">printDeque(d);</span><br><span class="line">cout &lt;&lt; "d的尺寸为：" &lt;&lt; d.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// push_front是deque最经常用的操作，在头部添加元素，deque最核心的地方就在于此，头插可以节约大量时间。</span><br><span class="line">// push_back和vector相同，在尾部添加元素。</span><br><span class="line">d.push_front(1);</span><br><span class="line">d.push_back(4);</span><br><span class="line">printDeque(d);</span><br><span class="line">cout &lt;&lt; "d的尺寸为：" &lt;&lt; d.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// 和push_front相反，pop_front时在头部移除元素，除了push_front和pop_front，deque和vector其他的内容几乎相同。</span><br><span class="line">//pop_back和vector也相同，在尾部移除元素。</span><br><span class="line">d.pop_front();</span><br><span class="line">d.pop_back();</span><br><span class="line">printDeque(d);</span><br><span class="line">cout &lt;&lt; "d的尺寸为：" &lt;&lt; d.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// 索引元素可以直接类似于数组的写法，也可以使用at进行索引</span><br><span class="line">cout &lt;&lt; "d[2] = " &lt;&lt; d[2] &lt;&lt; endl;</span><br><span class="line">// front获取deque第一个元素，等价于d[0]</span><br><span class="line">cout &lt;&lt; "第一个元素为：" &lt;&lt; d.front() &lt;&lt; endl;</span><br><span class="line">// back获取deque最后一个元素，等价于d[d.size() - 1]</span><br><span class="line">cout &lt;&lt; "最后一个元素为：" &lt;&lt; d.back() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// clear清空容器中的所有内容</span><br><span class="line">d.clear();</span><br><span class="line">printDeque(d);</span><br><span class="line">cout &lt;&lt; "d的尺寸为：" &lt;&lt; d.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++69.png" alt="1"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  当然了deque的相关操作还有很多很多，在这里也不可能一一讲解，但是常用的一些操作都已经介绍，尤其是empty，size，push_front，push_back，pop_front，pop_back，clear，back，索引和遍历这些操作，是deque最最常用的操作，在BFS有大量的应用，因为BFS需要在尾部插入元素，从头部弹出元素，使用vector不是很方便，因此deque可以大放异彩，有了deque容器，使得我们写代码时更加方便，请小伙伴们务必放在心上。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++容器(Deque)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>乘积最大子数组(Leetcode 152)</title>
    <link href="https://USTCcoder.github.io/2020/08/26/program%20Leetcode152/"/>
    <id>https://USTCcoder.github.io/2020/08/26/program Leetcode152/</id>
    <published>2020-08-26T05:36:09.000Z</published>
    <updated>2020-09-03T01:36:53.696Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode152.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这个题目和前两天说的一个最大子序和的题目类似，不过这个题目不是求和而是求积。求和和求积有什么区别呢？为啥这个题目的难度就是中等，而上一个题目的难度是简单呢。<strong>求和的题目不会涉及到符号的问题，如果前缀和小于0，则相加时不考虑前缀，这时上一个题目的核心思想。而这个题目的难点在于，可能出现负数乘负数产生正数的情况</strong>。</p><a id="more"></a><h1 id="DP"><a href="#DP" class="headerlink" title="DP"></a><font size="5" color="red">DP</font></h1><p>我们使用动态规划进行求解，这时我们<strong>不但要记录最大值的状态也要记录最小值的状态，对正数和负数进行分别讨论</strong>。dp_max[i]表示右端点选择第i个数可得的最大值，dp_min[i]表示右端点选择第i个数可得的最小值。可以得到状态转移方程</p><script type="math/tex; mode=display">dp_max[i] =  \begin{case} max(dp_max[i - 1] * nums[i] , nums[i]) & nums[i] > 0 \\ nums[i] & max(dp_min[i - 1] * nums[i], nums[i]) \le 0 \end{cases}</script><script type="math/tex; mode=display">dp_min[i] =  \begin{case} min(dp_min[i - 1] * nums[i] , nums[i]) & nums[i] > 0 \\ nums[i] & min(dp_max[i - 1] * nums[i], nums[i]) \le 0 \end{cases}</script><p>当考虑到所有情况后，就可以得到右端点为任何情况下的最大值，然后求dp_max数组的最大值即可。DP的<strong>时间复杂度为$O(n)$，空间复杂度为$O(n)$</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def maxProduct(self, nums):</span><br><span class="line">        lens = len(nums)</span><br><span class="line">        dp_max = [0] * lens</span><br><span class="line">        dp_min = [0] * lens</span><br><span class="line">        dp_max[0] = dp_min[0] = nums[0]</span><br><span class="line">        for i in range(1, lens):</span><br><span class="line">            if nums[i] &gt; 0:</span><br><span class="line">                dp_max[i] = max(dp_max[i - 1] * nums[i], nums[i])</span><br><span class="line">                dp_min[i] = min(dp_min[i - 1] * nums[i], nums[i])</span><br><span class="line">            else:</span><br><span class="line">                dp_max[i] = max(dp_min[i - 1] * nums[i], nums[i])</span><br><span class="line">                dp_min[i] = min(dp_max[i - 1] * nums[i], nums[i])</span><br><span class="line">        return max(dp_max)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="优化DP"><a href="#优化DP" class="headerlink" title="优化DP"></a><font size="5" color="red">优化DP</font></h1><p>我们发现<strong>每次只需要使用到dp_max[i - 1]和dp_min[i - 1]，因此可以使用一个变量pre_num保存dp[i - 1]，然后实时更新这个变量即可，并且更新同时记录最大值</strong>，这样可以节约空间复杂度。这种算法的<strong>时间复杂度为$O(n)$，空间复杂度为$O(1)$</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def maxProduct(self, nums):</span><br><span class="line">        pre_max = pre_min = res = nums[0]</span><br><span class="line">        for num in nums[1:]:</span><br><span class="line">            if num &gt; 0:</span><br><span class="line">                pre_max, pre_min = max(pre_max * num, num), min(pre_min * num, num)</span><br><span class="line">            else:</span><br><span class="line">                pre_max, pre_min = max(pre_min * num, num), min(pre_max * num, num)</span><br><span class="line">            res = max(pre_max, res)</span><br><span class="line">        return res</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  这个题目需要和leetcode53题一起做，并且仔细比较两个题目之间的差异，观察状态转移方程的写法。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 152&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>C++容器(Vector)</title>
    <link href="https://USTCcoder.github.io/2020/08/25/C++_stl_vector/"/>
    <id>https://USTCcoder.github.io/2020/08/25/C++_stl_vector/</id>
    <published>2020-08-25T15:53:58.000Z</published>
    <updated>2020-08-24T12:41:08.110Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++66.png" alt="2"></p><h1 id="C-容器-Vector"><a href="#C-容器-Vector" class="headerlink" title="C++容器(Vector)"></a><font size="5" color="red">C++容器(Vector)</font></h1><p>  容器是C++非常方便的功能，今天给小伙伴们介绍vector库，vector库是C++专门用于处理动态单端数组的库，里面内置了许多增删改查的算法，在刷题时常常使用它。<br><a id="more"></a></p><h1 id="vector容器"><a href="#vector容器" class="headerlink" title="vector容器"></a><font size="5">vector容器</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;vector&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">// 为了方便显示，在这里将打印操作定义为一个函数</span><br><span class="line">void printVector(vector&lt;int&gt; v) {</span><br><span class="line">cout &lt;&lt; "v：";</span><br><span class="line">for (vector&lt;int&gt;::iterator it = v.begin(); it != v.end(); it++) {</span><br><span class="line">cout &lt;&lt; *it &lt;&lt; " ";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">// 创建一个vector容器，因为vector是一个模板，可以存放任意类型的数据，但是需要指定数据类型。</span><br><span class="line">// 而且建立以后只能存放该类型的数据，还可以存放vector类型的数据，类似于二维数组。</span><br><span class="line">vector&lt;int&gt; v = { 1, 2, 3 };</span><br><span class="line">printVector(v);</span><br><span class="line"></span><br><span class="line">// empty判断容器是否为空，为空则返回true，否则返回false</span><br><span class="line">cout &lt;&lt; "v是否为空：" &lt;&lt; v.empty() &lt;&lt; endl;</span><br><span class="line">// capacity求容器的容量</span><br><span class="line">cout &lt;&lt; "v的容量为：" &lt;&lt; v.capacity() &lt;&lt; endl;</span><br><span class="line">// size求容器的大小，注意大小和容量虽然给人的感觉是一样的，但是它们完全不同</span><br><span class="line">// 大小是求得容器中元素的个数，而容量相当于目前申请的内存空间。因为vector是动态数组，因此为了扩展方便，一次性会申请很多内存，这样避免每添加一个元素申请一次内存。</span><br><span class="line">// 所以容量就是目前申请的内存，以但达到vector容量时，会自动再去申请一部分内存。</span><br><span class="line">cout &lt;&lt; "v的尺寸为：" &lt;&lt; v.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// resize是将vector调整大小，如果大于原始数组，则填充数值，达到resize的大小，否则会删除超出的部分。</span><br><span class="line">v.resize(10);</span><br><span class="line">printVector(v);</span><br><span class="line">cout &lt;&lt; "v的容量为：" &lt;&lt; v.capacity() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "v的尺寸为：" &lt;&lt; v.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// push_back是最经常用的操作，在末尾添加元素。</span><br><span class="line">v.push_back(4);</span><br><span class="line">printVector(v);</span><br><span class="line">cout &lt;&lt; "v的容量为：" &lt;&lt; v.capacity() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "v的尺寸为：" &lt;&lt; v.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// pop_back和push_back相反，将末尾元素移除。</span><br><span class="line">v.pop_back();</span><br><span class="line">printVector(v);</span><br><span class="line">cout &lt;&lt; "v的容量为：" &lt;&lt; v.capacity() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "v的尺寸为：" &lt;&lt; v.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// 索引元素可以直接类似于数组的写法，也可以使用at进行索引</span><br><span class="line">cout &lt;&lt; "v[2] = " &lt;&lt; v[2] &lt;&lt; endl;</span><br><span class="line">// front获取vector第一个元素，等价于v[0]</span><br><span class="line">cout &lt;&lt; "第一个元素为：" &lt;&lt; v.front() &lt;&lt; endl;</span><br><span class="line">// back()获取vector最后一个元素，等价于v[v.size() - 1]</span><br><span class="line">cout &lt;&lt; "最后一个元素为：" &lt;&lt; v.back() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">// clear清空容器中的所有内容，注意capacity和size的区别，内存一旦申请，常规的操作就不会释放，但是size不同。</span><br><span class="line">// size是元素的个数，因此以但清空vector，size就会清0.</span><br><span class="line">v.clear();</span><br><span class="line">printVector(v);</span><br><span class="line">cout &lt;&lt; "v的容量为：" &lt;&lt; v.capacity() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "v的尺寸为：" &lt;&lt; v.size() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++67.png" alt="1"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  当然了vector的相关操作还有很多很多，在这里也不可能一一讲解，但是常用的一些操作都已经介绍，尤其是empty，size，push_back，pop_back，clear，back，索引和遍历这些操作，是笔试，面试中的重中之重，有了vector容器，使得我们写代码时更加方便，请小伙伴们务必放在心上。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++容器(Vector)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>最大子序和(Leetcode 53)</title>
    <link href="https://USTCcoder.github.io/2020/08/24/program%20Leetcode53/"/>
    <id>https://USTCcoder.github.io/2020/08/24/program Leetcode53/</id>
    <published>2020-08-24T01:52:47.000Z</published>
    <updated>2020-09-03T01:35:44.985Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode53.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   从今天开始进入另一个有趣的部分——<strong>子序列相关题目</strong>，这是笔试面试中最最经典的问题之一，常用的解法是<strong>动态规划</strong>，小伙伴们一定要留心这类题目。虽然看起来难度并不大，但是遇到时可能会出现眼高手低的情况。</p><a id="more"></a><h1 id="DP"><a href="#DP" class="headerlink" title="DP"></a><font size="5" color="red">DP</font></h1><p>我们使用动态规划进行求解，dp[i]表示右端点选择第i个数可得的最大值，可以得到状态转移方程</p><script type="math/tex; mode=display">dp[i] =  \begin{case} nums[i] + dp[i - 1] & dp[i - 1] > 0 \\ nums[i] & dp[i - 1] \le 0 \end{cases}</script><p>当考虑到所有情况后，就可以得到右端点为任何情况下的最大值，然后求dp数组的最大值即可。DP的<strong>时间复杂度为$O(n)$，空间复杂度为$O(n)$</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def maxSubArray(self, nums):</span><br><span class="line">        lens = len(nums)</span><br><span class="line">        dp = [0] * lens</span><br><span class="line">        dp[0] = nums[0]</span><br><span class="line">        for i in range(1, lens):</span><br><span class="line">            if dp[i - 1] &lt;= 0:</span><br><span class="line">                dp[i] = nums[i]</span><br><span class="line">            else:</span><br><span class="line">                dp[i] = dp[i - 1] + nums[i]</span><br><span class="line">        return max(dp)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="优化DP"><a href="#优化DP" class="headerlink" title="优化DP"></a><font size="5" color="red">优化DP</font></h1><p>我们发现<strong>每次只需要使用到dp[i - 1]，因此可以使用一个变量pre_num保存dp[i - 1]，然后实时更新这个变量即可，并且更新同时记录最大值</strong>，这样可以节约空间复杂度。这种算法的<strong>时间复杂度为$O(n)$，空间复杂度为$O(1)$</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def maxSubArray(self, nums):</span><br><span class="line">        max_sum = pre_num = nums[0]</span><br><span class="line">        for num in nums[1:]:</span><br><span class="line">            if pre_num &lt; 0:</span><br><span class="line">                pre_num = 0</span><br><span class="line">            pre_num += num</span><br><span class="line">            max_sum = max(pre_num, max_sum)</span><br><span class="line">        return max_sum</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  这个题目是一个简单的子序列问题，牛刀小试，下面来看一看其他更加精妙的题目吧。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 53&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>C++容器(String)</title>
    <link href="https://USTCcoder.github.io/2020/08/23/C++_stl_string/"/>
    <id>https://USTCcoder.github.io/2020/08/23/C++_stl_string/</id>
    <published>2020-08-23T14:22:58.000Z</published>
    <updated>2020-08-30T01:30:57.026Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++63.png" alt="2"></p><h1 id="C-容器-String"><a href="#C-容器-String" class="headerlink" title="C++容器(String)"></a><font size="5" color="red">C++容器(String)</font></h1><p>  容器是C++非常方便的功能，今天给小伙伴们介绍string库，string库是C++专门用于处理字符串的库，里面内置了许多字符串的算法，在刷题时常常使用它。<br><a id="more"></a></p><h1 id="string容器"><a href="#string容器" class="headerlink" title="string容器"></a><font size="5">string容器</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">//创建时直接当成数据类型使用即可，不需要记构造函数，使用最广泛最方便。</span><br><span class="line">string a = "hello world!";</span><br><span class="line">string b = "hello";</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//length方法，求出字符串的长度。</span><br><span class="line">int lenA = a.length();</span><br><span class="line">cout &lt;&lt; "lenA = " &lt;&lt; lenA &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//字符串截取操作，返回一个字符串。如果不指定，默认截取到最后一个字符。</span><br><span class="line">string c = a.substr(3);</span><br><span class="line">string d = a.substr(3, 4);</span><br><span class="line">cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d = " &lt;&lt; d &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//字符串拼接操作，使用加法即可，不需要记append方法。</span><br><span class="line">string e = a + " hello C++";</span><br><span class="line">cout &lt;&lt; "e = " &lt;&lt; e &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//查找操作，find为从左到右查找，rfind为从右到左查找，如果不指定则从头开始查找，如果指定则从指定位置开始。</span><br><span class="line">int indexL = a.find("l");</span><br><span class="line">int indexL1 = a.find("l", 3);</span><br><span class="line">int indexR = a.rfind("l");</span><br><span class="line">cout &lt;&lt; "indexL = " &lt;&lt; indexL &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "indexL1 = " &lt;&lt; indexL1 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "indexR = " &lt;&lt; indexR &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//字符串替换操作，replace(pos, n, str)用str替换pos后面n个字符，注意这个操作会改变原字符串。</span><br><span class="line">string f = a.replace(6, 5, "C++");</span><br><span class="line">cout &lt;&lt; "f = " &lt;&lt; f &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//字符串字典序比较，按照ASCII码进行字符串比较，大于则返回1，小于则返回-1，等于返回0</span><br><span class="line">int compare = a.compare("Hello world!");</span><br><span class="line">cout &lt;&lt; "compare = " &lt;&lt; compare &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//索引操作，返回指定位置的字符，也可以使用at方法。</span><br><span class="line">char at = a[6];</span><br><span class="line">cout &lt;&lt; "at = " &lt;&lt; at &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//插入操作，insert(pos, str)在pos处插入str字符串，注意这个操作会改变原字符串。</span><br><span class="line">string g = a.insert(6, "world! hello ");</span><br><span class="line">cout &lt;&lt; "g = " &lt;&lt; g &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//删除操作，erase(pos, n)删除pos后面的n个字符，注意这个操作会改变原字符串。</span><br><span class="line">string h = a.erase(6, 7);</span><br><span class="line">cout &lt;&lt; "h = " &lt;&lt; h &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//字符串打印，有很多种方法，可以使用普通的方法，i从0遍历到length()，对于每一个i，使用索引方法获得。</span><br><span class="line">//在这里使用一种迭代器方法，begin()返回起始位置的迭代器对象，不可以直接访问，需要用指针才可以访问，end()同理。</span><br><span class="line">for (string::iterator it = a.begin(); it != a.end(); it++) {</span><br><span class="line">cout &lt;&lt; *it &lt;&lt; "-";</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++64.png" alt="1"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  当然了C++字符串处理操作还有很多很多，在这里也不可能一一讲解，但是常用的一些操作都已经介绍，尤其是截取，增加，查找，比较，插入，删除，替换这些操作，是笔试，面试中一旦考察字符串一定会用到的函数，请小伙伴们务必放在心上。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++容器(String)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++类模板</title>
    <link href="https://USTCcoder.github.io/2020/08/21/C++_class_template/"/>
    <id>https://USTCcoder.github.io/2020/08/21/C++_class_template/</id>
    <published>2020-08-21T15:17:33.000Z</published>
    <updated>2020-08-30T01:30:36.550Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++59.png" alt="2"></p><h1 id="C-类模板"><a href="#C-类模板" class="headerlink" title="C++类模板"></a><font size="5" color="red">C++类模板</font></h1><p>  前天介绍了C++中函数模板的相关知识，函数模板非常简单，使用也很方便，C++中还给我们提供了类模板的相关知识，但是相比较而言，类模板就非常复杂，知识点很多，也并不经常使用，小伙伴作为了解即可。<br><a id="more"></a></p><h1 id="类模板"><a href="#类模板" class="headerlink" title="类模板"></a><font size="5">类模板</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//类模板：建立一个通用类，类的成员数据类型不具体指定，定义方式和函数模板相同。</span><br><span class="line">//区别在于类模板没有自动类型推导，使用时必须指定类型。函数模板中的默认参数没有意义，类模板中默认参数有意义。</span><br><span class="line">//类模板和普通类的区别在于：类模板中的成员函数并不是创建类时创建，而是调用时创建，而普通类中的成员函数是在创建类时创建。</span><br><span class="line">template &lt;class TName, class TAge=int&gt;</span><br><span class="line">class Person</span><br><span class="line">{</span><br><span class="line">public:</span><br><span class="line">TName tName;</span><br><span class="line">TAge  tAge;</span><br><span class="line">Person(TName tName, TAge tAge) {</span><br><span class="line">this-&gt;tName = tName;</span><br><span class="line">this-&gt;tAge = tAge;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">void print() {</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; this-&gt;tName &lt;&lt; "，我的年龄是：" &lt;&lt; this-&gt;tAge &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">Person&lt;string&gt; p1("C++程序员", 30);</span><br><span class="line">p1.print();</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++60.png" alt="1"></p><h1 id="类模板作为函数参数"><a href="#类模板作为函数参数" class="headerlink" title="类模板作为函数参数"></a><font size="5">类模板作为函数参数</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">template &lt;class TName, class TAge=int&gt;</span><br><span class="line">class Person</span><br><span class="line">{</span><br><span class="line">public:</span><br><span class="line">TName tName;</span><br><span class="line">TAge  tAge;</span><br><span class="line">Person(TName tName, TAge tAge) {</span><br><span class="line">this-&gt;tName = tName;</span><br><span class="line">this-&gt;tAge = tAge;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">void print() {</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; this-&gt;tName &lt;&lt; "，我的年龄是：" &lt;&lt; this-&gt;tAge &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">//指定传入类型</span><br><span class="line">void test1(Person&lt;string&gt; &amp;p) {</span><br><span class="line">p.print();</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//参数模板化</span><br><span class="line">template&lt;class T&gt;</span><br><span class="line">void test2(Person&lt;T&gt; &amp;p) {</span><br><span class="line">p.print();</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//整体模板化</span><br><span class="line">template&lt;class P&gt;</span><br><span class="line">void test3(P &amp;p) {</span><br><span class="line">p.print();</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">Person&lt;string&gt; p1("C++程序员", 30);</span><br><span class="line">test1(p1);</span><br><span class="line"></span><br><span class="line">Person&lt;string&gt; p2("Java程序员", 26);</span><br><span class="line">test2(p2);</span><br><span class="line"></span><br><span class="line">Person&lt;string&gt; p3("Python程序员", 22);</span><br><span class="line">test3(p3);</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++61.png" alt="2"></p><h1 id="子类继承类模板"><a href="#子类继承类模板" class="headerlink" title="子类继承类模板"></a><font size="5">子类继承类模板</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">template &lt;class TName, class TAge=int&gt;</span><br><span class="line">class Person</span><br><span class="line">{</span><br><span class="line">public:</span><br><span class="line">TName tName;</span><br><span class="line">TAge  tAge;</span><br><span class="line">Person(TName tName, TAge tAge) {</span><br><span class="line">this-&gt;tName = tName;</span><br><span class="line">this-&gt;tAge = tAge;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">void print() {</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; this-&gt;tName &lt;&lt; "，我的年龄是：" &lt;&lt; this-&gt;tAge &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">//子类继承类模板有两种方式，第一种是指定父类的类型</span><br><span class="line">class Teacher : public Person&lt;string&gt; {</span><br><span class="line">public:</span><br><span class="line">Teacher(string s, int age):Person(s, age){}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">//第二种是灵活继承，子类也变成类模板。</span><br><span class="line">template &lt;class TName, class TAge=int&gt;</span><br><span class="line">class Student : public Person&lt;TName&gt; {</span><br><span class="line">public:</span><br><span class="line">Student(TName tName, TAge tAge):Person&lt;TName&gt; (tName, tAge) {}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">Teacher t("C++老师", 45);</span><br><span class="line">t.print();</span><br><span class="line"></span><br><span class="line">Student&lt;string&gt; s("C++学生", 30);</span><br><span class="line">s.print();</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++62.png" alt="4"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  类模板相对于函数模板较为复杂，模板元编程不推荐大家使用，对于本来就是菜鸡的小伙伴们，这无疑又强行增加难度，我们学习C++的高级编程主要关注点在于STL库和面向对象与设计模式之间的关系，因此后面介绍的内容，小伙伴们一定要认真学习。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++类模板&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++函数模板</title>
    <link href="https://USTCcoder.github.io/2020/08/19/C++_function_template/"/>
    <id>https://USTCcoder.github.io/2020/08/19/C++_function_template/</id>
    <published>2020-08-19T14:57:30.000Z</published>
    <updated>2020-08-16T15:16:39.521Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++57.png" alt="2"></p><h1 id="C-函数模板"><a href="#C-函数模板" class="headerlink" title="C++函数模板"></a><font size="5" color="red">C++函数模板</font></h1><p>  从今天开始，正式进入C++的高级篇章，之前C++基础介绍的重点内容是函数与指针，C++进阶介绍的重点是引用和面向对象，在高级部分主要介绍模板和STL相关知识，尤其STL是小伙伴们必须要掌握的知识。今天先介绍一下C++的函数模板。<br><a id="more"></a></p><h1 id="函数模板"><a href="#函数模板" class="headerlink" title="函数模板"></a><font size="5">函数模板</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//函数模板：建立一个通用函数，其函数返回值类型和形参类型可以不指定，用虚拟类型代表</span><br><span class="line">//定义函数模板时，要在函数声明或者定义的上一行写template关键字，表示下面定义的类型是函数模板。</span><br><span class="line">template &lt;class T&gt;</span><br><span class="line">void myPrint(T t) {</span><br><span class="line">cout &lt;&lt; t &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "模板函数执行" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">void myPrint(double a) {</span><br><span class="line">cout &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "普通函数执行" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a = 10;</span><br><span class="line">double b = 10.0;</span><br><span class="line">string s = "hello world!";</span><br><span class="line"></span><br><span class="line">//使用函数模板时有两种方法。</span><br><span class="line">//自动类型推导，将值传入函数中，会根据传入的类型自动确定模板中的类型。</span><br><span class="line">//显示指定类型，可以用函数名&lt;类型名&gt;(参数)指定传入的类型。myPrint&lt;int&gt;(a);也可以达到相同的效果。</span><br><span class="line">myPrint(a);</span><br><span class="line">myPrint(b);</span><br><span class="line">myPrint(s);</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++58.png" alt="1"></p><h1 id="函数模板与普通函数的区别"><a href="#函数模板与普通函数的区别" class="headerlink" title="函数模板与普通函数的区别"></a><font size="5">函数模板与普通函数的区别</font></h1><ol><li>普通函数可以发生隐式类型转换，函数模板中自动类型推导不可以，显示指定可以。</li><li>如果普通函数和函数模板都可以调用，则优先使用普通函数，可以通过显示指定空模板参数调用函数模板。</li><li>如果函数模板可以发生更好的匹配，则优先使用函数模板。</li><li>函数模板也可以发生重载。</li></ol><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  函数模板是一种高级的技巧，对于多种类似的参数输入，不需要重载大量的函数，减少了代码的冗余度，提高了复用性。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++函数模板&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>吃掉 N 个橘子的最少天数(Leetcode 5490)</title>
    <link href="https://USTCcoder.github.io/2020/08/18/program%20Leetcode5490/"/>
    <id>https://USTCcoder.github.io/2020/08/18/program Leetcode5490/</id>
    <published>2020-08-18T12:41:10.000Z</published>
    <updated>2020-09-02T02:16:48.573Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode5490.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这是第202周周赛的第四题，题目的意思也很简单，有一堆橘子，每天可以选择吃掉一个，当橘子个数为2的整数倍时，每天还可以选择吃掉一半，当橘子个数为3的整数倍时，每天还可以选择吃掉三分之二，求如何最快吃完这些橘子。</p><a id="more"></a><h1 id="BFS"><a href="#BFS" class="headerlink" title="BFS"></a><font size="5" color="red">BFS</font></h1><p>这道题目做法有很多，但是测试样例中，橘子的个数可以达到$2 \times 10^9$，这样就不能使用DP来做了。DP的思路非常清晰，dp[i] = min(dp[i - 1], dp[i // 2], dp[i // 3]) + 1，当然i需要是能整除2和3的，否则去掉对应的那一项即可，这里就不再论述。时间复杂度为O(n)。我们思考，因为吃掉一半或者三分之二的概率非常大，所以结果应该不会很大，所以我们可以使用广搜的方法进行求解。为了避免搜到重复的情况，我们建立一个字典保存当前已经搜到的情况。时间复杂度为O(log(n))，空间复杂度为O(log(n))。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from collections import deque</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Solution(object):</span><br><span class="line">    def minDays(self, n):</span><br><span class="line">        """</span><br><span class="line">        :type n: int</span><br><span class="line">        :rtype: int</span><br><span class="line">        """</span><br><span class="line">        dic = {n: 0}</span><br><span class="line">        queue = deque([n])</span><br><span class="line">        while 0 not in dic:</span><br><span class="line">            current = queue.popleft()</span><br><span class="line">            if current % 3 == 0 and current // 3 not in dic:</span><br><span class="line">                dic[current // 3] = dic[current] + 1</span><br><span class="line">                queue.append(current // 3)</span><br><span class="line">            if current % 2 == 0 and current // 2 not in dic:</span><br><span class="line">                dic[current // 2] = dic[current] + 1</span><br><span class="line">                queue.append(current // 2)</span><br><span class="line">            if current - 1 not in dic:</span><br><span class="line">                dic[current - 1] = dic[current] + 1</span><br><span class="line">                queue.append(current - 1)</span><br><span class="line">        return dic[0]</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="贪心-DFS"><a href="#贪心-DFS" class="headerlink" title="贪心+DFS"></a><font size="5" color="red">贪心+DFS</font></h1><p>DFS速度更快，但是更难以想到， 需要一些数学的思路。<br>能整除，我们就不吃一个，这是DFS的核心。如果n个橘子经过了k次吃一个的操作后，出现了除以3的操作，因此可以说明n - k是3的倍数，那么当k大于3时，这一定不是最优解，因为可以吃k - 3次一个橘子，然后除以3，再吃一个橘子。举个例子，101个橘子，如果每天吃一个橘子，吃了5天，然后剩余96个橘子，然后第六条吃了64个橘子，花费6天，橘子数目变为32。我们可以有一种更快到达32的方法，吃2个橘子，剩余99个橘子，然后第三天吃66个橘子，第四天吃一个橘子，花费4天，橘子数目变为32。</p><p>n个橘子经过k次吃一个的操作后，出现了除以2的操作同理。</p><p>因此我们采用一种贪心的算法，能整除则整除，不能整除先吃1个或2个然后再整除。使用记忆化的DFS，可以进一步加快搜索。时间复杂度为O(log(n))，空间复杂度为O(log(n))。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from functools import lru_cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Solution(object):</span><br><span class="line">    def minDays(self, n):</span><br><span class="line">        """</span><br><span class="line">        :type n: int</span><br><span class="line">        :rtype: int</span><br><span class="line">        """</span><br><span class="line">        @lru_cache(None)</span><br><span class="line">        def dfs(n):</span><br><span class="line">            if n &lt; 3:</span><br><span class="line">                return n</span><br><span class="line">            return min(dfs(n // 2) + n % 2, dfs(n // 3) + n % 3) + 1</span><br><span class="line">        return dfs(n)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  非常有趣的题目，我当时想到了使用BFS去求解，但是没有使用字典保存，导致时间复杂度过高。小伙伴们要吸取教训，记忆化是一个非常好的搜索方法，可以从指数级的时间复杂度变成线性复杂度，一定要掌握呀。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 5490&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>两球之间的磁力(Leetcode 5489)</title>
    <link href="https://USTCcoder.github.io/2020/08/16/program%20Leetcode5489/"/>
    <id>https://USTCcoder.github.io/2020/08/16/program Leetcode5489/</id>
    <published>2020-08-16T09:16:46.000Z</published>
    <updated>2020-09-02T02:16:54.131Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode5489.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这是第202周周赛的第三题，我觉得非常有价值。题目的意思很简单，通俗的说就是从数组中找m个节点，使得它们的最小距离最大，也就是说让它们两两之间距离最远。小伙伴们先思考应该如何解答。</p><a id="more"></a><h1 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a><font size="5" color="red">二分查找</font></h1><p>之前应该说过一句话，这里再强调一次，当遇到最小化最大值问题或者最大化最小值问题，首先考虑二分法。最小距离为1，最大距离为max(position)，当然可以优化，不过也没有太大意义，因为二分法以对数的速度收敛，很快就可以达到最优解。<br>我们可以二分查找它们之间的距离，当对于某一个距离使用贪心算法进行判断时，如果能够创建出m个节点，说明该距离是合适的，我们可以令left = mid，否则令right = mid - 1，但是我们要保证数组是有序的。这样做的时间复杂度为$O(n \times log(n))$，空间复杂度为O(1)。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def maxDistance(self, position, m):</span><br><span class="line">        """</span><br><span class="line">        :type position: List[int]</span><br><span class="line">        :type m: int</span><br><span class="line">        :rtype: int</span><br><span class="line">        """</span><br><span class="line">        position.sort()</span><br><span class="line">        left, right = 1, (position[-1] - position[0]) // (m - 1)</span><br><span class="line">        while left &lt; right:</span><br><span class="line">            mid, res, last, flag = (left + right + 1) // 2, 1, position[0], False</span><br><span class="line">            for x in position[1:]:</span><br><span class="line">                if x - last &gt;= mid:</span><br><span class="line">                    res += 1</span><br><span class="line">                    last = x</span><br><span class="line">                    if res &gt;= m:</span><br><span class="line">                        flag = True</span><br><span class="line">                        break</span><br><span class="line">            if flag:</span><br><span class="line">                left = mid</span><br><span class="line">            else:</span><br><span class="line">                right = mid - 1</span><br><span class="line">        return left</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  这种题是非常有价值的，因为二分查找是每一个程序员必备的技能，也是面试官常常喜欢考察的内容。而且题目难度适中，所以小伙伴们务必掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 5489&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>寻找重复数(Leetcode 287)</title>
    <link href="https://USTCcoder.github.io/2020/08/15/program%20Leetcode287/"/>
    <id>https://USTCcoder.github.io/2020/08/15/program Leetcode287/</id>
    <published>2020-08-15T14:51:22.000Z</published>
    <updated>2020-09-02T02:18:16.772Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode287.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这道题也是找数字的题目，但是不同的是，之前讲的都是在重复数字中找单独出现的数字，而这个题目是在单独的数字中，查找重复数字，又该如何去应对呢？</p><a id="more"></a><h1 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a><font size="5" color="red">二分查找</font></h1><p>因为题目中数字是从1到n，而且存在重复元素，元素的总个数为n+1。我们思考，如果元素总个数为n，并且无重复元素的情况下，那么不就是从1到n的随机排列吗？那么多了一个数，必然存在重复的情况，假设重复的元素为x，当$k \ge x$时，那么小于等于k的元素个数有k + 1个，当$k &lt; x$时，那么小于等于k的元素有k个。我们思考如果不是从1到n全都存在的情况下，即有一个数字重复出现很多次的情况，这种思路还正确吗？当然也是正确的，小伙伴们可以举个例子验证一下，我也只可意会不可言传。因此可以考虑二分法进行求解。<br>令k=mid，当mid代入时，数组中小于mid的元素个数大于mid时，说明$mid \ge x$，则令right = mid。否则令left = mid + 1。时间复杂度为O(nlog(n))，空间复杂度为O(1)。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def findDuplicate(self, nums):</span><br><span class="line">        n = len(nums)</span><br><span class="line">        left, right = 1, n - 1</span><br><span class="line">        while left &lt; right:</span><br><span class="line">            mid = (left + right) // 2</span><br><span class="line">            cnt = sum([x &lt;= mid for x in nums])</span><br><span class="line">            if cnt &gt; mid:</span><br><span class="line">                right = mid</span><br><span class="line">            else:</span><br><span class="line">                left = mid + 1</span><br><span class="line">        return left</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a><font size="5" color="red">位运算</font></h1><p>这个位运算很简单，从1到n，分别计算每一位上出现的总个数，因为有一个数x重复，假设x的某个二进制位为1，则数组中该位为1的数字个数一定大于从1到n所有数字中该位为1的数字个数，否则一定小于等于。因此我们按位进行比较，最后哪些位为1，就可以得到该数字的二进制表示。时间复杂度为O(nlog(n))，空间复杂度为O(1)。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def findDuplicate(self, nums):</span><br><span class="line">        n = len(nums)</span><br><span class="line">        k = len(bin(n)) - 2</span><br><span class="line">        res = 0</span><br><span class="line">        for i in range(k):</span><br><span class="line">            bit_nums, bit_origin = 0, 0</span><br><span class="line">            for x in nums:</span><br><span class="line">                if x &amp; (1 &lt;&lt; i):</span><br><span class="line">                    bit_nums += 1</span><br><span class="line">            for x in range(1, n):</span><br><span class="line">                if x &amp; (1 &lt;&lt; i):</span><br><span class="line">                    bit_origin += 1</span><br><span class="line">            if bit_origin &lt; bit_nums:</span><br><span class="line">                res |= 1 &lt;&lt; i</span><br><span class="line">        return res</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="双指针"><a href="#双指针" class="headerlink" title="双指针"></a><font size="5" color="red">双指针</font></h1><p>这题很难想出和双指针有什么关系，官方和一些民间大神总有一些骚想法，我们来看一看。因为数组的长度为n+1，数组的最大值为n，因此我们可以看成数组中每个元素保存的是下一个元素的索引，这样就相当于一个指针。因此有重复的数字，说明构成了环。我们需要找到环的入口处。<br>这个问题就转化成了一个经典的快慢指针问题。我们以题目中的例子进行讲解。<br><img src="/images/ALGORITHM/leetcode287_solve.png" alt="q"><br>时间复杂度为O(n)，空间复杂度为O(1)。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def findDuplicate(self, nums):</span><br><span class="line">        slow = 0</span><br><span class="line">        fast = 0</span><br><span class="line">        while True:</span><br><span class="line">            slow = nums[slow]</span><br><span class="line">            fast = nums[nums[fast]]</span><br><span class="line">            if slow == fast:</span><br><span class="line">                break</span><br><span class="line">        find = 0</span><br><span class="line">        while True:</span><br><span class="line">            find = nums[find]</span><br><span class="line">            slow = nums[slow]</span><br><span class="line">            if find == slow:</span><br><span class="line">                return find</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  这类题目基本上结束了，无论是从多次出现的数组中寻找单独出现的数字，还是从单独出现的数字中寻找多次出现的数字，都可以使用一些奇技淫巧优化算法。如果面试问到了，掌握最普通的暴力求解法是最最基本的，但是面试官既然出这个题目，一定不是想考察暴力解法，所以小伙伴们一定要掌握呦~</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 287&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>错误的集合(Leetcode 645)</title>
    <link href="https://USTCcoder.github.io/2020/08/13/program%20Leetcode645/"/>
    <id>https://USTCcoder.github.io/2020/08/13/program Leetcode645/</id>
    <published>2020-08-13T12:56:22.000Z</published>
    <updated>2020-09-02T02:18:33.248Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode645.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这个题目和昨天讲解的题目非常类似，有的小伙伴就会说骗人，根本不一样，那下面让我们来康一康吧。</p><a id="more"></a><h1 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a><font size="5" color="red">位运算</font></h1><p>题目中说集合S中的元素包含从1到n的整数，而且其中的某一个元素替换成了另一个。这其实就是相当于所有数字都出现两次，除了一个出现一次，一个出现三次。因为我们已知数据从1到n，我们可以额外添加一组从1到n的数据，假设x替换成了y，那么新数组为x出现1次，y出现3次，其他的数据都出现2次。是不是和昨天讲解的题目非常相似。只是昨天的数据已经给出了出现两次，今天的数据需要从1到n额外添加一次。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Solution(object):</span><br><span class="line">    def findErrorNums(self, nums):</span><br><span class="line">        """</span><br><span class="line">        :type nums: List[int]</span><br><span class="line">        :rtype: List[int]</span><br><span class="line">        """</span><br><span class="line">        n = len(nums)</span><br><span class="line">        a = reduce(lambda x, y: x ^ y, nums + list(range(1, n + 1)))</span><br><span class="line">        b = 1</span><br><span class="line">        part1 = 0</span><br><span class="line">        part2 = 0</span><br><span class="line">        while not (a &amp; b):</span><br><span class="line">            b &lt;&lt;= 1</span><br><span class="line">        for x in nums:</span><br><span class="line">            if x &amp; b:</span><br><span class="line">                part1 ^= x</span><br><span class="line">            else:</span><br><span class="line">                part2 ^= x</span><br><span class="line">        for x in range(1, n + 1):</span><br><span class="line">            if x &amp; b:</span><br><span class="line">                part1 ^= x</span><br><span class="line">            else:</span><br><span class="line">                part2 ^= x</span><br><span class="line">        for x in nums:</span><br><span class="line">            if part1 == x:</span><br><span class="line">                return [part1, part2]</span><br><span class="line">        return [part2, part1]</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  这几天的强化位运算找数字问题，让小伙伴们明白，遇到相同数字的问题，首先需要考虑异或的方法，往往可以节省大量的内存空间。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 645&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>只出现一次的数字 III(Leetcode 260)</title>
    <link href="https://USTCcoder.github.io/2020/08/12/program%20Leetcode260/"/>
    <id>https://USTCcoder.github.io/2020/08/12/program Leetcode260/</id>
    <published>2020-08-12T13:34:57.000Z</published>
    <updated>2020-09-02T02:18:10.178Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode260.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   前面的两道题目都是找出唯一出现的一个数字，如果有两个数字都出现一次，该如何求解呢？</p><a id="more"></a><h1 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a><font size="5" color="red">位运算</font></h1><p>对于136题来说，找出唯一出现的一个数字，通过异或可以直接求出。如果有两个数字x和y都出现了一次，那么所有数字异或的结果应该就是x和y的异或结果。因为x和y不相同，所以异或的结果一定不为0，所以一定在某个位上有1出现。我们从低到高找到第一次出现1的位置。于是可以得出一个结论。一定x在该为上位1，y为0或者x为0，y为1。所以就可以将这个数组分成part1和part2，part1中所有的元素在该位上都是1，part2中所有的元素在该位上都是0。因此对于part1能通过异或的方法找出唯一出现的一个数字，part2也能通过异或的方法找出唯一出现的一个数字。<br><img src="/images/ALGORITHM/leetcode260_solve.png" alt="q"><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import functools</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Solution(object):</span><br><span class="line">    def singleNumbers(self, nums):</span><br><span class="line">        """</span><br><span class="line">        :type nums: List[int]</span><br><span class="line">        :rtype: List[int]</span><br><span class="line">        """</span><br><span class="line">        p = functools.reduce(lambda x, y: x ^ y, nums)</span><br><span class="line">        p1 = p2 = 0</span><br><span class="line">        b = 1</span><br><span class="line">        while not (p &amp; b):</span><br><span class="line">            b &lt;&lt;= 1</span><br><span class="line">        for i in nums:</span><br><span class="line">            if i &amp; b:</span><br><span class="line">                p1 ^= i</span><br><span class="line">            else:</span><br><span class="line">                p2 ^= i</span><br><span class="line">        return [p1, p2]</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  走过路过不要错过，赶紧尝试尝试，新鲜出炉的位运算，小伙伴们一定要掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 260&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>只出现一次的数字 II(Leetcode 137)</title>
    <link href="https://USTCcoder.github.io/2020/08/10/program%20Leetcode137/"/>
    <id>https://USTCcoder.github.io/2020/08/10/program Leetcode137/</id>
    <published>2020-08-10T14:16:08.000Z</published>
    <updated>2020-09-02T02:18:02.382Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode137.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>  这个题目比昨天的题目难度大很多，是谷歌公司面试的一道题目，拍了拍自己的胸脯，幸好没去谷歌（狗头保命）。</p><a id="more"></a><h1 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a><font size="5" color="red">位运算</font></h1><p>想一下每个元素出现两次时是咋做的？是不是记录二进制位中1出现的个数，并对2求余。一个数如果某一位是1，出现2次，就为2，然后对2求余就为0。因此单独的一个数对2求余就为1，所以该数在哪些位置上为1，则最后的结果在哪些位置上也为1。设当前某一位为0，下一个数在该位上为1，那么该位的状态位1，下一个数在该为上位0，那么该位的状态位0。设当前某一位为1，下一个数在该位上为1，那么该位的状态位0，下一个数在该为上位0，那么该位的状态位1。这不正好符合异或的定义吗？</p><p>那我们考虑一下每个元素出现三次时的情况，也应该记录二进制位中1出现的个数，并对3求余，一个数如果某一位是1，出现3次，就为3，然后对3求余就为0。因此单独的一个数对3求余就为1，所以该数在哪些位置上为1，则最后的结果在哪些位置上也为1。但是出现了一个问题，出现2次，余数可以用0和1表示，那么出现3次如何表示呢？余数为0，1，2，需要使用3个状态表示，也就是需要两位表示。</p><p>下面借用Leetcode Krahets(K神)的图<br><img src="/images/ALGORITHM/leetcode137_solve.png" alt="q"></p><p>当two为0时，如果下一个数为0，则one的状态不变，如果下一个数为1，则one的状态反转。<br>当two为1时，无论下一个数时0还是1，one都为0。<br>可以列出下表进行推导one为0或者1时，two的变化，注意one为更新后的值。<br><img src="/images/ALGORITHM/leetcode137_solve1.png" alt="q"><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def singleNumber(self, nums: List[int]) -&gt; int:</span><br><span class="line">        ones, twos = 0, 0</span><br><span class="line">        for num in nums:</span><br><span class="line">            ones = ones ^ num &amp; ~twos</span><br><span class="line">            twos = twos ^ num &amp; ~ones</span><br><span class="line">        return ones</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  大佬的思路太漂亮，不禁感叹自己还是太菜啦~，我们不是要记住这一题，而是要记住状态转移的方式，然后列表推导出下一个状态的表达式，这个才是这个问题的核心，以后无论数据重复多少次都有类似的解法。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 137&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>只出现一次的数字(Leetcode 136)</title>
    <link href="https://USTCcoder.github.io/2020/08/09/program%20Leetcode136/"/>
    <id>https://USTCcoder.github.io/2020/08/09/program Leetcode136/</id>
    <published>2020-08-09T14:06:18.000Z</published>
    <updated>2020-09-02T02:17:59.198Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode136.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>  接下来的几天给小伙伴介绍一种新题型，当我看到这个题目的时候，我觉得很简单，但是往往面试或者笔试的时候，会对时间复杂度或者空间复杂度进行限制，让我们一起来康一康吧。</p><a id="more"></a><h1 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a><font size="5" color="red">位运算</font></h1><p>当然暴力求解很容易，用一个字典保存每一个值出现的次数即可。但是空间复杂度为O(n)，可不可以使用一种空间复杂度为O(1)的算法呢？位运算应运而生了，异或的特点是相同为0，那么两个相同的数字异或会得到0，0和x异或为x。如果所有的数都出现两次，那么将所有的数字异或的结果为0，如果只有一个数字出现了一次，那么异或的结果为该数字。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Solution:</span><br><span class="line">    def singleNumber(self, nums):</span><br><span class="line">        return reduce(lambda x, y: x ^ y, nums)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  一个Pythonic的写法，只需要一行代码，非常简洁。妙啊~，位运算往往就是这样，难度可能并不大，但是不容易想到，那接下来让我们看一看难度较大的题目吧。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 136&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>数字序列中某一位的数字(Leetcode 剑指Offer44)</title>
    <link href="https://USTCcoder.github.io/2020/08/07/program%20Leetcode_offer44/"/>
    <id>https://USTCcoder.github.io/2020/08/07/program Leetcode_offer44/</id>
    <published>2020-08-07T13:32:05.000Z</published>
    <updated>2020-09-02T02:17:34.137Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcodeoffer44.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这个题目应该和43题一起做，都是经典的找规律的题目，这个题目相比上一题简单一些，小伙伴们开动脑筋想一想。</p><a id="more"></a><h1 id="数学规律"><a href="#数学规律" class="headerlink" title="数学规律"></a><font size="5" color="red">数学规律</font></h1><p>我们一般按照位数进行找规律，0-9一位数中，每个数字只有一个字符，共10个字符。而10-99中，这90个两位数中，每个数字都有2个字符，共180个字符。而100-999中，共有900个数字，每个数字有3个字符，共2700个字符。我们将其累加，就可以得到从0开始到k位数的字符总和。如果n大于该数，说明还需要继续叠加，如果n小于等于该数，说明n在k位数之中。</p><p>我以2000这个数字举例，0-99有190个字符，0-999有2890个字符，因此2000在100-999之间，而100-999全都是3位数，因此2000-190=1810说明是从100开始的第1810个字符。1810除以3得到的整数位603，即从100开始的第603个数是602，余数为1，说明是602的第1个字符（索引从0开始计算），因此结果为0。时间复杂度为$O(log(n))$，空间复杂度为$O(log(n))$。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def findNthDigit(self, n: int) -&gt; int:</span><br><span class="line">        if n &lt; 10:</span><br><span class="line">            return n</span><br><span class="line">        current, next_ = 10, 190</span><br><span class="line">        k = 2</span><br><span class="line">        while next_ &lt; n:</span><br><span class="line">            k += 1</span><br><span class="line">            current = next_</span><br><span class="line">            next_ = next_ + 9 * k * 10 ** (k - 1)</span><br><span class="line">        number = (n - current) // k + 10 ** (k - 1)</span><br><span class="line">        remain = (n - current) % k</span><br><span class="line">        return int(str(number)[remain])</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  数学题目的规律往往需要按照数字的位数进行找规律，先找一位数，然后找两位数，然后找三位数，往往就可以发现一些规律。这种题不经常出现，但也是小伙伴们必须要掌握的，否则遇到就两眼一抹黑。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 剑指Offer44&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>1～n整数中1出现的次数(Leetcode 剑指Offer43)</title>
    <link href="https://USTCcoder.github.io/2020/08/05/program%20Leetcode_offer43/"/>
    <id>https://USTCcoder.github.io/2020/08/05/program Leetcode_offer43/</id>
    <published>2020-08-05T15:06:57.000Z</published>
    <updated>2020-09-02T02:17:30.317Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcodeoffer43.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这道题目典型的找规律，为什么这么说呢？从时间复杂度上就可以看出端倪，一般从数据量上就可以看出，这个题目的数据最大为$2^31$，说明用线性复杂度的算法都会超时，那么一定有特殊的方法进行求解。</p><a id="more"></a><h1 id="数学规律"><a href="#数学规律" class="headerlink" title="数学规律"></a><font size="5" color="red">数学规律</font></h1><p>我们一般按照位数进行找规律，0-9一位数中，只有1个1出现，而00-99中，这100个两位数中，有20个1出现，为什么呢？因为两位数的各位都是0-9，重复了10次，00-09，10-19，20-29，等等，一共10组，每一组的个位都有一个1，因此有10个，但是10-19这十个数中，十位也都是1，那么共有10 + 10 x 1 = 20个1。同理，我们可以得出000-999，这1000个三位数中，000-099，100-199，200-299，等等，一共10组，每一组的后两位都相当于00-99，因此共有10 x 20 = 200个，而且100-199，共有100个1，因此共有100 + 10 x 20 = 300个1。<br>根据这个规律，我们可以使用一种递归的思想，进行求解。<br>我以6666这个数字举例，6666可以分成6000+666因为最高位是6，因此可以看成从0-5999中所有1的个数，加上6000-6666中所有1的个数，0-5999的个数，可以从上面的规律中直接求解，0-999共有300个1，因此0-5999共有1000 + 6 <em> 300 = 2800个1，而6000-6666中，最高位都是6，可以看成0-666中所有的1的个数。就变成求2800 + 0-666中1的个数。再次递归，666可以分成600 + 66，0-599共有 100 + 6 </em> 20 = 220，因此变成求2800 + 220 + 0-66中1的个数。再次递归，66可以分成60 + 6，0-59共有10 + 6 + 1 = 16，因此变为2800 + 220 + 16 + 0-6中1的个数。再次递归，0-6中只有1个，总数为2800 + 220 + 16 + 1 = 3037个。<br>再举一个复杂一点的例子6106，6106可以分成6000 + 106，可以看成0-5999中所有1的个数共有2800个1，加上6000-6106中所有1的个数，因此变成求2800 + 0-106中1的个数。再次递归，106可以分成100 + 6，可以看成0-99中所有1的个数，加上100-106中所有1的个数，0-99中1的个数为20，因为没有超过200，所以不能加100，而且100-106中最高位是1，因此不能看成0-6中1的个数，而是要加上100-106中数字的个数，因为每一个数字的最高位都是1。这样就可以看成0-6中1的个数，因此为2800 + 20 + (106 - 100) + 1 + 0-6中1的个数，等于2828个。<br>因为一位数也满足这个条件，因此可以将一位数也放入该情形，不需要单独进行讨论，所以可以得到下面的代码。时间复杂度为$O(log(n))$，空间复杂度为$O(log(n))$。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    res = 0</span><br><span class="line">    def countDigitOne(self, n):</span><br><span class="line">        lens = len(str(n))</span><br><span class="line">        one_num = [0] * lens</span><br><span class="line">        for i in range(1, lens):</span><br><span class="line">            one_num[i] = one_num[i - 1] * 10 + 10 ** (i - 1)</span><br><span class="line">        self.res = 0</span><br><span class="line">        </span><br><span class="line">        def sub_question(n):</span><br><span class="line">            if n == 0:</span><br><span class="line">                return</span><br><span class="line">            array = [int(x) for x in str(n)]</span><br><span class="line">            lens = len(array)</span><br><span class="line">            high = 10 ** (lens - 1)</span><br><span class="line">            if array[0] == 1:</span><br><span class="line">                self.res += n - high + 1 + one_num[lens - 1]</span><br><span class="line">            elif array[0] != 0:</span><br><span class="line">                self.res += high + one_num[lens - 1] * array[0]</span><br><span class="line">            sub_question(n - high * array[0])</span><br><span class="line">        </span><br><span class="line">        sub_question(n)</span><br><span class="line">        return self.res</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  这个题目还是比较有趣的，难度较大，很多同学采用暴力法进行遍历统计1的个数，那么时间复杂度为$O(n \times log(n))$，在数据量很大的时候，是不合适的。这道题是不是非常有趣呢，小伙伴们务必掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 剑指Offer43&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>丑数 II(Leetcode 264)</title>
    <link href="https://USTCcoder.github.io/2020/08/04/program%20Leetcode264/"/>
    <id>https://USTCcoder.github.io/2020/08/04/program Leetcode264/</id>
    <published>2020-08-04T15:47:08.000Z</published>
    <updated>2020-09-02T02:18:13.546Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode264.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   非常有趣的一道题，看似简单，其实做起来很不容易想到最优的方法，我第一眼看上去就知道需要使用DP进行求解，然而提交时超出了时间限制，下面带小伙伴们看一看。</p><a id="more"></a><h1 id="DP-超时"><a href="#DP-超时" class="headerlink" title="DP(超时)"></a><font size="5" color="red">DP(超时)</font></h1><p>第i个数是丑数的前提是，i除以2或i除以3或i除以5是丑数，于是可以从2开始遍历。2除以2等于1，是丑数，因此将2加入哈希表中，3除以3等于1，是丑数，因此将3加入哈希表中，4除以2等于2，是丑数，因此将4加入哈希表中，以此类推。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def nthUglyNumber(self, n):</span><br><span class="line">        """</span><br><span class="line">        :type n: int</span><br><span class="line">        :rtype: int</span><br><span class="line">        """</span><br><span class="line">        dp = set()</span><br><span class="line">        dp.add(1)</span><br><span class="line">        current = 1</span><br><span class="line">        while len(dp) &lt; n:</span><br><span class="line">            current += 1</span><br><span class="line">            if current / 2 in dp or current / 3 in dp or current / 5 in dp:</span><br><span class="line">                dp.add(current)</span><br><span class="line">        return current</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="DP"><a href="#DP" class="headerlink" title="DP"></a><font size="5" color="red">DP</font></h1><p>上面的方法虽然可行，但是当n=1690时，第n个丑数为2123366400，超过了时间限制。我们需要找一种时间复杂度更小的算法。我们发现丑数除以2，除以3，除以5也都是丑数，因此我们只需要保存丑数序列即可，建立三个指针，都指向1，一个指针每次负责乘2，一个负责乘3，一个负责乘5，因为指针都是指向丑数，因此三个指针乘积结果最小的也就是当前的丑数。再比较这个数是由哪一个指针得到，说明这个指针应该相后移动一个距离，移到下一个可能产生丑数的地方。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def nthUglyNumber(self, n):</span><br><span class="line">        """</span><br><span class="line">        :type n: int</span><br><span class="line">        :rtype: int</span><br><span class="line">        """</span><br><span class="line">        p2, p3, p5 = 0, 0, 0</span><br><span class="line">        dp = [1] * n</span><br><span class="line">        for i in range(1, n):</span><br><span class="line">            mini = min(dp[p2] * 2, dp[p3] * 3, dp[p5] * 5)</span><br><span class="line">            if mini == dp[p2] * 2:</span><br><span class="line">                p2 += 1</span><br><span class="line">            if mini == dp[p3] * 3:</span><br><span class="line">                p3 += 1</span><br><span class="line">            if mini == dp[p5] * 5:</span><br><span class="line">                p5 += 1</span><br><span class="line">            dp[i] = mini</span><br><span class="line">        return dp[-1]</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  丑数是不是非常有趣呢？这个题目是一个三指针加动态规划的问题，希望小伙伴可以喜欢。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 264&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>表示数值的字符串(Leetcode 剑指Offer20)</title>
    <link href="https://USTCcoder.github.io/2020/08/03/program%20Leetcode_offer20/"/>
    <id>https://USTCcoder.github.io/2020/08/03/program Leetcode_offer20/</id>
    <published>2020-08-03T11:53:43.000Z</published>
    <updated>2020-09-02T02:09:40.719Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcodeoffer20.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   在某大厂的笔试中，我做过这道题目，不过当时的笔试较为简单，没有这个题目来的复杂。我们就来说一说这种判断是否合法的题目。</p><a id="more"></a><h1 id="有限状态自动机"><a href="#有限状态自动机" class="headerlink" title="有限状态自动机"></a><font size="5" color="red">有限状态自动机</font></h1><p>哇哦，名字就要起成我看不懂的样子，其实认真看下去也并不难，其主要是状态之间的来回转换。我们分析，合法的输入字符有哪些。<br>空格，符号，数字，小数点，e或者E。其他的都是不正确的输入。如果暴力if，当然也可以求解，在这里就不过多赘述，小伙伴们可以自行验证。<br>状态机，顾名思义，就是在状态之间来回转换，看一看能否到达最终的状态，如果可以则为有效输入，否则为无效输入。<br>下面在代码中对重要部分进行解读。<br><img src="/images/ALGORITHM/leetcodeoffer20_solve.png" alt="q"><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def isNumber(self, s):</span><br><span class="line">        dic = [</span><br><span class="line">        # 起始状态为0，后面可以为空格" 1"，正负号"-2"，数字"3"或者小数点".4"，其他的都是不合法的。</span><br><span class="line">        # 可以将起始处的空格定义为状态0，符号定义为状态1，数字定义为状态2，小数点定义为状态4。</span><br><span class="line">        {'blank': 0, 'sign': 1, 'digit': 2, 'dot': 4},</span><br><span class="line">        # 如果位于状态1，则为列表dic的索引1，出现了符号，后面可以为数字"-2"，或者小数点"-.4"，其他的都是不合法的。</span><br><span class="line">        # 数字已经定义了为状态2，小数点也定义了为状态4。</span><br><span class="line">        {'digit': 2, 'dot': 4},</span><br><span class="line">        # 如果位于状态2，则出现了数字，后面可以为数字"33"，小数点"3.4"，e"3e5"，空格"3 "，其他的都是不合法的。</span><br><span class="line">        # 数字已经定义了为状态2，小数点定义为状态3，因为状态4为前面无数字的小数点，状态3则为前面有数字的小数点，e定义为状态5，空格定义为状态8</span><br><span class="line">        {'digit': 2, 'dot': 3, 'e': 5, 'blank': 8},</span><br><span class="line">        # 有的小朋友感觉困惑？为什么需要两个状态的小数点，因为前面有数字的小数点，后面还可以跟数字"34"，e"3.e"，或者直接结束"3."</span><br><span class="line">        {'digit': 3, 'e': 5, 'blank': 8},</span><br><span class="line">        # 而前面没有数字的小数点，后面只能够跟数字，因此两个小数点的状态是不同的，所以需要不同的记录方式，符号位，数字位，空格都是相同的道理。</span><br><span class="line">        {'digit': 3},</span><br><span class="line">        # 如果位于状态5，出现了e，那么后面可以跟符号"3e+5"，或者数字"3e5"</span><br><span class="line">        # 符号定义为状态6，数字定义为状态7</span><br><span class="line">        {'sign': 6, 'digit': 7},</span><br><span class="line">        # 如果位于状态6，出现了符号，那么后面只能跟数字"3e+5"</span><br><span class="line">        # 数字定义为状态7</span><br><span class="line">        {'digit': 7},</span><br><span class="line">        # 如果位于状态7，出现了数字，那么后面可以跟数字"3e44"，或者空格"3e4 "</span><br><span class="line">        # 数字定义为状态7，空格定义为状态8</span><br><span class="line">        {'digit': 7, 'blank': 8},</span><br><span class="line">        # 如果位于状态8，出现了空格，那么代表结束，只能跟空格</span><br><span class="line">        {'blank': 8}</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        # 设置初始状态为0</span><br><span class="line">        p = 0</span><br><span class="line">        # 遍历所有的字符，进行状态的转化</span><br><span class="line">        for c in s:</span><br><span class="line">            if c == ' ':</span><br><span class="line">                state = 'blank'</span><br><span class="line">            elif c == '+' or c == '-':</span><br><span class="line">                state = 'sign'</span><br><span class="line">            elif c == '.':</span><br><span class="line">                state = 'dot'</span><br><span class="line">            elif c == 'e' or c == 'E':</span><br><span class="line">                state = 'e'</span><br><span class="line">            elif c.isdigit():</span><br><span class="line">                state = 'digit'</span><br><span class="line">            else:</span><br><span class="line">                state = 'ERROR'</span><br><span class="line">            # 如果下一个状态不在当前状态的表中，说明报错，则不可以表示</span><br><span class="line">            if state not in dic[p]:</span><br><span class="line">                return False</span><br><span class="line">            # 如果在则继续判断是否在下一个状态中。</span><br><span class="line">            p = dic[p][state]</span><br><span class="line">        # 最终的状态是位于2，3，7，8状态中的一个，即必须要以状态2的数字结尾，状态3的小数点结尾，状态7的数字结尾，状态8的空格结尾。</span><br><span class="line">        return p in {2, 3, 7, 8}</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  在这个文章里，详细的给小伙伴们捋了一下有限状态自动机的使用情景，小伙伴们需要认真审视自己的状态表，千万不要遗漏情况，要记得考虑一个字符的多种使用情景需要分配多种状态，记得这些事情，小伙伴们再也不用担心使用繁杂的if语句了。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 剑指Offer20&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>被围绕的区域(Leetcode 130)</title>
    <link href="https://USTCcoder.github.io/2020/08/02/program%20Leetcode130/"/>
    <id>https://USTCcoder.github.io/2020/08/02/program Leetcode130/</id>
    <published>2020-08-02T15:07:31.000Z</published>
    <updated>2020-09-02T02:17:55.779Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode130.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这道题目难度不大，是一个经典的图搜索问题，如何搜索才能达到最高的效率呢？这题有一个巧妙的方法。</p><a id="more"></a><h1 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a><font size="5" color="red">DFS</font></h1><p>图的搜索方法，深度优先搜索，但是这道题的技巧在于，从边缘点进行搜索，搜索到的点都是不被围绕的点。为了记录搜索的路径，常用方法是建立一个哈希表，存放已经经过的点，这里为了节省空间，使用了一种技巧，将搜索过的点改为A，那么下次再搜索到时也不会进行重复搜索，非常方便。时间复杂度为$O(m \times n)$，空间复杂度为$O(m \times n)$<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def solve(self, board):</span><br><span class="line">        """</span><br><span class="line">        Do not return anything, modify board in-place instead.</span><br><span class="line">        """</span><br><span class="line">        def dfs(x, y):</span><br><span class="line">            board[x][y] = "A"</span><br><span class="line">            for i, j in direction:</span><br><span class="line">                new_x, new_y = x + i, y + j</span><br><span class="line">                if 0 &lt;= new_x &lt; row and 0 &lt;= new_y &lt; col and board[new_x][new_y] == "O":</span><br><span class="line">                    dfs(new_x, new_y)</span><br><span class="line"></span><br><span class="line">        if not board:</span><br><span class="line">            return board</span><br><span class="line">        direction = [[0, 1], [0, -1], [1, 0], [-1, 0]]</span><br><span class="line">        row, col = len(board), len(board[0])</span><br><span class="line">        for i in range(row):</span><br><span class="line">            if board[i][0] == "O":</span><br><span class="line">                dfs(i, 0)</span><br><span class="line">            if board[i][col - 1] == "O":</span><br><span class="line">                dfs(i, col - 1)</span><br><span class="line">        for j in range(col):</span><br><span class="line">            if board[0][j] == "O":</span><br><span class="line">                dfs(0, j)</span><br><span class="line">            if board[row - 1][j] == "O":</span><br><span class="line">                dfs(row - 1, j)</span><br><span class="line">        for i in range(row):</span><br><span class="line">            for j in range(col):</span><br><span class="line">                if board[i][j] == "A":</span><br><span class="line">                    board[i][j] = "O"</span><br><span class="line">                elif board[i][j] == "O":</span><br><span class="line">                    board[i][j] = "X"</span><br><span class="line"></span><br><span class="line">        return board</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="BFS"><a href="#BFS" class="headerlink" title="BFS"></a><font size="5" color="red">BFS</font></h1><p>这道题也可以类似的使用BFS来进行求解，解题思路大致相同。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def solve(self, board):</span><br><span class="line">        """</span><br><span class="line">        Do not return anything, modify board in-place instead.</span><br><span class="line">        """</span><br><span class="line">        if not board:</span><br><span class="line">            return board</span><br><span class="line">        direction = [[0, 1], [0, -1], [1, 0], [-1, 0]]</span><br><span class="line">        row, col = len(board), len(board[0])</span><br><span class="line"></span><br><span class="line">        queue = deque()</span><br><span class="line"></span><br><span class="line">        for i in range(row):</span><br><span class="line">            if board[i][0] == "O":</span><br><span class="line">                queue.append((i, 0))</span><br><span class="line">            if board[i][col - 1] == "O":</span><br><span class="line">                queue.append((i, col - 1))</span><br><span class="line">        for j in range(col):</span><br><span class="line">            if board[0][j] == "O":</span><br><span class="line">                queue.append((0, j))</span><br><span class="line">            if board[row - 1][j] == "O":</span><br><span class="line">                queue.append((row - 1, j))</span><br><span class="line"></span><br><span class="line">        while queue:</span><br><span class="line">            x, y = queue.popleft()</span><br><span class="line">            board[x][y] = "A"</span><br><span class="line">            for i, j in direction:</span><br><span class="line">                new_x, new_y = x + i, y + j</span><br><span class="line">                if 0 &lt;= new_x &lt; row and 0 &lt;= new_y &lt; col and board[new_x][new_y] == "O":</span><br><span class="line">                    queue.append((new_x, new_y))</span><br><span class="line"></span><br><span class="line">        for i in range(row):</span><br><span class="line">            for j in range(col):</span><br><span class="line">                if board[i][j] == "A":</span><br><span class="line">                    board[i][j] = "O"</span><br><span class="line">                elif board[i][j] == "O":</span><br><span class="line">                    board[i][j] = "X"</span><br><span class="line"></span><br><span class="line">        return board</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  路径搜索问题已经不想重复强调了，重要！重要！重要！</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 130&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>C++面向对象完结</title>
    <link href="https://USTCcoder.github.io/2020/07/31/C++_oop_5/"/>
    <id>https://USTCcoder.github.io/2020/07/31/C++_oop_5/</id>
    <published>2020-07-31T15:33:45.000Z</published>
    <updated>2020-08-11T14:53:24.518Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++52.png" alt="2"></p><h1 id="C-面向对象完结"><a href="#C-面向对象完结" class="headerlink" title="C++面向对象完结"></a><font size="5" color="red">C++面向对象完结</font></h1><p>  C++面向对象终于来到了完结篇，最后剩下的内容也是最最重要的内容——多态和虚函数，虽然在自己写代码时很少用到，但是在企业级开发中是必不可少的部分，在最终章会为大家详细介绍。<br><a id="more"></a></p><h1 id="函数重写"><a href="#函数重写" class="headerlink" title="函数重写"></a><font size="5">函数重写</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">class Person {</span><br><span class="line">public:</span><br><span class="line"></span><br><span class="line">//父类中存在一个成员方法，名为program</span><br><span class="line">void program() {</span><br><span class="line">cout &lt;&lt; "我的名字是程序员，我会写代码" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">class CppProgrammer :public Person {</span><br><span class="line">public:</span><br><span class="line">//函数重写，发生在继承关系中，子类的方法名和参数列表和父类一样，则会发生函数重写。</span><br><span class="line">//特点是创建子类对象会优先调用子类方法。</span><br><span class="line">void program() {</span><br><span class="line">cout &lt;&lt; "我的名字是Cpp程序员，我会写Cpp代码" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">class JavaProgrammer :public Person {</span><br><span class="line">public:</span><br><span class="line"></span><br><span class="line">void program() {</span><br><span class="line">cout &lt;&lt; "我的名字是Java程序员，我会写Java代码" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">class PythonProgrammer :public Person {</span><br><span class="line">public:</span><br><span class="line"></span><br><span class="line">void program() {</span><br><span class="line">cout &lt;&lt; "我的名字是Python程序员，我会写Python代码" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">Person p;</span><br><span class="line">CppProgrammer cp;</span><br><span class="line">JavaProgrammer jp;</span><br><span class="line">PythonProgrammer pp;</span><br><span class="line"></span><br><span class="line">//调用父类对象的成员方法时，会使用父类的成员方法。</span><br><span class="line">p.program();</span><br><span class="line">//调用子类对象的成员方法时，会使用子类的成员方法。</span><br><span class="line">cp.program();</span><br><span class="line">jp.program();</span><br><span class="line">pp.program();</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++53.png" alt="1"></p><h1 id="多态"><a href="#多态" class="headerlink" title="多态"></a><font size="5">多态</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">class Person {</span><br><span class="line">public:</span><br><span class="line"></span><br><span class="line">//虚函数：在成员函数前面加上virtual关键字，称这个函数为虚函数。</span><br><span class="line">//如果将父类中的函数实现舍去，并且在函数的参数列表后面加上=0;则称为纯虚函数。</span><br><span class="line">//当有了纯虚函数，这个类就被称为抽象类，抽象类无法实例化对象，而且子类必须重写抽象类中的纯虚函数，否则也是抽象类。</span><br><span class="line">virtual void program() {</span><br><span class="line">cout &lt;&lt; "我的名字是程序员，我会写代码" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">class CppProgrammer :public Person {</span><br><span class="line">public:</span><br><span class="line"></span><br><span class="line">//多态的本质：父类指针或者引用指向子类对象，调用时会调用子类的成员方法。</span><br><span class="line">//多态的两个必要条件：有继承关系，子类要重写父类的虚函数。</span><br><span class="line">void program() {</span><br><span class="line">cout &lt;&lt; "我的名字是Cpp程序员，我会写Cpp代码" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">class JavaProgrammer :public Person {</span><br><span class="line">public:</span><br><span class="line"></span><br><span class="line">void program() {</span><br><span class="line">cout &lt;&lt; "我的名字是Java程序员，我会写Java代码" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">class PythonProgrammer :public Person {</span><br><span class="line">public:</span><br><span class="line"></span><br><span class="line">void program() {</span><br><span class="line">cout &lt;&lt; "我的名字是Python程序员，我会写Python代码" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">void test(Person&amp; p) {</span><br><span class="line">p.program();</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">//因为Person类中的program方法有具体的实现，因此不是一个抽象类，可以创建实例对象。</span><br><span class="line">Person p;</span><br><span class="line">CppProgrammer cp;</span><br><span class="line">JavaProgrammer jp;</span><br><span class="line">PythonProgrammer pp;</span><br><span class="line"></span><br><span class="line">//将父类的实例对象传给父类的引用，程序正常执行。</span><br><span class="line">test(p);</span><br><span class="line">//将子类的实例对象传给父类的引用，将产生多态，会执行子类的成员方法。</span><br><span class="line">test(cp);</span><br><span class="line">test(jp);</span><br><span class="line">test(pp);</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++54.png" alt="1"></p><h1 id="虚析构与纯虚析构"><a href="#虚析构与纯虚析构" class="headerlink" title="虚析构与纯虚析构"></a><font size="5">虚析构与纯虚析构</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">class Person {</span><br><span class="line">public:</span><br><span class="line"></span><br><span class="line">Person() {</span><br><span class="line">cout &lt;&lt; "父类的构造函数被调用" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">virtual void program() {</span><br><span class="line">cout &lt;&lt; "我的名字是程序员，我会写代码" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">~Person() {</span><br><span class="line">cout &lt;&lt; "父类的析构函数被调用" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">class CppProgrammer :public Person {</span><br><span class="line">public:</span><br><span class="line"></span><br><span class="line">CppProgrammer() {</span><br><span class="line">cout &lt;&lt; "子类的构造函数被调用" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">void program() {</span><br><span class="line">cout &lt;&lt; "我的名字是Cpp程序员，我会写Cpp代码" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">~CppProgrammer() {</span><br><span class="line">cout &lt;&lt; "子类的析构函数被调用" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">{</span><br><span class="line">//当使用new在堆区中开辟子类对象数据，并返回给父类指针时。</span><br><span class="line">//delete父类指针，无法看到子类的析构函数，这就不会调用子类的析构函数，因此有内存泄漏的风险。</span><br><span class="line">//解决办法有两种，一个是在父类的析构函数前加virtual关键字，变成一个虚析构函数。</span><br><span class="line">//第二种方法是将父类的析构函数变为一个纯虚析构函数，但是此时运行会产生错误，因为父类的一些成员变量可能无法析构，所以还要在类外通过作用域进行实现。</span><br><span class="line">//此时两者的区别是，虚析构函数的父类不是一个抽象类，而纯虚析构函数的父类是一个抽象类。</span><br><span class="line">Person* p = new CppProgrammer();</span><br><span class="line">p-&gt;program();</span><br><span class="line">delete p;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++55.png" alt="1"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  到今天为止，面向对象的所有内容都已经给小伙伴们分享完了，至于其中包含的奇技淫巧，需要小伙伴们慢慢磨砺，多态是设计模式的基础，以后会给小伙伴们详细介绍设计模式，在这里一定要掌握多态的用法。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++面向对象完结&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>整数拆分(Leetcode 343)</title>
    <link href="https://USTCcoder.github.io/2020/07/30/program%20Leetcode343/"/>
    <id>https://USTCcoder.github.io/2020/07/30/program Leetcode343/</id>
    <published>2020-07-30T09:27:41.000Z</published>
    <updated>2020-09-02T02:18:26.323Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode343.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   整数拆分问题是一个数学问题，如果指定拆分个数是2个的话，那么就是小伙伴们小学都学过的问题，可能小伙伴们会抬杠，小学怎么可能学过求二次函数最大值的问题呢？其实二拆分这个问题有另外一种描述：同样周长的正方形和长方形哪一个面积更大？Leetcode这一题将二拆分进行了推广，不指定拆分个数，这应该怎么做呢？暴力穷举当然是不行的，这是指数级的时间复杂度，应该如何优化呢？</p><a id="more"></a><h1 id="DP"><a href="#DP" class="headerlink" title="DP"></a><font size="5" color="red">DP</font></h1><p>我们使用动态规划进行求解，dp[i]表示第i个数分解可得的最大值，可以得到状态转移方程</p><script type="math/tex; mode=display">dp[i] = \max_{j = 1}^{i}(dp[i], dp[j] \times (i - j), j \times (i - j))</script><p>这个方案不难想到，小伙伴们千万不要忘记还有$i \times j$，DP的时间复杂度为$O(n^2)$，空间复杂度为$O(n)$，因为数字较小，因此很容易通过。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def integerBreak(self, n):</span><br><span class="line">        dp = [0] * (n + 1)</span><br><span class="line">        for i in range(2, n + 1):</span><br><span class="line">            for j in range(1, i):</span><br><span class="line">                dp[i] = max(dp[j] * (i - j), j * (i - j), dp[i])</span><br><span class="line">        return dp[-1]</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="数学方法"><a href="#数学方法" class="headerlink" title="数学方法"></a><font size="5" color="red">数学方法</font></h1><p>这个方法很巧妙，用一种归纳的方法进行这类问题的求解。有很多算法题，有一些较好的数学解法，这个题目可以给小伙伴们提供一个思路。<br>我们观察，如果一个数大于等于4，它的因子会出现什么情况？</p><ol><li>可不可能出现大于等于4的因子？答案是不可能的，假设存在这样一个数x，满足$x \ge 4$，那么$2(x - 2) \ge x$在x大于等于4时恒成立，因此可以进行拆分 。</li><li>2的个数不会超过3个，因为3个2的乘积为8，小于2个3的乘积9，因此如果可以拆分成3个2，那么不如拆分成2个3.</li><li>在大于等于5的情况中，不可能出现1，假设存在这样一个数x，满足$x - 1 \ge 4$，那么x - 1可以拆分成至少2个数，因此在其中一个增加1，乘积都会变大，所以不可能出现1的情况。<br>综上所述，当x大于等于5时，其因数只可能是2和3，且2的个数只能是0个，1个，2个，其余都是3，如果模3等于0，说明全部都是3，如果模3等于1，说明有2个2，如果模3等于2，说明有1个2。这种算法的时间复杂度为$O(1)$，空间复杂度为$O(1)$。<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def integerBreak(self, n: int) -&gt; int:</span><br><span class="line">        if n &lt;= 3:</span><br><span class="line">            return n - 1</span><br><span class="line">        k, r = n // 3, n % 3</span><br><span class="line">        if r == 0:</span><br><span class="line">            return 3 ** k</span><br><span class="line">        elif r == 1:</span><br><span class="line">            return 3 ** (k - 1) * 4</span><br><span class="line">        else:</span><br><span class="line">            return 3 ** k * 2</span><br></pre></td></tr></tbody></table></figure></li></ol><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  数学方法往往很难想到，这道题目数据量较小，因此可以通过DP求解，如果这题的n非常大，那么就需要我们考虑数学方法，现在笔试的数据越来越庞大了，小伙伴们要加油呀。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 343&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>C++面向对象拓展</title>
    <link href="https://USTCcoder.github.io/2020/07/28/C++_oop_4/"/>
    <id>https://USTCcoder.github.io/2020/07/28/C++_oop_4/</id>
    <published>2020-07-28T11:39:57.000Z</published>
    <updated>2020-08-11T14:53:20.828Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++47.png" alt="2"></p><h1 id="C-面向对象拓展"><a href="#C-面向对象拓展" class="headerlink" title="C++面向对象拓展"></a><font size="5" color="red">C++面向对象拓展</font></h1><p>  C++面向对象还有一部分拓展知识，包括运算符重载以及多继承产生的问题，也可以给最终章的虚函数做一个铺垫，在这里给大家做简单的介绍。<br><a id="more"></a></p><h1 id="运算符重载"><a href="#运算符重载" class="headerlink" title="运算符重载"></a><font size="5">运算符重载</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">class Person {</span><br><span class="line"></span><br><span class="line">public:</span><br><span class="line">string name;</span><br><span class="line">int age;</span><br><span class="line">int money;</span><br><span class="line"></span><br><span class="line">Person(string name, int age, int money) {</span><br><span class="line">this-&gt;name = name;</span><br><span class="line">this-&gt;age = age;</span><br><span class="line">this-&gt;money = money;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//operator加上要重载的运算符，称为运算符重载。</span><br><span class="line">//在这里重载+运算符，两个对象之间不能进行运算，但是重载了加号运算符之后，可以根据重载的内容实现两个对象的加法运算。</span><br><span class="line">int operator+(Person&amp; p) {</span><br><span class="line">return this-&gt;money + p.money;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//在这里重载==运算符，两个对象之间不能进行运算，但是重载了等于号运算符之后，可以根据重载的内容判断两个对象是否相等。</span><br><span class="line">bool operator==(Person&amp; p) {</span><br><span class="line">return this-&gt;age == p.age;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">Person p1("C++程序员", 28, 28000);</span><br><span class="line">Person p2("Python程序员", 22, 22000);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; p1.name &lt;&lt; " 我的年龄是：" &lt;&lt; p1.age &lt;&lt; " 我的工资是：" &lt;&lt; p1.money &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; p2.name &lt;&lt; " 我的年龄是：" &lt;&lt; p2.age &lt;&lt; " 我的工资是：" &lt;&lt; p2.money &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//重载后加法计算两个对象的money成员变量之和，因此会输出50000</span><br><span class="line">cout &lt;&lt; "我们的工资总数为：" &lt;&lt; p1 + p2 &lt;&lt; endl;;</span><br><span class="line">//重载等于号判断两个对象的年龄是否相等，因此会返回0</span><br><span class="line">cout &lt;&lt; "我们的年龄是否相等：" &lt;&lt; (p1 == p2) &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++48.png" alt="1"></p><h1 id="C-多继承"><a href="#C-多继承" class="headerlink" title="C++多继承"></a><font size="5">C++多继承</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">class A {</span><br><span class="line">public:</span><br><span class="line">//A类中包括name和a两个成员变量和A的有参构造函数，以及getName成员方法</span><br><span class="line">string name;</span><br><span class="line">int a;</span><br><span class="line"></span><br><span class="line">A(string name, int a) {</span><br><span class="line">this-&gt;name = name;</span><br><span class="line">this-&gt;a = a;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">string getName() {</span><br><span class="line">return name;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">class B {</span><br><span class="line">public:</span><br><span class="line">//B类中包括name和b两个成员变量和B的有参构造函数，以及getName成员方法</span><br><span class="line">string name;</span><br><span class="line">int b;</span><br><span class="line"></span><br><span class="line">B(string name, int b) {</span><br><span class="line">this-&gt;name = name;</span><br><span class="line">this-&gt;b = b;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">string getName() {</span><br><span class="line">return name;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">class C:public A, public B{</span><br><span class="line">public:</span><br><span class="line">//C类中包括name和c两个成员变量和C的有参构造函数，以及getName成员方法</span><br><span class="line">//C继承A也继承B，因此同时拥有A和B的成员，但是A和B的同名成员，需要使用作用域进行区分，如果子类也具有同名成员，则会覆盖父类的同名成员。</span><br><span class="line">string name;</span><br><span class="line">int c;</span><br><span class="line"></span><br><span class="line">//注意子类构造函数的写法，在参数列表后面加冒号，并写调用父类的构造函数，和Java，Python不同。</span><br><span class="line">C(string name_a, int a, string name_b, int b, string name_c, int c) :A(name_a, a), B(name_b, b) {</span><br><span class="line">this-&gt;name = name_c;</span><br><span class="line">this-&gt;c = c;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">string getName() {</span><br><span class="line">return name;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">};</span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">C c("A", 1, "B", 2, "C", 3);</span><br><span class="line">//对象c拥有类A的成员变量a</span><br><span class="line">cout &lt;&lt; "c.a = " &lt;&lt; c.a &lt;&lt; endl;</span><br><span class="line">//对象c拥有类B的成员变量b</span><br><span class="line">cout &lt;&lt; "c.b = " &lt;&lt; c.b &lt;&lt; endl;</span><br><span class="line">//对象c拥有类C的成员变量c</span><br><span class="line">cout &lt;&lt; "c.c = " &lt;&lt; c.c &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//类C拥有和父类同名的成员变量name，因此子类的成员变量会覆盖父类</span><br><span class="line">cout &lt;&lt; "c.name = " &lt;&lt; c.name &lt;&lt; endl;</span><br><span class="line">//类C拥有和父类同名的成员方法getName，因此子类的成员方法会覆盖父类</span><br><span class="line">cout &lt;&lt; "c.getName() = " &lt;&lt; c.getName() &lt;&lt; endl;</span><br><span class="line">//类C拥有和类A同名的成员变量name，因此要想访问类A的成员变量，需要加入作用域</span><br><span class="line">cout &lt;&lt; "c.A::name = " &lt;&lt; c.A::name &lt;&lt; endl;</span><br><span class="line">//类C拥有和类A同名的成员方法getName，因此要想访问类A的成员方法，需要加入作用域</span><br><span class="line">cout &lt;&lt; "c.A::getName() = " &lt;&lt; c.A::getName() &lt;&lt; endl;</span><br><span class="line">//类C拥有和类B同名的成员变量name，因此要想访问类B的成员变量，需要加入作用域</span><br><span class="line">cout &lt;&lt; "c.B:::name = " &lt;&lt; c.B::name &lt;&lt; endl;</span><br><span class="line">//类C拥有和类B同名的成员方法getName，因此要想访问类B的成员方法，需要加入作用域</span><br><span class="line">cout &lt;&lt; "c.B::getName() = " &lt;&lt; c.B::getName() &lt;&lt; endl;</span><br><span class="line">//类C拥有和父类同名的成员变量name，因此子类默认调用自己的成员变量，c.C::name等价于c.name</span><br><span class="line">cout &lt;&lt; "c.C::name = " &lt;&lt; c.C::name &lt;&lt; endl;</span><br><span class="line">//类C拥有和父类同名的成员方法getName，因此子类默认调用自己的成员方法，c.C::getName等价于c.getName</span><br><span class="line">cout &lt;&lt; "c.C::getName() = " &lt;&lt; c.C::getName() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++49.png" alt="1"></p><h1 id="C-菱形继承——解决方案"><a href="#C-菱形继承——解决方案" class="headerlink" title="C++菱形继承——解决方案"></a><font size="5">C++菱形继承——解决方案</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">class A {</span><br><span class="line">public:</span><br><span class="line">string name;</span><br><span class="line">int a;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">class B :public A {</span><br><span class="line">public:</span><br><span class="line">int b;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">class C :public A {</span><br><span class="line">public:</span><br><span class="line">int c;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">//D类继承B和C类，然而B和C都继承A类，构成了一种菱形继承关系。</span><br><span class="line">//A类中的name成员变量，在D中的name不知道是从B中继承而来的还是从C中继承而来的</span><br><span class="line">class D :public B, public C {</span><br><span class="line">public:</span><br><span class="line">int d;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">D d;</span><br><span class="line">//访问时和之前说过的一样，可以通过指定作用域来实现对哪一个父类的成员变量进行访问。</span><br><span class="line">//但是不可以直接使用d.name，这就编译器不知道是从哪一个类继承而来的成员变量，会报错。</span><br><span class="line">d.B::name = "B";</span><br><span class="line">d.C::name = "C";</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "d.B::name = " &lt;&lt; d.B::name &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d.C::name = " &lt;&lt; d.C::name &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++51.png" alt="1"></p><h1 id="C-菱形继承——解决方案-1"><a href="#C-菱形继承——解决方案-1" class="headerlink" title="C++菱形继承——解决方案"></a><font size="5">C++菱形继承——解决方案</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">class A {</span><br><span class="line">public:</span><br><span class="line">string name;</span><br><span class="line">int a;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">//对于上面发生的问题，通过在继承前面加上virtual关键字，代表虚继承。</span><br><span class="line">//小伙伴们可以理解为B和C不再保存A中的具体内容，而是保存了一份偏移地址。</span><br><span class="line">//当调用d.B::name时，会指向d.A::name，当调用d.C::name时，也会指向d.A::name，当调用d.name时也会指向d.A::name。</span><br><span class="line">class B :virtual public A {</span><br><span class="line">public:</span><br><span class="line">int b;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">class C :virtual public A {</span><br><span class="line">public:</span><br><span class="line">int c;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">class D :public B, public C {</span><br><span class="line">public:</span><br><span class="line">int d;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">D d;</span><br><span class="line">d.B::name = "B";</span><br><span class="line">d.C::name = "C";</span><br><span class="line"></span><br><span class="line">//因此d.B::name和d.C::name和d.name都是相同的结果。</span><br><span class="line">cout &lt;&lt; "d.B::name = " &lt;&lt; d.B::name &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d.C::name = " &lt;&lt; d.C::name &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d.name = " &lt;&lt; d.name &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">d.name = "D";</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "d.B::name = " &lt;&lt; d.B::name &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d.C::name = " &lt;&lt; d.C::name &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d.name = " &lt;&lt; d.name &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++51.png" alt="1"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  今天的内容是C++面向对象的拓展部分，比较重要的部分是运算符的重载，在后面STL中可能会再次见到它，而菱形继承方式，我们在使用面向对象时，要尽量避免碰到它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++面向对象拓展&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>矩阵中的最长递增路径(Leetcode 329)</title>
    <link href="https://USTCcoder.github.io/2020/07/26/program%20Leetcode329/"/>
    <id>https://USTCcoder.github.io/2020/07/26/program Leetcode329/</id>
    <published>2020-07-26T07:36:59.000Z</published>
    <updated>2020-09-02T02:18:20.668Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode329.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   最长递增路径问题，是一个有向图问题，而且要更为复杂一些，不是一个简单的单向图，而是<strong>多向图</strong>。要想找到最长的一条路径，必然要使用两种常见的搜索方法，<strong>DFS和BFS</strong>。但是这两个搜索方法都有一个特点，就是<strong>已知起始点然后进行搜索</strong>，如果不知道起始点，如何搜索得到最长路径呢？一个最暴力的方法是遍历所有起始点，从每一个点开始搜索，并且比较哪一个最长。这样的方法时间复杂度太高，浪费的太多的计算资源，必然无法通过所有的样例，那么如何保存计算过程中的计算结果呢？有一个专业术语称为<strong>记忆化</strong>，下面给小伙伴聊一聊记忆化的DFS。</p><a id="more"></a><h1 id="记忆化-DFS"><a href="#记忆化-DFS" class="headerlink" title="记忆化+DFS"></a><font size="5" color="red">记忆化+DFS</font></h1><p>Python常用库给我们提供了一种重要的装饰器，<strong>lru_cache，其位于functools包下</strong>。主要是用于<strong>函数的递归调用时，会记录递归调用的结果，这样下次遇到同样的参数时，可以直接调出，不需要再次递归调用。其本质是一个字典，键是参数值，值是递归调用的结果</strong>。例如使用递归求斐波那契数列时，计算f(5)会使用到f(3)+f(4)，如果不使用lru_cache装饰器，已经得到了f(3)时，递归调用f(4)时还会重新调用f(3)的，这样就会出现指数爆炸式的计算量增长。当计算f(100)时，会递归调用f(99)和f(98)，如果有了lru_cache，就可以从字典中直接取出99和98对应的值，此时的时间复杂度为线性的，而递归是指数级的。<br>这道题也类似，<strong>我们遍历所有的点，从第一个点A去寻找最长递增路径的长度，如果寻找到了某一个点B，那么会记录从点B出发的最长递增路径，那么下次当其他点也寻找到了这个点B时，就不用重新遍历点B。时间复杂度为$O(mn)$，空间复杂度为$O(mn)$，m和n为矩阵的行和列</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from functools import lru_cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Solution(object):</span><br><span class="line">    def longestIncreasingPath(self, matrix):</span><br><span class="line">        """</span><br><span class="line">        :type matrix: List[List[int]]</span><br><span class="line">        :rtype: int</span><br><span class="line">        """</span><br><span class="line">        if not matrix:</span><br><span class="line">            return 0</span><br><span class="line">        direction = [[1, 0], [-1, 0], [0, 1], [0, -1]]</span><br><span class="line">        m, n = len(matrix), len(matrix[0])</span><br><span class="line"></span><br><span class="line">        @lru_cache(None)</span><br><span class="line">        def dfs(i, j):</span><br><span class="line">            res = 1</span><br><span class="line">            for x, y in direction:</span><br><span class="line">                new_x, new_y = i + x, j + y</span><br><span class="line">                if 0 &lt;= new_x &lt; m and 0 &lt;= new_y &lt; n and matrix[i][j] &gt; matrix[new_x][new_y]:</span><br><span class="line">                    res = max(res, dfs(new_x, new_y) + 1)</span><br><span class="line">            return res</span><br><span class="line"></span><br><span class="line">        result = 0</span><br><span class="line">        for i in range(m):</span><br><span class="line">            for j in range(n):</span><br><span class="line">                result = max(result, dfs(i, j))</span><br><span class="line">        return result</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="多源BFS"><a href="#多源BFS" class="headerlink" title="多源BFS"></a><font size="5" color="red">多源BFS</font></h1><p>这道题目也可以使用<strong>多源BFS</strong>来求解，我们理解题意可以发现，<strong>路径的终点一定是无法向四周扩展的点，也就是说终点的值一定大于周围的值</strong>，玩过围棋的小伙伴们更加了解这个说法，可以说是气，说明这个棋子还有几口气，如果周围都是对方的子，说明这个子没有气了，这也是同样道理，因此我们<strong>可以遍历所有的点，对每一个点计算四个方向的值，计算每一个点的气。然后对于每一个气为0的点，作为源，然后进行BFS广度优先搜索，搜索出距离源点为1且满足小于当前值的点，并且更新这些点的气，气的值减1，如果气为0，则加入下一轮的寻找过程中(这一步是关键，为什么气为0则加入下一轮，是因为如果气不为0，说明还有其他点可以到达，说明现在就到达这个点的路径必然不是最长路径)，依次寻找距离为2的点……，直到所有点都寻找完毕，此时的距离就是最长的路径</strong>。<br>这个算法的<strong>时间复杂度为$O(mn)$，空间复杂度也是$O(mn)$</strong>，我就直接将官方的题解借鉴过来，供大家参考。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    DIRS = [(-1, 0), (1, 0), (0, -1), (0, 1)]</span><br><span class="line"></span><br><span class="line">    def longestIncreasingPath(self, matrix):</span><br><span class="line">        if not matrix:</span><br><span class="line">            return 0</span><br><span class="line"></span><br><span class="line">        rows, columns = len(matrix), len(matrix[0])</span><br><span class="line">        outdegrees = [[0] * columns for _ in range(rows)]</span><br><span class="line">        queue = collections.deque()</span><br><span class="line">        for i in range(rows):</span><br><span class="line">            for j in range(columns):</span><br><span class="line">                for dx, dy in Solution.DIRS:</span><br><span class="line">                    newRow, newColumn = i + dx, j + dy</span><br><span class="line">                    if 0 &lt;= newRow &lt; rows and 0 &lt;= newColumn &lt; columns and matrix[newRow][newColumn] &gt; matrix[i][j]:</span><br><span class="line">                        outdegrees[i][j] += 1</span><br><span class="line">                if outdegrees[i][j] == 0:</span><br><span class="line">                    queue.append((i, j))</span><br><span class="line"></span><br><span class="line">        ans = 0</span><br><span class="line">        while queue:</span><br><span class="line">            ans += 1</span><br><span class="line">            size = len(queue)</span><br><span class="line">            for _ in range(size):</span><br><span class="line">                row, column = queue.popleft()</span><br><span class="line">                for dx, dy in Solution.DIRS:</span><br><span class="line">                    newRow, newColumn = row + dx, column + dy</span><br><span class="line">                    if 0 &lt;= newRow &lt; rows and 0 &lt;= newColumn &lt; columns and matrix[newRow][newColumn] &lt; matrix[row][column]:</span><br><span class="line">                        outdegrees[newRow][newColumn] -= 1</span><br><span class="line">                        if outdegrees[newRow][newColumn] == 0:</span><br><span class="line">                            queue.append((newRow, newColumn))</span><br><span class="line"></span><br><span class="line">        return ans</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  多次强调，<strong>DFS和BFS问题是高频考点</strong>，现在的题目难度越来越高，因此在掌握了基本的DFS和BFS之后，还需要多刷题，<strong>掌握一些奇技淫巧</strong>，这样遇到图相关的问题才能从容应对。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 329&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>分割数组的最大值(Leetcode 410)</title>
    <link href="https://USTCcoder.github.io/2020/07/25/program%20Leetcode410/"/>
    <id>https://USTCcoder.github.io/2020/07/25/program Leetcode410/</id>
    <published>2020-07-25T06:59:35.000Z</published>
    <updated>2020-09-02T02:18:29.998Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode410.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这个题目非常有难度，<strong>要将一个数组拆分成若干个子数组，并且要求得到的各个子数组之和的最大值最小。也就是说我们要切割的非常巧妙，尽量让每一个子数组之和相同，这样就不会产生有的很大，有的很小的问题</strong>。</p><a id="more"></a><h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a><font size="5" color="red">动态规划</font></h1><p>我们假定f[i][j]代表前i个元素分成j个数组，得到的最优解。而且已知cum_sum为前缀和，前缀和就是前i个数字的总和。如cum_sum[i] = nums[0] + nums[1] + … + nums[i - 1]，当i=0时，cum_sum=0。那么我们可以<strong>得出状态转移方程f[i][j] = min(f[i][j], max(f[k][j-1], cum_sum[i] - cum_sum[k])) for k in range(i - 1)</strong>。具体描述一下，f[k][j-1]说明的是在从前i个数分成j个数组中，已经将前k个数分成了j-1个数组得到的最优解，然后其余的从第k+1个数到第i个数之和为cum_sum[i] - cum_sum[k]，然后求两者的最大值，为这次分割得到的子数组最大值。遍历所有的k，求得使这个最大值最小的值。因为对于每一个数组前缀i和子数组个数j都要遍历i次，求出f[i][j]的最优解。因此<strong>时间复杂度为$O(n^{2}m)$，空间复杂度为$O(nm)$，其中n为数组的长度，m为子数组的个数</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def splitArray(self, nums: List[int], m: int) -&gt; int:</span><br><span class="line">        cum_sum = [0]</span><br><span class="line">        n = len(nums)</span><br><span class="line">        for i in range(n):</span><br><span class="line">            cum_sum.append(cum_sum[-1] + nums[i])</span><br><span class="line">        </span><br><span class="line">        dp = [[float('inf') for _ in range(m + 1)] for _ in range(n + 1)]</span><br><span class="line">        dp[0][0] = 0</span><br><span class="line">        for i in range(1, n + 1):</span><br><span class="line">            for j in range(1, min(i, m) + 1):</span><br><span class="line">                for k in range(i):</span><br><span class="line">                    dp[i][j] = min(dp[i][j], max(dp[k][j - 1], cum_sum[i] - cum_sum[k]))</span><br><span class="line">        return dp[-1][-1]</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="二分查找-贪心"><a href="#二分查找-贪心" class="headerlink" title="二分查找+贪心"></a><font size="5" color="red">二分查找+贪心</font></h1><p>这道题最妙的地方不在于DP，而是<strong>二分查找+贪心</strong>的算法，<strong>官方题解中写道：「使……最大值尽可能小」是二分搜索题目常见的问法</strong>。我们<strong>已知子数组之和的上限和下限，要找到最小的满足条件的子数组之和，就可以采用二分法，让上限和下限逐渐逼近于最终的答案</strong>。因为这是一个连续的子数组之和问题，所以我们只需要<strong>选择一个值，然后让子数组之和尽可能接近这个值，但是不超过这个值，一但超过，说明进入了下一个子数组。如果到最后子数组的个数小于等于给定的子数组个数，则说明这个值是满足条件的，可能需要继续缩小。否则这个值是小的，应该继续增大</strong>。<br><strong>因为子数组的最大值为全部数字之和，记作sum，子数组的最小值为单个数字的最大值，记作max，当然还可以继续优化，可以选择max和sum整除m的较大者。因为sum平分成m份，至少每个数组之和都是sum除以m，因此至少为sum整除m，我们就按照官方题解来叙述。从sum和max之间进行二分查找。因此时间复杂度为$O(nlog(sum-max))$，空间复杂度为$O(1)$</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def splitArray(self, nums: List[int], m: int) -&gt; int:</span><br><span class="line"></span><br><span class="line">        def check(max_val):</span><br><span class="line">            current_sum = 0</span><br><span class="line">            current_group = 1</span><br><span class="line">            for x in nums:</span><br><span class="line">                current_sum += x</span><br><span class="line">                if current_sum &gt; max_val:</span><br><span class="line">                    current_group += 1</span><br><span class="line">                    current_sum = x</span><br><span class="line">                    if current_group &gt; m:</span><br><span class="line">                        return False</span><br><span class="line">            return True</span><br><span class="line"></span><br><span class="line">        left, right = max(nums), sum(nums)</span><br><span class="line">        while left &lt; right:</span><br><span class="line">            mid = (left + right) // 2</span><br><span class="line">            if check(mid):</span><br><span class="line">                right = mid</span><br><span class="line">            else:</span><br><span class="line">                left = mid + 1</span><br><span class="line">        return left</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  这道题目非常有趣，我们<strong>在学习或者使用二分的时候，都觉得没什么难度。但是这个题目，巧妙的利用了二分查找的特点，是二分查找的进阶应用，小伙伴们一定要灵活使用它</strong>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 410&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>除数博弈(Leetcode 1025)</title>
    <link href="https://USTCcoder.github.io/2020/07/24/program%20Leetcode1025/"/>
    <id>https://USTCcoder.github.io/2020/07/24/program Leetcode1025/</id>
    <published>2020-07-24T11:24:04.000Z</published>
    <updated>2020-09-02T02:19:01.082Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode1025.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这道题目是一个<strong>博弈问题</strong>，因为很难去遍历所有的情况，我们只能通过上一步对手的情况，选择我们相应的行为，因此博弈问题一般可以通过动态规划求解。</p><a id="more"></a><h1 id="状态转移方程的求解"><a href="#状态转移方程的求解" class="headerlink" title="状态转移方程的求解"></a><font size="5" color="red">状态转移方程的求解</font></h1><p>题目中说每次选取一个满足条件的数，进行减法替换。所以以可以构建一个从0到N的数组，保存到达这N个数字的胜负情况，博弈的题目要求是<strong>两个人都是最优策略</strong>，因此我们可以推断出，<strong>当判断第N个数时，如果从1到N的所有满足条件的数字有一个会导致败局时，那么第N个数就是必胜局，当从1到N的所有满足条件的数字全都是胜局时，那么第N个数就是必败局</strong>。因为爱丽丝先手，0到N中如果有一个数字K会导致必败局，那么爱丽丝就选择K，这样，鲍勃就会走入必败的局面，因此爱丽丝必胜，如果0到N的所有数字都是必胜局，那么爱丽丝无论如何选择，鲍勃都会到达必胜局，因此爱丽丝必败。</p><h1 id="DP"><a href="#DP" class="headerlink" title="DP"></a><font size="5" color="red">DP</font></h1><p>根据状态转移方程，我们可以轻易的求得该题，x从2遍历到N，对于每一个数x，计算出所有符合条件的数，因为要满足整除条件，因此符合条件的数从1遍历到x的平方根即可。所以时间复杂度为$O(n\sqrt{n})$，空间复杂度为$O(n)$。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def divisorGame(self, N: int) -&gt; bool:</span><br><span class="line">        dp = [False] * (N + 1)</span><br><span class="line"></span><br><span class="line">        for i in range(2, N + 1):</span><br><span class="line">            for x in range(1, int(i ** 0.5) + 1):</span><br><span class="line">                if i % x == 0:</span><br><span class="line">                    if not dp[i - x]:</span><br><span class="line">                        dp[i] = True</span><br><span class="line">                        break</span><br><span class="line">        return dp[-1]</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="归纳法"><a href="#归纳法" class="headerlink" title="归纳法"></a><font size="5" color="red">归纳法</font></h1><p>归纳法是最简单的方法，但不是我首推的方法，因为归纳法不能解决大多数问题，所以说并不是这个问题的通解。<br>我们已知1为必败态，因为找不到满足条件的数。<br>我们已知2为必胜态，因为我们可以找到因数1满足条件，而且1为必败态，因此爱丽丝选择1，就会让鲍勃陷入必败态。<br>我们尝试3，因为1是满足条件的唯一解，而且2为必胜态，所以3为必败态。<br>我们尝试4，因为3是必败态，所以爱丽丝会选择1，让鲍勃陷入必败态，爱丽丝是必胜态。<br>我们尝试5，因为1是满足条件的唯一解，而且4为必胜态，所以5为必败态。<br>……我们会发现，1，3，5，…是必败态，2，4，…是必胜态。所以我们猜测奇数态都是必败态，偶数态都是必胜态。<br>我们假设k为偶数，并且小于等于k的数都满足条件，奇数为必败态，偶数为必胜态。那么当爱丽丝处于k+1奇数时，由于奇数的因子必定为奇数，所以爱丽丝只能选择奇数。这样鲍勃会处于一个小于等于k的偶数，由假定可知偶数是必胜态，那么鲍勃必胜，爱丽丝必败，满足条件。<br>我们假设k为奇数，并且小于等于k的数都满足条件，奇数为必败态，偶数为必胜态。那么当爱丽丝处于k+1偶数时，爱丽丝可以选择1，这样鲍勃会处于等于k的奇数，由假定可知，奇数为必败态，那么鲍勃必败，爱丽丝必胜，也满足条件。<br>综上可知，我们的推论是正确的。因此只需要判断该数的奇偶，就可以直接给出结论，所以时间复杂度为$O(1)$，空间复杂度为$O(1)$。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def divisorGame(self, N: int) -&gt; bool:</span><br><span class="line">        return N % 2 == 0</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  博弈问题是非常有趣的编程题，其常规解法是使用动态规划，得到状态转移方程，由必胜和必败的相互状态转移，逐步求得最后结果的状态。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 1025&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>C++面向对象进阶</title>
    <link href="https://USTCcoder.github.io/2020/07/23/C++_oop_3/"/>
    <id>https://USTCcoder.github.io/2020/07/23/C++_oop_3/</id>
    <published>2020-07-23T12:26:24.000Z</published>
    <updated>2020-08-11T14:53:16.980Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++43.png" alt="2"></p><h1 id="C-面向对象进阶"><a href="#C-面向对象进阶" class="headerlink" title="C++面向对象进阶"></a><font size="5" color="red">C++面向对象进阶</font></h1><p>  前面介绍了面向对象的三种访问控制权限。今天带小伙们看一看C++面向对象中的static，const，friend关键字。<br><a id="more"></a></p><h1 id="static关键字"><a href="#static关键字" class="headerlink" title="static关键字"></a><font size="5">static关键字</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Person {</span><br><span class="line"></span><br><span class="line">public: </span><br><span class="line">//static修饰的成员变量称为静态成员变量，其特点是所有对象共享一份数据，也可称为类成员变量，说明这个成员变量不属于某一个对象，而是属于这个类。</span><br><span class="line">//在编译阶段分配内存，并且要在类内声明，在类外定义</span><br><span class="line">static string name;</span><br><span class="line"></span><br><span class="line">static int age;</span><br><span class="line"></span><br><span class="line">int score = 100;</span><br><span class="line"></span><br><span class="line">//static修饰的成员方法称为静态成员方法，其特点是所有对象共享一个函数，也可称为类成员函数，注意类成员函数只能访问类成员变量，不可以访问普通成员变量。</span><br><span class="line">static string getName() {</span><br><span class="line">return name;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">static int getAge() {</span><br><span class="line">return age;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int getScore() {</span><br><span class="line">return score;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">string Person::name = "程序员";</span><br><span class="line">int Person::age = 24;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">//使用静态成员方法时可以通过对象名.静态成员变量名，也可以通过类名::静态成员变量名，因为所有对象都拥有同一份数据，因此为了区分推荐使用类名::静态成员变量名</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; Person::name &lt;&lt; " 我的年龄是：" &lt;&lt; Person::age &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">Person p1;</span><br><span class="line">p1.name = "小学生";</span><br><span class="line">p1.age = 8;</span><br><span class="line">p1.score = 100;</span><br><span class="line"></span><br><span class="line">Person p2;</span><br><span class="line">p2.name = "高中生";</span><br><span class="line">p2.age = 18;</span><br><span class="line">p2.score = 150;</span><br><span class="line"></span><br><span class="line">//因为p1和p2拥有同一个名字和年龄，因此p1的名字和年龄都会变成和p2相同的数据，但是分数不会，因为分数是普通成员变量，每个对象都具有一份独特的数据。</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; p1.name &lt;&lt; " 我的年龄是：" &lt;&lt; p1.age &lt;&lt; " 我的成绩是：" &lt;&lt; p1.score &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; p2.name &lt;&lt; " 我的年龄是：" &lt;&lt; p2.age &lt;&lt; " 我的成绩是：" &lt;&lt; p2.score &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; Person::name &lt;&lt; " 我的年龄是：" &lt;&lt; Person::age &lt;&lt; endl;</span><br><span class="line">//访问成员方法是和成员变量相同，但是要注意静态成员方法不可以访问普通成员变量，在getAge成员方法中，不能够访问score普通成员变量。</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; p1.getName() &lt;&lt; " 我的年龄是：" &lt;&lt; p1.getAge() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; Person::getName() &lt;&lt; " 我的年龄是：" &lt;&lt; Person::getAge() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++44.png" alt="1"></p><h1 id="const关键字"><a href="#const关键字" class="headerlink" title="const关键字"></a><font size="5">const关键字</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Person {</span><br><span class="line"></span><br><span class="line">private:</span><br><span class="line">string name = "xxx";</span><br><span class="line">mutable int arms = 2;</span><br><span class="line"></span><br><span class="line">public:</span><br><span class="line">void setName(string name) {</span><br><span class="line">this-&gt;name = name;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//在成员方法的参数列表后面写上const关键字，称为常函数，此时相当于给this指针前面加入const关键字，因此指针指向的内容也不能发生改变，所以在常函数种不能够修改变量的值。</span><br><span class="line">//如果想要修改，则需要在对应的成员变量前加入mutable关键字。</span><br><span class="line">void setArms(int arms) const{</span><br><span class="line">this-&gt;arms = arms;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">string getName() {</span><br><span class="line">return name;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int getArms() const{</span><br><span class="line">return arms;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">Person p1;</span><br><span class="line">p1.setName("程序员");</span><br><span class="line">p1.setArms(2);</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; p1.getName() &lt;&lt; "我的手臂个数为：" &lt;&lt; p1.getArms() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//在声明对象前加const关键字，说明该对象为常对象，常对象只能调用常函数，也只能修改加入mutable修饰的成员变量。</span><br><span class="line">const Person p2;</span><br><span class="line">p2.setArms(1);</span><br><span class="line">cout &lt;&lt; "我的手臂个数为：" &lt;&lt; p2.getArms() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++45.png" alt="1"></p><h1 id="friend友元"><a href="#friend友元" class="headerlink" title="friend友元"></a><font size="5">friend友元</font></h1><h1 id="全局函数作为友元"><a href="#全局函数作为友元" class="headerlink" title="全局函数作为友元"></a><font size="4">全局函数作为友元</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Person {</span><br><span class="line"></span><br><span class="line">//通过将全局函数的声明写入类内，并在前面加上friend关键字，说明这个全局函数是这个类的好朋友，称作友元，可以访问其私有成员。</span><br><span class="line">friend int getAge(Person&amp; p);</span><br><span class="line">friend void setAge(Person&amp; p, int age);</span><br><span class="line"></span><br><span class="line">public:</span><br><span class="line">string name;</span><br><span class="line"></span><br><span class="line">private:</span><br><span class="line">int age;</span><br><span class="line"></span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">//setAge是全局函数，根据访问控制权限，在类外无法访问到类内的私有成员</span><br><span class="line">void setAge(Person&amp; p, int age) {</span><br><span class="line">p.age = age;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//getAge是全局函数，根据访问控制权限，在类外无法访问到类内的私有成员</span><br><span class="line">int getAge(Person&amp; p) {</span><br><span class="line">return p.age;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">Person p1;</span><br><span class="line">p1.name = "程序员";</span><br><span class="line">setAge(p1, 24);</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; p1.name &lt;&lt; " 我的年龄是：" &lt;&lt; getAge(p1) &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++46.png" alt="1"></p><h1 id="类作为友元"><a href="#类作为友元" class="headerlink" title="类作为友元"></a><font size="4">类作为友元</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Person {</span><br><span class="line"></span><br><span class="line">//将Friend类的声明写入类内，并在前面加上friend关键字，说明Friend是这个类的好朋友，称作友元，可以访问其私有成员。</span><br><span class="line">friend class Friend;</span><br><span class="line"></span><br><span class="line">public:</span><br><span class="line">string name;</span><br><span class="line"></span><br><span class="line">private:</span><br><span class="line">int age;</span><br><span class="line"></span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">//根据访问控制权限，Friend类无法访问Person类的私有成员</span><br><span class="line">class Friend {</span><br><span class="line"></span><br><span class="line">public:</span><br><span class="line">static void setAge(Person&amp; p, int age) {</span><br><span class="line">p.age = age;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">static int getAge(Person&amp; p) {</span><br><span class="line">return p.age;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">Person p1;</span><br><span class="line">p1.name = "程序员";</span><br><span class="line">Friend::setAge(p1, 24);</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; p1.name &lt;&lt; " 我的年龄是：" &lt;&lt; Friend::getAge(p1) &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++46.png" alt="1"></p><h1 id="成员函数作为友元"><a href="#成员函数作为友元" class="headerlink" title="成员函数作为友元"></a><font size="4">成员函数作为友元</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//要将Person的声明写在Friend的定义前面，要告诉编译器后面有Person类的定义，这样参数列表中才可以写入Person，否则编译器也会报错。</span><br><span class="line">class Person;</span><br><span class="line"></span><br><span class="line">//注意写法，要将Friend定义在前面，如果将Person写在前面，则编译器不知道Friend::setAge函数是什么。</span><br><span class="line">class Friend {</span><br><span class="line"></span><br><span class="line">public:</span><br><span class="line">static void setAge(Person&amp; p, int age);</span><br><span class="line">static int getAge(Person&amp; p);</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Person {</span><br><span class="line"></span><br><span class="line">//通过将成员函数的声明写入类内，并在前面加上friend关键字，说明这个成员函数是这个类的好朋友，称作友元，可以访问其私有成员。</span><br><span class="line">//要注意成员函数的前面要写清作用域，否则会当作全局函数作为友元。</span><br><span class="line">friend void Friend::setAge(Person&amp; p, int age);</span><br><span class="line">friend int Friend::getAge(Person&amp; p);</span><br><span class="line"></span><br><span class="line">public:</span><br><span class="line">string name;</span><br><span class="line"></span><br><span class="line">private:</span><br><span class="line">int age;</span><br><span class="line"></span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">//要注意Friend成员函数要在类外实现，且要放在Person类的定义之后，否则编译器只知道Person类的存在，却不知道Person类中有哪些成员，也会报错。</span><br><span class="line">void Friend::setAge(Person&amp; p, int age) {</span><br><span class="line">p.age = age;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int Friend::getAge(Person&amp; p) {</span><br><span class="line">return p.age;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">Person p1;</span><br><span class="line">p1.name = "程序员";</span><br><span class="line">Friend::setAge(p1, 24);</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; p1.name &lt;&lt; " 我的年龄是：" &lt;&lt; Friend::getAge(p1) &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++46.png" alt="1"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  今天的内容也非常重要，static关键字，要记得它不属于某一个对象，而是属于整个类，因此在以后的访问中尽量使用类名的方式进行访问，友元的三种使用方法大家要记得，尤其是第三种非常复杂，牵扯到函数和类的声明等问题，小伙伴们一定要理解它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++面向对象进阶&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++面向对象提高</title>
    <link href="https://USTCcoder.github.io/2020/07/21/C++_oop_2/"/>
    <id>https://USTCcoder.github.io/2020/07/21/C++_oop_2/</id>
    <published>2020-07-21T15:08:19.000Z</published>
    <updated>2020-08-11T14:53:13.109Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++41.png" alt="2"></p><h1 id="C-面向对象提高"><a href="#C-面向对象提高" class="headerlink" title="C++面向对象提高"></a><font size="5" color="red">C++面向对象提高</font></h1><p>  之前介绍了C++面向对象的基础内容，其中引入了public关键字，这时C++面向对象的一种访问控制权限，这非常重要，可以更灵活的使用我们创建的类和对象，保护类中的内容，不被其他人调用。今天带小伙们看一看C++中的三种访问控制权限，与Java稍有不同，Java中包括四类访问控制权限，以后再给小伙伴介绍。<br><a id="more"></a></p><h1 id="访问控制权限"><a href="#访问控制权限" class="headerlink" title="访问控制权限"></a><font size="5">访问控制权限</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">class Father {</span><br><span class="line"></span><br><span class="line">//成员关键字</span><br><span class="line">//public关键字：类内可以访问，子类可以访问，类外可以访问</span><br><span class="line">public:</span><br><span class="line">string name;</span><br><span class="line"></span><br><span class="line">Father() {</span><br><span class="line">cout &lt;&lt; "我是父类的构造函数" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">~Father() {</span><br><span class="line">cout &lt;&lt; "我是父类的析构函数" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">void setName(string name) {</span><br><span class="line">this-&gt;name = name;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">string getName() {</span><br><span class="line">return this-&gt;name;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">void setAge(int age) {</span><br><span class="line">this-&gt;age = age;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int getAge() {</span><br><span class="line">return this-&gt;age;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">void setMoney(int money) {</span><br><span class="line">this-&gt;money = money;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int getMoney() {</span><br><span class="line">return this-&gt;money;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//public关键字：类内可以访问，子类可以访问，类外不可以访问</span><br><span class="line">protected:</span><br><span class="line">int age;</span><br><span class="line"></span><br><span class="line">//public关键字：类内可以访问，子类不可以访问，类外不可以访问</span><br><span class="line">private:</span><br><span class="line">int money;</span><br><span class="line"></span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">//继承：面向对象的第二大特点，可以获取父类的部分成员，实现代码的复用。</span><br><span class="line">//语法是class 子类名:继承方式 父类名</span><br><span class="line">//继承关键字</span><br><span class="line">//public关键字：继承父类的public,protected成员变量和成员函数，并且保持不变</span><br><span class="line">//protected关键字：继承父类的public,protected成员变量和成员函数，并且都将变为protected类型</span><br><span class="line">//public关键字：继承父类的public,protected成员变量和成员函数，并且都将变为private类型</span><br><span class="line">class Son :public Father {</span><br><span class="line"></span><br><span class="line">public:</span><br><span class="line">//在子类中，可以定义父类没有的成员，在父类中无法获取子类成员，但是子类可以获取父类成员。</span><br><span class="line">int score;</span><br><span class="line"></span><br><span class="line">Son() {</span><br><span class="line">cout &lt;&lt; "我是子类的构造函数" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">~Son() {</span><br><span class="line">cout &lt;&lt; "我是子类的析构函数" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//因为name在父类是public的，因此在子类中可以访问</span><br><span class="line">void printName() {</span><br><span class="line">cout &lt;&lt; this-&gt;name;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//因为age在父类是protected的，因此在子类中可以访问</span><br><span class="line">//但是money成员变量，在父类中是private的，在子类中无法访问</span><br><span class="line">void printAge() {</span><br><span class="line">cout &lt;&lt; this-&gt;age;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">{</span><br><span class="line">//创建父类的对象，调用父类的构造函数</span><br><span class="line">Father f;</span><br><span class="line">//因为name是public的，所以可以直接进行访问和修改</span><br><span class="line">f.name = "father";</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; f.name &lt;&lt; endl;</span><br><span class="line">//因为age是protected的，所以不可以直接进行访问和修改</span><br><span class="line">//引入set和get方法，可以实现对protected或这private成员进行类内修改。</span><br><span class="line">f.setAge(40);</span><br><span class="line">cout &lt;&lt; "我的年龄是：" &lt;&lt; f.getAge() &lt;&lt; endl;</span><br><span class="line">//因为age是private的，所以不可以直接进行访问和修改</span><br><span class="line">f.setMoney(1000000);</span><br><span class="line">cout &lt;&lt; "我的存款是：" &lt;&lt; f.getMoney() &lt;&lt; endl;</span><br><span class="line">//当花括号结束时，创建的父类对象会被析构，调用析构方法</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "********************" &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">{</span><br><span class="line">//继承的特点：</span><br><span class="line">//1.父类中的所有非静态成员都会被子类继承，只是私有的会被编译器隐藏，在子类中不可见。</span><br><span class="line">//2.具有继承关系的构造函数，首先调用父类的构造函数，初始化属性，然后再调用子类的构造函数，析构时顺序相反。</span><br><span class="line">//3.子类与父类具有相同的成员时，默认访问子类的成员，可以通过作用域访问父类的成员，父类名::父类成员即可。</span><br><span class="line">//4.如果在子类中访问父类的静态成员，需要通过子类名::父类名::父类静态成员名。</span><br><span class="line">//创建子类对象，因此即会打印父类的构造函数，也会打印子类的构造函数。</span><br><span class="line">Son s;</span><br><span class="line">//name是父类的public属性，而且通过public继承，因此可以访问</span><br><span class="line">s.name = "son";</span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; s.name &lt;&lt; endl;</span><br><span class="line">//setAge是父类的public属性，而且通过public继承，因此可以访问</span><br><span class="line">s.setAge(20);</span><br><span class="line">cout &lt;&lt; "我的年龄是：" &lt;&lt; s.getAge() &lt;&lt; endl;</span><br><span class="line">//setMoney是父类的public属性，而且通过public继承，因此可以访问</span><br><span class="line">s.setMoney(100);</span><br><span class="line">cout &lt;&lt; "我的存款是：" &lt;&lt; s.getMoney() &lt;&lt; endl;</span><br><span class="line">s.score = 90;</span><br><span class="line">cout &lt;&lt; "我的成绩是：" &lt;&lt; s.score &lt;&lt; endl;</span><br><span class="line">//当花括号结束时，创建的子类对象会被析构，注意子类对象析构时的顺序</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++42.png" alt="1"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  今天给大家介绍三种C++的访问控制权限以及继承的基本概念，这对于C++的学习非常重要，因此小伙伴们务必掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++面向对象提高&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++面向对象基础</title>
    <link href="https://USTCcoder.github.io/2020/07/19/C++_oop_1/"/>
    <id>https://USTCcoder.github.io/2020/07/19/C++_oop_1/</id>
    <published>2020-07-19T15:55:46.000Z</published>
    <updated>2020-09-01T08:43:37.424Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++37.png" alt="2"></p><h1 id="C-面向对象基础"><a href="#C-面向对象基础" class="headerlink" title="C++面向对象基础"></a><font size="5" color="red">C++面向对象基础</font></h1><p>  面向对象的编程思想(Object Oriented Programming, OOP)是程序设计发展的必然阶段，在70年代初，人们使用面向过程的编程思想解决问题，但是随着时代的进步，人们发现这种编程思想非常繁琐，尤其是定义多个相同或相似的变量，需要进行非常冗余的代码编写。这就引入了OOP的观念，面向对象的思想是C++语言的核心内容，因此我们分成多个篇章进行叙述，今天主要给大家介绍类的创建和使用，以及封装，构造析构函数等内容。<br><a id="more"></a></p><h1 id="类的定义及使用"><a href="#类的定义及使用" class="headerlink" title="类的定义及使用"></a><font size="5">类的定义及使用</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//创建类时，需要使用class关键字加类名，然后在花括号中定义成员变量和成员方法。</span><br><span class="line">//是不是和结构体非常类似，最简单的类和结构体非常类似，但是类具有结构体所没有的特殊性质，在后面会一一为大家介绍。</span><br><span class="line">//这种将成员变量和成员方法写在一个类中，创建对象时，所有对象都具有这些成员变量，也可以使用成员方法，这种思想称为封装。</span><br><span class="line">class Person {</span><br><span class="line"></span><br><span class="line">public:</span><br><span class="line">string name;</span><br><span class="line">int age;</span><br><span class="line"></span><br><span class="line">void eat() {</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "I am hungry!" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">void sleep() {</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "I am sleepy!" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">//对象的创建，可以通过类名 变量名(参数列表)进行创建，但是如果没有参数则不能加括号，否则会和函数的声明发生冲突。</span><br><span class="line">Person p1;</span><br><span class="line"></span><br><span class="line">//对象的使用，如果想访问对象的成员变量或者成员方法和结构体相同，使用小数点进行访问。</span><br><span class="line">p1.name = "睡神";</span><br><span class="line">p1.age = 24;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; p1.name &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "我的年龄是：" &lt;&lt; p1.age &lt;&lt; endl;</span><br><span class="line">p1.eat();</span><br><span class="line">p1.sleep();</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++38.png" alt="1"></p><h1 id="构造函数和析构函数"><a href="#构造函数和析构函数" class="headerlink" title="构造函数和析构函数"></a><font size="5">构造函数和析构函数</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">class Person {</span><br><span class="line">public:</span><br><span class="line">string name="xxx";</span><br><span class="line">int age = 0;</span><br><span class="line"></span><br><span class="line">//构造函数是和类名同名的一种特殊的成员函数，当创建对象时会自动调用类的构造函数。</span><br><span class="line">//如果没有自定义构造函数，编译器会自动帮你创建一个无参的构造函数，里面是空实现。</span><br><span class="line">Person(string name, int age) {</span><br><span class="line">this-&gt;name = name;</span><br><span class="line">this-&gt;age = age;</span><br><span class="line">cout &lt;&lt; "我是" &lt;&lt; this-&gt;name &lt;&lt; "我有两个参数的构造函数，我被执行了~" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//构造函数的出现帮我们省去了每个成员变量重新赋值的操作，而且构造函数可以发生函数重载。</span><br><span class="line">//注意构造函数的写法，没有返回值。</span><br><span class="line">Person(string name) {</span><br><span class="line">this-&gt;name = name;</span><br><span class="line">cout &lt;&lt; "我是" &lt;&lt; this-&gt;name &lt;&lt; "我有一个参数的构造函数，我被执行了~" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">Person() {</span><br><span class="line">cout &lt;&lt; "我是" &lt;&lt; this-&gt;name &lt;&lt; "无参的构造函数，我被执行了~" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//析构函数和构造函数相反，是在类名前加一个波浪号，没有返回值，析构函数是在对象销毁时调用，因此没有参数，也不可以发生重载。</span><br><span class="line">~Person() {</span><br><span class="line">cout &lt;&lt; "我是" &lt;&lt; this-&gt;name &lt;&lt;"的析构函数，我被执行了~" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">//在一个花括号种写代码，则花括号为代码的作用域，当执行结束后，会销毁其中的变量，所以在执行结束后会自动调用对象的析构函数。</span><br><span class="line">{</span><br><span class="line">Person p1;</span><br><span class="line">Person p2("婴儿");</span><br><span class="line">Person p3("程序员", 24);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; p1.name &lt;&lt; "的年龄是：" &lt;&lt; p1.age &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; p2.name &lt;&lt; "的年龄是：" &lt;&lt; p2.age &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; p3.name &lt;&lt; "的年龄是：" &lt;&lt; p3.age &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++39.png" alt="2"></p><h1 id="拷贝构造函数"><a href="#拷贝构造函数" class="headerlink" title="拷贝构造函数"></a><font size="5">拷贝构造函数</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">class Person {</span><br><span class="line">public:</span><br><span class="line">string name="xxx";</span><br><span class="line">int age = 0;</span><br><span class="line"></span><br><span class="line">//拷贝构造函数的参数是一个同类型的对象，可以根据传入的对象创建一个相同的对象，为了防止对传入对象的误操作，可以在前面加const关键字修饰。</span><br><span class="line">//默认情况下编译器会提供一个默认无参构造函数，一个默认拷贝构造函数和一个默认析构函数。</span><br><span class="line">//如果程序员自定义了一个构造函数，则编译器不会提供默认的无参构造函数，但是会提供拷贝构造函数和析构函数。</span><br><span class="line">//如果程序员自定义了一个拷贝构造函数，则编译器不会再提供任何构造函数，只会提供析构函数。</span><br><span class="line">Person(const Person&amp; p) {</span><br><span class="line">this-&gt;name = p.name;</span><br><span class="line">this-&gt;age = p.age;</span><br><span class="line">cout &lt;&lt; "我是" &lt;&lt; this-&gt;name &lt;&lt; "的拷贝构造函数，我被执行了~" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">Person(string name, int age) {</span><br><span class="line">this-&gt;name = name;</span><br><span class="line">this-&gt;age = age;</span><br><span class="line">cout &lt;&lt; "我是" &lt;&lt; this-&gt;name &lt;&lt; "我有两个参数的构造函数，我被执行了~" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">~Person() {</span><br><span class="line">cout &lt;&lt; "我是" &lt;&lt; this-&gt;name &lt;&lt;"的析构函数，我被执行了~" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">{</span><br><span class="line">Person p1("程序员", 24);</span><br><span class="line">//使用p1对象拷贝构造一个p2对象，具有和p1对象相同的成员变量，但是两个对象，具有不同的地址值。</span><br><span class="line">Person p2(p1);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; p1.name &lt;&lt; "的年龄是：" &lt;&lt; p1.age &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; p2.name &lt;&lt; "的年龄是：" &lt;&lt; p2.age &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "p1的地址是：" &lt;&lt; &amp;p1 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "p2的地址是：" &lt;&lt; &amp;p2 &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++40.png" alt="2"></p><h1 id="深拷贝和浅拷贝"><a href="#深拷贝和浅拷贝" class="headerlink" title="深拷贝和浅拷贝"></a><font size="5">深拷贝和浅拷贝</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">class Person {</span><br><span class="line">public:</span><br><span class="line">string name = "xxx";</span><br><span class="line">int age = 0;</span><br><span class="line">int* p = NULL;</span><br><span class="line"></span><br><span class="line">//在构造函数中申请了一段内存，并且在析构函数中完成释放。</span><br><span class="line">//此时如果使用默认的拷贝构造函数，则新创建的对象是浅拷贝第一个对象，此时两个对象创建的内存是同一块内存。</span><br><span class="line">//这时在释放内存时，第一次释放成功，第二次释放就会失败，并且产生异常。</span><br><span class="line">//解决方法是重写拷贝构造函数，并且在构造函数中重新申请内存。</span><br><span class="line">Person(const Person&amp; p) {</span><br><span class="line">this-&gt;name = p.name;</span><br><span class="line">this-&gt;age = p.age;</span><br><span class="line">this-&gt;p = new int(10);</span><br><span class="line">cout &lt;&lt; "我是" &lt;&lt; this-&gt;name &lt;&lt; "的拷贝构造函数，我被执行了~" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//深拷贝：指当对象或者变量复制时，不是简单的赋值操作，而是内存的重新分配，即两个对象或者变量中的地址是不重复的。修改了原对象，拷贝构造对象不会发生改变。</span><br><span class="line">//浅拷贝：指当对象或者变量复制时，通过简单的赋值号，完成拷贝，两个对象或者变量可能指向同一块内存地址。修改了原对象，拷贝构造对象也会发生改变。</span><br><span class="line">Person(string name, int age) {</span><br><span class="line">this-&gt;name = name;</span><br><span class="line">this-&gt;age = age;</span><br><span class="line">this-&gt;p = new int(10);</span><br><span class="line">cout &lt;&lt; "我是" &lt;&lt; this-&gt;name &lt;&lt; "我有两个参数的构造函数，我被执行了~" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">~Person() {</span><br><span class="line">delete this-&gt;p;</span><br><span class="line">cout &lt;&lt; "我是" &lt;&lt; this-&gt;name &lt;&lt; "的析构函数，我被执行了~" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">{</span><br><span class="line">Person p1("程序员", 24);</span><br><span class="line">Person p2(p1);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; p1.name &lt;&lt; "的年龄是：" &lt;&lt; p1.age &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; p2.name &lt;&lt; "的年龄是：" &lt;&lt; p2.age &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "p1的地址是：" &lt;&lt; &amp;p1 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "p2的地址是：" &lt;&lt; &amp;p2 &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++40.png" alt="2"></p><h1 id="this指针"><a href="#this指针" class="headerlink" title="this指针"></a><font size="5">this指针</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">class Person {</span><br><span class="line">public:</span><br><span class="line">string name = "xxx";</span><br><span class="line">int age = 0;</span><br><span class="line"></span><br><span class="line">//this指针是隐含在每一个非静态成员函数中的一个指针，使用时无需定义，指代当前对象，其本质是一个指针常量，指针的指向是该对象，不可以修改，但是指针指向的值可以修改。</span><br><span class="line">//this指针的作用是指代该对象的成员变量或者成员函数，在构造函数中常有体现，当进行赋值时，由于作用域的原因，参数列表中的参数如果和成员变量同名，则会隐藏成员函数，如果想访问可以通过this指针。</span><br><span class="line">//当要返回当前对象时，可以使用return *this，返回值仍是一个对象，因此还可以继续调用成员变量或者成员方法，体现出链式编程的思想。</span><br><span class="line">Person(const Person&amp; p) {</span><br><span class="line">this-&gt;name = p.name;</span><br><span class="line">this-&gt;age = p.age;</span><br><span class="line">cout &lt;&lt; "我是" &lt;&lt; this-&gt;name &lt;&lt; "的拷贝构造函数，我被执行了~" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">Person(string name, int age) {</span><br><span class="line">this-&gt;name = name;</span><br><span class="line">this-&gt;age = age;</span><br><span class="line">cout &lt;&lt; "我是" &lt;&lt; this-&gt;name &lt;&lt; "我有两个参数的构造函数，我被执行了~" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">~Person() {</span><br><span class="line">cout &lt;&lt; "我是" &lt;&lt; this-&gt;name &lt;&lt; "的析构函数，我被执行了~" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">{</span><br><span class="line">Person p1("程序员", 24);</span><br><span class="line">Person p2(p1);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; p1.name &lt;&lt; "的年龄是：" &lt;&lt; p1.age &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; p2.name &lt;&lt; "的年龄是：" &lt;&lt; p2.age &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "p1的地址是：" &lt;&lt; &amp;p1 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "p2的地址是：" &lt;&lt; &amp;p2 &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++40.png" alt="2"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  在这里给小伙伴们介绍了C++的基本面向对象的概念，虽然不是很难，但是非常重要，在这里为了方便起见，使用了public关键字，下一节会为大家详细介绍几种访问权限关键字。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++面向对象基础&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++引用</title>
    <link href="https://USTCcoder.github.io/2020/07/17/C++_quote/"/>
    <id>https://USTCcoder.github.io/2020/07/17/C++_quote/</id>
    <published>2020-07-17T15:39:36.000Z</published>
    <updated>2020-08-11T14:53:37.603Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++33.png" alt="2"></p><h1 id="C-引用"><a href="#C-引用" class="headerlink" title="C++引用"></a><font size="5" color="red">C++引用</font></h1><p>  从今天开始，正式进入C++的内容中，引用时C++的代码风格，为了使代码简单清晰，C++引入了引用这个概念。为什么要引入呢？因为人们发现用指针指来指去，非常复杂，而且容易出错，所以提出了一种更加简单的形式。引用可以看成是起别名，就像小名一样，小伙伴们应该都有小名，在家里一般不会直呼其名，常常会给小孩子起一个小名。那么小名就是你，大名也是你，因此对小名进行操作时，相当于对这个变量进行了操作。<br><a id="more"></a></p><h1 id="引用的定义"><a href="#引用的定义" class="headerlink" title="引用的定义"></a><font size="5">引用的定义</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">//引用的定义就是在变量名前加一个取地址符&amp;，并且让其等于另一个变量，相当于给一个变量又起了一个小名</span><br><span class="line">//注意只能让一个变量名传给引用的数据类型，不可以是常量。而且引用只能在创建时进行赋值，不可以先创建然后再赋值。</span><br><span class="line">int a = 10;</span><br><span class="line">int&amp; b = a;</span><br><span class="line">int&amp; c = b;</span><br><span class="line"></span><br><span class="line">//我们可以看出，它们a，b，c变量具有相同的地址值，而且数据变化时，它们三个具有相同的变化。</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;c = " &lt;&lt; &amp;c &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">b = 20;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;c = " &lt;&lt; &amp;c &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">c += 10;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;c = " &lt;&lt; &amp;c &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++34.png" alt="1"></p><h1 id="引用本质的探寻"><a href="#引用本质的探寻" class="headerlink" title="引用本质的探寻"></a><font size="5">引用本质的探寻</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">struct Student</span><br><span class="line">{</span><br><span class="line">int a = 10;</span><br><span class="line">int&amp; b = a;</span><br><span class="line">int c = 20;</span><br><span class="line"></span><br><span class="line">//定义两个变量a和c，定义a的引用b，由先前的知识可以知道a和b具有相同的地址，c的地址和a不同。</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;c = " &lt;&lt; &amp;c &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//让c的值赋值给b会出现什么神奇现象呢？是b和c具有相同的地址？还是a和b具有相同地址？还是a，b，c都具有相同的地址呢？</span><br><span class="line">b = c;</span><br><span class="line"></span><br><span class="line">//原来a和b仍然具有相同的地址，a和c的地址不同。这是什么原因呢？</span><br><span class="line">//我们在指针部分讲解了常量指针和指针常量。这里b的值可以改变，但是b和a绑定了，b只能指向a，不能改变其指向。这种我们称之为指针常量。</span><br><span class="line">//引用本质就是指针常量，只不过我们隐藏了指针的定义和写法，更加简单。</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;c = " &lt;&lt; &amp;c &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++35.png" alt="2"></p><h1 id="引用作为函数参数"><a href="#引用作为函数参数" class="headerlink" title="引用作为函数参数"></a><font size="5">引用作为函数参数</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">void swap(int&amp; a, int&amp; b) {</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">int temp = a;</span><br><span class="line">a = b;</span><br><span class="line">b = temp;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a = 10;</span><br><span class="line">int b = 20;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//既然引用的本质是一个指针，那么必然满足地址传递的性质，所以在函数内部操作会影响到外部变量的值。</span><br><span class="line">//给小伙伴讲解一下调用函数背后的操作，首先将a和b的值传入函数列表，然后在函数参数中执行int&amp; a = a，int&amp; b = b</span><br><span class="line">//后面的a和b是实参，前面的a和b是形参，此时形参a就是实参a的小名，那么对小名a操作就等价于对大名a操作，所以外部的值会发生改变。</span><br><span class="line">swap(a, b);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++36.png" alt="4"></p><h1 id="new和delete"><a href="#new和delete" class="headerlink" title="new和delete"></a><font size="5">new和delete</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a1 = 10;</span><br><span class="line">int a2 = 20;</span><br><span class="line"></span><br><span class="line">//栈区：由编译器自动分配释放，存放函数的参数，局部变量等。</span><br><span class="line">//堆区：由程序员分配和释放，最后由操作系统进行回收。</span><br><span class="line">//在函数中的局部变量，不要返回其地址，因为函数调用结束后，会由编译器自动释放，会导致异常。</span><br><span class="line">//在堆区的数据由程序员分配和释放，分配的方式是使用new关键字，并返回开辟的地址，释放的方式是使用delete关键字。</span><br><span class="line">//如果开辟或释放数组则要使用中括号进行说明，new 数据类型[], delete[]。</span><br><span class="line">int* b1 = new int(10);</span><br><span class="line">int* b2 = new int(20);</span><br><span class="line">int* c1 = &amp;a1;</span><br><span class="line">int* c2 = &amp;a2;</span><br><span class="line"></span><br><span class="line">//可以看出b1，b2的地址非常接近，在一个区内，c1和c2的地址也非常接近，也在同一个区内。但是b1和c1的地址却不在同一个区，这就是堆区和栈区的区别。</span><br><span class="line">cout &lt;&lt; "b1 = " &lt;&lt; b1 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b2 = " &lt;&lt; b2 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "c1 = " &lt;&lt; c1 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "c2 = " &lt;&lt; c2 &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">delete b1;</span><br><span class="line">delete b2;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  引用是C++中的重要工具，在C++中推荐使用引用代替指针来传递参数，因为操作简单，使用方便，所以小伙伴们务必记住它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++引用&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++结构体</title>
    <link href="https://USTCcoder.github.io/2020/07/15/C++_struct/"/>
    <id>https://USTCcoder.github.io/2020/07/15/C++_struct/</id>
    <published>2020-07-15T15:04:37.000Z</published>
    <updated>2020-08-11T14:53:43.013Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++28.png" alt="2"></p><h1 id="C-结构体"><a href="#C-结构体" class="headerlink" title="C++结构体"></a><font size="5" color="red">C++结构体</font></h1><p>  结构体进一步体现了面向对象的思想，但是和真正的面向对象有很大的差别，没有继承和多态的思想，但是已经可以定义成员函数和成员变量了，而且创建结构体变量时，可以实现将成员变量和成员函数封装在一个变量之中，实现多个结构体变量具有相同的结构，这一点类似于面向对象中的类。<br><a id="more"></a></p><h1 id="结构体定义"><a href="#结构体定义" class="headerlink" title="结构体定义"></a><font size="5">结构体定义</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//结构体的定义包括成员变量和成员函数，关键的是成员变量可以是不同的类型，这为我们操作不同类型的数据提供了便利。</span><br><span class="line">//每一个结构体变量都具有相同的结构，但它们之间互不影响。</span><br><span class="line">struct Student</span><br><span class="line">{</span><br><span class="line">string name = "xxx";</span><br><span class="line">int age = 0;</span><br><span class="line"></span><br><span class="line">string getName() {</span><br><span class="line"></span><br><span class="line">return name;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int getAge() {</span><br><span class="line"></span><br><span class="line">return age;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">//结构体变量的创建和基本类型相似，第一种方式是先定义，后初始化。</span><br><span class="line">//但是和变量不同的是，如果成员变量没有默认值，编译器会自动添加默认值，不会报错，而基本类型没有默认值会报错。</span><br><span class="line">Student s;</span><br><span class="line">cout &lt;&lt; "s的姓名是：" &lt;&lt; s.getName() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "s的年龄是：" &lt;&lt; s.getAge() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//结构体变量的第二种创建方式，定义的同时进行初始化，还有第三种定义方式，在结构体的花括号后面写上变量名，但是不推荐，因为会让其他人很难发现定义的具体位置，掌握前两种定义方法即可，第三种作为了解。</span><br><span class="line">Student s1 = { "菜鸟", 24 };</span><br><span class="line">cout &lt;&lt; "s1的姓名是：" &lt;&lt; s1.getName() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "s1的年龄是：" &lt;&lt; s1.getAge() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "···十年之后···" &lt;&lt; endl;</span><br><span class="line">s1.name = "大神";</span><br><span class="line">s1.age = 34;</span><br><span class="line">cout &lt;&lt; "s1的姓名是：" &lt;&lt; s1.getName() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "s1的年龄是：" &lt;&lt; s1.getAge() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++29.png" alt="1"></p><h1 id="结构体数组"><a href="#结构体数组" class="headerlink" title="结构体数组"></a><font size="5">结构体数组</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">struct Student</span><br><span class="line">{</span><br><span class="line">string name = "xxx";</span><br><span class="line">int age = 0;</span><br><span class="line"></span><br><span class="line">string getName() {</span><br><span class="line"></span><br><span class="line">return name;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int getAge() {</span><br><span class="line"></span><br><span class="line">return age;</span><br><span class="line">}</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">//结构体数组和普通变量的数组有相同的定义方法，访问是也是通过索引进行访问。</span><br><span class="line">Student s[] = { {"C++程序员", 30}, {"Java程序员", 25}, {"Python程序员", 20} };</span><br><span class="line"></span><br><span class="line">for (int i = 0; i &lt; 3; i++) {</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "我的名字是：" &lt;&lt; s[i].getName() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "我的年龄是：" &lt;&lt; s[i].getAge() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++30.png" alt="2"></p><h1 id="指针作为结构体成员变量"><a href="#指针作为结构体成员变量" class="headerlink" title="指针作为结构体成员变量"></a><font size="5">指针作为结构体成员变量</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//在定义结构体时，将指针作为成员变量，并且指向另一个结构体变量</span><br><span class="line">struct LinkedList</span><br><span class="line">{</span><br><span class="line">int val;</span><br><span class="line">LinkedList* next;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">//创建链表数据类型，如果指针作为结构体的成员，那么(*s).xxx可以写为s-&gt;xxx。</span><br><span class="line">LinkedList l3 = { 3, NULL };</span><br><span class="line">LinkedList l2 = { 2, &amp;l3 };</span><br><span class="line">LinkedList l1 = { 1, &amp;l2 };</span><br><span class="line"></span><br><span class="line">LinkedList* head = &amp;l1;</span><br><span class="line"></span><br><span class="line">while (head) {</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; head-&gt;val &lt;&lt; " -&gt; ";</span><br><span class="line">head = head-&gt;next;</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; "NULL" &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++31.png" alt="4"></p><h1 id="结构体的嵌套定义"><a href="#结构体的嵌套定义" class="headerlink" title="结构体的嵌套定义"></a><font size="5">结构体的嵌套定义</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//在一个结构体中定义另一个结构体，可以实现结构体的嵌套定义，又可以保护内部结构体类型在外部不可以访问。</span><br><span class="line">struct Student</span><br><span class="line">{</span><br><span class="line">struct Score</span><br><span class="line">{</span><br><span class="line">int chinese;</span><br><span class="line">int math;</span><br><span class="line">int english;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">string name;</span><br><span class="line">int age;</span><br><span class="line">Score score;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">Student s = { "学霸", 24, {95, 100, 95} };</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; s.name &lt;&lt; "的年龄是" &lt;&lt; s.age &lt;&lt; "岁" &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "他的语文成绩是：" &lt;&lt; s.score.chinese &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "他的数学成绩是：" &lt;&lt; s.score.math &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "他的英语成绩是：" &lt;&lt; s.score.english &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++32.png" alt="4"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  结构体是C语言中功能强大的工具之一，在没有面向对象的思想中，结构体起到了非常重要的作用，可以实现复杂数据类型的定义，而且可以对多个变量，函数进行封装，也是链表，二叉树等数据结构的重要组成部分，小伙伴们一定要掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++结构体&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++指针</title>
    <link href="https://USTCcoder.github.io/2020/07/12/C++_algorithm/"/>
    <id>https://USTCcoder.github.io/2020/07/12/C++_algorithm/</id>
    <published>2020-07-12T12:46:33.000Z</published>
    <updated>2020-08-11T14:53:33.219Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++22.png" alt="2"></p><h1 id="C-指针"><a href="#C-指针" class="headerlink" title="C++指针"></a><font size="5" color="red">C++指针</font></h1><p>  指针是C/C++最精髓的内容，也是大多数程序员幼崽的童年阴影，噩梦的开始。当然也包括本菜鸟也深受其害，在这里我们不说的很详尽，因为我也是个freshman，这个博客的内容是普及一下指针的基本用法，至于复杂的情况，可以参考上面的三本宝典，都是C/C++程序员走上不归路的必读书籍。<br><a id="more"></a></p><h1 id="指针定义"><a href="#指针定义" class="headerlink" title="指针定义"></a><font size="5">指针定义</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">//指针和普通变量的定义一样，可以定义时初始化，也可以在使用时初始化。指针的意思是指向某一块内存空间，因此其代表的一个地址，前面加上*代表取出地址指向的值。</span><br><span class="line">int a = 10;</span><br><span class="line">int* p = &amp;a;</span><br><span class="line">int* p1;</span><br><span class="line">p1 = &amp;a;</span><br><span class="line"></span><br><span class="line">//可以看出p等价于&amp;a，在数组中已经介绍了取地址符&amp;，因此p指代的是a的地址，*p则指代地址中存放的值。</span><br><span class="line">cout &lt;&lt; "p = " &lt;&lt; p &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*p = " &lt;&lt; *p &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">int b = a;</span><br><span class="line">b += 2;</span><br><span class="line"></span><br><span class="line">//int b = a，说明创建一个变量，让其值等于a，但是b和a是两个不同的变量，因此b的变化和a无关，所以b += 2时，a仍保持不变。</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">*p += 2;</span><br><span class="line"></span><br><span class="line">//*p += 2，说明让p指向的内存空间中的值加2，但是p指向的内存空间就是a，相当于a的门牌号，因此*p += 2就等价于a += 2，所以a的值发生了变化。</span><br><span class="line">cout &lt;&lt; "p = " &lt;&lt; p &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*p = " &lt;&lt; *p &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++23.png" alt="1"></p><h1 id="const与指针"><a href="#const与指针" class="headerlink" title="const与指针"></a><font size="5">const与指针</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a = 10;</span><br><span class="line">int b = 20;</span><br><span class="line"></span><br><span class="line">//const修饰指针，称为常量指针，const修饰*p，说明指针指向的值是不可以改变的，但是可以改变指针的指向。</span><br><span class="line">const int* p1 = &amp;a;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "p1 = " &lt;&lt; p1 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*p1 = " &lt;&lt; *p1 &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">p1 = &amp;b;</span><br><span class="line"></span><br><span class="line">//p1指向了b，因此a的值没有变，p1为b的地址，p1指向的值为b的值</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "p1 = " &lt;&lt; p1 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*p1 = " &lt;&lt; *p1 &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">int c = 30;</span><br><span class="line">int d = 40;</span><br><span class="line"></span><br><span class="line">//const修饰常量，称为指针常量，const修饰p，说明指针的指向是不可以改变的，但是可以改变指针指向的值。</span><br><span class="line">int* const p2 = &amp;c;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d = " &lt;&lt; d &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;c = " &lt;&lt; &amp;c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;d = " &lt;&lt; &amp;d &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "p2 = " &lt;&lt; p2 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*p2 = " &lt;&lt; *p2 &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">*p2 = d;</span><br><span class="line"></span><br><span class="line">//p2为c的地址，现在将地址指向的值改为d，因此c变为d的值，p2的地址没有改变</span><br><span class="line">cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d = " &lt;&lt; d &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;c = " &lt;&lt; &amp;c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;d = " &lt;&lt; &amp;d &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "p2 = " &lt;&lt; p2 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*p2 = " &lt;&lt; *p2 &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++24.png" alt="2"></p><h1 id="指针和数组"><a href="#指针和数组" class="headerlink" title="指针和数组"></a><font size="5">指针和数组</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a[3] = { 1, 2, 3 };</span><br><span class="line"></span><br><span class="line">//在数组章节中说过，一维数组名就是数组的首地址，因此可以将一位数组名赋值给指针</span><br><span class="line">int* p = a;</span><br><span class="line"></span><br><span class="line">//访问元素时，可以通过指针的方式，p++代表指针向后移动一个单位，指针移动的单位根据指针类型确定，指针是什么类型则移动相应的大小。</span><br><span class="line">//这里指针为int类型，因此p++就相当于指针从数组第一个元素移动到第二个元素，因为数组也是按照内存大小存放的。</span><br><span class="line">for (int i = 0; i &lt; 3; i++) {</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; *p++ &lt;&lt; " ";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">p = a;</span><br><span class="line">//还可以通过类似数组的方式，通过指针索引访问数组元素。</span><br><span class="line">for (int i = 0; i &lt; 3; i++) {</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; p[i] &lt;&lt; " ";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">int b[3][3] = { 1, 2, 3, 4, 5, 6, 7, 8, 9 };</span><br><span class="line"></span><br><span class="line">//指向指针的指针，很好理解，指针p1是二维数组中第一行的首地址，虽然二维数组名也是一个地址，但是不能通过赋值操作直接给指针赋值。</span><br><span class="line">//指针p2是指针p1的地址，因此p2指向p1，p1指向第一行的首地址，所以*p2就是p1的值，**p2就是b[0][0]的值</span><br><span class="line">int* p1 = b[0];</span><br><span class="line">int** p2 = &amp;p1;</span><br><span class="line"></span><br><span class="line">for (int i = 0; i &lt; 3; i++) {</span><br><span class="line">for (int j = 0; j &lt; 3; j++) {</span><br><span class="line">//注意这里的写法是有讲究的，p2指向p1，因此在一维数组中p1++就等价于(*p2)++。</span><br><span class="line">//不能写成*p2++，*p2++指的是p2指针自加1，p2指针自加是没有意义的。</span><br><span class="line">cout &lt;&lt; *(*p2)++ &lt;&lt; " ";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int c[3][3] = { 1, 2, 3, 4, 5, 6, 7, 8, 9 };</span><br><span class="line"></span><br><span class="line">//指针数组，p3的每一个元素都是一个指针，分别指向数组c每一行的首地址，指针p4指向指针数组的首地址。</span><br><span class="line">int* p3[3] = { c[0], c[1], c[2] };</span><br><span class="line">int** p4 = p3;</span><br><span class="line"></span><br><span class="line">for (int i = 0; i &lt; 3; i++, p4++) {</span><br><span class="line">for (int j = 0; j &lt; 3; j++) {</span><br><span class="line">//p4是指针数组的首地址，*p4指的是p3[0]的值，而p3[0]又是数组c[0][0]的地址</span><br><span class="line">//因此*(*p4)是c[0][0]的值，做完之后(*p4)自增1，等价于p3[0]指向了c[0][1]。</span><br><span class="line">//注意在第一层循环中包含着p4++这个操作，当p3[0]指向c[0][2]以后，p4指向指针数组的下一个元素p3[1]，然后开始读取c数组第二行的元素。</span><br><span class="line">//如果将p4++删去，仍可以得到相同的答案，如何理解呢？因为二维数组也是按照顺序存放的，因此指针p3[0]读取到第一行最后一个元素后，自增时也会指向第二行的第一个元素，因此也是正确的。</span><br><span class="line">cout &lt;&lt; *(*p4)++ &lt;&lt; " ";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++25.png" alt="4"></p><h1 id="值传递"><a href="#值传递" class="headerlink" title="值传递"></a><font size="5">值传递</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//swap参数列表中的变量为普通变量</span><br><span class="line">void swap(int a, int b) {</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">int temp = a;</span><br><span class="line">a = b;</span><br><span class="line">b = temp;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a = 10;</span><br><span class="line">int b = 20;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//当调用函数时，传入变量值，则会将实参值拷贝到形参中去，因此形参会获得实参的值，但是它们的地址是不同的。</span><br><span class="line">//所有进行交换时，两个形参的值进行了交换，在调用结束后，形参的作用域结束，被销毁，并不会影响到实参的值。</span><br><span class="line">swap(a, b);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++26.png" alt="4"></p><h1 id="址传递"><a href="#址传递" class="headerlink" title="址传递"></a><font size="5">址传递</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//swap参数列表中的变量为指针变量</span><br><span class="line">void swap(int* a, int* b) {</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "*a = " &lt;&lt; *a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*b = " &lt;&lt; *b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">int temp = *a;</span><br><span class="line">*a = *b;</span><br><span class="line">*b = temp;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "*a = " &lt;&lt; *a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*b = " &lt;&lt; *b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a = 10;</span><br><span class="line">int b = 20;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//当调用函数时，传入地址值，则会将实参地址值拷贝到形参中去，因此形参会获得实参的地址值。</span><br><span class="line">//所有进行交换时，两个地址的指向的内容进行了交换，在调用结束后，形参的作用域结束，被销毁，但是地址指向的内容已经发生了改变，所以实参会发生变化。</span><br><span class="line">swap(&amp;a, &amp;b);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++27.png" alt="4"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  指针是C/C++中最具有魅力的地方，其实所有的高级语言都有指针，只不过将其包装了起来，让使用者感觉不到在操作指针，最简单的例子，Java中的this，和Python中的self，其本质都是一个指针，学好了指针可以让我们更加了解语言背后的动作，因此小伙伴们要克服恐惧，奥里给！</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++指针&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++指针</title>
    <link href="https://USTCcoder.github.io/2020/07/12/C++_point/"/>
    <id>https://USTCcoder.github.io/2020/07/12/C++_point/</id>
    <published>2020-07-12T12:46:33.000Z</published>
    <updated>2020-08-11T14:53:33.219Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++22.png" alt="2"></p><h1 id="C-指针"><a href="#C-指针" class="headerlink" title="C++指针"></a><font size="5" color="red">C++指针</font></h1><p>  指针是C/C++最精髓的内容，也是大多数程序员幼崽的童年阴影，噩梦的开始。当然也包括本菜鸟也深受其害，在这里我们不说的很详尽，因为我也是个freshman，这个博客的内容是普及一下指针的基本用法，至于复杂的情况，可以参考上面的三本宝典，都是C/C++程序员走上不归路的必读书籍。<br><a id="more"></a></p><h1 id="指针定义"><a href="#指针定义" class="headerlink" title="指针定义"></a><font size="5">指针定义</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">//指针和普通变量的定义一样，可以定义时初始化，也可以在使用时初始化。指针的意思是指向某一块内存空间，因此其代表的一个地址，前面加上*代表取出地址指向的值。</span><br><span class="line">int a = 10;</span><br><span class="line">int* p = &amp;a;</span><br><span class="line">int* p1;</span><br><span class="line">p1 = &amp;a;</span><br><span class="line"></span><br><span class="line">//可以看出p等价于&amp;a，在数组中已经介绍了取地址符&amp;，因此p指代的是a的地址，*p则指代地址中存放的值。</span><br><span class="line">cout &lt;&lt; "p = " &lt;&lt; p &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*p = " &lt;&lt; *p &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">int b = a;</span><br><span class="line">b += 2;</span><br><span class="line"></span><br><span class="line">//int b = a，说明创建一个变量，让其值等于a，但是b和a是两个不同的变量，因此b的变化和a无关，所以b += 2时，a仍保持不变。</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">*p += 2;</span><br><span class="line"></span><br><span class="line">//*p += 2，说明让p指向的内存空间中的值加2，但是p指向的内存空间就是a，相当于a的门牌号，因此*p += 2就等价于a += 2，所以a的值发生了变化。</span><br><span class="line">cout &lt;&lt; "p = " &lt;&lt; p &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*p = " &lt;&lt; *p &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++23.png" alt="1"></p><h1 id="const与指针"><a href="#const与指针" class="headerlink" title="const与指针"></a><font size="5">const与指针</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a = 10;</span><br><span class="line">int b = 20;</span><br><span class="line"></span><br><span class="line">//const修饰指针，称为常量指针，const修饰*p，说明指针指向的值是不可以改变的，但是可以改变指针的指向。</span><br><span class="line">const int* p1 = &amp;a;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "p1 = " &lt;&lt; p1 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*p1 = " &lt;&lt; *p1 &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">p1 = &amp;b;</span><br><span class="line"></span><br><span class="line">//p1指向了b，因此a的值没有变，p1为b的地址，p1指向的值为b的值</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "p1 = " &lt;&lt; p1 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*p1 = " &lt;&lt; *p1 &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">int c = 30;</span><br><span class="line">int d = 40;</span><br><span class="line"></span><br><span class="line">//const修饰常量，称为指针常量，const修饰p，说明指针的指向是不可以改变的，但是可以改变指针指向的值。</span><br><span class="line">int* const p2 = &amp;c;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d = " &lt;&lt; d &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;c = " &lt;&lt; &amp;c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;d = " &lt;&lt; &amp;d &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "p2 = " &lt;&lt; p2 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*p2 = " &lt;&lt; *p2 &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">*p2 = d;</span><br><span class="line"></span><br><span class="line">//p2为c的地址，现在将地址指向的值改为d，因此c变为d的值，p2的地址没有改变</span><br><span class="line">cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d = " &lt;&lt; d &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;c = " &lt;&lt; &amp;c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;d = " &lt;&lt; &amp;d &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "p2 = " &lt;&lt; p2 &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*p2 = " &lt;&lt; *p2 &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++24.png" alt="2"></p><h1 id="指针和数组"><a href="#指针和数组" class="headerlink" title="指针和数组"></a><font size="5">指针和数组</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a[3] = { 1, 2, 3 };</span><br><span class="line"></span><br><span class="line">//在数组章节中说过，一维数组名就是数组的首地址，因此可以将一位数组名赋值给指针</span><br><span class="line">int* p = a;</span><br><span class="line"></span><br><span class="line">//访问元素时，可以通过指针的方式，p++代表指针向后移动一个单位，指针移动的单位根据指针类型确定，指针是什么类型则移动相应的大小。</span><br><span class="line">//这里指针为int类型，因此p++就相当于指针从数组第一个元素移动到第二个元素，因为数组也是按照内存大小存放的。</span><br><span class="line">for (int i = 0; i &lt; 3; i++) {</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; *p++ &lt;&lt; " ";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">p = a;</span><br><span class="line">//还可以通过类似数组的方式，通过指针索引访问数组元素。</span><br><span class="line">for (int i = 0; i &lt; 3; i++) {</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; p[i] &lt;&lt; " ";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">int b[3][3] = { 1, 2, 3, 4, 5, 6, 7, 8, 9 };</span><br><span class="line"></span><br><span class="line">//指向指针的指针，很好理解，指针p1是二维数组中第一行的首地址，虽然二维数组名也是一个地址，但是不能通过赋值操作直接给指针赋值。</span><br><span class="line">//指针p2是指针p1的地址，因此p2指向p1，p1指向第一行的首地址，所以*p2就是p1的值，**p2就是b[0][0]的值</span><br><span class="line">int* p1 = b[0];</span><br><span class="line">int** p2 = &amp;p1;</span><br><span class="line"></span><br><span class="line">for (int i = 0; i &lt; 3; i++) {</span><br><span class="line">for (int j = 0; j &lt; 3; j++) {</span><br><span class="line">//注意这里的写法是有讲究的，p2指向p1，因此在一维数组中p1++就等价于(*p2)++。</span><br><span class="line">//不能写成*p2++，*p2++指的是p2指针自加1，p2指针自加是没有意义的。</span><br><span class="line">cout &lt;&lt; *(*p2)++ &lt;&lt; " ";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int c[3][3] = { 1, 2, 3, 4, 5, 6, 7, 8, 9 };</span><br><span class="line"></span><br><span class="line">//指针数组，p3的每一个元素都是一个指针，分别指向数组c每一行的首地址，指针p4指向指针数组的首地址。</span><br><span class="line">int* p3[3] = { c[0], c[1], c[2] };</span><br><span class="line">int** p4 = p3;</span><br><span class="line"></span><br><span class="line">for (int i = 0; i &lt; 3; i++, p4++) {</span><br><span class="line">for (int j = 0; j &lt; 3; j++) {</span><br><span class="line">//p4是指针数组的首地址，*p4指的是p3[0]的值，而p3[0]又是数组c[0][0]的地址</span><br><span class="line">//因此*(*p4)是c[0][0]的值，做完之后(*p4)自增1，等价于p3[0]指向了c[0][1]。</span><br><span class="line">//注意在第一层循环中包含着p4++这个操作，当p3[0]指向c[0][2]以后，p4指向指针数组的下一个元素p3[1]，然后开始读取c数组第二行的元素。</span><br><span class="line">//如果将p4++删去，仍可以得到相同的答案，如何理解呢？因为二维数组也是按照顺序存放的，因此指针p3[0]读取到第一行最后一个元素后，自增时也会指向第二行的第一个元素，因此也是正确的。</span><br><span class="line">cout &lt;&lt; *(*p4)++ &lt;&lt; " ";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++25.png" alt="4"></p><h1 id="值传递"><a href="#值传递" class="headerlink" title="值传递"></a><font size="5">值传递</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//swap参数列表中的变量为普通变量</span><br><span class="line">void swap(int a, int b) {</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">int temp = a;</span><br><span class="line">a = b;</span><br><span class="line">b = temp;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a = 10;</span><br><span class="line">int b = 20;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//当调用函数时，传入变量值，则会将实参值拷贝到形参中去，因此形参会获得实参的值，但是它们的地址是不同的。</span><br><span class="line">//所有进行交换时，两个形参的值进行了交换，在调用结束后，形参的作用域结束，被销毁，并不会影响到实参的值。</span><br><span class="line">swap(a, b);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++26.png" alt="4"></p><h1 id="址传递"><a href="#址传递" class="headerlink" title="址传递"></a><font size="5">址传递</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//swap参数列表中的变量为指针变量</span><br><span class="line">void swap(int* a, int* b) {</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "*a = " &lt;&lt; *a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*b = " &lt;&lt; *b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">int temp = *a;</span><br><span class="line">*a = *b;</span><br><span class="line">*b = temp;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "*a = " &lt;&lt; *a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "*b = " &lt;&lt; *b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a = 10;</span><br><span class="line">int b = 20;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//当调用函数时，传入地址值，则会将实参地址值拷贝到形参中去，因此形参会获得实参的地址值。</span><br><span class="line">//所有进行交换时，两个地址的指向的内容进行了交换，在调用结束后，形参的作用域结束，被销毁，但是地址指向的内容已经发生了改变，所以实参会发生变化。</span><br><span class="line">swap(&amp;a, &amp;b);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b = " &lt;&lt; b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;a = " &lt;&lt; &amp;a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "&amp;b = " &lt;&lt; &amp;b &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++27.png" alt="4"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  指针是C/C++中最具有魅力的地方，其实所有的高级语言都有指针，只不过将其包装了起来，让使用者感觉不到在操作指针，最简单的例子，Java中的this，和Python中的self，其本质都是一个指针，学好了指针可以让我们更加了解语言背后的动作，因此小伙伴们要克服恐惧，奥里给！</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++指针&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++函数</title>
    <link href="https://USTCcoder.github.io/2020/07/09/C++_function/"/>
    <id>https://USTCcoder.github.io/2020/07/09/C++_function/</id>
    <published>2020-07-09T15:18:06.000Z</published>
    <updated>2020-08-11T14:52:56.718Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++18.png" alt="2"></p><h1 id="C-函数"><a href="#C-函数" class="headerlink" title="C++函数"></a><font size="5" color="red">C++函数</font></h1><p>  函数是面向过程的程序设计精髓，也是所有语言中最重要的一个内容，学好函数，可以设计出优雅的程序，下面给小伙伴们介绍C++函数的定义，调用，参数传递，声明，默认参数，占位参数和函数的重载。<br><a id="more"></a></p><h1 id="函数定义"><a href="#函数定义" class="headerlink" title="函数定义"></a><font size="5">函数定义</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//函数小伙伴们其实并不陌生，在第一行C++代码中就用到了函数的知识，main函数，main函数是程序运行的起始位置，程序必须要有main函数才可以运行</span><br><span class="line">//函数的定义包括返回值类型，函数名，参数列表和函数体。这里单独强调返回值，返回值代表函数运行结束后返回到调用处时产生的数据</span><br><span class="line">//如果没有返回值，类型要写void，可以不写return，如果有返回值，一定要写return</span><br><span class="line">//函数的参数列表要写清楚参数的类型，这和Python有很大的区别</span><br><span class="line">int add(int a, int b) {</span><br><span class="line"></span><br><span class="line">int c = a + b;</span><br><span class="line"></span><br><span class="line">return c;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a = 5;</span><br><span class="line">int b = 3;</span><br><span class="line"></span><br><span class="line">//函数的调用使用函数名(参数)，如果有返回值可以用变量接收</span><br><span class="line">int c = add(a, b);</span><br><span class="line">int d = add(c, b);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "c = a + b = " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d = c + b = " &lt;&lt; d &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++19.png" alt="1"></p><h1 id="函数声明"><a href="#函数声明" class="headerlink" title="函数声明"></a><font size="5">函数声明</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//有时为了更好的看清楚main函数的具体位置，而且让其他人能够能清晰的看出这个文件中的函数具体有哪些，可以在文件开始写清楚函数的声明。</span><br><span class="line">//函数的声明和定义类似，参数列表可以只写参数的类型，并且后面没有花括号，而用分号代替，声明的作用就是告诉编译器，后面有这个函数的具体实现，否则编译器会报错。</span><br><span class="line">int add(int, int);</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a = 5;</span><br><span class="line">int b = 3;</span><br><span class="line"></span><br><span class="line">//函数的调用使用函数名(参数)，如果有返回值可以用变量接收</span><br><span class="line">int c = add(a, b);</span><br><span class="line">int d = add(c, b);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "c = a + b = " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d = c + b = " &lt;&lt; d &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int add(int a, int b) {</span><br><span class="line"></span><br><span class="line">int c = a + b;</span><br><span class="line"></span><br><span class="line">return c;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++19.png" alt="2"></p><h1 id="默认参数"><a href="#默认参数" class="headerlink" title="默认参数"></a><font size="5">默认参数</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//默认参数是指在参数列表中，指定了参数的指，这样传递时可以不传递该参数，会使用默认的参数代入函数进行计算。</span><br><span class="line">//注意默认参数只能出现在参数列表的末尾，允许出现多个默认参数。</span><br><span class="line">double calcArea(int r, double pi=3.1415926) {</span><br><span class="line"></span><br><span class="line">double area = pi * r * r;</span><br><span class="line"></span><br><span class="line">return area;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">double c = calcArea(3);</span><br><span class="line">double d = calcArea(3, 3.14);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "圆c的面积为 " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "圆d的面积为 " &lt;&lt; d &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++20.png" alt="4"></p><h1 id="占位参数"><a href="#占位参数" class="headerlink" title="占位参数"></a><font size="5">占位参数</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//占位参数是指在参数列表中，占了一个位置，传递参数时必须要传入这个参数，否则会报错，但是在函数内无法使用，因为没有变量接收这个参数。</span><br><span class="line">//占位参数使用概率不高，见到时能够认识它即可。</span><br><span class="line">double calcArea(int r, int, double pi=3.1415926) {</span><br><span class="line"></span><br><span class="line">double area = pi * r * r;</span><br><span class="line"></span><br><span class="line">return area;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">double c = calcArea(3, 0);</span><br><span class="line">double d = calcArea(3, 1, 3.14);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "圆c的面积为 " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "圆d的面积为 " &lt;&lt; d &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++20.png" alt="4"></p><h1 id="函数重载"><a href="#函数重载" class="headerlink" title="函数重载"></a><font size="5">函数重载</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">//函数重载是指，一个函数名有不同的功能实现，当传入的参数个数不同，类型不同或者顺序不同时，函数可以发生重载，调用时，根据和哪一个函数更匹配则调用相应的函数。</span><br><span class="line">//特别注意，参数名和返回值不同不能成为函数重载的发生条件，编译器会报错。</span><br><span class="line">double calcArea(int r, double pi) {</span><br><span class="line"></span><br><span class="line">double area = pi * r * r;</span><br><span class="line"></span><br><span class="line">return area;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">double calcArea(int r, double pi, int) {</span><br><span class="line"></span><br><span class="line">double area = pi * r * r;</span><br><span class="line"></span><br><span class="line">return area;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">double calcArea(double r, double pi) {</span><br><span class="line"></span><br><span class="line">double area = pi * r * r;</span><br><span class="line"></span><br><span class="line">return area;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">double calcArea(double pi, int r) {</span><br><span class="line"></span><br><span class="line">double area = pi * r * r;</span><br><span class="line"></span><br><span class="line">return area;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">double c = calcArea(3, 3.14);</span><br><span class="line">double d = calcArea(3, 3.14, 0);</span><br><span class="line">double e = calcArea(3.0, 3.14);</span><br><span class="line">double f = calcArea(3.14, 3);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "圆c的面积为 " &lt;&lt; c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "圆d的面积为 " &lt;&lt; d &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "圆e的面积为 " &lt;&lt; e &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "圆f的面积为 " &lt;&lt; f &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++21.png" alt="4"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  函数是我们面向过程编程的重要思想，同时也是面向对象中封装特性的体现，有了函数我们可以节约大量的时间和空间管理我们的代码，提高了代码的复用率，但是我们要注意编程习惯，尽量一个函数实现一个功能，不要将多个功能写在同一个函数之中。。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++函数&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++数组</title>
    <link href="https://USTCcoder.github.io/2020/07/07/C++_array/"/>
    <id>https://USTCcoder.github.io/2020/07/07/C++_array/</id>
    <published>2020-07-07T14:39:36.000Z</published>
    <updated>2020-08-11T14:52:49.075Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++17.png" alt="2"></p><h1 id="C-数组"><a href="#C-数组" class="headerlink" title="C++数组"></a><font size="5" color="red">C++数组</font></h1><p>  在前面已经介绍了C++的运算符和流程控制语句，这里主要介绍C++的数组，包括一维数组，二维数组以及数组名和地址之间的关系。<br><a id="more"></a></p><h1 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a><font size="5">创建数组</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">//C++创建数组有三种常见方式，可以先定义然后赋初值，也可以定义的时候赋初值</span><br><span class="line">//创建时数组的大小必须要固定，而且不能用变量作为数组的长度，int a[b]是错误的语法，虽然第三种方式没有指定数组大小，但是编译器会根据值的个数自动确定</span><br><span class="line">int a[5];</span><br><span class="line"></span><br><span class="line">for (int i = 0; i &lt; 5; i++) {</span><br><span class="line">a[i] = i;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int b[5] = { 1, 2, 3, 4, 5 };</span><br><span class="line">int c[] = { 1, 2, 3, 4, 5 };</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for (int i = 0; i &lt; 5; i++) {</span><br><span class="line">printf("a[%d] = %d ", i, a[i]);</span><br><span class="line">printf("b[%d] = %d ", i, b[i]);</span><br><span class="line">printf("c[%d] = %d\n", i, c[i]);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++14.png" alt="1"></p><h1 id="数组名和数组地址"><a href="#数组名和数组地址" class="headerlink" title="数组名和数组地址"></a><font size="5">数组名和数组地址</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">int a[5] = { 1, 2, 3, 4, 5 };</span><br><span class="line">char c[3] = { 'a', 'b', 'c' };</span><br><span class="line"></span><br><span class="line">//数组的特点是每个元素都是相同的数据类型，并且元素存放在连续的内存空间上</span><br><span class="line">//int的大小为4个字节，a数组有5个元素，因此是20个字节。char的大小为1个字节，c数组有3个元素，因此是3个字节</span><br><span class="line">cout &lt;&lt; "元素a[0]的大小为" &lt;&lt; sizeof(a[0]) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "数组a的大小为" &lt;&lt; sizeof(a) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "元素c[0]的大小为" &lt;&lt; sizeof(c[0]) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "数组c的大小为" &lt;&lt; sizeof(c) &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//元素a[0]和元素a[1]索引相差1，因此大小相差4个字节，数组名代表的是数组首个元素的地址，&amp;符号为取地址符号</span><br><span class="line">cout &lt;&lt; "元素a[0]的地址为" &lt;&lt; &amp;a[0] &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "元素a[1]的地址为" &lt;&lt; &amp;a[1] &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "数组名的值为" &lt;&lt; a &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++15.png" alt="2"></p><h1 id="二维数组"><a href="#二维数组" class="headerlink" title="二维数组"></a><font size="5">二维数组</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">//二位数组和一位数组类似，有相似的创建方法</span><br><span class="line">int a[3][4];</span><br><span class="line"></span><br><span class="line">for (int i = 0; i &lt; 3; i++) {</span><br><span class="line">for (int j = 0; j &lt; 4; j++) {</span><br><span class="line">a[i][j] = i * j + 1;</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">int b[3][4] = { {1, 2, 3, 4}, {5, 6, 7, 8}, {9, 10, 11, 12} };</span><br><span class="line">int c[3][4] = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 };</span><br><span class="line">int d[][4] = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 };</span><br><span class="line"></span><br><span class="line">//二维数组可以看成一位数组中每个元素都是一个一维数组，因此a[3][4]可以看成3个一维数组的组合，每个一维数组有4个元素</span><br><span class="line">//因此二维数组的大小是一维数组的3倍，一维数组的大小是单个元素的4倍</span><br><span class="line">cout &lt;&lt; "元素a[0][0]的大小为" &lt;&lt; sizeof(a[0][0]) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "一维数组a[0]的大小为" &lt;&lt; sizeof(a[0]) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "二维数组a的大小为" &lt;&lt; sizeof(a) &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">//因为数据是按照内存顺序存放的，所以先存放第一个一维数组，因此a[0][0]和a[1][0]的地址相差4个int大小，是16个字节</span><br><span class="line">//二维数组名等于一维数组名a[0]等于第一个元素a[0][0]的地址</span><br><span class="line">cout &lt;&lt; "元素a[0][0]的地址为" &lt;&lt; &amp;a[0][0] &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "元素a[1][0]的地址为" &lt;&lt; &amp;a[1][0] &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "一维数组名a[0]的值为" &lt;&lt; a[0] &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "一维数组名a[1]的值为" &lt;&lt; a[1] &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "二维数组名a的值为" &lt;&lt; a &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++16.png" alt="4"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  数组是我们存放数据的好方法，就如同抽屉一样，每一个抽屉都放置同样的物品，数组的学习非常重要，也是第一次接触到地址的概念，无论以后从事什么样的研究，数组的使用都是必不可少的，因此需要熟练掌握。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++数组&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++流程控制</title>
    <link href="https://USTCcoder.github.io/2020/07/04/C++_control/"/>
    <id>https://USTCcoder.github.io/2020/07/04/C++_control/</id>
    <published>2020-07-04T14:57:25.000Z</published>
    <updated>2020-08-11T14:52:52.887Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python4.jpg" alt="2"></p><h1 id="C-流程控制"><a href="#C-流程控制" class="headerlink" title="C++流程控制"></a><font size="5" color="red">C++流程控制</font></h1><p>  在前面已经介绍了C++的运算符，这里主要介绍C++的流程控制，包括if条件语句，switch条件语句，while循环，do…while循环，for循环，goto语句，以及continue和break跳转语句。<br><a id="more"></a></p><h1 id="C-条件语句"><a href="#C-条件语句" class="headerlink" title="C++条件语句"></a><font size="5" color="red">C++条件语句</font></h1><h2 id="if条件语句"><a href="#if条件语句" class="headerlink" title="if条件语句"></a><font size="4">if条件语句</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line"></span><br><span class="line">int a = 10;</span><br><span class="line"></span><br><span class="line">//if单分支语句</span><br><span class="line">if (a &gt; 5) {</span><br><span class="line">cout &lt;&lt; "a &gt; 5" &lt;&lt; endl;;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//if...else双分支语句</span><br><span class="line">if (a &gt; 20) {</span><br><span class="line">cout &lt;&lt; "a &gt; 20" &lt;&lt; endl;</span><br><span class="line">}else{</span><br><span class="line">cout &lt;&lt; "a &lt;= 20" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">//if...else if...else多分支语句</span><br><span class="line">if (a &gt; 20) {</span><br><span class="line">cout &lt;&lt; "a &gt; 20" &lt;&lt; endl;</span><br><span class="line">}else if (a &lt; 0) {</span><br><span class="line">cout &lt;&lt; "a &lt; 0" &lt;&lt; endl;</span><br><span class="line">}else{</span><br><span class="line">cout &lt;&lt; "0 &lt;= a &lt;= 20" &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++8.png" alt="1"></p><h2 id="switch-case语句"><a href="#switch-case语句" class="headerlink" title="switch case语句"></a><font size="4">switch case语句</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">int a = 6;</span><br><span class="line"></span><br><span class="line">//switch中的表达式可以为整型或者字符型，不能为浮点型，字符串型，或者一段区间。而且switch语句具有穿透效果，直至遇到break才会跳出。</span><br><span class="line">switch (a){</span><br><span class="line"></span><br><span class="line">case 1:</span><br><span class="line">cout &lt;&lt; "Monday" &lt;&lt; endl;</span><br><span class="line">break;</span><br><span class="line">case 2:</span><br><span class="line">cout &lt;&lt; "Tuesday" &lt;&lt; endl;</span><br><span class="line">break;</span><br><span class="line">case 3:</span><br><span class="line">cout &lt;&lt; "Wednesday" &lt;&lt; endl;</span><br><span class="line">break;</span><br><span class="line">case 4:</span><br><span class="line">cout &lt;&lt; "Thursday" &lt;&lt; endl;</span><br><span class="line">break;</span><br><span class="line">case 5:</span><br><span class="line">cout &lt;&lt; "Friday" &lt;&lt; endl;</span><br><span class="line">break;</span><br><span class="line">case 6:</span><br><span class="line">cout &lt;&lt; "Saturday is free" &lt;&lt; endl;</span><br><span class="line">case 7:</span><br><span class="line">cout &lt;&lt; "Sunday is free" &lt;&lt; endl;</span><br><span class="line">break;</span><br><span class="line">default:</span><br><span class="line">cout &lt;&lt; "Wrong" &lt;&lt; endl;</span><br><span class="line">break;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++9.png" alt="2"></p><h2 id="while循环"><a href="#while循环" class="headerlink" title="while循环"></a><font size="4">while循环</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">int i = 1;</span><br><span class="line">int sum = 0;</span><br><span class="line"></span><br><span class="line">//while循环，每次循环开始时，判断表达式的内容，为真时执行循环体，否则跳出循环</span><br><span class="line">while (i &lt; 10) {</span><br><span class="line">sum += i++;</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; "1 + 2 + 3 + ... + 9 = " &lt;&lt; sum &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++10.png" alt="4"></p><h2 id="C-自增自减运算符"><a href="#C-自增自减运算符" class="headerlink" title="C++自增自减运算符"></a><font size="4">C++自增自减运算符</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">int i = 1;</span><br><span class="line">int sum = 0;</span><br><span class="line"></span><br><span class="line">//do...while循环，每次循环结束后，判断表达式的内容，为真时执行下一次循环体，否则跳出循环</span><br><span class="line">//注意do...while语句，一定会执行一次，而且最后有分号。而while语句，可能一次都不会执行。</span><br><span class="line">do{</span><br><span class="line">sum += i++;</span><br><span class="line">} while (i &lt; 10);</span><br><span class="line">cout &lt;&lt; "1 + 2 + 3 + ... + 9 = " &lt;&lt; sum &lt;&lt; endl;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++10.png" alt="5"></p><h2 id="for循环"><a href="#for循环" class="headerlink" title="for循环"></a><font size="4">for循环</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">int sum = 0;</span><br><span class="line"></span><br><span class="line">//for循环，括号中有三个表达式，分别为初始化表达式1，布尔表达式2，步进表达式3，循环体内容记为4</span><br><span class="line">//for循环的执行过程为1-&gt;2-&gt;4-&gt;3-&gt;2-&gt;4-&gt;3-&gt;...-&gt;2，注意表达式1，2，3都可以省略，但是分号不可以省略。</span><br><span class="line">//三种循环往往可以得到相同的结果，如果确定执行次数一般使用for循环，如果不确定执行次数则可以使用while循环</span><br><span class="line">for (int i = 1; i &lt; 10; i++) {</span><br><span class="line">sum += i;</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; "1 + 2 + 3 + ... + 9 = " &lt;&lt; sum &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++10.png" alt="6"></p><h2 id="goto语句"><a href="#goto语句" class="headerlink" title="goto语句"></a><font size="4">goto语句</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">int sum = 0;</span><br><span class="line"></span><br><span class="line">//goto语句，用于强行跳转到指定语句，大多用在多层循环的跳出，其余情况下避免使用goto语句，会导致编码风格混乱</span><br><span class="line">for (int i = 0; i &lt; 10; i++) {</span><br><span class="line">for (int j = 0; j &lt; 10; j++) {</span><br><span class="line">if (i == 6 &amp;&amp; j == 6) {</span><br><span class="line">goto print;</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; i * 10 + j &lt;&lt; "\t";</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">print:</span><br><span class="line">cout &lt;&lt; "......" &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++11.png" alt="7"></p><h2 id="continue关键字"><a href="#continue关键字" class="headerlink" title="continue关键字"></a><font size="4">continue关键字</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">int sum = 0;</span><br><span class="line"></span><br><span class="line">//continue关键字，立即结束本次循环体中的内容，继续进行下一次循环</span><br><span class="line">for (int i = 0; i &lt; 10; i++) {</span><br><span class="line">if (i == 5) {</span><br><span class="line">continue;</span><br><span class="line">}</span><br><span class="line">sum += i;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "1 + 2 + 3 + 4 + 6 + 7 + 8 + 9 = " &lt;&lt; sum &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++12.png" alt="4"></p><h2 id="break关键字"><a href="#break关键字" class="headerlink" title="break关键字"></a><font size="4">break关键字</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">int sum = 0;</span><br><span class="line"></span><br><span class="line">//break关键字，立即结束该层循环，如果是多层循环嵌套，则进行上一层循环的下一次循环</span><br><span class="line">//break语句除了用于循环结构，还可以用于switch语句中，跳出选择结构</span><br><span class="line">for (int i = 0; i &lt; 10; i++) {</span><br><span class="line">if (i == 5) {</span><br><span class="line">break;</span><br><span class="line">}</span><br><span class="line">sum += i;</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; "1 + 2 + 3 + 4 = " &lt;&lt; sum &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++13.png" alt="5"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  流程控制每种语言都大同小异，因为流程控制是所有语言的基础，只有掌握不同的流程控制语句，才能达到我们想要的目的，虽然难度较小，但是非常重要，无论以后从事什么样的研究，流程控制都是必不可少的，因此需要熟练掌握。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++流程控制&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++运算符</title>
    <link href="https://USTCcoder.github.io/2020/07/01/C++_operator/"/>
    <id>https://USTCcoder.github.io/2020/07/01/C++_operator/</id>
    <published>2020-07-01T14:53:32.000Z</published>
    <updated>2020-08-11T14:53:29.387Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python3.jpg" alt="2"></p><h1 id="C-运算符"><a href="#C-运算符" class="headerlink" title="C++运算符"></a><font size="5" color="red">C++运算符</font></h1><p>  在前面已经介绍了C++的由来，这里主要介绍C++的运算符，包括赋值运算符，算术运算符，关系运算符，逻辑运算符，三目运算符。<br><a id="more"></a></p><h1 id="C-创建变量"><a href="#C-创建变量" class="headerlink" title="C++创建变量"></a><font size="4">C++创建变量</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">// C++中的变量创建时必须写清楚数据类型，也可以先定义，然后在其他地方赋初始值</span><br><span class="line">short s = 1;</span><br><span class="line">int i = 20;</span><br><span class="line">long l = 300;</span><br><span class="line">long long ll = 4000L;</span><br><span class="line">char c = 'A';</span><br><span class="line">float f = 3.14f;</span><br><span class="line">double d = 1.234567;</span><br><span class="line">bool b;</span><br><span class="line">b = true</span><br><span class="line"></span><br><span class="line">// 可以用sizeof(obj) 查看创建的变量大小，typeid(obj).name()可以查看变量的类型</span><br><span class="line">cout &lt;&lt; "s=" &lt;&lt; s &lt;&lt; "  类型为" &lt;&lt; typeid(s).name() &lt;&lt; "  所占内存大小为" &lt;&lt; sizeof(s) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "i=" &lt;&lt; i &lt;&lt; "  类型为" &lt;&lt; typeid(i).name() &lt;&lt; "  所占内存大小为" &lt;&lt; sizeof(i) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "l=" &lt;&lt; l &lt;&lt; "  类型为" &lt;&lt; typeid(l).name() &lt;&lt; "  所占内存大小为" &lt;&lt; sizeof(l) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "ll=" &lt;&lt; ll &lt;&lt; "  类型为" &lt;&lt; typeid(ll).name() &lt;&lt; "  所占内存大小为" &lt;&lt; sizeof(ll) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "c=" &lt;&lt; c &lt;&lt; "  类型为" &lt;&lt; typeid(c).name() &lt;&lt; "  所占内存大小为" &lt;&lt; sizeof(c) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "f=" &lt;&lt; f &lt;&lt; "  类型为" &lt;&lt; typeid(f).name() &lt;&lt; "  所占内存大小为" &lt;&lt; sizeof(f) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d=" &lt;&lt; d &lt;&lt; "  类型为" &lt;&lt; typeid(d).name() &lt;&lt; "  所占内存大小为" &lt;&lt; sizeof(d) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b=" &lt;&lt; b &lt;&lt; "  类型为" &lt;&lt; typeid(b).name() &lt;&lt; "  所占内存大小为" &lt;&lt; sizeof(b) &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++2.png" alt="1"></p><h1 id="C-算术运算"><a href="#C-算术运算" class="headerlink" title="C++算术运算"></a><font size="4">C++算术运算</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">// +(加)，-(减)，*(乘)，/(除)，%(求余)，整数除法结果只能得到整数，注意字符型和布尔型也可以参与运算，字符型的值为ASCII码对应的值，布尔型的true代表1，false代表0</span><br><span class="line">//进行计算时要注意数据的范围，以及低范围和高范围数据类型进行运算时，数据会发生自动类型转换到高范围，高范围数据向低范围数据要强制类型转换</span><br><span class="line">int a = 2;</span><br><span class="line">long l = 999;</span><br><span class="line">char c = 'A';</span><br><span class="line">bool b = false;</span><br><span class="line">double d = 3.14159;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "a + c =" &lt;&lt; a + c &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a * b =" &lt;&lt; a * b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "l / a =" &lt;&lt; l / a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "l % a =" &lt;&lt; l % a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a * a * d =" &lt;&lt; a * a * d &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "a * a * d =" &lt;&lt; (int)(a * a * d) &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++3.png" alt="2"></p><h1 id="C-关系运算"><a href="#C-关系运算" class="headerlink" title="C++关系运算"></a><font size="4">C++关系运算</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">// &gt;(大于)，&lt;(小于)，&gt;=(大于等于)，&lt;=(小于等于)，==(等于)，!=(不等于)</span><br><span class="line">int a = 99;</span><br><span class="line">int b = 'b';</span><br><span class="line">char c = 'c';</span><br><span class="line">bool d = a == c;</span><br><span class="line">bool e = b &gt; c;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; "99是否等于c？ " &lt;&lt; d &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "b是否d大于c " &lt;&lt; e &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++4.png" alt="4"></p><h1 id="C-自增自减运算符"><a href="#C-自增自减运算符" class="headerlink" title="C++自增自减运算符"></a><font size="4">C++自增自减运算符</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">// ++(自增运算符)，--(自减运算符)，++在前代表先进行加1，然后将值代入表达式，++在后代表先将值代入表达式，然后再进行加1，自减操作符同理。</span><br><span class="line">int a = 1;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; a &lt;&lt; " 此时a=1" &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; a++ &lt;&lt; " ++在后，先打印a=1，然后a=a+1" &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; a &lt;&lt; " 此时a=2" &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; ++a &lt;&lt; " ++在前，先计算a=a+1，然后再打印a=3" &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; a &lt;&lt; " 此时a=3" &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; a-- &lt;&lt; " --在后，先打印a=3，然后a=a-1" &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; a &lt;&lt; " 此时a=2" &lt;&lt; endl;;</span><br><span class="line">cout &lt;&lt; --a &lt;&lt; " --在前，先计算a=a-1，然后再打印a=1" &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; a &lt;&lt; " 此时a=1" &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++5.png" alt="5"></p><h1 id="C-逻辑运算"><a href="#C-逻辑运算" class="headerlink" title="C++逻辑运算"></a><font size="4">C++逻辑运算</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">// &amp;&amp;(与)，||(或)，!(非)，注意与或操作只要可以判断出最后结果则停止，具有短路效果。</span><br><span class="line">//如果与操作的第一个条件为假，则不执行第二个条件，如果或操作的第一个条件为真，则不执行第二个条件。</span><br><span class="line">int a = 5;</span><br><span class="line">int b = 6;</span><br><span class="line">int c = 7;</span><br><span class="line">int i = 0;</span><br><span class="line">bool d = a &gt; b || ++i; //此时或操作的第一个表达式为假，需要执行++i，因此i=1</span><br><span class="line">cout &lt;&lt; "i = " &lt;&lt; i &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "d = " &lt;&lt; d &lt;&lt; endl;</span><br><span class="line">bool e = a &gt; b &amp;&amp; ++i; //此时与操作的第一个表达式为假，不需要执行++i，因此i=1</span><br><span class="line">cout &lt;&lt; "i = " &lt;&lt; i &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; "e = " &lt;&lt; e &lt;&lt; endl;</span><br><span class="line">bool f = !d;</span><br><span class="line">cout &lt;&lt; "f = " &lt;&lt; f &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++6.png" alt="6"></p><h1 id="C-三目运算符"><a href="#C-三目运算符" class="headerlink" title="C++三目运算符"></a><font size="4">C++三目运算符</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main() {</span><br><span class="line">// 条件?表达式1:表达式2，当条件成立时，执行表达式1的内容，否则执行表达式2的内容。</span><br><span class="line">int a = 5 &gt; 3 ? 2 : 1; // 5 &gt; 3成立，因此a = 2</span><br><span class="line">cout &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">int b = ++a &gt;= a++ ? ++a : a++; // ++a中a的值为3，a++中a的值为2，此时++a &gt;= a++成立，当条件判断结束后，a自增了两次，变为4，然后b = ++a，a先自增为5，并且赋值给b</span><br><span class="line">cout &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; b &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/C++7.png" alt="7"></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  运算符操作每种语言都大同小异，因为运算符是所有语言的基础，学习每一种语言都离不开运算操作，虽然难度较小，但是非常重要，无论以后从事什么样的研究，都是必不可少的，因此需要熟练掌握。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++运算符&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>C++介绍</title>
    <link href="https://USTCcoder.github.io/2020/06/28/C++_introduction/"/>
    <id>https://USTCcoder.github.io/2020/06/28/C++_introduction/</id>
    <published>2020-06-28T10:27:08.000Z</published>
    <updated>2020-07-19T12:48:04.216Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/C++1.png" alt="0"></p><h1 id="C-介绍"><a href="#C-介绍" class="headerlink" title="C++介绍"></a><font size="5" color="red">C++介绍</font></h1><p>  C++的创始人为美国人本贾尼·斯特劳斯特卢普(Bjarne Stroustrup)，1979年，Bjame Sgoustrup来到了Bell实验室，开始从事将C改良的工作，1983年该语言被正式命名为C++。C++是C语言的继承，既可以面向过程也可以面向对象。可以说是现在高级语言的鼻祖。<br><a id="more"></a></p><p><img src="/images/LANGUAGE/C++.png" alt="0"></p><h1 id="语言的比较"><a href="#语言的比较" class="headerlink" title="语言的比较"></a><font size="5" color="red">语言的比较</font></h1><p>  <font size="3">将其他语言翻译成机器语言的工具称为编译器，编译的方式有两种，一种是编译，一种是解释</font><br>  <font size="3">编译型语言：C/C++，Pascal等语言都属于编译型语言，先由编译器生成可执行文件，运行时不需要重新编译，直接使用编译的结果即可，因此程序执行效率高，跨平台能力差。</font><br>  <font size="3">解释型语言：Java，C++等语言都属于解释型语言，运行时由解释器逐行解释每一句源代码，每次运行都需要解释一次，因此程序执行效率低，跨平台能力强。</font><br><img src="/images/LANGUAGE/python1.jpg" alt="1"></p><h1 id="C-特点"><a href="#C-特点" class="headerlink" title="C++特点"></a><font size="5" color="red">C++特点</font></h1><p>  <font size="3">在C语言的基础上进行扩充和完善，兼容了C的面向过程，也成为了一种面向对象的程序设计语言。</font><br>  <font size="3">功能强大，接近底层，但是也最为复杂，这是C++语言的优点同时也是它的缺点。</font></p><h1 id="C-小结"><a href="#C-小结" class="headerlink" title="C++小结"></a><font size="5" color="red">C++小结</font></h1><p>  C++是大多数高校同学们所接触的第一门语言，因为它经典而且可以学习更多底层的知识体系，所以是我们学习编程的第一个关卡。如果不做C++开发工程师并且具有很长时间的工作经验，很难将这门语言学习透彻，有的小伙伴就会问为什么我们还要学习C++呢？我不做C++开发工程师是不是可以不用学习C++了呢？我认为无论在什么时候，只要是程序员，都需要和C++,Java等语言打交道，因为你的项目总要落地，总要应用在不同的场景之中(服务器开发，游戏开发，图形图像处理，嵌入式等等)，所以了解其他的语言可以更好的和其他部门的同事合作。想要在程序员的道路上越走越远，那么你就需要掌握多种语言，少年，你渴望力量吗<del>~</del>~</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;C++介绍&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="C++" scheme="https://USTCcoder.github.io/categories/C/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/C/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>YOLO-V3</title>
    <link href="https://USTCcoder.github.io/2020/06/26/Object%20detection%20YOLO-V3/"/>
    <id>https://USTCcoder.github.io/2020/06/26/Object detection YOLO-V3/</id>
    <published>2020-06-26T11:04:28.000Z</published>
    <updated>2020-07-12T11:11:36.202Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">YOLO-V3</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>YOLO-V3(You Only Look Once)</strong>:于<strong>2018年发表</strong>上。You Only Look Once体现了较高的检测速度，是一步法的典型代表，也是YOLO系列的第三个版本。<a id="more"></a></p><p><img src="/images/Object_detection/YOLO-V3.png" alt="YOLO-V3"></p><h1 id="YOLO-V3和SSD的区别"><a href="#YOLO-V3和SSD的区别" class="headerlink" title="YOLO-V3和SSD的区别"></a><font size="5" color="red">YOLO-V3和SSD的区别</font></h1><p>  <font size="3"><strong>特征提取网络不同</strong>，SSD的特征提取网络为<strong>VGG</strong>，YOLO-V3中的特征提取网络是ResNet的改进版本<strong>Darknet-53</strong>，实现深层次的特征融合。</font><br>  <font size="3"><strong>先验框不同</strong>，SSD是<strong>根据每一层的尺寸和长宽比计算出来</strong>的，YOLO-V3中每一层的先验框是<strong>根据大量数据聚类</strong>而得的。</font><br>  <font size="3"><strong>编解码函数不同</strong>，SSD的预测的是<strong>中心和宽高的相对偏移量</strong>，YOLO-V3预测的是<strong>中心的绝对偏移量，宽高的相对偏移量</strong></font><br>  <font size="3"><strong>损失函数不同</strong>，SSD采用<strong>Smooth-L1-Loss损失函数和多分类交叉熵损失函数</strong>。YOLO-V3采用<strong>MSE和二分类交叉熵损失函数</strong></font><br>  <font size="3"><strong>预测结果不同</strong>，SSD是<strong>在多个类别上求Softmax，选择最高的一个类别作为预测类别</strong>，YOLO-V3是<strong>通过Sigmoid函数，用置信度和预测结果相乘，超过阈值即可认为有目标存在，因此可以预测多个物体存在于一个预测框的情况</strong>。</font><br>  <font size="3"><strong>正负样本数量不同</strong>，SSD<strong>正样本的数量是根据真实框的先验框的IOU确定的，只要大于设定值就视为正样本，然后设置正负样本的比例为1：3确定负样本的个数</strong>，YOLO-V3<strong>选择与真实框最接近的一个先验框作为正样本，然后从剩余样本中选择IOU小于设定值的作为负样本</strong></font></p><h1 id="YOLO-V3图像分析"><a href="#YOLO-V3图像分析" class="headerlink" title="YOLO-V3图像分析"></a><font size="5" color="red">YOLO-V3图像分析</font></h1><p><img src="/images/Object_detection/YOLO-V3_A.png" alt="YOLO-V3"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, **kwargs):</span><br><span class="line">        super(Conv_Bn_Relu, self).__init__(**kwargs)</span><br><span class="line">        self.blocks = keras.Sequential()</span><br><span class="line">        self.blocks.add(keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_regularizer=keras.regularizers.l2(5e-4)))</span><br><span class="line">        self.blocks.add(keras.layers.BatchNormalization())</span><br><span class="line">        self.blocks.add(keras.layers.LeakyReLU(0.1))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.blocks(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def block(x, filters, times, name):</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.ZeroPadding2D((1, 1), name='{}_zeropadding'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(filters, (3, 3), (2, 2), 'valid', name='{}_conv_bn_relu'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    for i in range(times):</span><br><span class="line">        shortcut = x</span><br><span class="line">        x = compose(Conv_Bn_Relu(filters // 2, (1, 1), (1, 1), 'same', name='{}_resblock{}_conv1'.format(name, i + 1)),</span><br><span class="line">                    Conv_Bn_Relu(filters, (3, 3), (1, 1), 'same', name='{}_resblock{}_conv2'.format(name, i + 1)))(x)</span><br><span class="line">        x = keras.layers.Add(name='{}_resblock{}_add'.format(name, i + 1))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def five_conv(x, filters, name):</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_Relu(filters, (1, 1), (1, 1), 'same', name='{}_conv_bn_relu1'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(filters * 2, (3, 3), (1, 1), 'same', name='{}_conv_bn_relu2'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(filters, (1, 1), (1, 1), 'same', name='{}_conv_bn_relu3'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(filters * 2, (3, 3), (1, 1), 'same', name='{}_conv_bn_relu4'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(filters, (1, 1), (1, 1), 'same', name='{}_conv_bn_relu5'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def yolo_v3(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x1 = Conv_Bn_Relu(32, (3, 3), (1, 1), 'same', name='conv_bn_relu')(x)</span><br><span class="line">    x1 = block(x1, 64, 1, name='block1')</span><br><span class="line">    x1 = block(x1, 128, 2, name='block2')</span><br><span class="line">    x1 = block(x1, 256, 8, name='block3')</span><br><span class="line"></span><br><span class="line">    x2 = block(x1, 512, 8, name='block4')</span><br><span class="line"></span><br><span class="line">    x3 = block(x2, 1024, 4, name='block5')</span><br><span class="line">    feature3 = five_conv(x3, 512, name='feature3')</span><br><span class="line"></span><br><span class="line">    pred3 = compose(Conv_Bn_Relu(1024, (3, 3), (1, 1), 'same', name='pred3_conv1'),</span><br><span class="line">                    keras.layers.Conv2D(3 * 85, (1, 1), (1, 1), 'same', name='pred3_conv2'),</span><br><span class="line">                    keras.layers.Flatten(name='pred3_flatten'))(feature3)</span><br><span class="line"></span><br><span class="line">    upsampling2 = compose(Conv_Bn_Relu(256, (1, 1), (1, 1), 'same', name='conv_bn_relu2'),</span><br><span class="line">                          keras.layers.UpSampling2D((2, 2), name='upsampling2'))(feature3)</span><br><span class="line">    concatenate2 = keras.layers.Concatenate(name='concatenate2')([upsampling2, x2])</span><br><span class="line">    feature2 = five_conv(concatenate2, 256, name='feature2')</span><br><span class="line">    pred2 = compose(Conv_Bn_Relu(512, (3, 3), (1, 1), 'same', name='pred2_conv1'),</span><br><span class="line">                    keras.layers.Conv2D(3 * 85, (1, 1), (1, 1), 'same', name='pred2_conv2'),</span><br><span class="line">                    keras.layers.Flatten(name='pred2_flatten'))(feature2)</span><br><span class="line"></span><br><span class="line">    upsampling1 = compose(Conv_Bn_Relu(128, (1, 1), (1, 1), 'same', name='conv_bn_relu1'),</span><br><span class="line">                          keras.layers.UpSampling2D((2, 2), name='upsampling1'))(feature2)</span><br><span class="line">    concatenate1 = keras.layers.Concatenate(name='concatenate1')([upsampling1, x1])</span><br><span class="line">    feature1 = five_conv(concatenate1, 128, name='feature1')</span><br><span class="line">    pred1 = compose(Conv_Bn_Relu(256, (3, 3), (1, 1), 'same', name='pred1_conv1'),</span><br><span class="line">                    keras.layers.Conv2D(3 * 85, (1, 1), (1, 1), 'same', name='pred1_conv2'),</span><br><span class="line">                    keras.layers.Flatten(name='pred1_flatten'))(feature1)</span><br><span class="line"></span><br><span class="line">    concatenate = keras.layers.Concatenate(name='concatenate')([pred1, pred2, pred3])</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Reshape((10647, 85), name='reshape')(concatenate)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, output, name='YOLO-V3')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = yolo_v3(input_shape=(416, 416, 3))</span><br><span class="line">    model.build(input_shape=(None, 416, 416, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Object_detection/YOLO-V3_R.png" alt="YOLO-V3"></p><h1 id="Shape数据集完整实战"><a href="#Shape数据集完整实战" class="headerlink" title="Shape数据集完整实战"></a><font size="5" color="red">Shape数据集完整实战</font></h1><h2 id="文件路径关系说明"><a href="#文件路径关系说明" class="headerlink" title="文件路径关系说明"></a>文件路径关系说明</h2><ul><li>project<ul><li>shape<ul><li>train_imgs(训练集图像文件夹)</li><li>annotations(训练集标签文件夹)</li><li>test_imgs(测试集图像文件夹)</li></ul></li><li>YOLO-V3_weight(模型权重文件夹)</li><li>YOLO-V3_test_result(测试集结果文件夹)</li><li>YOLO-V3.py</li></ul></li></ul><h2 id="实战步骤说明"><a href="#实战步骤说明" class="headerlink" title="实战步骤说明"></a>实战步骤说明</h2><ol><li>目标检测和语义分割是两种不同类型的工程项目，目标检测实战处理比语义分割困难的多，首先要<strong>读取真实框信息</strong>，将其保存下来，为了后面编码使用。</li><li><strong>建立先验框</strong>，根据网络结构，在不同特征层上建立不同的先验框，先验框的总个数为每个回归分类特征层的像素点个数x每个像素点上的先验框个数。以论文中的先验框为例，特征层有3个，大小分别为52x52，26x26，13x13，特征层上每个像素点的先验框个数都是3个。<br><img src="/images/Object_detection/YOLO-V3_P.png" alt="anchor"><script type="math/tex; mode=display">52^2 \times 3+26^2 \times 3+13^2 \times 3=10647</script>故先验框总数为10647个。</li><li>根据真实框的信息，和所有先验框计算IOU，将IOU最大的先验框作为正样本。然后进行<strong>编码</strong>，在置信度上面置1说明该位置有目标，对应目标类别置信度置1，其他目标类别置信度置0，并计算正样本先验框的中心坐标与宽高和真实框的中心坐标与宽高之间的差异。输出(batch_size, num_prior, 4 + 1 + num_class + 1)，num_prior为先验框的个数，每个先验框有4 + 1 + num_class + 1个值，4代表中心坐标和宽高相对真实框的差异，1代表属于有目标的置信度，num_class代表属于某一个类别的置信度，最后一个1代表真实框与先验框的IOU，方便计算损失时得到负样本。编码的目的是得到真实框对应的神经网络的输出应该是什么样子，然后让两者尽可能的接近。<br><strong>IOU(Intersection Over Union，交并比)</strong>：用于<strong>评估语义分割算法性能的指标是平均IOU</strong>，交并比也非常好理解，算法的结果与真实物体进行<strong>交运算的结果除以进行并运算的结果</strong>。通过下图可以直观的看出IOU的计算方法。<br><img src="/images/Semantic_segmentation/Dataset_I.png" alt="IOU"></li><li><strong>设计损失函数</strong>，因为先验框中大部分都是负样本，因此不能直接计算损失函数，<strong>选择与真实框最接近的一个先验框作为正样本，然后从剩余样本中选择IOU小于设定值的作为负样本</strong>。</li><li>搭建神经网络，<strong>设置合适参数</strong>，进行训练。</li><li>预测时，需要根据神经网络的输出进行<strong>逆向解码(编码的反过程)</strong>，根据置信度，选择<strong>背景置信度乘类别置信度大于设定值的先验框作为候选框</strong>，然后<strong>根据先验框的坐标和4个回归参数确定候选框的左上角和右下角坐标</strong>。对<strong>每一类候选框进行NMS得到预测框</strong>，并且在图像上<strong>画出预测框</strong>，并且<strong>标出置信度</strong>即可完成目标检测任务。<br><strong>NMS(Non-Maximum Suppression，非极大值抑制)</strong>：简单地说，<strong>不是最大的我不要</strong>，在目标检测中，往往图像上存在大量先验框，会导致很多附近的框都会预测出同一个物体，但是我们<strong>只保留最大的一个预测结果</strong>，这就是非极大值抑制。<br>步骤：<br>(1)<strong>从最大概率矩形框F开始</strong>，分别判断A~E与F的IOU是否大于某个设定的阈值，<strong>假设B、D与F的重叠度超过阈值，那么就扔掉B、D</strong>；并<strong>标记第一个矩形框F</strong>，是我们保留下来的。<br>(2)<strong>从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框</strong>。<br>(3)<strong>重复步骤(2)，直到所有的框都被抛弃或者保留</strong>。<br><img src="/images/Object_detection/Dataset_N.png" alt="NMS"></li></ol><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>神经网络的输出维度为(batch_size, num_prior, 4 + 1 + num_class)，<strong>此数据集为3类，因此最后一个维度是8</strong>。每个先验框有8个索引，前面4个索引代表先验框的回归参数，用来对先验框进行调整得到预测框，索引为4代表有目标的置信度，索引为5代表圆形，索引为6代表三角形，索引为7代表正方形。</li><li>实际的工程应用中，常常还需要对数据集进行<strong>大小调整和增强</strong>，在这里为了简单起见，没有进行复杂的操作，小伙伴们应用中要记得根据自己的需要，对图像进行<strong>resize或者padding</strong>，然后<strong>旋转</strong>，<strong>对比度增强</strong>，<strong>仿射运算</strong>等等操作，增加模型的鲁棒性，并且实际中的图像不一定按照顺序命名的，因此应用中也要注意图像读取的文件名。</li><li>设置了<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li>使用<strong>yield</strong>关键字，产生可迭代对象，不用将所有的数据都保存下来，大大节约内存。</li><li>其中将1000个数据，分成800个训练集，100个验证集和100个测试集，小伙伴们可以自行修改。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>YOLO-V3的<strong>特征提取网络为Darknet-53</strong>，小伙伴们可以参考特征提取网络部分内容，选择其他的网络进行特征提取，比较不同网络参数量，运行速度，最终结果之间的差异。</li><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li><strong>根据实际的图像大小，选择合适的特征层数，先验框的形状，先验框数量，以及各种阈值</strong></li><li><strong>anchor尺寸的确定</strong>，anchor通过聚类方法确定，anchor的大小对于检测效果有很大的影响，小伙伴们可以尝试不同的anchor，看一看测试的结果。<br>11.TF2.0是一个不太稳定的版本，在训练时，常常出现卡顿情况，在损失函数中前面加上一句打印损失函数的值，就不会发生卡顿。。。喵喵喵~~~。</li><li>因为这个博客是对学习的一些总结和记录，意在和学习者探讨和交流，并且给准备入门的同学一些手把手的教学，因此关于目标检测的算法参数设计，我都是自己尝试的，不是针对于这个数据集最优的参数，大家可以根据自己的实际需要修改网络结构。</li></ol><h2 id="完整实战代码"><a href="#完整实战代码" class="headerlink" title="完整实战代码"></a>完整实战代码</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br></pre></td><td class="code"><pre><span class="line">import colorsys</span><br><span class="line">import os</span><br><span class="line">import xml.etree.ElementTree as ET</span><br><span class="line">from functools import reduce</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取先验框函数</span><br><span class="line">def get_prior(layer_id):</span><br><span class="line">    layer_id = layer_id - 1</span><br><span class="line"></span><br><span class="line">    box_widths = [x[1] for x in anchors[layer_id]]</span><br><span class="line">    box_heights = [x[0] for x in anchors[layer_id]]</span><br><span class="line"></span><br><span class="line">    step_x = img_size[1] / feature_map[layer_id]</span><br><span class="line">    step_y = img_size[0] / feature_map[layer_id]</span><br><span class="line">    linx = np.linspace(0.5 * step_x, img_size[1] - 0.5 * step_x, feature_map[layer_id])</span><br><span class="line">    liny = np.linspace(0.5 * step_y, img_size[0] - 0.5 * step_y, feature_map[layer_id])</span><br><span class="line"></span><br><span class="line">    centers_x, centers_y = np.meshgrid(linx, liny)</span><br><span class="line">    centers_x = centers_x.reshape(-1, 1)</span><br><span class="line">    centers_y = centers_y.reshape(-1, 1)</span><br><span class="line"></span><br><span class="line">    # 获得先验框的中心坐标</span><br><span class="line">    prior_center = np.concatenate((centers_x, centers_y), axis=1)</span><br><span class="line">    prior_center = np.tile(prior_center, (1, prior[layer_id] * 2))</span><br><span class="line"></span><br><span class="line">    prior_lt_rb = prior_center.copy()</span><br><span class="line"></span><br><span class="line">    # 获得先验框的左上右下</span><br><span class="line">    prior_lt_rb[:, ::4] -= box_widths</span><br><span class="line">    prior_lt_rb[:, 1::4] -= box_heights</span><br><span class="line">    prior_lt_rb[:, 2::4] += box_widths</span><br><span class="line">    prior_lt_rb[:, 3::4] += box_heights</span><br><span class="line"></span><br><span class="line">    # 归一化到[0, 1]</span><br><span class="line">    prior_lt_rb[:, ::2] /= img_size[1]</span><br><span class="line">    prior_lt_rb[:, 1::2] /= img_size[0]</span><br><span class="line">    prior_lt_rb = prior_lt_rb.reshape(-1, 4)</span><br><span class="line">    prior_lt_rb = np.minimum(np.maximum(prior_lt_rb, 0.0), 1.0)</span><br><span class="line"></span><br><span class="line">    prior_center_wh = np.zeros_like(prior_lt_rb)</span><br><span class="line">    # 获得先验框的宽和高</span><br><span class="line">    prior_center_wh[:, 0] = 0.5 * (prior_lt_rb[:, 2] + prior_lt_rb[:, 0])</span><br><span class="line">    prior_center_wh[:, 1] = 0.5 * (prior_lt_rb[:, 3] + prior_lt_rb[:, 1])</span><br><span class="line">    prior_center_wh[:, 2] = prior_lt_rb[:, 2] - prior_lt_rb[:, 0]</span><br><span class="line">    prior_center_wh[:, 3] = prior_lt_rb[:, 3] - prior_lt_rb[:, 1]</span><br><span class="line"></span><br><span class="line">    return prior_center_wh.astype(np.float32), prior_lt_rb.astype(np.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 从xml文件中获取bounding-box信息</span><br><span class="line">def get_bbox(image_id, bbox_path, annotations_path):</span><br><span class="line">    with open(bbox_path, 'w') as f:</span><br><span class="line">        for id in image_id:</span><br><span class="line">            # 图片路径</span><br><span class="line">            info = os.getcwd() + imgs_path[1:] + '\\' + str(id) + '.jpg'</span><br><span class="line">            in_file = open(annotations_path + '\\' + str(id) + '.xml', encoding='utf-8')</span><br><span class="line">            tree = ET.parse(in_file)</span><br><span class="line">            root = tree.getroot()</span><br><span class="line"></span><br><span class="line">            for obj in root.iter('object'):</span><br><span class="line">                difficult = obj.find('difficult').text</span><br><span class="line">                cls = obj.find('name').text</span><br><span class="line">                if cls not in classes or int(difficult) == 1:</span><br><span class="line">                    continue</span><br><span class="line">                cls_id = classes.index(cls)</span><br><span class="line">                xmlbox = obj.find('bndbox')</span><br><span class="line">                b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))</span><br><span class="line">                info += " " + ",".join([str(x) for x in b]) + ',' + str(cls_id)</span><br><span class="line">            f.writelines(info + '\n')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, **kwargs):</span><br><span class="line">        super(Conv_Bn_Relu, self).__init__(**kwargs)</span><br><span class="line">        self.blocks = keras.Sequential()</span><br><span class="line">        self.blocks.add(keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_regularizer=keras.regularizers.l2(5e-4)))</span><br><span class="line">        self.blocks.add(keras.layers.BatchNormalization())</span><br><span class="line">        self.blocks.add(keras.layers.LeakyReLU(0.1))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.blocks(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def block(x, filters, times, name):</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.ZeroPadding2D((1, 1), name='{}_zeropadding'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(filters, (3, 3), (2, 2), 'valid', name='{}_conv_bn_relu'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    for i in range(times):</span><br><span class="line">        shortcut = x</span><br><span class="line">        x = compose(Conv_Bn_Relu(filters // 2, (1, 1), (1, 1), 'same', name='{}_resblock{}_conv1'.format(name, i + 1)),</span><br><span class="line">                    Conv_Bn_Relu(filters, (3, 3), (1, 1), 'same', name='{}_resblock{}_conv2'.format(name, i + 1)))(x)</span><br><span class="line">        x = keras.layers.Add(name='{}_resblock{}_add'.format(name, i + 1))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def five_conv(x, filters, name):</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_Relu(filters, (1, 1), (1, 1), 'same', name='{}_conv_bn_relu1'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(filters * 2, (3, 3), (1, 1), 'same', name='{}_conv_bn_relu2'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(filters, (1, 1), (1, 1), 'same', name='{}_conv_bn_relu3'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(filters * 2, (3, 3), (1, 1), 'same', name='{}_conv_bn_relu4'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(filters, (1, 1), (1, 1), 'same', name='{}_conv_bn_relu5'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def small_yolo_v3(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x1 = Conv_Bn_Relu(16, (3, 3), (1, 1), 'same', name='conv_bn_relu')(x)</span><br><span class="line">    x1 = block(x1, 32, 2, name='block2')</span><br><span class="line">    x1 = block(x1, 64, 2, name='block3')</span><br><span class="line"></span><br><span class="line">    x2 = block(x1, 128, 2, name='block4')</span><br><span class="line"></span><br><span class="line">    x3 = block(x2, 256, 2, name='block5')</span><br><span class="line">    feature3 = five_conv(x3, 128, name='feature3')</span><br><span class="line">    pred_reg3 = compose(Conv_Bn_Relu(256, (3, 3), (1, 1), 'same', name='pred3_reg_conv1'),</span><br><span class="line">                    keras.layers.Conv2D(2 * 4, (1, 1), (1, 1), 'same', name='pred3_reg_conv2'),</span><br><span class="line">                    keras.layers.Flatten(name='pred3_reg_flatten'))(feature3)</span><br><span class="line"></span><br><span class="line">    pred_conf3 = compose(Conv_Bn_Relu(256, (3, 3), (1, 1), 'same', name='pred3_conf_conv1'),</span><br><span class="line">                    keras.layers.Conv2D(2 * num_class, (1, 1), (1, 1), 'same', name='pred3_conf_conv2'),</span><br><span class="line">                    keras.layers.Flatten(name='pred3_conf_flatten'))(feature3)</span><br><span class="line"></span><br><span class="line">    upsampling2 = compose(Conv_Bn_Relu(64, (1, 1), (1, 1), 'same', name='conv_bn_relu2'),</span><br><span class="line">                          keras.layers.UpSampling2D((2, 2), name='upsampling2'))(feature3)</span><br><span class="line">    concatenate2 = keras.layers.Concatenate(name='concatenate2')([upsampling2, x2])</span><br><span class="line">    feature2 = five_conv(concatenate2, 64, name='feature2')</span><br><span class="line">    pred_reg2 = compose(Conv_Bn_Relu(128, (3, 3), (1, 1), 'same', name='pred2_reg_conv1'),</span><br><span class="line">                        keras.layers.Conv2D(2 * 4, (1, 1), (1, 1), 'same', name='pred2_reg_conv2'),</span><br><span class="line">                        keras.layers.Flatten(name='pred2_reg_flatten'))(feature2)</span><br><span class="line"></span><br><span class="line">    pred_conf2 = compose(Conv_Bn_Relu(128, (3, 3), (1, 1), 'same', name='pred2_conf_conv1'),</span><br><span class="line">                         keras.layers.Conv2D(2 * num_class, (1, 1), (1, 1), 'same', name='pred2_conf_conv2'),</span><br><span class="line">                         keras.layers.Flatten(name='pred2_conf_flatten'))(feature2)</span><br><span class="line"></span><br><span class="line">    upsampling1 = compose(Conv_Bn_Relu(32, (1, 1), (1, 1), 'same', name='conv_bn_relu1'),</span><br><span class="line">                          keras.layers.UpSampling2D((2, 2), name='upsampling1'))(feature2)</span><br><span class="line">    concatenate1 = keras.layers.Concatenate(name='concatenate1')([upsampling1, x1])</span><br><span class="line">    feature1 = five_conv(concatenate1, 32, name='feature1')</span><br><span class="line">    pred_reg1 = compose(Conv_Bn_Relu(64, (3, 3), (1, 1), 'same', name='pred1_reg_conv1'),</span><br><span class="line">                        keras.layers.Conv2D(2 * 4, (1, 1), (1, 1), 'same', name='pred1_reg_conv2'),</span><br><span class="line">                        keras.layers.Flatten(name='pred1_reg_flatten'))(feature1)</span><br><span class="line"></span><br><span class="line">    pred_conf1 = compose(Conv_Bn_Relu(64, (3, 3), (1, 1), 'same', name='pred1_conf_conv1'),</span><br><span class="line">                         keras.layers.Conv2D(2 * num_class, (1, 1), (1, 1), 'same', name='pred1_conf_conv2'),</span><br><span class="line">                         keras.layers.Flatten(name='pred1_conf_flatten'))(feature1)</span><br><span class="line"></span><br><span class="line">    concatenate_reg = keras.layers.Concatenate(name='concatenate_reg')([pred_reg1, pred_reg2, pred_reg3])</span><br><span class="line">    concatenate_cls = keras.layers.Concatenate(name='concatenate_cls')([pred_conf1, pred_conf2, pred_conf3])</span><br><span class="line"></span><br><span class="line">    reshape_reg = keras.layers.Reshape((num_prior, 4), name='reshape_reg')(concatenate_reg)</span><br><span class="line">    reshape_cls = keras.layers.Reshape((num_prior, num_class), name='reshape_cls')(concatenate_cls)</span><br><span class="line"></span><br><span class="line">    softmax_cls = keras.layers.Activation('sigmoid', name='sigmoid_cls')(reshape_cls)</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Concatenate(name='concatenate')([reshape_reg, softmax_cls])</span><br><span class="line"></span><br><span class="line">    # 输出维度是[batch_size, 先验框的总数num_prior, 先验框的位置回归 + 物体的置信度 + 先验框的预测类别]，这里是[8, 1008, 8]</span><br><span class="line">    model = keras.Model(input_tensor, output, name='YOLO-V3')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 计算IOU函数</span><br><span class="line">def iou(box):</span><br><span class="line">    inter_upleft = np.maximum(prior_lt_rb[:, :2], box[:2])</span><br><span class="line">    inter_botright = np.minimum(prior_lt_rb[:, 2:4], box[2:])</span><br><span class="line"></span><br><span class="line">    inter_wh = inter_botright - inter_upleft</span><br><span class="line">    inter_wh = np.maximum(inter_wh, 0)</span><br><span class="line">    inter = inter_wh[:, 0] * inter_wh[:, 1]</span><br><span class="line">    # 真实框的面积</span><br><span class="line">    area_true = (box[2] - box[0]) * (box[3] - box[1])</span><br><span class="line">    # 先验框的面积</span><br><span class="line">    area_gt = (prior_lt_rb[:, 2] - prior_lt_rb[:, 0]) * (prior_lt_rb[:, 3] - prior_lt_rb[:, 1])</span><br><span class="line">    # 计算iou</span><br><span class="line">    union = area_true + area_gt - inter</span><br><span class="line"></span><br><span class="line">    iou = inter / union</span><br><span class="line"></span><br><span class="line">    return iou</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 根据真实框bounding-box编码函数</span><br><span class="line">def encoder(box):</span><br><span class="line">    iou_val = iou(box)</span><br><span class="line">    encoded_box = np.zeros((num_prior, 5))</span><br><span class="line"></span><br><span class="line">    # 找到每一个真实框，重合程度较高的先验框</span><br><span class="line">    assign_mask = iou_val &gt; overlap_threshold</span><br><span class="line">    encoded_box[:, -1][assign_mask] = iou_val[assign_mask]</span><br><span class="line"></span><br><span class="line">    # 先计算真实框的中心与长宽</span><br><span class="line">    encoded_box[:, 0:2] = (0.5 * (box[:2] + box[2:]) - prior_center_wh[:, :2]) * feature_shape</span><br><span class="line">    encoded_box[:, 2:4] = tf.math.log((box[2:] - box[:2]) / prior_center_wh[:, 2:])</span><br><span class="line"></span><br><span class="line">    return encoded_box</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取网络输出标签数据，即作为损失函数的真实输入y_true</span><br><span class="line">def assign_boxes(boxes):</span><br><span class="line">    # 大小为num_box * (4 + num_class + 1)，4代表4个位置回归，1代表iou</span><br><span class="line">    assignment = np.zeros((num_prior, 4 + num_class + 1))</span><br><span class="line">    if len(boxes) == 0:</span><br><span class="line">        return assignment</span><br><span class="line">    # 对每一个真实框都进行iou计算</span><br><span class="line">    encoded_boxes = np.apply_along_axis(f_encode, 1, boxes[:, :4])</span><br><span class="line">    # 每一个真实框的编码后的值，和iou</span><br><span class="line">    encoded_boxes = encoded_boxes.reshape(-1, num_prior, 5)</span><br><span class="line">    # 取重合程度最大的先验框，并且获取这个先验框的index</span><br><span class="line">    best_iou_idx = encoded_boxes[:, :, -1].argmax(axis=1)</span><br><span class="line"></span><br><span class="line">    # 前面4列代表中心和宽高</span><br><span class="line">    assignment[:, :4][best_iou_idx] = encoded_boxes[np.arange(len(best_iou_idx)), best_iou_idx, :4]</span><br><span class="line">    # 中间num_class代表标签信息</span><br><span class="line">    assignment[:, 4:-1][best_iou_idx] = boxes[..., 4:]</span><br><span class="line">    # 最后一列代表iou</span><br><span class="line">    assignment[:, -1] = encoded_boxes[:, :, -1].max(axis=0)</span><br><span class="line">    return assignment</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 通过yield获取可迭代对象</span><br><span class="line">def generate_arrays_from_file(train_data, batch_size):</span><br><span class="line">    # 获取总长度</span><br><span class="line">    n = len(train_data)</span><br><span class="line">    i = 0</span><br><span class="line">    while True:</span><br><span class="line">        X_train = []</span><br><span class="line">        Y_train = []</span><br><span class="line">        # 获取一个batch_size大小的数据</span><br><span class="line">        while len(X_train) &lt; batch_size:</span><br><span class="line">            if i == 0:</span><br><span class="line">                np.random.shuffle(train_data)</span><br><span class="line">            # 从文件中读取图像</span><br><span class="line">            img = cv.imread(imgs_path + '\\' + str(train_data[i]) + '.jpg')</span><br><span class="line">            img = img / 127.5 - 1</span><br><span class="line">            info = np.array([list(map(int, x.split(','))) for x in bounding_info[train_data[i]].split()[3:]])</span><br><span class="line">            if not len(info):</span><br><span class="line">                i = (i + 1) % n</span><br><span class="line">                continue</span><br><span class="line">            box = (info[:, :4] + 1).astype(np.float32)</span><br><span class="line">            box[:, [0, 2]] = box[:, [0, 2]] / img_size[1]</span><br><span class="line">            box[:, [1, 3]] = box[:, [1, 3]] / img_size[0]</span><br><span class="line">            label = np.eye(num_class)[np.array(info[:, 4] + 1, np.int32)]</span><br><span class="line">            label[:, 0] = 1</span><br><span class="line">            if ((box[:, 0] - box[:, 2]) &gt;= 0).any() or ((box[:, 1] - box[:, 3]) &gt;= 0).any():</span><br><span class="line">                i = (i + 1) % n</span><br><span class="line">                continue</span><br><span class="line">            box = np.concatenate([box, label], axis=-1)</span><br><span class="line">            X_train.append(img)</span><br><span class="line">            y = assign_boxes(box)</span><br><span class="line">            Y_train.append(y)</span><br><span class="line">            i = (i + 1) % n</span><br><span class="line">        yield tf.constant(X_train), tf.constant(Y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 定义损失函数</span><br><span class="line">@tf.function</span><br><span class="line">def compute_loss(y_true, y_pred):</span><br><span class="line"></span><br><span class="line">    y_true = tf.reshape(y_true, (-1, 9))</span><br><span class="line">    y_pred = tf.reshape(y_pred, (-1, 8))</span><br><span class="line"></span><br><span class="line">    pos = tf.equal(y_true[:, 4], 1)</span><br><span class="line">    neg = tf.logical_and(tf.equal(y_true[:, 4], 0), tf.less(y_true[:, -1], overlap_threshold))</span><br><span class="line"></span><br><span class="line">    y_true_pos = tf.boolean_mask(y_true[:, :-1], axis=0, mask=pos)</span><br><span class="line">    y_true_neg = tf.boolean_mask(y_true[:, :-1], axis=0, mask=neg)</span><br><span class="line">    y_pred_pos = tf.boolean_mask(y_pred, axis=0, mask=pos)</span><br><span class="line">    y_pred_neg = tf.boolean_mask(y_pred, axis=0, mask=neg)</span><br><span class="line">    y_true_valid = tf.concat([y_true_pos, y_true_neg], axis=0)</span><br><span class="line">    y_pred_valid = tf.concat([y_pred_pos, y_pred_neg], axis=0)</span><br><span class="line">    reg_loss = tf.reduce_mean((y_true_pos[:, :4] - y_pred_pos[:, :4]) ** 2)</span><br><span class="line">    conf_loss = tf.reduce_mean(keras.losses.binary_crossentropy(y_true_valid[:, 4:], y_pred_valid[:, 4:]))</span><br><span class="line">    tf.print(conf_loss)</span><br><span class="line">    return reg_loss + conf_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 根据网络预测解码函数，获得候选框</span><br><span class="line">def decoder(loc):</span><br><span class="line">    # 获得先验框的中心与宽高</span><br><span class="line">    prior_center_x = prior_center_wh[:, 0]</span><br><span class="line">    prior_center_y = prior_center_wh[:, 1]</span><br><span class="line">    prior_width = prior_center_wh[:, 2]</span><br><span class="line">    prior_height = prior_center_wh[:, 3]</span><br><span class="line"></span><br><span class="line">    # 获得真实框的中心与宽高</span><br><span class="line">    decode_bbox_center_x = (loc[:, 0] / feature_shape[:, 0] + prior_center_x)</span><br><span class="line">    decode_bbox_center_y = (loc[:, 1] / feature_shape[:, 1] + prior_center_y)</span><br><span class="line">    decode_bbox_width = np.exp(loc[:, 2]) * prior_width</span><br><span class="line">    decode_bbox_height = np.exp(loc[:, 3]) * prior_height</span><br><span class="line"></span><br><span class="line">    # 获取真实框的左上角与右下角</span><br><span class="line">    decode_bbox_xmin = decode_bbox_center_x - 0.5 * decode_bbox_width</span><br><span class="line">    decode_bbox_ymin = decode_bbox_center_y - 0.5 * decode_bbox_height</span><br><span class="line">    decode_bbox_xmax = decode_bbox_center_x + 0.5 * decode_bbox_width</span><br><span class="line">    decode_bbox_ymax = decode_bbox_center_y + 0.5 * decode_bbox_height</span><br><span class="line"></span><br><span class="line">    # 真实框的左上角与右下角进行堆叠</span><br><span class="line">    decode_bbox = np.concatenate((decode_bbox_xmin[:, np.newaxis], decode_bbox_ymin[:, np.newaxis], decode_bbox_xmax[:, np.newaxis], decode_bbox_ymax[:, np.newaxis]), axis=-1)</span><br><span class="line">    # 防止超出0与1</span><br><span class="line">    decode_bbox = np.minimum(np.maximum(decode_bbox, 0.0), 1.0)</span><br><span class="line">    return decode_bbox</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 将候选框进行非极大值抑制，获得最终的预测框</span><br><span class="line">def detection_out(pred):</span><br><span class="line">    # 回归网络预测结果</span><br><span class="line">    mbox_loc = pred[:, :4]</span><br><span class="line">    # 分类网络预测结果</span><br><span class="line">    mbox_conf = pred[:, 4:]</span><br><span class="line">    results = []</span><br><span class="line">    # 对每一个图像进行处理</span><br><span class="line">    decode_bbox = decoder(mbox_loc)</span><br><span class="line">    for c in range(1, num_class):</span><br><span class="line">        c_confs = mbox_conf[:, c] * mbox_conf[:, 0]</span><br><span class="line">        c_confs_mask = c_confs &gt; confidence_threshold</span><br><span class="line">        if len(c_confs[c_confs_mask]) &gt; 0:</span><br><span class="line">            # 取出得分高于confidence_threshold的框</span><br><span class="line">            boxes_to_process = decode_bbox[c_confs_mask]</span><br><span class="line">            confs_to_process = c_confs[c_confs_mask]</span><br><span class="line">            # 进行iou的非极大抑制</span><br><span class="line">            idx = tf.image.non_max_suppression(boxes_to_process.astype(np.float32), confs_to_process, max_output_size=keep_top_k, iou_threshold=nms_thresh)</span><br><span class="line">            idx = idx.numpy()</span><br><span class="line">            # 取出在非极大抑制中效果较好的内容</span><br><span class="line">            box = boxes_to_process[idx]</span><br><span class="line">            confs = confs_to_process[idx][:, np.newaxis]</span><br><span class="line">            # 将label、置信度、框的位置进行堆叠。</span><br><span class="line">            labels = c * np.ones((len(idx), 1))</span><br><span class="line">            c_pred = np.concatenate((labels, confs, box), axis=1)</span><br><span class="line">            # 添加进result里</span><br><span class="line">            results.extend(c_pred)</span><br><span class="line">    if len(results) &gt; 0:</span><br><span class="line">        # 按照置信度进行排序</span><br><span class="line">        results = np.array(results)</span><br><span class="line">        arg = np.argsort(results[:, 1])[::-1][:keep_top_k]</span><br><span class="line">        results = results[arg]</span><br><span class="line">    return results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 将图像进行预测并画框</span><br><span class="line">def detect_image(filename):</span><br><span class="line"></span><br><span class="line">    test_img = cv.imread(filename)</span><br><span class="line">    preds = tf.squeeze(model.predict(tf.constant([test_img / 127.5 - 1])), axis=0).numpy()</span><br><span class="line"></span><br><span class="line">    # 将预测结果进行解码</span><br><span class="line">    results = detection_out(preds)</span><br><span class="line"></span><br><span class="line">    if len(results) &lt;= 0:</span><br><span class="line">        return test_img</span><br><span class="line">    print(filename)</span><br><span class="line">    # 筛选出其中得分高于confidence的框</span><br><span class="line">    det_label = results[:, 0]</span><br><span class="line">    det_conf = results[:, 1]</span><br><span class="line">    det_xmin, det_ymin, det_xmax, det_ymax = results[:, 2], results[:, 3], results[:, 4], results[:, 5]</span><br><span class="line">    indices = [index for index, conf in enumerate(det_conf) if conf &gt;= confidence_threshold]</span><br><span class="line">    top_conf = det_conf[indices]</span><br><span class="line">    top_label_indices = det_label[indices].tolist()</span><br><span class="line">    top_xmin = np.expand_dims(det_xmin[indices], -1) * img_size[1]</span><br><span class="line">    top_ymin = np.expand_dims(det_ymin[indices], -1) * img_size[0]</span><br><span class="line">    top_xmax = np.expand_dims(det_xmax[indices], -1) * img_size[1]</span><br><span class="line">    top_ymax = np.expand_dims(det_ymax[indices], -1) * img_size[0]</span><br><span class="line">    boxes = np.concatenate([top_xmin, top_ymin, top_xmax, top_ymax], axis=-1)</span><br><span class="line"></span><br><span class="line">    font = cv.FONT_HERSHEY_SIMPLEX</span><br><span class="line"></span><br><span class="line">    for i, c in enumerate(top_label_indices):</span><br><span class="line">        cls = int(c) - 1</span><br><span class="line">        predicted_class = classes[cls]</span><br><span class="line">        score = top_conf[i]</span><br><span class="line"></span><br><span class="line">        left, top, right, bottom = boxes[i]</span><br><span class="line">        left = left - expand</span><br><span class="line">        top = top - expand</span><br><span class="line">        right = right + expand</span><br><span class="line">        bottom = bottom + expand</span><br><span class="line"></span><br><span class="line">        left = max(0, np.floor(left + 0.5).astype('int32'))</span><br><span class="line">        top = max(0, np.floor(top + 0.5).astype('int32'))</span><br><span class="line">        right = min(img_size[1], np.floor(right + 0.5).astype('int32'))</span><br><span class="line">        bottom = min(img_size[0], np.floor(bottom + 0.5).astype('int32'))</span><br><span class="line"></span><br><span class="line">        # 画框</span><br><span class="line">        label = '{} {:.2f}'.format(predicted_class, score)</span><br><span class="line"></span><br><span class="line">        cv.rectangle(test_img, (left, top), (right, bottom), colors[cls], 1)</span><br><span class="line">        cv.putText(test_img, label, (left, top - int(label_size * 10)), font, label_size, colors[cls], 1)</span><br><span class="line">    return test_img</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    neg_pos_ratio = 3</span><br><span class="line">    # 包括背景的类别数目</span><br><span class="line">    num_class = 4</span><br><span class="line">    train_data = list(range(800))</span><br><span class="line">    validation_data = list(range(800, 900))</span><br><span class="line">    test_data = range(900, 1000)</span><br><span class="line">    epochs = 50</span><br><span class="line">    batch_size = 8</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    img_size = (128, 128)</span><br><span class="line">    classes = ["circle", "triangle", "square"]</span><br><span class="line">    # 每个特征图上每个像素先验框的个数</span><br><span class="line">    prior = [2, 2, 2]</span><br><span class="line">    # 特征图的大小</span><br><span class="line">    feature_map = [32, 16, 8]</span><br><span class="line">    # anchor的长宽</span><br><span class="line">    anchors = [[(4, 4), (8, 8)], [(16, 16), (24, 24)], [(36, 36), (64, 64)]]</span><br><span class="line">    # 先验框的个数</span><br><span class="line">    num_prior = sum([prior[x] * feature_map[x] ** 2 for x in range(len(prior))])</span><br><span class="line">    # 获取所有先验框</span><br><span class="line">    prior_center_wh = []</span><br><span class="line">    prior_lt_rb = []</span><br><span class="line">    feature_shape = []</span><br><span class="line">    for i in range(len(prior)):</span><br><span class="line">        c_wh, tl_br = get_prior(i + 1)</span><br><span class="line">        prior_center_wh.append(c_wh)</span><br><span class="line">        prior_lt_rb.append(tl_br)</span><br><span class="line">        feature_shape.append(np.broadcast_to(feature_map[i], (feature_map[i] ** 2 * prior[i], 2)))</span><br><span class="line">    # 1008 * 4</span><br><span class="line">    prior_center_wh = np.vstack(prior_center_wh)</span><br><span class="line">    # 1008 * 4</span><br><span class="line">    prior_lt_rb = np.vstack(prior_lt_rb)</span><br><span class="line">    # 1008 * 2</span><br><span class="line">    feature_shape = np.vstack(feature_shape)</span><br><span class="line">    # IOU超过0.5的视为正样本</span><br><span class="line">    overlap_threshold = 0.3</span><br><span class="line">    # 编码函数</span><br><span class="line">    f_encode = encoder</span><br><span class="line">    # 画框设置不同的颜色</span><br><span class="line">    hsv_tuples = [(x / (num_class - 1), 1., 1.) for x in range(num_class - 1)]</span><br><span class="line">    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))</span><br><span class="line">    colors = list(map(lambda x: (int(x[1] * 255), int(x[2] * 255), int(x[0] * 255)), colors))</span><br><span class="line">    # 设置图像检测最多的框数目</span><br><span class="line">    keep_top_k = 5</span><br><span class="line">    # 设置检测置信度，大于该值认为有物体</span><br><span class="line">    confidence_threshold = 0.5</span><br><span class="line">    # 非极大值抑制阈值，重叠度不得大于该值</span><br><span class="line">    nms_thresh = 0.5</span><br><span class="line">    # 预测框不要紧贴物体，向外扩展像素大小</span><br><span class="line">    expand = 5</span><br><span class="line">    # 标签大小</span><br><span class="line">    label_size = 0.3</span><br><span class="line"></span><br><span class="line">    imgs_path = r'.\shape\train_imgs'</span><br><span class="line">    annotations_path = r'.\shape\annotations'</span><br><span class="line">    test_path = r'.\shape\test_imgs'</span><br><span class="line">    save_path = r'.\Yolo_V3_test_result'</span><br><span class="line">    weight_path = r'.\Yolo_V3_weight'</span><br><span class="line">    bbox_path = r'.\shape\bbox.txt'</span><br><span class="line"></span><br><span class="line">    # 将xml存储的bbox转换为bbox.txt文件，内容为file_path + bbox + class_id</span><br><span class="line">    if 'bbox.txt' not in os.listdir(r'.\shape'):</span><br><span class="line">        get_bbox(train_data + validation_data, bbox_path, annotations_path)</span><br><span class="line"></span><br><span class="line">    with open(bbox_path, 'r') as f:</span><br><span class="line">        bounding_info = f.readlines()</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        os.mkdir(save_path)</span><br><span class="line">    except FileExistsError:</span><br><span class="line">        print(save_path + 'has been exist')</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        os.mkdir(weight_path)</span><br><span class="line">    except FileExistsError:</span><br><span class="line">        print(weight_path + 'has been exist')</span><br><span class="line"></span><br><span class="line">    model = small_yolo_v3(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(batch_size, img_size[0], img_size[1], 3))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    optimizor = keras.optimizers.Adam(lr=1e-4)</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=optimizor, loss=compute_loss)</span><br><span class="line"></span><br><span class="line">    # 保存的方式，3世代保存一次</span><br><span class="line">    checkpoint_period = keras.callbacks.ModelCheckpoint(</span><br><span class="line">        weight_path + '\\' + 'ep{epoch:03d}-loss{loss:.3f}.h5',</span><br><span class="line">        monitor='loss',</span><br><span class="line">        save_weights_only=True,</span><br><span class="line">        save_best_only=True,</span><br><span class="line">        period=3</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 学习率下降的方式，val_loss3次不下降就下降学习率继续训练</span><br><span class="line">    reduce_lr = keras.callbacks.ReduceLROnPlateau(</span><br><span class="line">        monitor='loss',</span><br><span class="line">        factor=0.5,</span><br><span class="line">        patience=3,</span><br><span class="line">        verbose=1</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 是否需要早停，当val_loss一直不下降的时候意味着模型基本训练完毕，可以停止</span><br><span class="line">    early_stopping = keras.callbacks.EarlyStopping(</span><br><span class="line">        monitor='loss',</span><br><span class="line">        min_delta=0,</span><br><span class="line">        patience=10,</span><br><span class="line">        verbose=1</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    model.fit_generator(generate_arrays_from_file(train_data, batch_size),</span><br><span class="line">                        steps_per_epoch=max(1, len(train_data) // batch_size),</span><br><span class="line">                        validation_data=generate_arrays_from_file(validation_data, batch_size),</span><br><span class="line">                        validation_steps=max(1, len(validation_data) // batch_size),</span><br><span class="line">                        epochs=epochs,</span><br><span class="line">                        callbacks=[checkpoint_period, reduce_lr, early_stopping])</span><br><span class="line"></span><br><span class="line">    for name in test_data:</span><br><span class="line">        test_img_path = test_path + '\\' + str(name) + '.jpg'</span><br><span class="line">        save_img_path = save_path + '\\' + str(name) + '.png'</span><br><span class="line">        test_img = detect_image(test_img_path)</span><br><span class="line">        cv.imwrite(save_img_path, test_img)</span><br></pre></td></tr></tbody></table></figure><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Object_detection/YOLO-V3_T.png" alt="YOLO-V3"></p><h1 id="YOLO-V3小结"><a href="#YOLO-V3小结" class="headerlink" title="YOLO-V3小结"></a><font size="5" color="red">YOLO-V3小结</font></h1><p>  YOLO-V3是一种简单的目标检测网络，从上图可以看出YOLO-V3模型的参数量有62M，由于其<strong>结构简单，效果稳定</strong>，因此很多场合仍然使用YOLO-V3作为目标检测算法。YOLO-V3作为一步法目标检测的元老级模型，是小伙伴们需要掌握的一个模型。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;YOLO-V3&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>BigGAN</title>
    <link href="https://USTCcoder.github.io/2020/06/20/generative_adversarial%20BigGAN/"/>
    <id>https://USTCcoder.github.io/2020/06/20/generative_adversarial BigGAN/</id>
    <published>2020-06-20T07:12:13.000Z</published>
    <updated>2020-06-17T09:40:25.485Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">BigGAN</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>BigGAN</strong>:于<strong>2019年</strong>发表于<strong>ICLR</strong>，<strong>被誉为史上最强GAN图像生成器，BigGAN为什么那么强，因为引入了大量的黑科技，目前GitHub上面都是基于Pytorch的实现，而且代码特别繁琐，这里向小伙伴们介绍我的TensorFlow2.0简易版实现</strong>。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/biggan.png" alt="biggan"></p><h1 id="BigGAN的特点"><a href="#BigGAN的特点" class="headerlink" title="BigGAN的特点"></a><font size="5" color="red">BigGAN的特点</font></h1><p>  <font size="3"><strong>BigGAN可以认为SNGAN和SAGAN的结合，为了解决WGAN中的1-Lipshcitz问题，BigGAN在生成器和判别器中都借鉴了SNGAN中的Spectral Normalization(频谱归一化)的思想，而且借鉴了SAGAN的Self-Attention(注意力机制)</strong>。</font><br>  <font size="3"><strong>Truncation Trick(截断技巧)，将噪声向量进行截断，可以提高样本的质量，但是降低了样本的多样性</strong>。</font><br>  <font size="3"><strong>Orthogonal Regularization(正交正则化)，可以降低权重系数之间的干扰</strong>。</font><br>  <font size="3"><strong>Class-Conditional-BatchNorm(类条件批归一化)，在归一化时引入分类信息，可以生成指定类型的图像</strong>。</font><br>  <font size="3"><strong>Hierarchical latent spaces(分层潜在空间)，输入噪声分布在网络的各个层，并不只作用于第一层</strong>。</font><br>  <font size="3"><strong>使用了ResNet网络结构，其中有输入和输出尺寸相同的Resblock层，输入尺寸宽高缩小两倍的Resblock_down层和输入尺寸宽高增大两倍的Resblock_up层</strong>。</font><br>  <font size="3"><strong>batch大，参数量大，训练时间长，在这里我只展示网络结构和一些细节，训练过程我就跳过了</strong>。</font></p><h1 id="128x128网络结构"><a href="#128x128网络结构" class="headerlink" title="128x128网络结构"></a><font size="5" color="red">128x128网络结构</font></h1><p><img src="/images/Generative_adversarial/biggan_S.png" alt="BigGAN"></p><h1 id="BigGAN图像分析"><a href="#BigGAN图像分析" class="headerlink" title="BigGAN图像分析"></a><font size="5" color="red">BigGAN图像分析</font></h1><p><img src="/images/Generative_adversarial/BigGAN-generator.png" alt="generator"><br><img src="/images/Generative_adversarial/BigGAN-discriminator.png" alt="discriminator"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">try:</span><br><span class="line">    import tensorflow.python.keras as keras</span><br><span class="line">except:</span><br><span class="line">    import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def orthogonal_regularizer(scale):</span><br><span class="line"></span><br><span class="line">    def ortho_reg(w):</span><br><span class="line">        shape = w.get_shape().as_list()</span><br><span class="line">        c = shape[-1]</span><br><span class="line"></span><br><span class="line">        w = tf.reshape(w, [-1, c])</span><br><span class="line"></span><br><span class="line">        identity = tf.eye(c)</span><br><span class="line"></span><br><span class="line">        w_transpose = tf.transpose(w)</span><br><span class="line">        w_mul = tf.matmul(w_transpose, w)</span><br><span class="line">        reg = tf.subtract(w_mul, identity)</span><br><span class="line"></span><br><span class="line">        ortho_loss = tf.nn.l2_loss(reg)</span><br><span class="line"></span><br><span class="line">        return scale * ortho_loss</span><br><span class="line"></span><br><span class="line">    return ortho_reg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def orthogonal_regularizer_fully(scale):</span><br><span class="line"></span><br><span class="line">    def ortho_reg_fully(w):</span><br><span class="line">        _, c = w.get_shape().as_list()</span><br><span class="line"></span><br><span class="line">        identity = tf.eye(c)</span><br><span class="line">        w_transpose = tf.transpose(w)</span><br><span class="line">        w_mul = tf.matmul(w_transpose, w)</span><br><span class="line">        reg = tf.subtract(w_mul, identity)</span><br><span class="line"></span><br><span class="line">        ortho_loss = tf.nn.l2_loss(reg)</span><br><span class="line"></span><br><span class="line">        return scale * ortho_loss</span><br><span class="line"></span><br><span class="line">    return ortho_reg_fully</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class SpectralNorm(keras.layers.Layer):</span><br><span class="line">    def __init__(self, iteration=1, **kwargs):</span><br><span class="line">        super(SpectralNorm, self).__init__(**kwargs, dynamic=True)</span><br><span class="line">        self.iteration = iteration</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        self.u = self.add_variable(shape=[1, input_shape[-1]],</span><br><span class="line">                                   initializer=tf.initializers.TruncatedNormal(1.),</span><br><span class="line">                                   trainable=False)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        shape = tf.shape(inputs)</span><br><span class="line">        w = tf.reshape(inputs, shape=[-1, shape[-1]])</span><br><span class="line">        u_hat = self.u</span><br><span class="line">        for i in range(self.iteration):</span><br><span class="line">            v_hat = tf.nn.l2_normalize(tf.matmul(u_hat, tf.transpose(w)))</span><br><span class="line">            u_hat = tf.nn.l2_normalize(tf.matmul(v_hat, w))</span><br><span class="line"></span><br><span class="line">        u_hat = tf.stop_gradient(u_hat)</span><br><span class="line">        v_hat = tf.stop_gradient(v_hat)</span><br><span class="line"></span><br><span class="line">        sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))</span><br><span class="line">        with tf.control_dependencies([self.u.assign(u_hat)]):</span><br><span class="line">            w_norm = w / sigma</span><br><span class="line">            w_norm = tf.reshape(w_norm, inputs.get_shape())</span><br><span class="line">        return w_norm</span><br><span class="line"></span><br><span class="line">    def compute_output_shape(self, input_shape):</span><br><span class="line"></span><br><span class="line">        return input_shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ClassConditionalBatchNorm(keras.layers.Layer):</span><br><span class="line"></span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        super(ClassConditionalBatchNorm, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        self.beta_dense = keras.layers.Dense(units=input_shape[0][-1])</span><br><span class="line">        self.gamma_dense = keras.layers.Dense(units=input_shape[0][-1])</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, is_training=True):</span><br><span class="line"></span><br><span class="line">        x, condition = inputs</span><br><span class="line">        #</span><br><span class="line">        split = keras.layers.Flatten()(condition)</span><br><span class="line">        beta = self.beta_dense(split)</span><br><span class="line">        gamma = self.gamma_dense(split)</span><br><span class="line"></span><br><span class="line">        beta = tf.reshape(beta, shape=[-1, 1, 1, x.shape[-1]])</span><br><span class="line">        gamma = tf.reshape(gamma, shape=[-1, 1, 1, x.shape[-1]])</span><br><span class="line"></span><br><span class="line">        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], keepdims=True)</span><br><span class="line"></span><br><span class="line">        return (x - batch_mean) / batch_var * gamma + beta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyConv(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(MyConv, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.filters = filters</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">        self.strides = strides</span><br><span class="line">        self.padding = padding</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        self.w = self.add_weight(name='kernel',</span><br><span class="line">                                 shape=(self.kernel_size, self.kernel_size, input_shape[-1], self.filters),</span><br><span class="line">                                 initializer=weight_init, regularizer=weight_regularizer)</span><br><span class="line">        self.b = self.add_weight(name='bias', shape=(self.filters,), initializer=keras.initializers.Zeros())</span><br><span class="line"></span><br><span class="line">        if self._name.find('sn') != -1:</span><br><span class="line">            self.u = self.add_weight(shape=[1, self.w.shape[-1]], initializer=tf.initializers.TruncatedNormal(1.))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        if self._name.find('sn') != -1:</span><br><span class="line">            shape = tf.shape(self.w)</span><br><span class="line">            w = tf.reshape(self.w, shape=[-1, shape[-1]])</span><br><span class="line">            u_hat = self.u</span><br><span class="line">            for i in range(1):</span><br><span class="line">                v_hat = tf.nn.l2_normalize(tf.matmul(u_hat, tf.transpose(w)))</span><br><span class="line">                u_hat = tf.nn.l2_normalize(tf.matmul(v_hat, w))</span><br><span class="line"></span><br><span class="line">            sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))</span><br><span class="line"></span><br><span class="line">            with tf.control_dependencies([self.u.assign(u_hat)]):</span><br><span class="line">                w_norm = w / sigma</span><br><span class="line">                w_norm = tf.reshape(w_norm, self.w.get_shape())</span><br><span class="line"></span><br><span class="line">            return tf.nn.bias_add(tf.nn.conv2d(inputs, w_norm, (1, self.strides, self.strides, 1), self.padding), self.b)</span><br><span class="line"></span><br><span class="line">        return tf.nn.bias_add(tf.nn.conv2d(inputs, self.w, (1, self.strides, self.strides, 1), self.padding), self.b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyDense(keras.layers.Layer):</span><br><span class="line">    def __init__(self, units, name):</span><br><span class="line">        super(MyDense, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.units = units</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        self.w = self.add_weight(name='kernel', shape=(input_shape[-1], self.units),</span><br><span class="line">                                 initializer=weight_init, regularizer=weight_regularizer_fully)</span><br><span class="line">        self.b = self.add_weight(name='bias', shape=(self.units,), initializer=keras.initializers.Zeros())</span><br><span class="line"></span><br><span class="line">        if self._name.find('sn') != -1:</span><br><span class="line">            self.u = self.add_weight(shape=[1, self.w.shape[-1]], initializer=tf.initializers.TruncatedNormal(1.))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        if self._name.find('sn') != -1:</span><br><span class="line">            shape = tf.shape(self.w)</span><br><span class="line">            w = tf.reshape(self.w, shape=[-1, shape[-1]])</span><br><span class="line">            u_hat = self.u</span><br><span class="line">            for i in range(1):</span><br><span class="line">                v_hat = tf.nn.l2_normalize(tf.matmul(u_hat, tf.transpose(w)))</span><br><span class="line">                u_hat = tf.nn.l2_normalize(tf.matmul(v_hat, w))</span><br><span class="line"></span><br><span class="line">            sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))</span><br><span class="line"></span><br><span class="line">            with tf.control_dependencies([self.u.assign(u_hat)]):</span><br><span class="line">                w_norm = w / sigma</span><br><span class="line">                w_norm = tf.reshape(w_norm, self.w.get_shape())</span><br><span class="line"></span><br><span class="line">            return tf.matmul(inputs, w_norm) + self.b</span><br><span class="line"></span><br><span class="line">        return tf.matmul(inputs, self.w) + self.b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Resblock(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, name):</span><br><span class="line">        super(Resblock, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential([MyConv(filters, 3, 1, 'SAME', name='{}_part1_snconv1'.format(name)),</span><br><span class="line">                                       keras.layers.BatchNormalization(momentum=0.8, name='{}_bn1'.format(name)),</span><br><span class="line">                                       keras.layers.ReLU(name='{}_relu'.format(name)),</span><br><span class="line">                                       MyConv(filters, 3, 1, 'SAME', name='{}_part1_snconv2'.format(name)),</span><br><span class="line">                                       keras.layers.BatchNormalization(momentum=0.8, name='{}_bn2'.format(name))])</span><br><span class="line">        self.add = keras.layers.Add(name='{}_add'.format(name))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        x = self.block(inputs)</span><br><span class="line">        output = self.add([x, inputs])</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Resblock_Down(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, name):</span><br><span class="line">        super(Resblock_Down, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block1 = keras.Sequential([keras.layers.ReLU(name='{}_part1_relu1'.format(name)),</span><br><span class="line">                                        MyConv(filters, 3, 1, 'SAME', name='{}_part1_snconv1'.format(name)),</span><br><span class="line">                                        keras.layers.ReLU(name='{}_part1_relu2'.format(name)),</span><br><span class="line">                                        MyConv(filters, 3, 1, 'SAME', name='{}_part1_snconv2'.format(name)),</span><br><span class="line">                                        keras.layers.AveragePooling2D((2, 2), name='{}_part1_averagepool'.format(name))])</span><br><span class="line"></span><br><span class="line">        self.block2 = keras.Sequential([MyConv(filters, 1, 1, 'SAME', name='{}_part2_snconv'.format(name)),</span><br><span class="line">                                        keras.layers.AveragePooling2D((2, 2), name='{}_part2_averagepool'.format(name))])</span><br><span class="line"></span><br><span class="line">        self.add = keras.layers.Add(name='{}_add'.format(name))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        x1 = self.block1(inputs)</span><br><span class="line">        x2 = self.block2(inputs)</span><br><span class="line">        output = self.add([x1, x2])</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Resblock_Up(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, name):</span><br><span class="line">        super(Resblock_Up, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.cbn1 = ClassConditionalBatchNorm(name='{}_part1_cbn1'.format(name))</span><br><span class="line">        self.cbn2 = ClassConditionalBatchNorm(name='{}_part1_cbn2'.format(name))</span><br><span class="line">        self.block1_1 = keras.Sequential([keras.layers.ReLU(name='{}_part1_relu1'.format(name)),</span><br><span class="line">                                          keras.layers.UpSampling2D((2, 2), name='{}_part1_upsampling'.format(name)),</span><br><span class="line">                                          MyConv(filters, 3, 1, 'SAME', name='{}_part1_snconv1'.format(name))])</span><br><span class="line"></span><br><span class="line">        self.block1_2 = keras.Sequential([keras.layers.ReLU(name='{}_part1_relu2'.format(name)),</span><br><span class="line">                                          MyConv(filters, 3, 1, 'SAME', name='{}_part1_snconv2'.format(name))])</span><br><span class="line"></span><br><span class="line">        self.block2 = keras.Sequential([keras.layers.UpSampling2D((2, 2), name='{}_part2_upsampling'.format(name)),</span><br><span class="line">                                        MyConv(filters, 1, 1, 'SAME', name='{}_part2_snconv1'.format(name))])</span><br><span class="line"></span><br><span class="line">        self.add = keras.layers.Add(name='{}_add'.format(name))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        x, z = inputs</span><br><span class="line">        x1 = self.cbn1([x, z])</span><br><span class="line">        x1 = self.block1_1(x1)</span><br><span class="line">        x1 = self.cbn2([x1, z])</span><br><span class="line">        x1 = self.block1_2(x1)</span><br><span class="line">        x2 = self.block2(x)</span><br><span class="line">        output = self.add([x1, x2])</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class SAblock(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, name):</span><br><span class="line">        super(SAblock, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.theta = MyConv(filters // 8, 1, 1, 'SAME', name='{}_theta'.format(name))</span><br><span class="line">        self.phi = MyConv(filters // 8, 1, 1, 'SAME', name='{}_phi'.format(name))</span><br><span class="line">        self.g = MyConv(filters, 1, 1, 'SAME', name='{}_g'.format(name))</span><br><span class="line">        self.o = MyConv(filters, 1, 1, 'SAME', name='{}_conv4'.format(name))</span><br><span class="line">        self.gamma = tf.Variable([0.])</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        theta = self.theta(inputs)</span><br><span class="line">        theta = tf.reshape(theta, (-1, theta.shape[1] * theta.shape[2], theta.shape[-1]), name='{}_theta_reshape'.format(self._name))</span><br><span class="line">        phi = self.phi(inputs)</span><br><span class="line">        phi = tf.reshape(phi, (-1, phi.shape[1] * phi.shape[2], phi.shape[-1]), name='{}_phi_reshape'.format(self._name))</span><br><span class="line">        g = self.g(inputs)</span><br><span class="line">        g = tf.reshape(g, (-1, g.shape[1] * g.shape[2], g.shape[-1]), name='{}_g_reshape'.format(self._name))</span><br><span class="line">        theta_phi = tf.matmul(theta, phi, transpose_b=True, name='{}_theta_dot_phi'.format(self._name))</span><br><span class="line">        theta_phi = tf.nn.softmax(theta_phi, name='{}_softmax'.format(self.name))</span><br><span class="line">        theta_phi_g = tf.matmul(theta_phi, g, name='{}_theta_phi_dot_g'.format(self._name))</span><br><span class="line">        theta_phi_g = tf.reshape(theta_phi_g, shape=(-1, inputs.shape[1], inputs.shape[2], inputs.shape[3]), name='{}_theta_phi_g_reshape'.format(self._name))</span><br><span class="line">        o = self.o(theta_phi_g)</span><br><span class="line"></span><br><span class="line">        return o * self.gamma + inputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generator(input_shape_noise, input_shape_label):</span><br><span class="line">    input_tensor_noise = keras.layers.Input(input_shape_noise, name='input_noise')</span><br><span class="line">    input_tensor_label = keras.layers.Input(input_shape_label, name='input_label')</span><br><span class="line"></span><br><span class="line">    embedding_tensor = compose(keras.layers.Embedding(1000, 120, name='embedding'),</span><br><span class="line">                               keras.layers.Flatten(name='flatten'))(input_tensor_label)</span><br><span class="line"></span><br><span class="line">    noise_split = tf.split(input_tensor_noise, 6, -1, name='split')</span><br><span class="line">    for i in range(1, 6):</span><br><span class="line">        noise_split[i] = keras.layers.Concatenate(name='concatenate{}'.format(i + 1))([noise_split[i], embedding_tensor])</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Dense(1024 * 16, activation='relu', name='dense_relu'),</span><br><span class="line">                keras.layers.Reshape((4, 4, 1024), name='reshape'))(noise_split[0])</span><br><span class="line">    x = Resblock_Up(1024, name='resblockup1')([x, noise_split[1]])</span><br><span class="line">    x = Resblock_Up(512, name='resblockup2')([x, noise_split[2]])</span><br><span class="line">    x = Resblock_Up(256, name='resblockup3')([x, noise_split[3]])</span><br><span class="line">    x = Resblock_Up(128, name='resblockup4')([x, noise_split[4]])</span><br><span class="line">    x = SAblock(128, name='sablock')(x)</span><br><span class="line">    x = Resblock_Up(64, name='resblockup5')([x, noise_split[5]])</span><br><span class="line">    x = compose(keras.layers.BatchNormalization(momentum=0.8, name='bn'),</span><br><span class="line">                keras.layers.ReLU(name='relu'),</span><br><span class="line">                MyConv(3, 3, 1, 'SAME', name='conv'),</span><br><span class="line">                keras.layers.Activation('tanh', name='tanh'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model([input_tensor_noise, input_tensor_label], x, name='BigGAN-Generator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discriminator(input_shape_image, input_shape_label):</span><br><span class="line">    input_tensor_image = keras.layers.Input(input_shape_image, name='input_image')</span><br><span class="line">    input_tensor_label = keras.layers.Input(input_shape_label, name='input_label')</span><br><span class="line"></span><br><span class="line">    x = compose(Resblock_Down(64, name='resblockdown1'),</span><br><span class="line">                SAblock(64, name='sablock'),</span><br><span class="line">                Resblock_Down(128, name='resblockdown2'),</span><br><span class="line">                Resblock_Down(256, name='resblockdown3'),</span><br><span class="line">                Resblock_Down(512, name='resblockdown4'),</span><br><span class="line">                Resblock_Down(1024, name='resblockdown5'),</span><br><span class="line">                Resblock(1024, name='resblock6'))(input_tensor_image)</span><br><span class="line"></span><br><span class="line">    x = tf.reduce_sum(x, axis=[1, 2], name='global_sumpool')</span><br><span class="line">    output_tensor = keras.layers.Dense(1, name='dense')(x)</span><br><span class="line"></span><br><span class="line">    embedding_tensor = compose(keras.layers.Embedding(1000, 1024, name='embedding'),</span><br><span class="line">                               keras.layers.Flatten(name='flatten'))(input_tensor_label)</span><br><span class="line">    output_tensor = output_tensor + tf.reduce_sum(embedding_tensor * x, 1, keepdims=True, name='reduce_sum')</span><br><span class="line"></span><br><span class="line">    model = keras.Model([input_tensor_image, input_tensor_label], output_tensor, name='BigGAN-Discriminator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def biggan(input_shape_noise, input_shape_image, input_shape_label, model_g, model_d):</span><br><span class="line">    input_noise = keras.layers.Input(input_shape_noise, name='input_noise')</span><br><span class="line">    input_real_image = keras.layers.Input(input_shape_image, name='input_image')</span><br><span class="line">    input_label = keras.layers.Input(input_shape_label, name='input_label')</span><br><span class="line"></span><br><span class="line">    model_g.trainable = False</span><br><span class="line">    fake = model_g([input_noise, input_label])</span><br><span class="line">    real_conf = model_d([input_real_image, input_label])</span><br><span class="line">    fake_conf = model_d([fake, input_label])</span><br><span class="line"></span><br><span class="line">    model_discriminator = keras.Model([input_noise, input_real_image, input_label], [real_conf, fake_conf], name='BigGAN-discriminator')</span><br><span class="line">    model_discriminator.compile(optimizer=optimizer_d, loss=[d_loss, d_loss], loss_weights=[1, 1])</span><br><span class="line"></span><br><span class="line">    model_g.trainable = True</span><br><span class="line">    model_d.trainable = False</span><br><span class="line"></span><br><span class="line">    model_generator = keras.Model([input_noise, input_label], fake_conf, name='BigGAN-generator')</span><br><span class="line">    model_generator.compile(optimizer=optimizer_g, loss=g_loss)</span><br><span class="line"></span><br><span class="line">    return model_generator, model_discriminator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def d_loss(y_true, y_pred):</span><br><span class="line"></span><br><span class="line">    return tf.reduce_mean(tf.nn.relu(1 - y_true * y_pred))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def g_loss(y_true, y_pred):</span><br><span class="line"></span><br><span class="line">    return -tf.reduce_mean(y_pred)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    weight_init = tf.initializers.TruncatedNormal(mean=0.0, stddev=0.02)</span><br><span class="line">    weight_regularizer = orthogonal_regularizer(0.0001)</span><br><span class="line">    weight_regularizer_fully = orthogonal_regularizer_fully(0.0001)</span><br><span class="line"></span><br><span class="line">    optimizer_g = keras.optimizers.Adam(0.00005, 0, 0.999, epsilon=1e-5)</span><br><span class="line">    optimizer_d = keras.optimizers.Adam(0.0002, 0, 0.999, epsilon=1e-5)</span><br><span class="line"></span><br><span class="line">    model_d = discriminator(input_shape_image=(128, 128, 3), input_shape_label=(1,))</span><br><span class="line"></span><br><span class="line">    model_g = generator(input_shape_noise=(120,), input_shape_label=(1,))</span><br><span class="line"></span><br><span class="line">    model_g.build(input_shape=[(120,), (1,)])</span><br><span class="line">    model_g.summary()</span><br><span class="line">    keras.utils.plot_model(model_g, 'BigGAN-generator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_d.build(input_shape=[(128, 128, 3), (1,)])</span><br><span class="line">    model_d.summary()</span><br><span class="line">    keras.utils.plot_model(model_d, 'BigGAN-discriminator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_generator, model_discriminator = biggan(input_shape_noise=(120,), input_shape_image=(128, 128, 3), input_shape_label=(1,), model_g=model_g, model_d=model_d)</span><br><span class="line"></span><br><span class="line">    model_generator.build(input_shape=[(120,), (1,)])</span><br><span class="line">    model_generator.summary()</span><br><span class="line">    keras.utils.plot_model(model_generator, 'BigGAN-generate.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_discriminator.build(input_shape=[(120,), (128, 128, 3), (1,)])</span><br><span class="line">    model_discriminator.summary()</span><br><span class="line">    keras.utils.plot_model(model_discriminator, 'BigGAN-discriminate.png', show_shapes=True, show_layer_names=True)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Generative_adversarial/biggan_R.png" alt="biggan"></p><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Generative_adversarial/biggan_T.png" alt="biggan"></p><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>可以设置一些<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li><strong>BigGAN对于网络结构，优化器参数，网络层的一些超参数都是非常敏感的，效果不好不容易发现原因，这可能需要较多的工程实践经验</strong>。</li><li><strong>先创建判别器，然后进行compile，这样判别器就固定了，然后创建生成器时，不要训练判别器，需要将判别器的trainable改成False，此时不会影响之前固定的判别器</strong>，这个可以<strong>通过模型的_collection_collected_trainable_weights属性查看</strong>，如果该属性为空，则模型不训练，否则模型可以训练，compile之后，该属性固定，无论后面如何修改trainable，只要不重新compile，都不影响训练。</li><li>代码中正交正则化使用了闭包的概念，有关闭包的使用，可以参考我的另一篇博客，Closure &amp; Decorators(闭包和装饰器)</li><li><strong>这个模型效果太好，生成的图片甚至比真实图片还要好，一些纹理，背景细节都可以完美呈现，但是想自己实现训练过程，非常困难，因此建议小伙伴了解就可以，不用亲自实践</strong>。</li></ol><h1 id="BigGAN小结"><a href="#BigGAN小结" class="headerlink" title="BigGAN小结"></a><font size="5" color="red">BigGAN小结</font></h1><p>  <strong>BigGAN分为很多版本，有128的图像版本，256的图像版本和512的图像版本，具体模型结构都很类似，但是参数量指数级增长</strong>。这是最小的BigGAN版本，<strong>参数量都可以达到80M，虽然VGG16的参数量有一亿多，但是网络结构简单，因此训练反而快，而BigGAN含有很多细节操作，会花费较长的时间，因此训练起来非常慢</strong>，BigGAN不是单打独斗，在特点中已经分析了，可以看成<strong>SNGAN和SAGAN的共同作品</strong>，因此关于其中的数学推导可以参考网络的其他资源，在这里也不过多赘述，作为史上最强的GAN图像生成器，小伙伴们一定要了解它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;BigGAN&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>最佳观光组合(Leetcode 1014)</title>
    <link href="https://USTCcoder.github.io/2020/06/17/program%20Leetcode1014/"/>
    <id>https://USTCcoder.github.io/2020/06/17/program Leetcode1014/</id>
    <published>2020-06-17T10:44:37.000Z</published>
    <updated>2020-09-02T02:18:57.885Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode1014.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这题思路明显，但是并不简单，暴力法没有难度，但是无法通过所有样例，要寻找一种时间复杂度为O(n)的算法解题。</p><a id="more"></a><h1 id="暴力法"><a href="#暴力法" class="headerlink" title="暴力法"></a><font size="5" color="red">暴力法</font></h1><p>暴力法可以根据题目中给出的公式进行直接求解，<strong>遍历所有的景点对，两层循环，时间复杂度为$O(n^2)$，空间复杂度为$O(1)$，在这里给出了一种更加Pythonic的写法，一行代码实现</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def maxScoreSightseeingPair(self, A):</span><br><span class="line">        """</span><br><span class="line">        :type A: List[int]</span><br><span class="line">        :rtype: int</span><br><span class="line">        """</span><br><span class="line">        return max([A[i] + A[j] + i - j for i in range(len(A)) for j in range(i + 1, len(A))])</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a><font size="5" color="red">动态规划</font></h1><p>上面的暴力法虽然简单，容易想到，但是无法通过所有样例，因此我们需要降低时间复杂度，发现在比较中浪费了大量的计算资源，每一个点都需要比较n次，所以时间复杂度为$O(n^2)$，可不可以记录之前的状态，每个点比较一次呢？我们发现<strong>公式可以分为两个部分，一部分是A[i] + i，另一部分是A[j] - j，最终将两部分相加即可。可以推出状态转移方程</strong>。</p><script type="math/tex; mode=display">\begin{case} F = max(F, P + A[i] - i) \\ P = max(P, A[i] + i) \end{case}</script><p><strong>F为最佳观光组合，P为选择当前第i个点时，在i之前的所有点的最优选择，即上面的第一部分，记录A[i] + [i]的最大值为P，然后遍历所有点一次即可</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def maxScoreSightseeingPair(self, A):</span><br><span class="line">        """</span><br><span class="line">        :type A: List[int]</span><br><span class="line">        :rtype: int</span><br><span class="line">        """</span><br><span class="line">        pre_val = A[0]</span><br><span class="line">        res = 0</span><br><span class="line">        for i in range(1, len(A)):</span><br><span class="line">            res = max(res, pre_val + A[i] - i)</span><br><span class="line">            pre_val = max(pre_val, A[i] + i)</span><br><span class="line">        return res</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  思路简单的题目一般都会有更巧妙的解法，<strong>对于数学问题，可能利用数学公式推出巧妙解，这个难度太大，或者是利用单调栈进行求解，或者利用动态规划进行求解</strong>。数学问题一般描述简单，难度适中，适合面试考察，因此小伙伴们尤其要注意。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 1014&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>pix2pix</title>
    <link href="https://USTCcoder.github.io/2020/06/14/generative_adversarial%20pix2pix/"/>
    <id>https://USTCcoder.github.io/2020/06/14/generative_adversarial pix2pix/</id>
    <published>2020-06-14T06:07:21.000Z</published>
    <updated>2020-07-19T07:03:32.258Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">pix2pix</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>pix2pix</strong>:于<strong>2017年</strong>发表在<strong>CVPR</strong>上，可以实现图像的风格迁移，风格迁移是GAN网络提出后才出现在人们视野里面的图像处理算法，在生成式对抗网络问世之前，人们很难通过传统的图像处理算法实现风格迁移，今天带小伙伴们看一看瞧一瞧。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/pix2pix.png" alt="pix2pix"></p><h1 id="pix2pix的特点"><a href="#pix2pix的特点" class="headerlink" title="pix2pix的特点"></a><font size="5" color="red">pix2pix的特点</font></h1><p>  <font size="3"><strong>类似于半个DiscoGAN，只是从风格A转换到风格B，没有从风格B转换到风格A</strong>。</font><br>  <font size="3"><strong>网络结构也类似于DiscoGAN，使用了UNet结构</strong>。</font><br>  <font size="3"><strong>生成器损失函数采用绝对误差，判别器损失函数采用均方误差</strong>。</font><br>  <font size="3"><strong>对生成器损失函数的权重进行调节，使网络更多关注于生成的图像质量</strong>。</font></p><h1 id="pix2pix图像分析"><a href="#pix2pix图像分析" class="headerlink" title="pix2pix图像分析"></a><font size="5" color="red">pix2pix图像分析</font></h1><p><img src="/images/Generative_adversarial/pix2pix-generator.png" alt="generator"><br><img src="/images/Generative_adversarial/pix2pix-discriminator.png" alt="discriminator"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import glob</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Relu_In(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Relu_In, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential([keras.layers.Conv2D(filters, kernel_size, strides, padding),</span><br><span class="line">                                       keras.layers.LeakyReLU(0.2)])</span><br><span class="line">        if name.find('bn') != -1:</span><br><span class="line">            self.block.add(keras.layers.BatchNormalization(momentum=0.8))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.block(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Upsampling_Conv_Relu_In_Concatenate(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Upsampling_Conv_Relu_In_Concatenate, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential([keras.layers.UpSampling2D((2, 2)),</span><br><span class="line">                                       keras.layers.Conv2D(filters, kernel_size, strides, padding, activation='relu'),</span><br><span class="line">                                       keras.layers.BatchNormalization(momentum=0.8)])</span><br><span class="line">        self.concatenate = keras.layers.Concatenate()</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        x, shortcut = inputs</span><br><span class="line">        x = self.block(x)</span><br><span class="line">        output = self.concatenate([x, shortcut])</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generator(input_shape, name):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x1 = Conv_Relu_In(64, (4, 4), (2, 2), 'same', name='conv_leakyrelu1')(x)</span><br><span class="line">    x2 = Conv_Relu_In(128, (4, 4), (2, 2), 'same', name='conv_leakyrelu_bn2')(x1)</span><br><span class="line">    x3 = Conv_Relu_In(256, (4, 4), (2, 2), 'same', name='conv_leakyrelu_bn3')(x2)</span><br><span class="line">    x4 = Conv_Relu_In(512, (4, 4), (2, 2), 'same', name='conv_leakyrelu_bn4')(x3)</span><br><span class="line">    x5 = Conv_Relu_In(512, (4, 4), (2, 2), 'same', name='conv_leakyrelu_bn5')(x4)</span><br><span class="line">    x6 = Conv_Relu_In(512, (4, 4), (2, 2), 'same', name='conv_leakyrelu_bn6')(x5)</span><br><span class="line">    x7 = Conv_Relu_In(512, (4, 4), (2, 2), 'same', name='conv_leakyrelu_bn7')(x6)</span><br><span class="line"></span><br><span class="line">    y6 = Upsampling_Conv_Relu_In_Concatenate(512, (4, 4), (1, 1), 'same', name='upsampling_conv_relu_bn_concatenate1')([x7, x6])</span><br><span class="line">    y5 = Upsampling_Conv_Relu_In_Concatenate(512, (4, 4), (1, 1), 'same', name='upsampling_conv_relu_bn_concatenate2')([y6, x5])</span><br><span class="line">    y4 = Upsampling_Conv_Relu_In_Concatenate(512, (4, 4), (1, 1), 'same', name='upsampling_conv_relu_bn_concatenate3')([y5, x4])</span><br><span class="line">    y3 = Upsampling_Conv_Relu_In_Concatenate(256, (4, 4), (1, 1), 'same', name='upsampling_conv_relu_bn_concatenate4')([y4, x3])</span><br><span class="line">    y2 = Upsampling_Conv_Relu_In_Concatenate(128, (4, 4), (1, 1), 'same', name='upsampling_conv_relu_bn_concatenate5')([y3, x2])</span><br><span class="line">    y1 = Upsampling_Conv_Relu_In_Concatenate(64, (4, 4), (1, 1), 'same', name='upsampling_conv_relu_bn_concatenate6')([y2, x1])</span><br><span class="line"></span><br><span class="line">    y = compose(keras.layers.UpSampling2D((2, 2), name='upsampling'),</span><br><span class="line">                keras.layers.Conv2D(3, (4, 4), (1, 1), 'same', activation='tanh', name='conv_tanh'))(y1)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, y, name=name)</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discriminator(input_shape, name):</span><br><span class="line">    input_tensor1 = keras.layers.Input(input_shape, name='input1')</span><br><span class="line">    input_tensor2 = keras.layers.Input(input_shape, name='input2')</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='concatenate')([input_tensor1, input_tensor2])</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Relu_In(64, (4, 4), (2, 2), 'same', name='conv_leakyrelu1'),</span><br><span class="line">                Conv_Relu_In(128, (4, 4), (2, 2), 'same', name='conv_leakyrelu_bn2'),</span><br><span class="line">                Conv_Relu_In(256, (4, 4), (2, 2), 'same', name='conv_leakyrelu_bn3'),</span><br><span class="line">                Conv_Relu_In(512, (4, 4), (2, 2), 'same', name='conv_leakyrelu_bn4'),</span><br><span class="line">                keras.layers.Conv2D(1, (4, 4), (1, 1), 'same', name='conv'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model([input_tensor1, input_tensor2], x, name=name)</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def pix2pix(input_shapeA, input_shapeB, model_g, model_d):</span><br><span class="line">    input_tensorA = keras.layers.Input(input_shapeA, name='input_A')</span><br><span class="line">    input_tensorB = keras.layers.Input(input_shapeB, name='input_B')</span><br><span class="line"></span><br><span class="line">    # 输入风格B生成的风格A类型的图像</span><br><span class="line">    fake_A = model_g(input_tensorB)</span><br><span class="line"></span><br><span class="line">    model_d.trainable = False</span><br><span class="line"></span><br><span class="line">    conf_A = model_d([fake_A, input_tensorB])</span><br><span class="line"></span><br><span class="line">    model = keras.Model([input_tensorA, input_tensorB], [conf_A, fake_A], name='Pix2Pix')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def read_data(data_path, batch_size):</span><br><span class="line">    filename = glob.glob(data_path + '\\*.jpg')</span><br><span class="line">    choose_name = np.random.choice(filename, batch_size)</span><br><span class="line"></span><br><span class="line">    image_A, image_B = [], []</span><br><span class="line">    for i in range(batch_size):</span><br><span class="line">        image = cv.imread(choose_name[i]).astype(np.float32)</span><br><span class="line">        image_A.append(image[:, 256:, :])</span><br><span class="line">        image_B.append(image[:, :256, :])</span><br><span class="line"></span><br><span class="line">    image_A = np.array(image_A) / 127.5 - 1</span><br><span class="line">    image_B = np.array(image_B) / 127.5 - 1</span><br><span class="line"></span><br><span class="line">    return image_A, image_B</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    batch_size = 2</span><br><span class="line">    epochs = 2000</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    img_size = (256, 256)</span><br><span class="line">    data_path = r'.\edges2shoes\train'</span><br><span class="line">    save_path = r'.\pix2pix'</span><br><span class="line">    if not os.path.exists(save_path):</span><br><span class="line">        os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(0.0002, 0.5)</span><br><span class="line">    loss = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    real_dmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    fake_dmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    gmse = keras.metrics.MeanSquaredError()</span><br><span class="line"></span><br><span class="line">    model_d = discriminator(input_shape=(img_size[0], img_size[1], 3), name='pix2pix-Discriminator')</span><br><span class="line">    model_d.compile(optimizer=optimizer, loss='mse')</span><br><span class="line"></span><br><span class="line">    model_g = generator(input_shape=(img_size[0], img_size[1], 3), name='pix2pix-Generator')</span><br><span class="line"></span><br><span class="line">    model_g.build(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line">    model_g.summary()</span><br><span class="line">    keras.utils.plot_model(model_g, 'pix2pix-generator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_d.build(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line">    model_d.summary()</span><br><span class="line">    keras.utils.plot_model(model_d, 'pix2pix-discriminator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model = pix2pix(input_shapeA=(img_size[0], img_size[1], 3), input_shapeB=(img_size[0], img_size[1], 3), model_g=model_g, model_d=model_d)</span><br><span class="line">    model.compile(optimizer=optimizer, loss=['mse', 'mae'], loss_weights=[1, 100])</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=[(img_size[0], img_size[1], 3), (img_size[0], img_size[1], 3)])</span><br><span class="line">    model.summary()</span><br><span class="line">    keras.utils.plot_model(model, 'pix2pix.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        image_A, image_B = read_data(data_path, batch_size)</span><br><span class="line"></span><br><span class="line">        fake_A = model_g(image_B)</span><br><span class="line"></span><br><span class="line">        real_dmse(np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model_d([image_A, image_B]))</span><br><span class="line">        fake_dmse(np.zeros((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model_d([fake_A, image_B]))</span><br><span class="line">        gmse(np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model([image_A, image_B])[0])</span><br><span class="line"></span><br><span class="line">        real_dloss = model_d.train_on_batch([image_A, image_B], np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)))</span><br><span class="line">        fake_dloss = model_d.train_on_batch([fake_A, image_B], np.zeros((batch_size, img_size[0] // 16, img_size[1] // 16, 1)))</span><br><span class="line"></span><br><span class="line">        gloss = model.train_on_batch([image_A, image_B], [np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), image_A])</span><br><span class="line"></span><br><span class="line">        if epoch % 20 == 0:</span><br><span class="line">            print('epoch = {}, real_dmse = {}, fake_dmse = {}, gmse = {}'.format(epoch, real_dmse.result(), fake_dmse.result(), gmse.result()))</span><br><span class="line">            real_dmse.reset_states()</span><br><span class="line">            fake_dmse.reset_states()</span><br><span class="line">            gmse.reset_states()</span><br><span class="line">            image_A, image_B = read_data(data_path, batch_size=1)</span><br><span class="line">            fake_A = ((model_g(image_B).numpy().squeeze() + 1) * 127.5).astype(np.uint8)</span><br><span class="line">            image_A = ((image_A.squeeze() + 1) * 127.5).astype(np.uint8)</span><br><span class="line">            image_B = ((image_B.squeeze() + 1) * 127.5).astype(np.uint8)</span><br><span class="line">            cv.imwrite(save_path + '\\epoch{}.jpg'.format(epoch), np.concatenate([image_A, image_B, fake_A], axis=1))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Generative_adversarial/pix2pix_R.png" alt="pix2pix"></p><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Generative_adversarial/pix2pix_T.png" alt="pix2pix"></p><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>可以设置一些<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li><strong>pix2pix对于网络结构，优化器参数，网络层的一些超参数都是非常敏感的，效果不好不容易发现原因，这可能需要较多的工程实践经验</strong>。</li><li><strong>先创建判别器，然后进行compile，这样判别器就固定了，然后创建生成器时，不要训练判别器，需要将判别器的trainable改成False，此时不会影响之前固定的判别器</strong>，这个可以<strong>通过模型的_collection_collected_trainable_weights属性查看</strong>，如果该属性为空，则模型不训练，否则模型可以训练，compile之后，该属性固定，无论后面如何修改trainable，只要不重新compile，都不影响训练。</li><li>在pix2pix的测试图像中，为了体现模型的效果，第一个图片为风格A的鞋子，第二个图片为风格B的鞋子，第三个图片为由风格B生成的风格A的鞋子，这里只是训练了2000代，而且每一代只有2个图像就可以看出pix2pix的效果。小伙伴们可以选择更大的数据集，更加快速的GPU，训练更长的时间，这样风格迁移的效果就会更加明显。</li></ol><h1 id="pix2pix小结"><a href="#pix2pix小结" class="headerlink" title="pix2pix小结"></a><font size="5" color="red">pix2pix小结</font></h1><p>  pix2pix是一种有效的风格迁移生成式对抗网络，网络结构，损失函数都和DiscoGAN几乎相同，但是DiscoGAN的生成器和判别器都是两个，可以实现AB风格的互换，而pix2pix只有一个生成器和判别器，因此只能完成风格的单向转换，因此参数量也是DiscoGAN的一半，从上图可以看出<strong>pix2pix模型的参数量只有43M</strong>，如果数据集足够，还可以生成人物表情包，是不是非常有趣呢？小伙伴们一定要掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;pix2pix&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>DiscoGAN</title>
    <link href="https://USTCcoder.github.io/2020/06/12/generative_adversarial%20DiscoGAN/"/>
    <id>https://USTCcoder.github.io/2020/06/12/generative_adversarial DiscoGAN/</id>
    <published>2020-06-12T05:21:18.000Z</published>
    <updated>2020-06-07T15:25:26.159Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">DiscoGAN</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>DiscoGAN(Discover Cross-Domain Relations with Generative Adversarial Networks)</strong>:于<strong>2017年</strong>发表在<strong>ICML</strong>上，可以实现图像的风格迁移，风格迁移是GAN网络提出后才出现在人们视野里面的图像处理算法，在生成式对抗网络问世之前，人们很难通过传统的图像处理算法实现风格迁移，今天带小伙伴们看一看瞧一瞧。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/discogan.png" alt="discogan"></p><h1 id="DiscoGAN理论思想"><a href="#DiscoGAN理论思想" class="headerlink" title="DiscoGAN理论思想"></a><font size="5" color="red">DiscoGAN理论思想</font></h1><p>DiscoGAN引入了4个网络结构，分别是生成器GAB，生成器GBA，判别器DA，判别器DB。<br>GAB的输入是风格A的图像，输出是风格B的图像，目的是将风格A的图像转换为风格B的图像。<br>GBA的输入是风格B的图像，输出是风格A的图像，目的是将风格B的图像转换为风格A的图像。<br>DA的输入是风格A的图像，输出是对输入图像的分类，目的是判断输入图像是否为由B转换的风格A的图像<br>DB的输入是风格B的图像，输出是对输入图像的分类，目的是判断输入图像是否为由A转换的风格B的图像</p><p>其中的图像名称有很多，在这里进行简单的介绍。<br>image_A, image_B指数据集中读取的真实图像，使用DA和DB进行预测时，结果应该是全1。<br>fake_A指imge_B由GBA生成的风格A类型的图像，使用DA预测时，希望应该是全1，fake_B指imge_A由GAB生成的风格B类型的图像，使用DB预测时，希望应该是全1。<br>recon_A指fake_B由GBA生成风格A类型的图像，也就是原图image_A经过GAB，再经过GBA生成风格A的图像，希望和image_A越接近越好。<br>recon_B指fake_A由GAB生成风格B类型的图像，也就是原图image_B经过GBA，再经过GAB生成风格B的图像，希望和image_B越接近越好。</p><h1 id="DiscoGAN的特点"><a href="#DiscoGAN的特点" class="headerlink" title="DiscoGAN的特点"></a><font size="5" color="red">DiscoGAN的特点</font></h1><p>  <font size="3"><strong>使用InstanceNormalization代替BatchNormalization</strong>。</font><br>  <font size="3"><strong>生成器使用UNet网络结构对图像进行深层特征提取</strong>。</font><br>  <font size="3"><strong>生成器损失函数采用绝对误差，判别器损失函数采用均方误差</strong>。</font><br>  <font size="3"><strong>对生成器损失函数的权重进行调节，使网络更多关注于生成的图像质量</strong>。</font></p><h1 id="DiscoGAN图像分析"><a href="#DiscoGAN图像分析" class="headerlink" title="DiscoGAN图像分析"></a><font size="5" color="red">DiscoGAN图像分析</font></h1><p><img src="/images/Generative_adversarial/DiscoGAN-generator.png" alt="generator"><br><img src="/images/Generative_adversarial/DiscoGAN-discriminator.png" alt="discriminator"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import glob</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class InstanceNormalization(keras.layers.Layer):</span><br><span class="line">    def __init__(self, beta_initializer='zeros', gamma_initializer='ones',</span><br><span class="line">                 beta_regularizer=None, gamma_regularizer=None,</span><br><span class="line">                 beta_constraint=None, gamma_constraint=None, epsilon=1e-5,</span><br><span class="line">                 **kwargs):</span><br><span class="line">        super(InstanceNormalization, self).__init__(**kwargs)</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.beta_initializer = keras.initializers.get(beta_initializer)</span><br><span class="line">        self.gamma_initializer = keras.initializers.get(gamma_initializer)</span><br><span class="line">        self.beta_regularizer = keras.regularizers.get(beta_regularizer)</span><br><span class="line">        self.gamma_regularizer = keras.regularizers.get(gamma_regularizer)</span><br><span class="line">        self.beta_constraint = keras.constraints.get(beta_constraint)</span><br><span class="line">        self.gamma_constraint = keras.constraints.get(gamma_constraint)</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        assert len(input_shape) == 4</span><br><span class="line">        self.gamma = self.add_weight(shape=(input_shape[-1],), name='gamma', initializer=self.gamma_initializer,</span><br><span class="line">                                     regularizer=self.gamma_regularizer, constraint=self.gamma_constraint)</span><br><span class="line">        self.beta = self.add_weight(shape=(input_shape[-1],), name='beta', initializer=self.beta_initializer,</span><br><span class="line">                                    regularizer=self.beta_regularizer, constraint=self.beta_constraint)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        mean, variance = tf.nn.moments(inputs, axes=[1, 2])</span><br><span class="line">        mean = tf.reshape(mean, shape=[-1, 1, 1, inputs.shape[-1]])</span><br><span class="line">        variance = tf.reshape(variance, shape=[-1, 1, 1, inputs.shape[-1]])</span><br><span class="line">        outputs = (inputs - mean) / tf.sqrt(variance + self.epsilon)</span><br><span class="line">        return outputs * self.gamma + self.beta</span><br><span class="line"></span><br><span class="line">    def get_config(self):</span><br><span class="line">        config = {</span><br><span class="line">            'epsilon': self.epsilon,</span><br><span class="line">            'beta_initializer': keras.initializers.serialize(self.beta_initializer),</span><br><span class="line">            'gamma_initializer': keras.initializers.serialize(self.gamma_initializer),</span><br><span class="line">            'beta_regularizer': keras.regularizers.serialize(self.beta_regularizer),</span><br><span class="line">            'gamma_regularizer': keras.regularizers.serialize(self.gamma_regularizer),</span><br><span class="line">            'beta_constraint': keras.constraints.serialize(self.beta_constraint),</span><br><span class="line">            'gamma_constraint': keras.constraints.serialize(self.gamma_constraint)</span><br><span class="line">        }</span><br><span class="line">        base_config = super(InstanceNormalization, self).get_config()</span><br><span class="line"></span><br><span class="line">        return dict(list(base_config.items()) + list(config.items()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Relu_In(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Relu_In, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential([keras.layers.Conv2D(filters, kernel_size, strides, padding),</span><br><span class="line">                                       keras.layers.LeakyReLU(0.2)])</span><br><span class="line">        if name.find('in') != -1:</span><br><span class="line">            self.block.add(InstanceNormalization())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.block(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Upsampling_Conv_Relu_In_Concatenate(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Upsampling_Conv_Relu_In_Concatenate, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential([keras.layers.UpSampling2D((2, 2)),</span><br><span class="line">                                       keras.layers.Conv2D(filters, kernel_size, strides, padding, activation='relu'),</span><br><span class="line">                                       InstanceNormalization()])</span><br><span class="line">        self.concatenate = keras.layers.Concatenate()</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        x, shortcut = inputs</span><br><span class="line">        x = self.block(x)</span><br><span class="line">        output = self.concatenate([x, shortcut])</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generator(input_shape, name):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x1 = Conv_Relu_In(64, (4, 4), (2, 2), 'same', name='conv_leakyrelu1')(x)</span><br><span class="line">    x2 = Conv_Relu_In(128, (4, 4), (2, 2), 'same', name='conv_leakyrelu_in2')(x1)</span><br><span class="line">    x3 = Conv_Relu_In(256, (4, 4), (2, 2), 'same', name='conv_leakyrelu_in3')(x2)</span><br><span class="line">    x4 = Conv_Relu_In(512, (4, 4), (2, 2), 'same', name='conv_leakyrelu_in4')(x3)</span><br><span class="line">    x5 = Conv_Relu_In(512, (4, 4), (2, 2), 'same', name='conv_leakyrelu_in5')(x4)</span><br><span class="line">    x6 = Conv_Relu_In(512, (4, 4), (2, 2), 'same', name='conv_leakyrelu_in6')(x5)</span><br><span class="line">    x7 = Conv_Relu_In(512, (4, 4), (2, 2), 'same', name='conv_leakyrelu_in7')(x6)</span><br><span class="line"></span><br><span class="line">    y6 = Upsampling_Conv_Relu_In_Concatenate(512, (4, 4), (1, 1), 'same', name='upsampling_conv_relu_in_concatenate1')([x7, x6])</span><br><span class="line">    y5 = Upsampling_Conv_Relu_In_Concatenate(512, (4, 4), (1, 1), 'same', name='upsampling_conv_relu_in_concatenate2')([y6, x5])</span><br><span class="line">    y4 = Upsampling_Conv_Relu_In_Concatenate(512, (4, 4), (1, 1), 'same', name='upsampling_conv_relu_in_concatenate3')([y5, x4])</span><br><span class="line">    y3 = Upsampling_Conv_Relu_In_Concatenate(256, (4, 4), (1, 1), 'same', name='upsampling_conv_relu_in_concatenate4')([y4, x3])</span><br><span class="line">    y2 = Upsampling_Conv_Relu_In_Concatenate(128, (4, 4), (1, 1), 'same', name='upsampling_conv_relu_in_concatenate5')([y3, x2])</span><br><span class="line">    y1 = Upsampling_Conv_Relu_In_Concatenate(64, (4, 4), (1, 1), 'same', name='upsampling_conv_relu_in_concatenate6')([y2, x1])</span><br><span class="line"></span><br><span class="line">    y = compose(keras.layers.UpSampling2D((2, 2), name='upsampling'),</span><br><span class="line">                keras.layers.Conv2D(3, (4, 4), (1, 1), 'same', activation='tanh', name='conv_tanh'))(y1)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, y, name=name)</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discriminator(input_shape, name):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Relu_In(64, (4, 4), (2, 2), 'same', name='conv_leakyrelu1'),</span><br><span class="line">                Conv_Relu_In(128, (4, 4), (2, 2), 'same', name='conv_leakyrelu_in2'),</span><br><span class="line">                Conv_Relu_In(256, (4, 4), (2, 2), 'same', name='conv_leakyrelu_in3'),</span><br><span class="line">                Conv_Relu_In(512, (4, 4), (2, 2), 'same', name='conv_leakyrelu_in4'),</span><br><span class="line">                keras.layers.Conv2D(1, (4, 4), (1, 1), 'same', name='conv'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name=name)</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discogan(input_shapeA, input_shapeB, model_gAB, model_gBA, model_dA, model_dB):</span><br><span class="line">    input_tensorA = keras.layers.Input(input_shapeA, name='input_A')</span><br><span class="line">    input_tensorB = keras.layers.Input(input_shapeB, name='input_B')</span><br><span class="line"></span><br><span class="line">    # 输入风格B由BA生成的风格A类型的图像和输入风格A由AB生成的风格B类型的图像，称为假A和假B</span><br><span class="line">    fake_A = model_gBA(input_tensorB)</span><br><span class="line">    fake_B = model_gAB(input_tensorA)</span><br><span class="line"></span><br><span class="line">    # 输入假风格B由BA生成的重建风格A和假风格A由AB生成的重建风格B，称为重建A和重建B</span><br><span class="line">    recon_A = model_gBA(fake_B)</span><br><span class="line">    recon_B = model_gAB(fake_A)</span><br><span class="line"></span><br><span class="line">    model_dA.trainable = False</span><br><span class="line">    model_dB.trainable = False</span><br><span class="line"></span><br><span class="line">    conf_A = model_dA(fake_A)</span><br><span class="line">    conf_B = model_dB(fake_B)</span><br><span class="line"></span><br><span class="line">    model = keras.Model([input_tensorA, input_tensorB], [conf_A, conf_B, fake_A, fake_B, recon_A, recon_B], name='DiscoGAN')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def read_data(data_path, img_size, batch_size):</span><br><span class="line">    filename = glob.glob(data_path + '\\*.jpg')</span><br><span class="line">    choose_name = np.random.choice(filename, batch_size)</span><br><span class="line"></span><br><span class="line">    image_A, image_B = [], []</span><br><span class="line">    for i in range(batch_size):</span><br><span class="line">        image = cv.imread(choose_name[i]).astype(np.float32)</span><br><span class="line">        image_A.append(cv.resize(image[:, 256:, :], img_size))</span><br><span class="line">        image_B.append(cv.resize(image[:, :256, :], img_size))</span><br><span class="line"></span><br><span class="line">    image_A = np.array(image_A) / 127.5 - 1</span><br><span class="line">    image_B = np.array(image_B) / 127.5 - 1</span><br><span class="line"></span><br><span class="line">    return image_A, image_B</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    batch_size = 2</span><br><span class="line">    epochs = 2000</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    img_size = (128, 128)</span><br><span class="line">    data_path = r'.\edges2shoes\train'</span><br><span class="line">    save_path = r'.\discogan'</span><br><span class="line">    if not os.path.exists(save_path):</span><br><span class="line">        os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(0.0002, 0.5)</span><br><span class="line">    loss = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    real_dAmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    fake_dAmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    real_dBmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    fake_dBmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    gAmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    gBmse = keras.metrics.MeanSquaredError()</span><br><span class="line"></span><br><span class="line">    model_dA = discriminator(input_shape=(img_size[0], img_size[1], 3), name='DiscoGAN-DiscriminatorA')</span><br><span class="line">    model_dA.compile(optimizer=optimizer, loss='mse')</span><br><span class="line">    model_dB = discriminator(input_shape=(img_size[0], img_size[1], 3), name='DiscoGAN-DiscriminatorB')</span><br><span class="line">    model_dB.compile(optimizer=optimizer, loss='mse')</span><br><span class="line"></span><br><span class="line">    model_gAB = generator(input_shape=(img_size[0], img_size[1], 3), name='DiscoGAN-GeneratorAB')</span><br><span class="line">    model_gBA = generator(input_shape=(img_size[0], img_size[1], 3), name='DiscoGAN-GeneratorBA')</span><br><span class="line"></span><br><span class="line">    model_gAB.build(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line">    model_gAB.summary()</span><br><span class="line">    keras.utils.plot_model(model_gAB, 'DiscoGAN-generatorAB.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_gBA.build(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line">    model_gBA.summary()</span><br><span class="line">    keras.utils.plot_model(model_gBA, 'DiscoGAN-generatorBA.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_dA.build(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line">    model_dA.summary()</span><br><span class="line">    keras.utils.plot_model(model_dA, 'DiscoGAN-discriminatorA.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_dB.build(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line">    model_dB.summary()</span><br><span class="line">    keras.utils.plot_model(model_dB, 'DiscoGAN-discriminatorB.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model = discogan(input_shapeA=(img_size[0], img_size[1], 3), input_shapeB=(img_size[0], img_size[1], 3), model_gAB=model_gAB, model_gBA=model_gBA, model_dA=model_dA, model_dB=model_dB)</span><br><span class="line">    model.compile(optimizer=optimizer, loss=['mse', 'mse', 'mae', 'mae', 'mae', 'mae'], loss_weights=[0.5, 0.5, 5, 5, 5, 5])</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=[(img_size[0], img_size[1], 3), (img_size[0], img_size[1], 3)])</span><br><span class="line">    model.summary()</span><br><span class="line">    keras.utils.plot_model(model, 'DiscoGAN.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        image_A, image_B = read_data(data_path, img_size, batch_size)</span><br><span class="line"></span><br><span class="line">        fake_A = model_gBA(image_B)</span><br><span class="line">        fake_B = model_gAB(image_A)</span><br><span class="line"></span><br><span class="line">        real_dAmse(np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model_dA(image_A))</span><br><span class="line">        fake_dAmse(np.zeros((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model_dA(fake_A))</span><br><span class="line">        real_dBmse(np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model_dB(image_B))</span><br><span class="line">        fake_dBmse(np.zeros((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model_dB(fake_B))</span><br><span class="line">        gAmse(np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model([image_A, image_B])[0])</span><br><span class="line">        gBmse(np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model([image_A, image_B])[1])</span><br><span class="line"></span><br><span class="line">        real_dAloss = model_dA.train_on_batch(image_A, np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)))</span><br><span class="line">        fake_dAloss = model_dA.train_on_batch(fake_A, np.zeros((batch_size, img_size[0] // 16, img_size[1] // 16, 1)))</span><br><span class="line">        real_dBloss = model_dB.train_on_batch(image_B, np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)))</span><br><span class="line">        fake_dBloss = model_dB.train_on_batch(fake_B, np.zeros((batch_size, img_size[0] // 16, img_size[1] // 16, 1)))</span><br><span class="line"></span><br><span class="line">        gloss = model.train_on_batch([image_A, image_B], [np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), image_A, image_B, image_A, image_B])</span><br><span class="line"></span><br><span class="line">        if epoch % 20 == 0:</span><br><span class="line">            print('epoch = {}, real_dAmse = {}, fake_dAmse = {}, real_dBmse = {}, fake_dBmse = {}, gAmse = {}, gBmse = {}'.format(epoch, real_dAmse.result(), fake_dAmse.result(), real_dBmse.result(), fake_dBmse.result(), gAmse.result(), gBmse.result()))</span><br><span class="line">            real_dAmse.reset_states()</span><br><span class="line">            fake_dAmse.reset_states()</span><br><span class="line">            real_dBmse.reset_states()</span><br><span class="line">            fake_dBmse.reset_states()</span><br><span class="line">            gAmse.reset_states()</span><br><span class="line">            gBmse.reset_states()</span><br><span class="line">            image_A, image_B = read_data(data_path, img_size, batch_size=1)</span><br><span class="line">            fake_A = ((model_gBA(image_B).numpy().squeeze() + 1) * 127.5).astype(np.uint8)</span><br><span class="line">            fake_B = ((model_gAB(image_A).numpy().squeeze() + 1) * 127.5).astype(np.uint8)</span><br><span class="line">            image_A = ((image_A.squeeze() + 1) * 127.5).astype(np.uint8)</span><br><span class="line">            image_B = ((image_B.squeeze() + 1) * 127.5).astype(np.uint8)</span><br><span class="line">            cv.imwrite(save_path + '\\epoch{}.jpg'.format(epoch), np.concatenate([np.concatenate([image_B, fake_A], axis=1), np.concatenate([image_A, fake_B], axis=1)], axis=0))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Generative_adversarial/discogan_R.png" alt="discogan"></p><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Generative_adversarial/discogan_T.png" alt="discogan"></p><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>可以设置一些<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li><strong>DiscoGAN对于网络结构，优化器参数，网络层的一些超参数都是非常敏感的，效果不好不容易发现原因，这可能需要较多的工程实践经验</strong>。</li><li><strong>先创建判别器，然后进行compile，这样判别器就固定了，然后创建生成器时，不要训练判别器，需要将判别器的trainable改成False，此时不会影响之前固定的判别器</strong>，这个可以<strong>通过模型的_collection_collected_trainable_weights属性查看</strong>，如果该属性为空，则模型不训练，否则模型可以训练，compile之后，该属性固定，无论后面如何修改trainable，只要不重新compile，都不影响训练。</li><li>在DiscoGAN的测试图像中，为了体现模型的效果，第一行的奇数个为鞋子的轮廓，第一行的偶数个为转换风格后的鞋子图像，第二行的奇数个为鞋子图像，第二行的偶数个为转换风格后的鞋子轮廓，这里只是训练了2000代，而且每一代只有2个图像就可以看出DiscoGAN的效果。小伙伴们可以选择更大的数据集，更加快速的GPU，训练更长的时间，这样风格迁移的效果就会更加明显。</li></ol><h1 id="DiscoGAN小结"><a href="#DiscoGAN小结" class="headerlink" title="DiscoGAN小结"></a><font size="5" color="red">DiscoGAN小结</font></h1><p>  DiscoGAN是一种有效的风格迁移生成式对抗网络，和CycleGAN模型非常相似，只是更换了部分损失函数，网络结构从ResNet更换为UNet，从上图可以看出<strong>DiscoGAN模型的参数量有89M</strong>，可以实现任意风格之间的迁移，如果数据集足够，还可以生成人物表情包，是不是非常有趣呢？小伙伴们一定要掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;DiscoGAN&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>每日温度(Leetcode 739)</title>
    <link href="https://USTCcoder.github.io/2020/06/11/program%20Leetcode739/"/>
    <id>https://USTCcoder.github.io/2020/06/11/program Leetcode739/</id>
    <published>2020-06-11T08:18:43.000Z</published>
    <updated>2020-09-02T02:18:37.429Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode739.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   题意很简单，找出比当前数字大的下一个数字的距离，是不是可以按照我们的想法来解题呢？遍历每一个元素，找出比它大的下一个元素的位置。</p><a id="more"></a><h1 id="暴力法"><a href="#暴力法" class="headerlink" title="暴力法"></a><font size="5" color="red">暴力法</font></h1><p>最直观的想法，从第一个元素开始遍历，对每一个元素遍历它后面的所有元素，如果找到比它大的数字，则索引相减并break，从下一个元素开始寻找，否则置为0。暴力法的时间复杂度为$O(n^2)$，空间复杂度为$O(n)$。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def dailyTemperatures(self, T):</span><br><span class="line">        """</span><br><span class="line">        :type T: List[int]</span><br><span class="line">        :rtype: List[int]</span><br><span class="line">        """</span><br><span class="line">        lens = len(T)</span><br><span class="line">        res = [0] * lens</span><br><span class="line">        for i in range(lens):</span><br><span class="line">            for j in range(i + 1, lens):</span><br><span class="line">                if T[j] &gt; T[i]:</span><br><span class="line">                    res[i] = j - i</span><br><span class="line">                    break</span><br><span class="line">        return res</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="优化暴力法"><a href="#优化暴力法" class="headerlink" title="优化暴力法"></a><font size="5" color="red">优化暴力法</font></h1><p>暴力法虽然可以求解，但是无法通过，因为时间复杂度太高，我们观察到题目中的信息，温度的范围在[30, 100]之间，因此我们对于每一个数，找后面比它大的数，哪个出现的最早即可。以题目的示例，第一个数为73，那么只需要寻找74到100，看哪一个数出现的最早即可。从后向前计算，并且用字典记录当前值出现的索引，这样寻找时直接查找是否存在于字典中，不需要遍历，因此可以将暴力法的第二层循环从n次查找变为最多100次的字典查找，时间复杂度为$O(mn)$，其中m最大为100，空间复杂度为$O(n)$，官方题解中有Pythonic的写法，在此我就不献丑了。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def dailyTemperatures(self, T):</span><br><span class="line">        """</span><br><span class="line">        :type T: List[int]</span><br><span class="line">        :rtype: List[int]</span><br><span class="line">        """</span><br><span class="line">        n = len(T)</span><br><span class="line">        ans, nxt, big = [0] * n, dict(), 10**9</span><br><span class="line">        for i in range(n - 1, -1, -1):</span><br><span class="line">            print(i)</span><br><span class="line">            warmer_index = min(nxt.get(t, big) for t in range(T[i] + 1, 102))</span><br><span class="line">            if warmer_index != big:</span><br><span class="line">                ans[i] = warmer_index - i</span><br><span class="line">            nxt[T[i]] = i</span><br><span class="line">        return ans</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="单调栈"><a href="#单调栈" class="headerlink" title="单调栈"></a><font size="5" color="red">单调栈</font></h1><p>这道题遍历法肯定不是最优的解法，单调栈是这类问题的最优解，下面我用图示说明如何使用单调栈求解这个问题。<br><img src="/images/ALGORITHM/leetcode739_stack.png" alt="stack"><br>单调栈的时间复杂度为$O(n)$，空间复杂度为$O(n)$。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def dailyTemperatures(self, T):</span><br><span class="line">        """</span><br><span class="line">        :type T: List[int]</span><br><span class="line">        :rtype: List[int]</span><br><span class="line">        """</span><br><span class="line">        lens = len(T)</span><br><span class="line">        stack = []</span><br><span class="line">        res = [0] * lens</span><br><span class="line">        for i, v in enumerate(T):</span><br><span class="line">            while stack and T[stack[-1]] &lt; v:</span><br><span class="line">                tmp_i = stack.pop()</span><br><span class="line">                res[tmp_i] = i - tmp_i</span><br><span class="line">            stack.append(i)</span><br><span class="line">        return res</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  单调栈算法时解决数字问题的常用算法，因为单调栈只需要遍历一次数组，因此非常高效，所以小伙伴们一定要掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 739&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>CycleGAN</title>
    <link href="https://USTCcoder.github.io/2020/06/11/generative_adversarial%20CycleGAN/"/>
    <id>https://USTCcoder.github.io/2020/06/11/generative_adversarial CycleGAN/</id>
    <published>2020-06-11T07:19:21.000Z</published>
    <updated>2020-06-07T15:25:43.837Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">CycleGAN</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>CycleGAN(Cycle Consistent Generative Adversarial Networks, 循环一致生成式对抗网络)</strong>:于<strong>2017年</strong>发表在<strong>ICCV</strong>上，可以实现图像的风格迁移，风格迁移是GAN网络提出后才出现在人们视野里面的图像处理算法，在生成式对抗网络问世之前，人们很难通过传统的图像处理算法实现风格迁移，今天带小伙伴们看一看瞧一瞧。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/cyclegan.png" alt="cyclegan"></p><h1 id="CycleGAN理论思想"><a href="#CycleGAN理论思想" class="headerlink" title="CycleGAN理论思想"></a><font size="5" color="red">CycleGAN理论思想</font></h1><p>CycleGAN引入了4个网络结构，分别是生成器GAB，生成器GBA，判别器DA，判别器DB。<br>GAB的输入是风格A的图像，输出是风格B的图像，目的是将风格A的图像转换为风格B的图像。<br>GBA的输入是风格B的图像，输出是风格A的图像，目的是将风格B的图像转换为风格A的图像。<br>DA的输入是风格A的图像，输出是对输入图像的分类，目的是判断输入图像是否为由B转换的风格A的图像<br>DB的输入是风格B的图像，输出是对输入图像的分类，目的是判断输入图像是否为由A转换的风格B的图像</p><p>其中的图像名称有很多，在这里进行简单的介绍。<br>image_A, image_B指数据集中读取的真实图像，使用DA和DB进行预测时，结果应该是全1。<br>fake_A指imge_B由GBA生成的风格A类型的图像，使用DA预测时，希望应该是全1，fake_B指imge_A由GAB生成的风格B类型的图像，使用DB预测时，希望应该是全1。<br>recon_A指fake_B由GBA生成风格A类型的图像，也就是原图image_A经过GAB，再经过GBA生成风格A的图像，希望和image_A越接近越好。<br>recon_B指fake_A由GAB生成风格B类型的图像，也就是原图image_B经过GBA，再经过GAB生成风格B的图像，希望和image_B越接近越好。<br>self_A指image_A由GBA生成的风格A类型的图像，因为GBA是将风格B的图像转换为风格A的图像，输入风格A的图像，应该是不会产生变化，希望和image_A越接近越好。<br>self_B指image_B由GAB生成的风格B类型的图像，因为GAB是将风格A的图像转换为风格B的图像，输入风格B的图像，应该是不会产生变化，希望和image_B越接近越好。</p><h1 id="CycelGAN的特点"><a href="#CycelGAN的特点" class="headerlink" title="CycelGAN的特点"></a><font size="5" color="red">CycelGAN的特点</font></h1><p>  <font size="3"><strong>使用InstanceNormalization代替BatchNormalization</strong>。</font><br>  <font size="3"><strong>生成器使用下采样+ResNet结构+上采样对图像进行深层特征提取</strong>。</font><br>  <font size="3"><strong>生成器损失函数采用绝对误差，判别器损失函数采用均方误差</strong>。</font><br>  <font size="3"><strong>对生成器损失函数的权重进行调节，使网络更多关注于生成的图像质量</strong>。</font></p><h1 id="CycleGAN图像分析"><a href="#CycleGAN图像分析" class="headerlink" title="CycleGAN图像分析"></a><font size="5" color="red">CycleGAN图像分析</font></h1><p><img src="/images/Generative_adversarial/CycleGAN-generator.png" alt="generator"><br><img src="/images/Generative_adversarial/CycleGAN-discriminator.png" alt="discriminator"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import glob</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class InstanceNormalization(keras.layers.Layer):</span><br><span class="line">    def __init__(self, beta_initializer='zeros', gamma_initializer='ones',</span><br><span class="line">                 beta_regularizer=None, gamma_regularizer=None,</span><br><span class="line">                 beta_constraint=None, gamma_constraint=None, epsilon=1e-5,</span><br><span class="line">                 **kwargs):</span><br><span class="line">        super(InstanceNormalization, self).__init__(**kwargs)</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.beta_initializer = keras.initializers.get(beta_initializer)</span><br><span class="line">        self.gamma_initializer = keras.initializers.get(gamma_initializer)</span><br><span class="line">        self.beta_regularizer = keras.regularizers.get(beta_regularizer)</span><br><span class="line">        self.gamma_regularizer = keras.regularizers.get(gamma_regularizer)</span><br><span class="line">        self.beta_constraint = keras.constraints.get(beta_constraint)</span><br><span class="line">        self.gamma_constraint = keras.constraints.get(gamma_constraint)</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        assert len(input_shape) == 4</span><br><span class="line">        self.gamma = self.add_weight(shape=(input_shape[-1],), name='gamma', initializer=self.gamma_initializer,</span><br><span class="line">                                     regularizer=self.gamma_regularizer, constraint=self.gamma_constraint)</span><br><span class="line">        self.beta = self.add_weight(shape=(input_shape[-1],), name='beta', initializer=self.beta_initializer,</span><br><span class="line">                                    regularizer=self.beta_regularizer, constraint=self.beta_constraint)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        mean, variance = tf.nn.moments(inputs, axes=[1, 2])</span><br><span class="line">        mean = tf.reshape(mean, shape=[-1, 1, 1, inputs.shape[-1]])</span><br><span class="line">        variance = tf.reshape(variance, shape=[-1, 1, 1, inputs.shape[-1]])</span><br><span class="line">        outputs = (inputs - mean) / tf.sqrt(variance + self.epsilon)</span><br><span class="line">        return outputs * self.gamma + self.beta</span><br><span class="line"></span><br><span class="line">    def get_config(self):</span><br><span class="line">        config = {</span><br><span class="line">            'epsilon': self.epsilon,</span><br><span class="line">            'beta_initializer': keras.initializers.serialize(self.beta_initializer),</span><br><span class="line">            'gamma_initializer': keras.initializers.serialize(self.gamma_initializer),</span><br><span class="line">            'beta_regularizer': keras.regularizers.serialize(self.beta_regularizer),</span><br><span class="line">            'gamma_regularizer': keras.regularizers.serialize(self.gamma_regularizer),</span><br><span class="line">            'beta_constraint': keras.constraints.serialize(self.beta_constraint),</span><br><span class="line">            'gamma_constraint': keras.constraints.serialize(self.gamma_constraint)</span><br><span class="line">        }</span><br><span class="line">        base_config = super(InstanceNormalization, self).get_config()</span><br><span class="line"></span><br><span class="line">        return dict(list(base_config.items()) + list(config.items()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_In_Relu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_In_Relu, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential([keras.layers.Conv2D(filters, kernel_size, strides, padding)])</span><br><span class="line">        if name.find('in') != -1:</span><br><span class="line">            self.block.add(InstanceNormalization())</span><br><span class="line">        if name.find('leakyrelu') != -1:</span><br><span class="line">            self.block.add(keras.layers.LeakyReLU(0.2))</span><br><span class="line">        elif name.find('relu') != -1:</span><br><span class="line">            self.block.add(keras.layers.ReLU())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.block(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def identity_block(x, filters, kernel_size, strides, padding, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = compose(Conv_In_Relu(filters, kernel_size, strides, padding, name='{}_conv_in_relu1'.format(name)),</span><br><span class="line">                Conv_In_Relu(filters, kernel_size, strides, padding, name='{}_conv_in2'.format(name)))(x)</span><br><span class="line">    x = keras.layers.Add(name='{}_add'.format(name))([x, shortcut])</span><br><span class="line">    x = keras.layers.ReLU(name='{}_relu'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generator(input_shape, name):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_In_Relu(64, (7, 7), (1, 1), 'same', name='conv_in_relu1'),</span><br><span class="line">                Conv_In_Relu(128, (3, 3), (2, 2), 'same', name='conv_in_relu2'),</span><br><span class="line">                Conv_In_Relu(256, (3, 3), (2, 2), 'same', name='conv_in_relu3'))(x)</span><br><span class="line"></span><br><span class="line">    for i in range(9):</span><br><span class="line">        x = identity_block(x, 256, (3, 3), (1, 1), 'same', name='identity_block{}'.format(i + 1))</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.UpSampling2D((2, 2), name='upsampling1'),</span><br><span class="line">                Conv_In_Relu(128, (3, 3), (1, 1), 'same', name='conv_in_relu4'),</span><br><span class="line">                keras.layers.UpSampling2D((2, 2), name='upsampling2'),</span><br><span class="line">                Conv_In_Relu(64, (3, 3), (1, 1), 'same', name='conv_in_relu5'),</span><br><span class="line">                keras.layers.Conv2D(3, (3, 3), (1, 1), 'same', activation='tanh', name='conv_tanh'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name=name)</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discriminator(input_shape, name):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_In_Relu(64, (4, 4), (2, 2), 'same', name='conv_leakyrelu1'),</span><br><span class="line">                Conv_In_Relu(128, (4, 4), (2, 2), 'same', name='conv_in_leakyrelu2'),</span><br><span class="line">                Conv_In_Relu(256, (4, 4), (2, 2), 'same', name='conv_in_leakyrelu3'),</span><br><span class="line">                Conv_In_Relu(512, (4, 4), (2, 2), 'same', name='conv_in_leakyrelu4'),</span><br><span class="line">                keras.layers.Conv2D(1, (3, 3), (1, 1), 'same', name='conv'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name=name)</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def cyclegan(input_shapeA, input_shapeB, model_gAB, model_gBA, model_dA, model_dB):</span><br><span class="line">    input_tensorA = keras.layers.Input(input_shapeA, name='input_A')</span><br><span class="line">    input_tensorB = keras.layers.Input(input_shapeB, name='input_B')</span><br><span class="line"></span><br><span class="line">    # 输入风格B由BA生成的风格A类型的图像和输入风格A由AB生成的风格B类型的图像，称为假A和假B</span><br><span class="line">    fake_A = model_gBA(input_tensorB)</span><br><span class="line">    fake_B = model_gAB(input_tensorA)</span><br><span class="line"></span><br><span class="line">    # 输入假风格B由BA生成的重建风格A和假风格A由AB生成的重建风格B，称为重建A和重建B</span><br><span class="line">    recon_A = model_gBA(fake_B)</span><br><span class="line">    recon_B = model_gAB(fake_A)</span><br><span class="line"></span><br><span class="line">    # 输入风格B由AB生成的风格B类型的图像和输入风格A由BA生成的风格A类型的图像，称为自身B和自身A</span><br><span class="line">    self_A = model_gBA(input_tensorA)</span><br><span class="line">    self_B = model_gAB(input_tensorB)</span><br><span class="line"></span><br><span class="line">    model_dA.trainable = False</span><br><span class="line">    model_dB.trainable = False</span><br><span class="line"></span><br><span class="line">    conf_A = model_dA(fake_A)</span><br><span class="line">    conf_B = model_dB(fake_B)</span><br><span class="line"></span><br><span class="line">    model = keras.Model([input_tensorA, input_tensorB], [conf_A, conf_B, recon_A, recon_B, self_A, self_B], name='CycleGAN')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def read_data(data_path, img_size, batch_size):</span><br><span class="line">    filename_A = glob.glob(data_path + 'A\\*.jpg')</span><br><span class="line">    filename_B = glob.glob(data_path + 'B\\*.jpg')</span><br><span class="line">    choose_name_A = np.random.choice(filename_A, batch_size)</span><br><span class="line">    choose_name_B = np.random.choice(filename_B, batch_size)</span><br><span class="line"></span><br><span class="line">    image_A, image_B = [], []</span><br><span class="line">    for i in range(batch_size):</span><br><span class="line">        A = cv.imread(choose_name_A[i]).astype(np.float32)</span><br><span class="line">        B = cv.imread(choose_name_B[i]).astype(np.float32)</span><br><span class="line">        image_A.append(cv.resize(A, img_size))</span><br><span class="line">        image_B.append(cv.resize(B, img_size))</span><br><span class="line"></span><br><span class="line">    image_A = np.array(image_A) / 127.5 - 1</span><br><span class="line">    image_B = np.array(image_B) / 127.5 - 1</span><br><span class="line"></span><br><span class="line">    return image_A, image_B</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    batch_size = 2</span><br><span class="line">    epochs = 2000</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    img_size = (128, 128)</span><br><span class="line">    data_path = r'.\monet2photo\train'</span><br><span class="line">    save_path = r'.\cyclegan'</span><br><span class="line">    if not os.path.exists(save_path):</span><br><span class="line">        os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(0.0002, 0.5)</span><br><span class="line">    loss = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    real_dAmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    fake_dAmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    real_dBmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    fake_dBmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    gAmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    gBmse = keras.metrics.MeanSquaredError()</span><br><span class="line"></span><br><span class="line">    model_dA = discriminator(input_shape=(img_size[0], img_size[1], 3), name='CycleGAN-DiscriminatorA')</span><br><span class="line">    model_dA.compile(optimizer=optimizer, loss='mse')</span><br><span class="line">    model_dB = discriminator(input_shape=(img_size[0], img_size[1], 3), name='CycleGAN-DiscriminatorB')</span><br><span class="line">    model_dB.compile(optimizer=optimizer, loss='mse')</span><br><span class="line"></span><br><span class="line">    model_gAB = generator(input_shape=(img_size[0], img_size[1], 3), name='CycleGAN-GeneratorAB')</span><br><span class="line">    model_gBA = generator(input_shape=(img_size[0], img_size[1], 3), name='CycleGAN-GeneratorBA')</span><br><span class="line"></span><br><span class="line">    model_gAB.build(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line">    model_gAB.summary()</span><br><span class="line">    keras.utils.plot_model(model_gAB, 'CycleGAN-generatorAB.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_gBA.build(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line">    model_gBA.summary()</span><br><span class="line">    keras.utils.plot_model(model_gBA, 'CycleGAN-generatorBA.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_dA.build(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line">    model_dA.summary()</span><br><span class="line">    keras.utils.plot_model(model_dA, 'CycleGAN-discriminatorA.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_dB.build(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line">    model_dB.summary()</span><br><span class="line">    keras.utils.plot_model(model_dB, 'CycleGAN-discriminatorB.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model = cyclegan(input_shapeA=(img_size[0], img_size[1], 3), input_shapeB=(img_size[0], img_size[1], 3), model_gAB=model_gAB, model_gBA=model_gBA, model_dA=model_dA, model_dB=model_dB)</span><br><span class="line">    model.compile(optimizer=optimizer, loss=['mse', 'mse', 'mae', 'mae', 'mae', 'mae'], loss_weights=[0.5, 0.5, 5, 5, 2.5, 2.5])</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=[(img_size[0], img_size[1], 3), (img_size[0], img_size[1], 3)])</span><br><span class="line">    model.summary()</span><br><span class="line">    keras.utils.plot_model(model, 'CycleGAN.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        image_A, image_B = read_data(data_path, img_size, batch_size)</span><br><span class="line"></span><br><span class="line">        fake_A = model_gBA(image_B)</span><br><span class="line">        fake_B = model_gAB(image_A)</span><br><span class="line"></span><br><span class="line">        real_dAmse(np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model_dA(image_A))</span><br><span class="line">        fake_dAmse(np.zeros((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model_dA(fake_A))</span><br><span class="line">        real_dBmse(np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model_dB(image_B))</span><br><span class="line">        fake_dBmse(np.zeros((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model_dB(fake_B))</span><br><span class="line">        gAmse(np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model([image_A, image_B])[0])</span><br><span class="line">        gBmse(np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), model([image_A, image_B])[1])</span><br><span class="line"></span><br><span class="line">        real_dAloss = model_dA.train_on_batch(image_A, np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)))</span><br><span class="line">        fake_dAloss = model_dA.train_on_batch(fake_A, np.zeros((batch_size, img_size[0] // 16, img_size[1] // 16, 1)))</span><br><span class="line">        real_dBloss = model_dB.train_on_batch(image_B, np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)))</span><br><span class="line">        fake_dBloss = model_dB.train_on_batch(fake_B, np.zeros((batch_size, img_size[0] // 16, img_size[1] // 16, 1)))</span><br><span class="line"></span><br><span class="line">        gloss = model.train_on_batch([image_A, image_B], [np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), np.ones((batch_size, img_size[0] // 16, img_size[1] // 16, 1)), image_A, image_B, image_A, image_B])</span><br><span class="line"></span><br><span class="line">        if epoch % 20 == 0:</span><br><span class="line">            print('epoch = {}, real_dAmse = {}, fake_dAmse = {}, real_dBmse = {}, fake_dBmse = {}, gAmse = {}, gBmse = {}'.format(epoch, real_dAmse.result(), fake_dAmse.result(), real_dBmse.result(), fake_dBmse.result(), gAmse.result(), gBmse.result()))</span><br><span class="line">            real_dAmse.reset_states()</span><br><span class="line">            fake_dAmse.reset_states()</span><br><span class="line">            real_dBmse.reset_states()</span><br><span class="line">            fake_dBmse.reset_states()</span><br><span class="line">            gAmse.reset_states()</span><br><span class="line">            gBmse.reset_states()</span><br><span class="line">            image_A, image_B = read_data(data_path, img_size, batch_size=1)</span><br><span class="line">            fake_A = ((model_gBA(image_B).numpy().squeeze() + 1) * 127.5).astype(np.uint8)</span><br><span class="line">            fake_B = ((model_gAB(image_A).numpy().squeeze() + 1) * 127.5).astype(np.uint8)</span><br><span class="line">            image_A = ((image_A.squeeze() + 1) * 127.5).astype(np.uint8)</span><br><span class="line">            image_B = ((image_B.squeeze() + 1) * 127.5).astype(np.uint8)</span><br><span class="line">            cv.imwrite(save_path + '\\epoch{}.jpg'.format(epoch), np.concatenate([np.concatenate([image_B, fake_A], axis=1), np.concatenate([image_A, fake_B], axis=1)], axis=0))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Generative_adversarial/cyclegan_R.png" alt="cyclegan"></p><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Generative_adversarial/cyclegan_T.png" alt="cyclegan"></p><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>可以设置一些<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li><strong>CycleGAN对于网络结构，优化器参数，网络层的一些超参数都是非常敏感的，效果不好不容易发现原因，这可能需要较多的工程实践经验</strong>。</li><li><strong>先创建判别器，然后进行compile，这样判别器就固定了，然后创建生成器时，不要训练判别器，需要将判别器的trainable改成False，此时不会影响之前固定的判别器</strong>，这个可以<strong>通过模型的_collection_collected_trainable_weights属性查看</strong>，如果该属性为空，则模型不训练，否则模型可以训练，compile之后，该属性固定，无论后面如何修改trainable，只要不重新compile，都不影响训练。</li><li>在CycleGAN的测试图像中，为了体现模型的效果，第一行的奇数个为拍摄的照片，第一行的偶数个为转换风格后的莫奈风格画作，第二行的奇数个为莫奈风格的画作，第二行的偶数个为转换风格后的照片，这里只是训练了2000代，而且每一代只有2个图像就可以看出CycleGAN的效果。小伙伴们可以选择更大的数据集，更加快速的GPU，训练更长的时间，这样风格迁移的效果就会更加明显。</li></ol><h1 id="CycleGAN小结"><a href="#CycleGAN小结" class="headerlink" title="CycleGAN小结"></a><font size="5" color="red">CycleGAN小结</font></h1><p>  CycleGAN是一种有效的风格迁移生成式对抗网络，从上图可以看出<strong>CycleGAN模型的参数量只有28M</strong>，可以实现任意风格之间的迁移，如果数据集足够，还可以生成人物表情包，是不是非常有趣呢？小伙伴们一定要掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;CycleGAN&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>SRGAN</title>
    <link href="https://USTCcoder.github.io/2020/06/09/generative_adversarial%20SRGAN/"/>
    <id>https://USTCcoder.github.io/2020/06/09/generative_adversarial SRGAN/</id>
    <published>2020-06-09T01:48:17.000Z</published>
    <updated>2020-07-19T06:17:32.553Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">SRGAN</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>SRGAN(Supre Resolution Generative Adversarial Networks, 超分辨率生成式对抗网络)</strong>:于<strong>2016年</strong>发表在<strong>CVPR</strong>上，图像处理的一个重要任务就是超分辨率，图像的大小是由分辨率决定的，常见的分辨率有128x128，256x256，512x512，1024x1024，<strong>像素越大，说明像素点越多，图像的表达能力更强，细节更加明显</strong>，看起来也会更加舒适。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/srgan.png" alt="srgan"></p><h1 id="分辨率提升的方法"><a href="#分辨率提升的方法" class="headerlink" title="分辨率提升的方法"></a><font size="5" color="red">分辨率提升的方法</font></h1><p>在GAN问世以前，人们就做了许多关于分辨率提升的方法，其中<strong>最简单的也是最常用的方法就是插值法，在opencv库中有imresize函数</strong>，其中<strong>可以指定目标图像的尺寸，而且可以选择插值方法，一般选择双线性插值</strong>，但是插值方法有一个<strong>最致命的问题—模糊</strong>，插值的原理就是根据周围的点进行估计，因此插值的结果会导致某个区域的值都i非常接近，会导致照片模糊。</p><p>随着GAN的发展，人们发现GAN既然能够生成图像，能不能生成更高分辨率的图像，答案是肯定的，今天给小伙伴们介绍SRGAN的原理。<br>SRGAN<strong>引入了三个网络，一个是生成器，一个是判别器，还有一个是特征提取器(VGG19)</strong><br>生成器的<strong>输入是低分辨率图像，输出是高分辨率图像，目的是根据输入的低分辨率图像生成高分辨率图像</strong>。<br>判别器的<strong>输入是高分辨率图像，输出是对输入图像的分类，目的是判断输入的图像是生成的高分辨率图像还是原始的高分辨率图像</strong>。<br>特征提取器的<strong>输入是高分辨率图像，输出是对高分辨率图像的特征提取，目的是使生成的图像和原始的高分辨率图像具有相同的特征</strong>。</p><h1 id="SRGAN的特点"><a href="#SRGAN的特点" class="headerlink" title="SRGAN的特点"></a><font size="5" color="red">SRGAN的特点</font></h1><p>  <font size="3"><strong>生成器使用ResNet结构+上采样对图像进行分辨率提升</strong>。</font><br>  <font size="3"><strong>引入特征提取网络VGG19，对高分辨率特征进行提取</strong>。</font><br>  <font size="3"><strong>特征提取损失函数采用均方误差，判别器损失函数采用二分类交叉熵</strong>。</font><br>  <font size="3"><strong>对生成器损失函数的权重进行调节，使网络更多关注于生成的图像质量</strong>。</font></p><h1 id="SRGAN图像分析"><a href="#SRGAN图像分析" class="headerlink" title="SRGAN图像分析"></a><font size="5" color="red">SRGAN图像分析</font></h1><p><img src="/images/Generative_adversarial/SRGAN-generator.png" alt="generator"><br><img src="/images/Generative_adversarial/SRGAN-discriminator.png" alt="discriminator"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import glob</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def resblock(x, filters, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = compose(keras.layers.Conv2D(filters, (3, 3), (1, 1), 'same', name='{}_conv1'.format(name)),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='{}_bn1'.format(name)),</span><br><span class="line">                keras.layers.ReLU(name='{}_relu1'.format(name)),</span><br><span class="line">                keras.layers.Conv2D(filters, (3, 3), (1, 1), 'same', name='{}_conv2'.format(name)),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='{}_bn2'.format(name)))(x)</span><br><span class="line">    x = keras.layers.Add(name='{}_add'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_LeakyRelu_Bn(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_LeakyRelu_Bn, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential([keras.layers.Conv2D(filters, kernel_size, strides, padding),</span><br><span class="line">                                       keras.layers.LeakyReLU(0.2)])</span><br><span class="line">        if name.find('bn') != -1:</span><br><span class="line">            self.block.add(keras.layers.BatchNormalization(momentum=0.8))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.block(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def vgg(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    vgg = keras.applications.VGG19()</span><br><span class="line">    vgg.outputs = [vgg.layers[9].output]</span><br><span class="line">    x = vgg(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='VGG19')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Conv2D(64, (9, 9), (1, 1), 'same', name='conv1')(x)</span><br><span class="line">    shortcut = x</span><br><span class="line"></span><br><span class="line">    for i in range(16):</span><br><span class="line">        x = resblock(x, 64, name='resblock{}'.format(i + 1))</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', name='conv2'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn2'))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Add(name='add2')([x, shortcut])</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.UpSampling2D((2, 2), name='upsampling1'),</span><br><span class="line">                keras.layers.Conv2D(256, (3, 3), (1, 1), 'same', activation='relu', name='conv3_relu'),</span><br><span class="line">                keras.layers.UpSampling2D((2, 2), name='upsampling2'),</span><br><span class="line">                keras.layers.Conv2D(256, (3, 3), (1, 1), 'same', activation='relu', name='conv4_relu'),</span><br><span class="line">                keras.layers.Conv2D(3, (9, 9), (1, 1), 'same', activation='tanh', name='conv5_tanh'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='SRGAN-Generator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discriminator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_LeakyRelu_Bn(64, (3, 3), (1, 1), 'same', name='conv_leakyrelu1'),</span><br><span class="line">                Conv_LeakyRelu_Bn(64, (3, 3), (2, 2), 'same', name='conv_leakyrelu_bn2'),</span><br><span class="line">                Conv_LeakyRelu_Bn(128, (3, 3), (1, 1), 'same', name='conv_leakyrelu_bn3'),</span><br><span class="line">                Conv_LeakyRelu_Bn(128, (3, 3), (2, 2), 'same', name='conv_leakyrelu_bn4'),</span><br><span class="line">                Conv_LeakyRelu_Bn(256, (3, 3), (1, 1), 'same', name='conv_leakyrelu_bn5'),</span><br><span class="line">                Conv_LeakyRelu_Bn(256, (3, 3), (2, 2), 'same', name='conv_leakyrelu_bn6'),</span><br><span class="line">                Conv_LeakyRelu_Bn(512, (3, 3), (1, 1), 'same', name='conv_leakyrelu_bn7'),</span><br><span class="line">                Conv_LeakyRelu_Bn(512, (3, 3), (2, 2), 'same', name='conv_leakyrelu_bn8'),</span><br><span class="line">                Conv_LeakyRelu_Bn(1024, (1, 1), (1, 1), 'same', name='conv_leakyrelu9'),</span><br><span class="line">                keras.layers.Conv2D(1, (1, 1), (1, 1), 'same', activation='sigmoid', name='conv_sigmoid'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='SRGAN-Discriminator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def srgan(input_shape, model_vgg, model_g, model_d):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    model_vgg.trainable = False</span><br><span class="line">    model_d.trainable = False</span><br><span class="line"></span><br><span class="line">    fake_image = model_g(x)</span><br><span class="line">    fake_feature = model_vgg(fake_image)</span><br><span class="line">    conf = model_d(fake_image)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, [conf, fake_feature], name='SRGAN')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def read_data(data_path, high_resolution, low_resolution, batch_size):</span><br><span class="line">    filename = glob.glob(data_path)</span><br><span class="line">    choose_name = np.random.choice(filename, batch_size)</span><br><span class="line"></span><br><span class="line">    hr_image, lr_image = [], []</span><br><span class="line">    for name in choose_name:</span><br><span class="line">        image = cv.imread(name).astype(np.float32)</span><br><span class="line">        hr_image.append(cv.resize(image, high_resolution))</span><br><span class="line">        lr_image.append(cv.resize(image, low_resolution))</span><br><span class="line"></span><br><span class="line">    hr_image = np.array(hr_image) / 127.5 - 1</span><br><span class="line">    lr_image = np.array(lr_image) / 127.5 - 1</span><br><span class="line"></span><br><span class="line">    return hr_image, lr_image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    batch_size = 2</span><br><span class="line">    epochs = 2000</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    low_resolution = (56, 56)</span><br><span class="line">    high_resolution = (224, 224)</span><br><span class="line">    data_path = r'.\monet2photo\trainB\*.jpg'</span><br><span class="line">    save_path = r'.\srgan'</span><br><span class="line">    if not os.path.exists(save_path):</span><br><span class="line">        os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(0.0002, 0.5)</span><br><span class="line">    loss = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    real_dacc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    fake_dacc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    gacc = keras.metrics.BinaryAccuracy()</span><br><span class="line"></span><br><span class="line">    model_vgg = vgg(input_shape=(high_resolution[0], high_resolution[1], 3))</span><br><span class="line"></span><br><span class="line">    model_d = discriminator(input_shape=(high_resolution[0], high_resolution[1], 3))</span><br><span class="line">    model_d.compile(optimizer=optimizer, loss='binary_crossentropy')</span><br><span class="line"></span><br><span class="line">    model_g = generator(input_shape=(low_resolution[0], low_resolution[1], 3))</span><br><span class="line"></span><br><span class="line">    model_vgg.build(input_shape=(high_resolution[0], high_resolution[1], 3))</span><br><span class="line">    model_vgg.summary()</span><br><span class="line">    keras.utils.plot_model(model_vgg, 'SRGAN-vgg19.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_g.build(input_shape=(low_resolution[0], low_resolution[1], 3))</span><br><span class="line">    model_g.summary()</span><br><span class="line">    keras.utils.plot_model(model_g, 'SRGAN-generator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_d.build(input_shape=(high_resolution[0], high_resolution[1], 3))</span><br><span class="line">    model_d.summary()</span><br><span class="line">    keras.utils.plot_model(model_d, 'SRGAN-discriminator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model = srgan(input_shape=(low_resolution[0], low_resolution[1], 3), model_vgg=model_vgg, model_g=model_g, model_d=model_d)</span><br><span class="line">    model.compile(optimizer=optimizer, loss=['binary_crossentropy', 'mse'], loss_weights=[1, 100])</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(low_resolution[0], low_resolution[1], 3))</span><br><span class="line">    model.summary()</span><br><span class="line">    keras.utils.plot_model(model, 'SRGAN.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        real_hr_image, real_lr_image = read_data(data_path, high_resolution, low_resolution, batch_size)</span><br><span class="line"></span><br><span class="line">        real_hr_feature = model_vgg(real_hr_image)</span><br><span class="line">        fake_hr_image = model_g(real_lr_image)</span><br><span class="line"></span><br><span class="line">        real_dacc(np.ones((batch_size, high_resolution[0] // 16, high_resolution[1] // 16, 1)), model_d(real_hr_image))</span><br><span class="line">        fake_dacc(np.zeros((batch_size, high_resolution[0] // 16, high_resolution[1] // 16, 1)), model_d(fake_hr_image))</span><br><span class="line">        gacc(np.ones((batch_size, high_resolution[0] // 16, high_resolution[1] // 16, 1)), model(real_lr_image)[0])</span><br><span class="line"></span><br><span class="line">        real_dloss = model_d.train_on_batch(real_hr_image, np.ones((batch_size, high_resolution[0] // 16, high_resolution[1] // 16, 1)))</span><br><span class="line">        fake_dloss = model_d.train_on_batch(fake_hr_image, np.zeros((batch_size, high_resolution[0] // 16, high_resolution[1] // 16, 1)))</span><br><span class="line">        gloss = model.train_on_batch(real_lr_image, [np.ones((batch_size, high_resolution[0] // 16, high_resolution[1] // 16, 1)), real_hr_feature])</span><br><span class="line"></span><br><span class="line">        if epoch % 20 == 0:</span><br><span class="line">            print('epoch = {}, real_dacc = {}, fake_dacc = {}, gacc = {}'.format(epoch, real_dacc.result(), fake_dacc.result(), gacc.result()))</span><br><span class="line">            real_dacc.reset_states()</span><br><span class="line">            fake_dacc.reset_states()</span><br><span class="line">            gacc.reset_states()</span><br><span class="line">            real_hr_image, real_lr_image = read_data(data_path, high_resolution, low_resolution, batch_size=1)</span><br><span class="line">            fake_hr_image = ((model_g(real_lr_image).numpy().squeeze() + 1) * 127.5).astype(np.uint8)</span><br><span class="line">            scale_hr_image = ((cv.resize(real_lr_image.squeeze(), high_resolution) + 1) * 127.5).astype(np.uint8)</span><br><span class="line">            cv.imwrite(save_path + '\\epoch{}.jpg'.format(epoch), np.concatenate([scale_hr_image, fake_hr_image], axis=1))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Generative_adversarial/srgan_R.png" alt="srgan"></p><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Generative_adversarial/srgan_T.png" alt="srgan"></p><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>可以设置一些<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li><strong>SRGAN对于网络结构，优化器参数，网络层的一些超参数都是非常敏感的，效果不好不容易发现原因，这可能需要较多的工程实践经验</strong>。</li><li><strong>先创建判别器，然后进行compile，这样判别器就固定了，然后创建生成器时，不要训练判别器，需要将判别器的trainable改成False，此时不会影响之前固定的判别器</strong>，这个可以<strong>通过模型的_collection_collected_trainable_weights属性查看</strong>，如果该属性为空，则模型不训练，否则模型可以训练，compile之后，该属性固定，无论后面如何修改trainable，只要不重新compile，都不影响训练。</li><li>要注意<strong>使用VGG19特征提取网络权重时，因为VGG19的输入尺寸是224x224x3的，因此图像的尺寸要匹配，如果想生成更大尺寸的高分辨率图像，需要自己训练一个合适的特征提取网络权重</strong>。</li><li>在SRGAN的测试图像中，为了说明模型的优越性，左边为低分辨率图像直接resize到高分辨率图像的结果，右边为SRGAN生成的高分辨率图像的结果。只是训练了2000代，每一代只有2个图像就可以看出SRGAN的效果。小伙伴们可以选择更大的数据集，更加快速的GPU，训练更长的时间，这两种算法之间的差距会更加明显。</li></ol><h1 id="SRGAN小结"><a href="#SRGAN小结" class="headerlink" title="SRGAN小结"></a><font size="5" color="red">SRGAN小结</font></h1><p>  SRGAN是一种有效的超分辨率生成式对抗网络，从上图可以看出<strong>SRGAN模型的参数量只有7M</strong>，最近AI老图像复原引起了人们的注意，训练好SRGAN模型后可以运用在AI老图像复原，可以将原来拍摄的低分辨率的图像转化为清晰的高分辨率图像，是不是非常有趣呢？</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;SRGAN&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>等式方程的可满足性(Leetcode 990)</title>
    <link href="https://USTCcoder.github.io/2020/06/08/program%20Leetcode990/"/>
    <id>https://USTCcoder.github.io/2020/06/08/program Leetcode990/</id>
    <published>2020-06-08T08:54:39.000Z</published>
    <updated>2020-09-02T02:18:54.235Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode990.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这道题目是一个集合问题，相等的变量位于同一个集合之中，判断不相等的变量是否存在于同一个集合之中，集合之间的查询要想到并查集。</p><a id="more"></a><h1 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a><font size="5" color="red">并查集</font></h1><p>先将相等关系进行处理，把所有的变量分成不同的集合，最后再遍历不等关系，判断不相等的两个遍历是否存在于同一个集合之中。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">class UnionFind:</span><br><span class="line">    def __init__(self, equations):</span><br><span class="line">        self.parents = {}</span><br><span class="line">        for x, _, _, y in equations:</span><br><span class="line">            self.parents[x] = x</span><br><span class="line">            self.parents[y] = y</span><br><span class="line"></span><br><span class="line">    def find(self, x):</span><br><span class="line">        if self.parents[x] != x:</span><br><span class="line">            self.parents[x] = self.find(self.parents[x])</span><br><span class="line">        return self.parents[x]</span><br><span class="line"></span><br><span class="line">    def union(self, x, y):</span><br><span class="line">        root_x, root_y = self.find(x), self.find(y)</span><br><span class="line">        if root_x != root_y:</span><br><span class="line">            self.parents[root_x] = self.parents[root_y]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Solution:</span><br><span class="line">    def equationsPossible(self, equations):</span><br><span class="line">        uf = UnionFind(equations)</span><br><span class="line">        for x, c, _, y in equations:</span><br><span class="line">            if c == '=':</span><br><span class="line">                uf.union(x, y)</span><br><span class="line">        for x, c, _, y in equations:</span><br><span class="line">            if c == '!' and uf.find(x) == uf.find(y):</span><br><span class="line">                return False</span><br><span class="line">        return True</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  此题还可以使用BFS或者DFS求解，类似于岛屿问题，将这些变量分成不同的集合，但是因为还需要判断变量之间的不等号关系，即判断变量之间是否不在同一个集合，如果使用BFS或者DFS还需要将每一组不等号去搜索两个变量是否为同一个集合，而并查集每个元素都有一个老大，因此比较两个老大是否为同一个就可以比较是否在同一个集合中，思路清晰，代码也很简单。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 990&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>单词接龙 II(Leetcode 126)</title>
    <link href="https://USTCcoder.github.io/2020/06/07/program%20Leetcode126/"/>
    <id>https://USTCcoder.github.io/2020/06/07/program Leetcode126/</id>
    <published>2020-06-07T07:11:49.000Z</published>
    <updated>2020-09-02T02:17:49.253Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode126.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这道题目是一个<strong>路径搜索问题，每次只能改变一个字母，因此每一个单词都有邻接关系，求出从起始单词到终点单词的路径中最短的所有路径</strong>，这题<strong>难点不在于路径搜索，而是如何最高效的确定路径</strong>，因为要判断两个单词是否有邻接关系，遍历所有字符，而且逐个字符判断，<strong>时间复杂度为$O(c * n^2)$</strong>，有没有更好更快的方法呢？</p><a id="more"></a><h1 id="邻接关系的确定"><a href="#邻接关系的确定" class="headerlink" title="邻接关系的确定"></a><font size="5" color="red">邻接关系的确定</font></h1><p>如果两两挑选，并且逐个字符判断，则<strong>时间复杂度为$O(c * n^2)$</strong>，如果建立一个哈希表，里面存储缺省一个字符的所有匹配单词，则可以在<strong>时间复杂度为$O(c * n)$</strong>的情况下确定邻接关系。<br><img src="/images/ALGORITHM/leetcode126_route.png" alt="route"></p><h1 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a><font size="5" color="red">DFS</font></h1><p>采用DFS来求解，沿着起始单词进行路径搜索，如果搜索到终点则记录长度，并且将路径保存，如果当前长度大于最优长度或者出现环，则剪枝，不继续搜索。最后从所有符合的路径中挑选出长度为最优长度的即可，因为初始最优长度设为无穷大，如果存在大量的环，DFS会耗费非常多的时间，因此不是最优的解法。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">from collections import deque, defaultdict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Solution(object):</span><br><span class="line">    def findLadders(self, beginWord, endWord, wordList):</span><br><span class="line">        """</span><br><span class="line">        :type beginWord: str</span><br><span class="line">        :type endWord: str</span><br><span class="line">        :type wordList: List[str]</span><br><span class="line">        :rtype: List[List[str]]</span><br><span class="line">        """</span><br><span class="line">        best_route = []</span><br><span class="line">        best_n = float('inf')</span><br><span class="line">        lens = len(beginWord)</span><br><span class="line">        relation = defaultdict(list)</span><br><span class="line">        list(map(lambda word: [relation[word[:i] + '*' + word[i + 1:]].append(word) for i in range(lens)], wordList))</span><br><span class="line"></span><br><span class="line">        def dfs(current, n):</span><br><span class="line">            nonlocal best_n</span><br><span class="line">            if n &gt; best_n or current[-1] in current[:-1]:</span><br><span class="line">                return</span><br><span class="line">            if current[-1] == endWord:</span><br><span class="line">                best_route.append(current)</span><br><span class="line">                best_n = len(current)</span><br><span class="line">                return</span><br><span class="line">            for i in range(lens):</span><br><span class="line">                for next_word in relation[current[-1][:i] + '*' + current[-1][i + 1:]]:</span><br><span class="line">                    dfs(current + [next_word], n + 1)</span><br><span class="line"></span><br><span class="line">        dfs([beginWord], 1)</span><br><span class="line">        return [x for x in best_route if len(x) == best_n]</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="BFS"><a href="#BFS" class="headerlink" title="BFS"></a><font size="5" color="red">BFS</font></h1><p>想到最短路径就要想到广度优先搜索，因为广度优先搜索是从起点开始一层一层向外搜索，因此最先搜索到终点的路径一定是最短路径，为什么这道题目BFS比DFS快呢，因为BFS经过的路径不需要再次经过，所以建立一个passed集合，经过的点都记录在里面，下次直接跳过即可，注意这道题目要求所有的最短路径，只能每一层寻找完之后再加入passed集合，如果A-B-C是可以的，这时就将C加入passed集合，那么A-D-C就会被跳过，因此要将当前层都遍历后再修改passed集合。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def findLadders(self, beginWord, endWord, wordList):</span><br><span class="line">        """</span><br><span class="line">        :type beginWord: str</span><br><span class="line">        :type endWord: str</span><br><span class="line">        :type wordList: List[str]</span><br><span class="line">        :rtype: List[List[str]]</span><br><span class="line">        """</span><br><span class="line">        lens = len(beginWord)</span><br><span class="line">        relation = defaultdict(list)</span><br><span class="line">        list(map(lambda word: [relation[word[:i] + '*' + word[i + 1:]].append(word) for i in range(lens)], wordList))</span><br><span class="line">        current_passed = set()</span><br><span class="line">        passed = set().union([beginWord])</span><br><span class="line">        current_queue = deque([[beginWord]])</span><br><span class="line">        next_queue = deque()</span><br><span class="line">        flag = False</span><br><span class="line">        while True:</span><br><span class="line">            while current_queue:</span><br><span class="line">                current = current_queue.popleft()</span><br><span class="line">                for i in range(lens):</span><br><span class="line">                    for word in relation[current[-1][:i] + '*' + current[-1][i + 1:]]:</span><br><span class="line">                        if word not in passed:</span><br><span class="line">                            if word == endWord:</span><br><span class="line">                                flag = True</span><br><span class="line">                            current_passed.add(word)</span><br><span class="line">                            next_queue.append(current + [word])</span><br><span class="line">            if not next_queue:</span><br><span class="line">                return []</span><br><span class="line">            if flag:</span><br><span class="line">                return [x for x in next_queue if x[-1] == endWord]</span><br><span class="line">            else:</span><br><span class="line">                passed.update(current_passed)</span><br><span class="line">                current_queue = next_queue</span><br><span class="line">                next_queue = deque()</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  路径搜索问题太经典了，多次强调BFS和DFS一定要牢记，两者绝大部分情况是可以相互转换的，但是可能存在着效率不同的问题，小伙伴们多多刷题，多多总结。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 126&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>WGAN-GP</title>
    <link href="https://USTCcoder.github.io/2020/06/07/generative_adversarial%20WGAN-GP/"/>
    <id>https://USTCcoder.github.io/2020/06/07/generative_adversarial WGAN-GP/</id>
    <published>2020-06-07T02:01:52.000Z</published>
    <updated>2020-06-09T00:14:11.107Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">WGAN-GP</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>WGAN-GP(Wasserstein Generative Adversarial Networks-Gradient Penalty)</strong>:于<strong>2017年</strong>发表于<strong>NIPS</strong>，<strong>是WGAN的升级版本，WGAN理论的前提是1-Liposchitz条件，WGAN中使用的方法是权重裁剪，这不是一个非常好的办法，WGAN-GP使用了一种GP(Gradient Penalty, 梯度惩罚)的方法替代权重裁剪，构建了一个更加稳定，收敛更快，质量更高的生成式对抗网络</strong>。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/wgan-gp.png" alt="wgan-gp"></p><h1 id="权重裁剪的缺陷"><a href="#权重裁剪的缺陷" class="headerlink" title="权重裁剪的缺陷"></a><font size="5" color="red">权重裁剪的缺陷</font></h1><p>  <font size="3"><strong>极大的限制了网络的表现能力，因为对权重进行的范围限制，使得网络很难模拟出想要得到的函数</strong>。</font><br>  <font size="3"><strong>容易出现梯度爆炸和梯度消失的现象，而且本来生成式对抗网络的参数设置就很敏感，这样更难收敛</strong>。</font></p><h1 id="WGAN-GP的特点"><a href="#WGAN-GP的特点" class="headerlink" title="WGAN-GP的特点"></a><font size="5" color="red">WGAN-GP的特点</font></h1><p>  <font size="3"><strong>保持GAN的网络结构不变，将判别器网络最后的sigmoid删去</strong>。</font><br>  <font size="3"><strong>使用随机采样作为惩罚项，不用WGAN中的权重裁剪</strong>。</font><br>  <font size="3"><strong>重新采用Adam优化器，不存在WGAN种Adam稳定性不高的问题</strong>。</font></p><h1 id="WGAN-GP图像分析"><a href="#WGAN-GP图像分析" class="headerlink" title="WGAN-GP图像分析"></a><font size="5" color="red">WGAN-GP图像分析</font></h1><p><img src="/images/Generative_adversarial/WGANGP-generator.png" alt="generator"><br><img src="/images/Generative_adversarial/WGANGP-discriminator.png" alt="discriminator"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from functools import reduce, partial</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Dense(256, activation='relu', name='dense_relu1'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8),</span><br><span class="line">                keras.layers.Dense(512, activation='relu', name='dense_relu2'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8),</span><br><span class="line">                keras.layers.Dense(1024, activation='relu', name='dense_relu3'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8),</span><br><span class="line">                keras.layers.Dense(784, activation='tanh', name='dense_tanh'),</span><br><span class="line">                keras.layers.Reshape((28, 28, 1)))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='WGANGP-Generator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discriminator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Flatten(name='flatten'),</span><br><span class="line">                keras.layers.Dense(512, activation='relu', name='dense_relu1'),</span><br><span class="line">                keras.layers.Dense(256, activation='relu', name='dense_relu2'),</span><br><span class="line">                keras.layers.Dense(1, name='dense'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='WGANGP-Discriminator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Weight_Addition(keras.layers.Layer):</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        super(Weight_Addition, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        r = np.random.uniform(0, 1, (batch_size, 1, 1, 1))</span><br><span class="line"></span><br><span class="line">        return inputs[0] * r + inputs[1] * (1 - r)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def wgan_gp(input_image_shape, input_noise_shape, model_g, model_d):</span><br><span class="line">    input_image = keras.layers.Input(input_image_shape, name='input_image')</span><br><span class="line">    input_noise_d = keras.layers.Input(input_noise_shape, name='input_noise_discriminator')</span><br><span class="line">    input_noise_g = keras.layers.Input(input_noise_shape, name='input_noise_generator')</span><br><span class="line"></span><br><span class="line">    model_g.trainable = False</span><br><span class="line">    fake_image = model_g(input_noise_d)</span><br><span class="line">    real_conf = model_d(input_image)</span><br><span class="line">    fake_conf = model_d(fake_image)</span><br><span class="line">    random_sample = Weight_Addition(name='random_sample')([input_image, fake_image])</span><br><span class="line">    sample_conf = model_d(random_sample)</span><br><span class="line">    partial_gp_loss = partial(gp_loss, random_sample=random_sample)</span><br><span class="line">    partial_gp_loss.__name__ = 'gp_loss'</span><br><span class="line">    model_discriminator = keras.Model([input_image, input_noise_d], [real_conf, fake_conf, sample_conf], name='WGAN-GP-discriminator')</span><br><span class="line">    model_discriminator.compile(optimizer=optimizer_d, loss=[wasserstein_loss, wasserstein_loss, partial_gp_loss], loss_weights=[1, 1, 10])</span><br><span class="line"></span><br><span class="line">    model_g.trainable = True</span><br><span class="line">    model_d.trainable = False</span><br><span class="line"></span><br><span class="line">    image = model_g(input_noise_g)</span><br><span class="line">    conf = model_d(image)</span><br><span class="line"></span><br><span class="line">    model_generator = keras.Model(input_noise_g, conf, name='WGAN-GP-generator')</span><br><span class="line">    model_generator.compile(optimizer=optimizer_g, loss=wasserstein_loss)</span><br><span class="line"></span><br><span class="line">    return model_generator, model_discriminator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def save_picture(image, save_path, picture_num):</span><br><span class="line">    image = ((image + 1) * 127.5).astype(np.uint8)</span><br><span class="line">    image = np.concatenate([image[i * picture_num:(i + 1) * picture_num] for i in range(picture_num)], axis=2)</span><br><span class="line">    image = np.concatenate([image[i] for i in range(picture_num)], axis=0)</span><br><span class="line">    cv.imwrite(save_path, image)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def gp_loss(y_true, y_pred, random_sample):</span><br><span class="line">    gradients = tf.gradients(y_pred, random_sample)[0]</span><br><span class="line">    return tf.reduce_mean((tf.norm(gradients, 2) - 1.) ** 2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def wasserstein_loss(y_true, y_pred):</span><br><span class="line"></span><br><span class="line">    return -tf.reduce_mean(y_true * y_pred)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    (x, _), (_, _) = keras.datasets.mnist.load_data()</span><br><span class="line">    batch_size = 256</span><br><span class="line">    epochs = 50</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    save_path = r'.\wgangp'</span><br><span class="line">    if not os.path.exists(save_path):</span><br><span class="line">        os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">    x = x[..., np.newaxis].astype(np.float32) / 127.5 - 1</span><br><span class="line">    x = tf.data.Dataset.from_tensor_slices(x).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    optimizer_g = keras.optimizers.Adam(0.00005, 0.5)</span><br><span class="line">    optimizer_d = keras.optimizers.Adam(0.0002, 0.5)</span><br><span class="line"></span><br><span class="line">    real_dmean = keras.metrics.Mean()</span><br><span class="line">    fake_dmean = keras.metrics.Mean()</span><br><span class="line">    gmean = keras.metrics.Mean()</span><br><span class="line"></span><br><span class="line">    model_d = discriminator(input_shape=(28, 28, 1))</span><br><span class="line"></span><br><span class="line">    model_g = generator(input_shape=(100,))</span><br><span class="line"></span><br><span class="line">    model_g.build(input_shape=(100,))</span><br><span class="line">    model_g.summary()</span><br><span class="line">    keras.utils.plot_model(model_g, 'WGANGP-generator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_d.build(input_shape=(28, 28, 1))</span><br><span class="line">    model_d.summary()</span><br><span class="line">    keras.utils.plot_model(model_d, 'WGANGP-discriminator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_generator, model_discriminator = wgan_gp(input_image_shape=(28, 28, 1), input_noise_shape=(100,), model_g=model_g, model_d=model_d)</span><br><span class="line"></span><br><span class="line">    model_generator.build(input_shape=(100,))</span><br><span class="line">    model_generator.summary()</span><br><span class="line">    keras.utils.plot_model(model_generator, 'WGAN-GP-generator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_discriminator.build(input_shape=[(28, 28, 1), (100,)])</span><br><span class="line">    model_discriminator.summary()</span><br><span class="line">    keras.utils.plot_model(model_discriminator, 'WGAN-GP-discriminator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line">    </span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        x = x.shuffle(np.random.randint(0, 10000))</span><br><span class="line">        x_db = iter(x)</span><br><span class="line"></span><br><span class="line">        for step, real_image in enumerate(x_db):</span><br><span class="line">            if real_image.shape[0] != batch_size:</span><br><span class="line">                continue</span><br><span class="line">            noise = np.random.normal(0, 1, (real_image.shape[0], 100))</span><br><span class="line"></span><br><span class="line">            real_dmean(model_discriminator([real_image, noise])[0])</span><br><span class="line">            fake_dmean(model_discriminator([real_image, noise])[1])</span><br><span class="line">            gmean(model_generator(noise))</span><br><span class="line"></span><br><span class="line">            d_loss = model_discriminator.train_on_batch([real_image, noise], [np.ones((real_image.shape[0], 1)), -np.ones((real_image.shape[0], 1)), np.zeros((real_image.shape[0], 1))])</span><br><span class="line">            g_loss = model_generator.train_on_batch(noise, np.ones((real_image.shape[0], 1)))</span><br><span class="line"></span><br><span class="line">            if step % 20 == 0:</span><br><span class="line">                print('epoch = {}, step = {}, real_dmean = {}, fake_dmean = {}, gmean = {}'.format(epoch, step, real_dmean.result(), fake_dmean.result(), gmean.result()))</span><br><span class="line">                real_dmean.reset_states()</span><br><span class="line">                fake_dmean.reset_states()</span><br><span class="line">                gmean.reset_states()</span><br><span class="line">                fake_data = np.random.normal(0, 1, (100, 100))</span><br><span class="line">                fake_image = model_g(fake_data)</span><br><span class="line">                save_picture(fake_image.numpy(), save_path + '\\epoch{}_step{}.jpg'.format(epoch, step), 10)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Generative_adversarial/wgan-gp_R.png" alt="wgan-gp"></p><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Generative_adversarial/wgan-gp_T.png" alt="wgan-gp"></p><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>可以设置一些<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li><strong>WGAN-GP对于网络结构，优化器参数，网络层的一些超参数都是非常敏感的，效果不好不容易发现原因，这可能需要较多的工程实践经验</strong>。</li><li><strong>先创建判别器，然后进行compile，这样判别器就固定了，然后创建生成器时，不要训练判别器，需要将判别器的trainable改成False，此时不会影响之前固定的判别器</strong>，这个可以<strong>通过模型的_collection_collected_trainable_weights属性查看</strong>，如果该属性为空，则模型不训练，否则模型可以训练，compile之后，该属性固定，无论后面如何修改trainable，只要不重新compile，都不影响训练。</li><li>代码中使用了partial偏函数的概念，是functools中的内容，有关偏函数的使用，可以参考我的另一篇博客，Function(函数)</li><li><strong>本博客中的WGAN-GP是在GAN的基础上进行修改，当然小伙伴们也可以尝试在DCGAN，CGAN等模型上进行尝试</strong>。</li></ol><h1 id="WGAN-GP小结"><a href="#WGAN-GP小结" class="headerlink" title="WGAN-GP小结"></a><font size="5" color="red">WGAN-GP小结</font></h1><p>  因为<strong>WGAN-GP基本没有修改网络结构，因此网络参数和GAN完全相同</strong>，WGAN-GP在提出时对网络的损失函数进行了大量的分析，<strong>使用随机采样的思想代替权重裁剪，增加了网络的表现能力和稳定性，这种GP思想对后面生成式对抗网络的发展有着巨大的推动作用</strong>，小伙伴们可以跳过WGAN的学习，但是WGAN-GP是需要我们了解的模型。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;WGAN-GP&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>WGAN</title>
    <link href="https://USTCcoder.github.io/2020/06/05/generative_adversarial%20WGAN/"/>
    <id>https://USTCcoder.github.io/2020/06/05/generative_adversarial WGAN/</id>
    <published>2020-06-05T08:40:51.000Z</published>
    <updated>2020-06-06T02:24:55.719Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">WGAN</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>WGAN(Wasserstein Generative Adversarial Networks)</strong>:于<strong>2017年</strong>提出，<strong>和LSGAN类似，没有对网络结构做太多修改，分析了GAN网络中判别器效果越好，生成器梯度消失越严重的问题，而且提出了一种新的损失函数，构建了一个更加稳定，收敛更快，质量更高的生成式对抗网络</strong>。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/wgan.png" alt="wgan"></p><h1 id="WGAN特点"><a href="#WGAN特点" class="headerlink" title="WGAN特点"></a><font size="5" color="red">WGAN特点</font></h1><p>  <font size="3"><strong>保持GAN的网络结构不变，将判别器网络最后的sigmoid删去</strong>。</font><br>  <font size="3"><strong>将损失函数中的log删去</strong>。</font><br>  <font size="3"><strong>每次更新判别器的参数，将参数绝对值截断到一个固定常数c</strong>。</font><br>  <font size="3"><strong>不使用基于动量的优化算法(Adam)，推荐使用RMSProp，SGD等方法</strong>。</font></p><h1 id="WGAN图像分析"><a href="#WGAN图像分析" class="headerlink" title="WGAN图像分析"></a><font size="5" color="red">WGAN图像分析</font></h1><p><img src="/images/Generative_adversarial/WGAN-generator.png" alt="generator"><br><img src="/images/Generative_adversarial/WGAN-discriminator.png" alt="discriminator"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Dense(256, activation='relu', name='dense_relu1'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn1'),</span><br><span class="line">                keras.layers.Dense(512, activation='relu', name='dense_relu2'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn2'),</span><br><span class="line">                keras.layers.Dense(1024, activation='relu', name='dense_relu3'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn3'),</span><br><span class="line">                keras.layers.Dense(784, activation='tanh', name='dense_tanh'),</span><br><span class="line">                keras.layers.Reshape((28, 28, 1), name='reshape'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='WGAN-Generator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discriminator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Flatten(name='flatten'),</span><br><span class="line">                keras.layers.Dense(512, activation='relu', name='dense_relu1'),</span><br><span class="line">                keras.layers.Dense(256, activation='relu', name='dense_relu2'),</span><br><span class="line">                keras.layers.Dense(1, name='dense'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='WGAN-Discriminator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def wgan(input_shape, model_g, model_d):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = model_g(x)</span><br><span class="line">    model_d.trainable = False</span><br><span class="line">    x = model_d(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='WGAN')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def wasserstein_loss(y_true, y_pred):</span><br><span class="line">    return -tf.reduce_mean(y_true * y_pred)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def save_picture(image, save_path, picture_num):</span><br><span class="line">    image = ((image + 1) * 127.5).astype(np.uint8)</span><br><span class="line">    image = np.concatenate([image[i * picture_num:(i + 1) * picture_num] for i in range(picture_num)], axis=2)</span><br><span class="line">    image = np.concatenate([image[i] for i in range(picture_num)], axis=0)</span><br><span class="line">    cv.imwrite(save_path, image)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    (x, _), (_, _) = keras.datasets.mnist.load_data()</span><br><span class="line">    batch_size = 256</span><br><span class="line">    epochs = 50</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    c = 0.01</span><br><span class="line">    save_path = r'.\wgan'</span><br><span class="line">    if not os.path.exists(save_path):</span><br><span class="line">        os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">    x = x[..., np.newaxis].astype(np.float32) / 127.5 - 1</span><br><span class="line">    x = tf.data.Dataset.from_tensor_slices(x).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    optimizer_g = keras.optimizers.RMSprop(0.00005)</span><br><span class="line">    optimizer_d = keras.optimizers.RMSprop(0.0002)</span><br><span class="line"></span><br><span class="line">    real_dmean = keras.metrics.Mean()</span><br><span class="line">    fake_dmean = keras.metrics.Mean()</span><br><span class="line">    gmean = keras.metrics.Mean()</span><br><span class="line"></span><br><span class="line">    model_d = discriminator(input_shape=(28, 28, 1))</span><br><span class="line">    model_d.compile(optimizer=optimizer_d, loss=wasserstein_loss)</span><br><span class="line"></span><br><span class="line">    model_g = generator(input_shape=(100,))</span><br><span class="line"></span><br><span class="line">    model_g.build(input_shape=(100,))</span><br><span class="line">    model_g.summary()</span><br><span class="line">    keras.utils.plot_model(model_g, 'WGAN-generator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_d.build(input_shape=(28, 28, 1))</span><br><span class="line">    model_d.summary()</span><br><span class="line">    keras.utils.plot_model(model_d, 'WGAN-discriminator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model = wgan(input_shape=(100,), model_g=model_g, model_d=model_d)</span><br><span class="line">    model.compile(optimizer=optimizer_g, loss=wasserstein_loss)</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(100,))</span><br><span class="line">    model.summary()</span><br><span class="line">    keras.utils.plot_model(model, 'WGAN.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        x = x.shuffle(np.random.randint(0, 10000))</span><br><span class="line">        x_db = iter(x)</span><br><span class="line"></span><br><span class="line">        for step, real_image in enumerate(x_db):</span><br><span class="line">            noise = np.random.normal(0, 1, (real_image.shape[0], 100))</span><br><span class="line">            fake_image = model_g(noise)</span><br><span class="line"></span><br><span class="line">            real_dmean(model_d(real_image))</span><br><span class="line">            fake_dmean(model_d(fake_image))</span><br><span class="line">            gmean(model_d(fake_image))</span><br><span class="line"></span><br><span class="line">            real_dloss = model_d.train_on_batch(real_image, np.ones((real_image.shape[0], 1)))</span><br><span class="line">            fake_dloss = model_d.train_on_batch(fake_image, -np.ones((real_image.shape[0], 1)))</span><br><span class="line"></span><br><span class="line">            _ = [x.assign(tf.clip_by_value(x, -c, c)) for x in model_d.variables]</span><br><span class="line"></span><br><span class="line">            gloss = model.train_on_batch(noise, np.ones((real_image.shape[0], 1)))</span><br><span class="line"></span><br><span class="line">            if step % 20 == 0:</span><br><span class="line">                print('epoch = {}, step = {}, real_dmean = {}, fake_dmean = {}, gmean = {}'.format(epoch, step, real_dmean.result(), fake_dmean.result(), gmean.result()))</span><br><span class="line">                real_dmean.reset_states()</span><br><span class="line">                fake_dmean.reset_states()</span><br><span class="line">                gmean.reset_states()</span><br><span class="line">                fake_data = np.random.normal(0, 1, (100, 100))</span><br><span class="line">                fake_image = model_g(fake_data)</span><br><span class="line">                save_picture(fake_image.numpy(), save_path + '\\epoch{}_step{}.jpg'.format(epoch, step), 10)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Generative_adversarial/wgan_R.png" alt="wgan"></p><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Generative_adversarial/wgan_T.png" alt="wgan"></p><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>可以设置一些<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li><strong>WGAN对于网络结构，优化器参数，网络层的一些超参数都是非常敏感的，效果不好不容易发现原因，这可能需要较多的工程实践经验</strong>。</li><li><strong>先创建判别器，然后进行compile，这样判别器就固定了，然后创建生成器时，不要训练判别器，需要将判别器的trainable改成False，此时不会影响之前固定的判别器</strong>，这个可以<strong>通过模型的_collection_collected_trainable_weights属性查看</strong>，如果该属性为空，则模型不训练，否则模型可以训练，compile之后，该属性固定，无论后面如何修改trainable，只要不重新compile，都不影响训练。</li><li><strong>本博客中的WGAN是在GAN的基础上进行修改，当然小伙伴们也可以尝试在DCGAN，CGAN等模型上进行尝试，可能一些超参数设置的不是非常合理，所以WGAN的效果不是特别好，小伙伴们在使用时可以自己修改</strong>。</li></ol><h1 id="WGAN小结"><a href="#WGAN小结" class="headerlink" title="WGAN小结"></a><font size="5" color="red">WGAN小结</font></h1><p>  <strong>WGAN在提出时对网络的损失函数进行了大量的分析</strong>，引入W距离，Lipschitz常数等等，我不是大佬，也不对数学公式进行过多的阐述，可能我说了会让小伙伴们更加迷糊，因此<strong>有需要的小伙伴们可以去网上搜索相关资料</strong>。因为<strong>WGAN基本没有修改网络结构，因此网络参数和GAN完全相同</strong>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;WGAN&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>LSGAN</title>
    <link href="https://USTCcoder.github.io/2020/06/04/generative_adversarial%20LSGAN/"/>
    <id>https://USTCcoder.github.io/2020/06/04/generative_adversarial LSGAN/</id>
    <published>2020-06-04T09:03:33.000Z</published>
    <updated>2020-06-07T05:40:02.752Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">LSGAN</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>LSGAN(Least Squares Generative Adversarial Networks, 最小二乘生成式对抗网络)</strong>:于<strong>2016年</strong>提出，<strong>分析了GAN网络中使用的交叉熵损失函数时可能会导致在饱和区收敛速度太慢，而且提出了一种新的最小二乘损失函数代替，构建了一个更加稳定，收敛更快，质量更高的生成式对抗网络</strong>。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/lsgan.png" alt="acgan"></p><h1 id="LSGAN特点"><a href="#LSGAN特点" class="headerlink" title="LSGAN特点"></a><font size="5" color="red">LSGAN特点</font></h1><p>  <font size="3"><strong>保持GAN的网络结构不变，仅仅将判别器网络最后的sigmoid删去，并且将损失函数由二分类交叉熵修改为均方差</strong>。</font></p><h1 id="LSGAN图像分析"><a href="#LSGAN图像分析" class="headerlink" title="LSGAN图像分析"></a><font size="5" color="red">LSGAN图像分析</font></h1><p><img src="/images/Generative_adversarial/LSGAN-generator.png" alt="generator"><br><img src="/images/Generative_adversarial/LSGAN-discriminator.png" alt="discriminator"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Dense(256, activation='relu', name='dense_relu1'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn1'),</span><br><span class="line">                keras.layers.Dense(512, activation='relu', name='dense_relu2'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn2'),</span><br><span class="line">                keras.layers.Dense(1024, activation='relu', name='dense_relu3'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn3'),</span><br><span class="line">                keras.layers.Dense(784, activation='tanh', name='dense_tanh'),</span><br><span class="line">                keras.layers.Reshape((28, 28, 1), name='reshape'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='LSGAN-Generator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discriminator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Flatten(name='flatten'),</span><br><span class="line">                keras.layers.Dense(512, activation='relu', name='dense_relu1'),</span><br><span class="line">                keras.layers.Dense(256, activation='relu', name='dense_relu2'),</span><br><span class="line">                keras.layers.Dense(1, name='dense'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='LSGAN-Discriminator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lsgan(input_shape, model_g, model_d):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = model_g(x)</span><br><span class="line">    model_d.trainable = False</span><br><span class="line">    x = model_d(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='LSGAN')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def save_picture(image, save_path, picture_num):</span><br><span class="line">    image = ((image + 1) * 127.5).astype(np.uint8)</span><br><span class="line">    image = np.concatenate([image[i * picture_num:(i + 1) * picture_num] for i in range(picture_num)], axis=2)</span><br><span class="line">    image = np.concatenate([image[i] for i in range(picture_num)], axis=0)</span><br><span class="line">    cv.imwrite(save_path, image)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    (x, _), (_, _) = keras.datasets.mnist.load_data()</span><br><span class="line">    batch_size = 256</span><br><span class="line">    epochs = 20</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    save_path = r'.\lsgan'</span><br><span class="line">    if not os.path.exists(save_path):</span><br><span class="line">        os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">    x = x[..., np.newaxis].astype(np.float) / 127.5 - 1</span><br><span class="line">    x = tf.data.Dataset.from_tensor_slices(x).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(0.0002, 0.5)</span><br><span class="line">    loss = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    real_dmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    fake_dmse = keras.metrics.MeanSquaredError()</span><br><span class="line">    gmse = keras.metrics.MeanSquaredError()</span><br><span class="line"></span><br><span class="line">    model_d = discriminator(input_shape=(28, 28, 1))</span><br><span class="line">    model_d.compile(optimizer=optimizer, loss='mse')</span><br><span class="line"></span><br><span class="line">    model_g = generator(input_shape=(100,))</span><br><span class="line"></span><br><span class="line">    model_g.build(input_shape=(100,))</span><br><span class="line">    model_g.summary()</span><br><span class="line">    keras.utils.plot_model(model_g, 'LSGAN-generator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_d.build(input_shape=(28, 28, 1))</span><br><span class="line">    model_d.summary()</span><br><span class="line">    keras.utils.plot_model(model_d, 'LSGAN-discriminator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model = lsgan(input_shape=(100,), model_g=model_g, model_d=model_d)</span><br><span class="line">    model.compile(optimizer=optimizer, loss='mse')</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(100,))</span><br><span class="line">    model.summary()</span><br><span class="line">    keras.utils.plot_model(model, 'LSGAN.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        x = x.shuffle(np.random.randint(0, 10000))</span><br><span class="line">        x_db = iter(x)</span><br><span class="line"></span><br><span class="line">        for step, real_image in enumerate(x_db):</span><br><span class="line">            noise = np.random.normal(0, 1, (real_image.shape[0], 100))</span><br><span class="line">            fake_image = model_g(noise)</span><br><span class="line"></span><br><span class="line">            real_dmse(np.ones((real_image.shape[0], 1)), model_d(real_image))</span><br><span class="line">            fake_dmse(np.zeros((real_image.shape[0], 1)), model_d(fake_image))</span><br><span class="line">            gmse(np.ones((real_image.shape[0], 1)), model(noise))</span><br><span class="line"></span><br><span class="line">            real_dloss = model_d.train_on_batch(real_image, np.ones((real_image.shape[0], 1)))</span><br><span class="line">            fake_dloss = model_d.train_on_batch(fake_image, np.zeros((real_image.shape[0], 1)))</span><br><span class="line">            gloss = model.train_on_batch(noise, np.ones((real_image.shape[0], 1)))</span><br><span class="line"></span><br><span class="line">            if step % 20 == 0:</span><br><span class="line">                print('epoch = {}, step = {}, real_dmse = {}, fake_dmse = {}, gmse = {}'.format(epoch, step, real_dmse.result(), fake_dmse.result(), gmse.result()))</span><br><span class="line">                real_dmse.reset_states()</span><br><span class="line">                fake_dmse.reset_states()</span><br><span class="line">                gmse.reset_states()</span><br><span class="line">                fake_data = np.random.normal(0, 1, (100, 100))</span><br><span class="line">                fake_image = model_g(fake_data)</span><br><span class="line">                save_picture(fake_image.numpy(), save_path + '\\epoch{}_step{}.jpg'.format(epoch, step), 10)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Generative_adversarial/lsgan_R.png" alt="lsgan"></p><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Generative_adversarial/lsgan_T.png" alt="lsgan"></p><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>可以设置一些<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li><strong>LSGAN对于网络结构，优化器参数，网络层的一些超参数都是非常敏感的，效果不好不容易发现原因，这可能需要较多的工程实践经验</strong>。</li><li><strong>先创建判别器，然后进行compile，这样判别器就固定了，然后创建生成器时，不要训练判别器，需要将判别器的trainable改成False，此时不会影响之前固定的判别器</strong>，这个可以<strong>通过模型的_collection_collected_trainable_weights属性查看</strong>，如果该属性为空，则模型不训练，否则模型可以训练，compile之后，该属性固定，无论后面如何修改trainable，只要不重新compile，都不影响训练。</li><li><strong>本博客中的LSGAN是在GAN的基础上修改了sigmoid函数和损失函数，当然小伙伴们也可以尝试在DCGAN，CGAN等模型上进行尝试</strong>。</li></ol><h1 id="LSGAN小结"><a href="#LSGAN小结" class="headerlink" title="LSGAN小结"></a><font size="5" color="red">LSGAN小结</font></h1><p>  <strong>LSGAN在提出时对网络的损失函数进行了大量的分析</strong>，我不是大佬，也不对数学公式进行过多的阐述，可能我说了会让小伙伴们更加迷糊，因此<strong>有需要的小伙伴们可以去网上搜索相关资料</strong>。因为<strong>LSGAN基本没有修改网络结构，只是更换了一个激活函数和损失函数，因此网络参数和GAN完全相同</strong>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;LSGAN&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>COGAN</title>
    <link href="https://USTCcoder.github.io/2020/06/03/generative_adversarial%20COGAN/"/>
    <id>https://USTCcoder.github.io/2020/06/03/generative_adversarial COGAN/</id>
    <published>2020-06-03T05:13:51.000Z</published>
    <updated>2020-06-06T03:09:13.587Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">COGAN</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>COGAN(Coupled Generative Adversarial Networks, 耦合生成式对抗网络)</strong>:于<strong>2016年</strong>发表在<strong>NIPS</strong>上，只有两个以上模型才能称之为耦合，因此COGAN中存在两个生成模型和两个判别模型，而且共用某些网络层，实现耦合效果。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/cogan.png" alt="cogan"></p><h1 id="COGAN特点"><a href="#COGAN特点" class="headerlink" title="COGAN特点"></a><font size="5" color="red">COGAN特点</font></h1><p>  <font size="3"><strong>引入两个GAN模型，并且共用某些网络层，实现少参数多功能的效果</strong>。</font></p><h1 id="COGAN图像分析"><a href="#COGAN图像分析" class="headerlink" title="COGAN图像分析"></a><font size="5" color="red">COGAN图像分析</font></h1><p><img src="/images/Generative_adversarial/COGAN-generator1.png" alt="generator"><br><img src="/images/Generative_adversarial/COGAN-discriminator1.png" alt="discriminator"><br><img src="/images/Generative_adversarial/COGAN-COdiscriminator.png" alt="discriminator"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Dense(1568, activation='relu', name='dense_relu'),</span><br><span class="line">                keras.layers.Reshape((7, 7, 32), name='reshape'),</span><br><span class="line">                keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', name='conv1'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn1'),</span><br><span class="line">                keras.layers.ReLU(name='relu1'),</span><br><span class="line">                keras.layers.UpSampling2D((2, 2), name='upsampling1'),</span><br><span class="line">                keras.layers.Conv2D(128, (3, 3), (1, 1), 'same', name='conv2'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn2'),</span><br><span class="line">                keras.layers.ReLU(name='relu2'),</span><br><span class="line">                keras.layers.UpSampling2D((2, 2), name='upsampling2'),</span><br><span class="line">                keras.layers.Conv2D(128, (3, 3), (1, 1), 'same', name='conv3'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn3'),</span><br><span class="line">                keras.layers.ReLU(name='relu3'))(x)</span><br><span class="line"></span><br><span class="line">    x1 = compose(keras.layers.Conv2D(64, (1, 1), (1, 1), 'same', name='conv_part1_4'),</span><br><span class="line">                 keras.layers.BatchNormalization(momentum=0.8, name='bn_part1_4'),</span><br><span class="line">                 keras.layers.ReLU(name='relu_part1_4'),</span><br><span class="line">                 keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', name='conv_part1_5'),</span><br><span class="line">                 keras.layers.BatchNormalization(momentum=0.8, name='bn_part1_5'),</span><br><span class="line">                 keras.layers.ReLU(name='relu_part1_5'),</span><br><span class="line">                 keras.layers.Conv2D(64, (1, 1), (1, 1), 'same', name='conv_part1_6'),</span><br><span class="line">                 keras.layers.BatchNormalization(momentum=0.8, name='bn_part1_6'),</span><br><span class="line">                 keras.layers.ReLU(name='relu_part1_6'),</span><br><span class="line">                 keras.layers.Conv2D(1, (1, 1), (1, 1), 'same', activation='tanh', name='conv_part1_tanh'))(x)</span><br><span class="line"></span><br><span class="line">    x2 = compose(keras.layers.Conv2D(64, (1, 1), (1, 1), 'same', name='conv_part2_4'),</span><br><span class="line">                 keras.layers.BatchNormalization(momentum=0.8, name='bn_part2_4'),</span><br><span class="line">                 keras.layers.ReLU(name='relu_part2_4'),</span><br><span class="line">                 keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', name='conv_part2_5'),</span><br><span class="line">                 keras.layers.BatchNormalization(momentum=0.8, name='bn_part2_5'),</span><br><span class="line">                 keras.layers.ReLU(name='relu_part2_5'),</span><br><span class="line">                 keras.layers.Conv2D(64, (1, 1), (1, 1), 'same', name='conv_part2_6'),</span><br><span class="line">                 keras.layers.BatchNormalization(momentum=0.8, name='bn_part2_6'),</span><br><span class="line">                 keras.layers.ReLU(name='relu_part2_6'),</span><br><span class="line">                 keras.layers.Conv2D(1, (1, 1), (1, 1), 'same', activation='tanh', name='conv_part2_tanh'))(x)</span><br><span class="line"></span><br><span class="line">    model_part1 = keras.Model(input_tensor, x1, name='COGAN-Generator1')</span><br><span class="line">    model_part2 = keras.Model(input_tensor, x2, name='COGAN-Generator2')</span><br><span class="line"></span><br><span class="line">    return model_part1, model_part2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discriminator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    input_tensor1 = keras.layers.Input(input_shape, name='input1')</span><br><span class="line">    input_tensor2 = keras.layers.Input(input_shape, name='input2')</span><br><span class="line"></span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(64, (3, 3), (2, 2), 'same', name='conv1'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn1'),</span><br><span class="line">                keras.layers.LeakyReLU(0.2, name='leakyrelu1'),</span><br><span class="line">                keras.layers.Conv2D(128, (3, 3), (2, 2), 'same', name='conv2'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn2'),</span><br><span class="line">                keras.layers.LeakyReLU(0.2, name='leakyrelu2'),</span><br><span class="line">                keras.layers.Conv2D(64, (3, 3), (2, 2), 'same', name='conv3'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn3'),</span><br><span class="line">                keras.layers.LeakyReLU(0.2, name='leakyrelu3'),</span><br><span class="line">                keras.layers.GlobalAveragePooling2D(name='global_averagepool'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='COGAN-CODiscriminator')</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(28, 28, 1))</span><br><span class="line">    model.summary()</span><br><span class="line">    keras.utils.plot_model(model, 'COGAN-COdiscriminator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    x1 = model(input_tensor1)</span><br><span class="line">    x1 = keras.layers.Dense(1, activation='sigmoid', name='dense_part1_sigmoid')(x1)</span><br><span class="line"></span><br><span class="line">    model_part1 = keras.Model(input_tensor1, x1, name='COGAN-Discriminator1')</span><br><span class="line"></span><br><span class="line">    x2 = model(input_tensor2)</span><br><span class="line">    x2 = keras.layers.Dense(1, activation='sigmoid', name='dense_part2_sigmoid')(x2)</span><br><span class="line"></span><br><span class="line">    model_part2 = keras.Model(input_tensor2, x2, name='COGAN-Discriminator2')</span><br><span class="line"></span><br><span class="line">    return model_part1, model_part2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def cogan(input_shape, model_g1, model_g2, model_d1, model_d2):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape)</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x1 = model_g1(x)</span><br><span class="line">    x2 = model_g2(x)</span><br><span class="line"></span><br><span class="line">    model_d1.trainable = False</span><br><span class="line">    model_d2.trainable = False</span><br><span class="line"></span><br><span class="line">    x1 = model_d1(x1)</span><br><span class="line">    x2 = model_d1(x2)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, [x1, x2], name='COGAN')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def save_picture(image, save_path, picture_num):</span><br><span class="line">    image = ((image + 1) * 127.5).astype(np.uint8)</span><br><span class="line">    image = np.concatenate([image[i * picture_num:(i + 1) * picture_num] for i in range(picture_num)], axis=2)</span><br><span class="line">    image = np.concatenate([image[i] for i in range(picture_num)], axis=0)</span><br><span class="line">    cv.imwrite(save_path, image)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    (x, _), (_, _) = keras.datasets.mnist.load_data()</span><br><span class="line">    batch_size = 256</span><br><span class="line">    epochs = 20</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    save_path = r'.\cogan'</span><br><span class="line">    if not os.path.exists(save_path):</span><br><span class="line">        os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">    x = x[..., np.newaxis].astype(np.float32) / 127.5 - 1</span><br><span class="line">    x = tf.data.Dataset.from_tensor_slices(x).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(0.0002, 0.5)</span><br><span class="line">    loss = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    real_d1acc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    fake_d1acc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    real_d2acc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    fake_d2acc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    g1acc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    g2acc = keras.metrics.BinaryAccuracy()</span><br><span class="line"></span><br><span class="line">    model_d1, model_d2 = discriminator(input_shape=(28, 28, 1))</span><br><span class="line">    model_d1.compile(optimizer=optimizer, loss='binary_crossentropy')</span><br><span class="line">    model_d2.compile(optimizer=optimizer, loss='binary_crossentropy')</span><br><span class="line"></span><br><span class="line">    model_g1, model_g2 = generator(input_shape=(100,))</span><br><span class="line"></span><br><span class="line">    model_g1.build(input_shape=(100,))</span><br><span class="line">    model_g1.summary()</span><br><span class="line">    keras.utils.plot_model(model_g1, 'COGAN-generator1.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_g2.build(input_shape=(100,))</span><br><span class="line">    model_g2.summary()</span><br><span class="line">    keras.utils.plot_model(model_g2, 'COGAN-generator2.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_d1.build(input_shape=(28, 28, 1))</span><br><span class="line">    model_d1.summary()</span><br><span class="line">    keras.utils.plot_model(model_d1, 'COGAN-discriminator1.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_d2.build(input_shape=(28, 28, 1))</span><br><span class="line">    model_d2.summary()</span><br><span class="line">    keras.utils.plot_model(model_d2, 'COGAN-discriminator2.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model = cogan(input_shape=(100,), model_g1=model_g1, model_g2=model_g2, model_d1=model_d1, model_d2=model_d2)</span><br><span class="line">    model.compile(optimizer=optimizer, loss=['binary_crossentropy', 'binary_crossentropy'])</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(100,))</span><br><span class="line">    model.summary()</span><br><span class="line">    keras.utils.plot_model(model, 'COGAN.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        x = x.shuffle(np.random.randint(0, 10000))</span><br><span class="line">        x_db = iter(x)</span><br><span class="line"></span><br><span class="line">        for step, real_image in enumerate(x_db):</span><br><span class="line">            noise = np.random.normal(0, 1, (real_image.shape[0], 100))</span><br><span class="line">            fake_image1 = model_g1(noise)</span><br><span class="line">            fake_image2 = model_g2(noise)</span><br><span class="line"></span><br><span class="line">            real_d1acc(np.ones((real_image.shape[0], 1)), model_d1(real_image))</span><br><span class="line">            fake_d1acc(np.zeros((real_image.shape[0], 1)), model_d1(fake_image1))</span><br><span class="line">            real_d2acc(np.ones((real_image.shape[0], 1)), model_d2(real_image))</span><br><span class="line">            fake_d2acc(np.zeros((real_image.shape[0], 1)), model_d2(fake_image2))</span><br><span class="line">            g1acc(np.ones((real_image.shape[0], 1)), model(noise)[0])</span><br><span class="line">            g2acc(np.ones((real_image.shape[0], 1)), model(noise)[1])</span><br><span class="line"></span><br><span class="line">            real_d1loss = model_d1.train_on_batch(real_image, np.ones((real_image.shape[0], 1)))</span><br><span class="line">            fake_d1loss = model_d1.train_on_batch(fake_image1, np.zeros((real_image.shape[0], 1)))</span><br><span class="line">            real_d2loss = model_d2.train_on_batch(real_image, np.ones((real_image.shape[0], 1)))</span><br><span class="line">            fake_d2loss = model_d2.train_on_batch(fake_image2, np.zeros((real_image.shape[0], 1)))</span><br><span class="line">            gloss = model.train_on_batch(noise, [np.ones((real_image.shape[0], 1)), np.ones((real_image.shape[0], 1))])</span><br><span class="line"></span><br><span class="line">            if step % 20 == 0:</span><br><span class="line">                print('epoch = {}, step = {}, real_d1acc = {}, fake_d1acc = {}, real_d1acc = {}, fake_d1acc = {}, g1acc = {}, g2acc = {}'.format(epoch, step, real_d1acc.result(), fake_d1acc.result(), real_d2acc.result(), fake_d2acc.result(), g1acc.result(), g2acc.result()))</span><br><span class="line">                real_d1acc.reset_states()</span><br><span class="line">                fake_d1acc.reset_states()</span><br><span class="line">                real_d2acc.reset_states()</span><br><span class="line">                fake_d2acc.reset_states()</span><br><span class="line">                g1acc.reset_states()</span><br><span class="line">                g2acc.reset_states()</span><br><span class="line">                fake_data = np.random.normal(0, 1, (100, 100))</span><br><span class="line">                fake_image = model_g1(fake_data)</span><br><span class="line">                save_picture(fake_image.numpy(), save_path + '\\epoch{}_step{}.jpg'.format(epoch, step), 10)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Generative_adversarial/cogan_R.png" alt="cogan"></p><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Generative_adversarial/cogan_T.png" alt="cogan"></p><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>可以设置一些<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li><strong>COGAN对于网络结构，优化器参数，网络层的一些超参数都是非常敏感的，效果不好不容易发现原因，这可能需要较多的工程实践经验</strong>。</li><li><strong>先创建判别器，然后进行compile，这样判别器就固定了，然后创建生成器时，不要训练判别器，需要将判别器的trainable改成False，此时不会影响之前固定的判别器</strong>，这个可以<strong>通过模型的_collection_collected_trainable_weights属性查看</strong>，如果该属性为空，则模型不训练，否则模型可以训练，compile之后，该属性固定，无论后面如何修改trainable，只要不重新compile，都不影响训练。</li><li><strong>COGAN中引入了两个GAN网络，其目的不是实现一种功能，虽然在这里我是实现了一种功能，其实他们共用的目的是节约特征提取网络参数，可以让一个GAN来生成某一种图像，另一个GAN来生成另一种图像，如model_g1生成手写数字，model_g2生成镜像手写数字，如果正常训练两个GAN模型，分别生成手写数字和镜像手写数字，同样的网络结构需要1.2M的参数量，而耦合之后的参数量约为0.6M，而且模型越大效果越明显</strong>。</li></ol><h1 id="COGAN小结"><a href="#COGAN小结" class="headerlink" title="COGAN小结"></a><font size="5" color="red">COGAN小结</font></h1><p>  COGAN是一种有效的耦合生成式对抗网络，从上图可以看出<strong>COGAN模型的参数量只有0.6M</strong>，是一种可以同时完成多任务的网络模型，小伙伴们一定要掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;COGAN&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>ACGAN</title>
    <link href="https://USTCcoder.github.io/2020/06/01/generative_adversarial%20ACGAN/"/>
    <id>https://USTCcoder.github.io/2020/06/01/generative_adversarial ACGAN/</id>
    <published>2020-06-01T07:36:50.000Z</published>
    <updated>2020-06-06T03:12:21.067Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">ACGAN</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>ACGAN(Auxiliary Classifier Generative Adversarial Networks, 辅助分类器生成式对抗网络)</strong>:于<strong>2016年</strong>提出，是CGAN类型网络的升级版本，<strong>引入了Embedding层对类别标签进行处理，而且增加了类别分类网络</strong>，因此称之为辅助分类器生成式对抗网络。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/acgan.png" alt="acgan"></p><h1 id="ACGAN特点"><a href="#ACGAN特点" class="headerlink" title="ACGAN特点"></a><font size="5" color="red">ACGAN特点</font></h1><p>  <font size="3"><strong>类似于DCGAN和CGAN的结合，将卷积使用在CGAN网络之中</strong>。</font><br>  <font size="3"><strong>引入了Embedding层对类别标签进行处理，Embedding层可以将输入的数字转化为一维向量</strong>。</font><br>  <font size="3"><strong>判别器中不但对真假置信度进行loss计算，而且使用了辅助类别分类器，对判别出的类别进行loss计算</strong>。</font></p><h1 id="ACGAN图像分析"><a href="#ACGAN图像分析" class="headerlink" title="ACGAN图像分析"></a><font size="5" color="red">ACGAN图像分析</font></h1><p><img src="/images/Generative_adversarial/ACGAN-generator.png" alt="generator"><br><img src="/images/Generative_adversarial/ACGAN-discriminator.png" alt="discriminator"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generator(input_label_shape, input_noise_shape):</span><br><span class="line">    label = keras.layers.Input(input_label_shape, name='input_label')</span><br><span class="line">    label_tensor = compose(keras.layers.Embedding(10, 100, name='embedding'),</span><br><span class="line">                           keras.layers.Flatten(name='flatten'))(label)</span><br><span class="line">    noise = keras.layers.Input(input_noise_shape, name='input_noise')</span><br><span class="line">    x = keras.layers.Multiply(name='multiply')([label_tensor, noise])</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Dense(1568, activation='relu', name='dense_relu'),</span><br><span class="line">                keras.layers.Reshape((7, 7, 32), name='reshape'),</span><br><span class="line">                keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', name='conv1'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn1'),</span><br><span class="line">                keras.layers.ReLU(name='relu1'),</span><br><span class="line">                keras.layers.UpSampling2D((2, 2), name='upsampling1'),</span><br><span class="line">                keras.layers.Conv2D(128, (3, 3), (1, 1), 'same', name='conv2'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn2'),</span><br><span class="line">                keras.layers.ReLU(name='relu2'),</span><br><span class="line">                keras.layers.UpSampling2D((2, 2), name='upsampling2'),</span><br><span class="line">                keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', name='conv3'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn3'),</span><br><span class="line">                keras.layers.ReLU(name='relu3'),</span><br><span class="line">                keras.layers.Conv2D(1, (3, 3), (1, 1), 'same', activation='tanh', name='conv_tanh'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model([noise, label], x, name='ACGAN-Generator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discriminator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(32, (3, 3), (2, 2), 'same'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8),</span><br><span class="line">                keras.layers.LeakyReLU(0.2),</span><br><span class="line">                keras.layers.Conv2D(64, (3, 3), (2, 2), 'same'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8),</span><br><span class="line">                keras.layers.LeakyReLU(0.2),</span><br><span class="line">                keras.layers.Conv2D(128, (3, 3), (2, 2), 'same'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8),</span><br><span class="line">                keras.layers.LeakyReLU(0.2),</span><br><span class="line">                keras.layers.GlobalAveragePooling2D(name='global_averagepool'))(x)</span><br><span class="line"></span><br><span class="line">    conf = keras.layers.Dense(1, activation='sigmoid', name='dense_sigmoid')(x)</span><br><span class="line">    label = keras.layers.Dense(10, activation='softmax', name='dense_softmax')(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, [conf, label], name='ACGAN-Discriminator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def acgan(input_label_shape, input_noise_shape, model_g, model_d):</span><br><span class="line">    label = keras.layers.Input(input_label_shape, name='input_label')</span><br><span class="line">    noise = keras.layers.Input(input_noise_shape, name='input_noise')</span><br><span class="line"></span><br><span class="line">    x = model_g([noise, label])</span><br><span class="line">    model_d.trainable = False</span><br><span class="line">    conf, pred_label = model_d(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model([noise, label], [conf, pred_label], name='ACGAN')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def save_picture(image, save_path, picture_num):</span><br><span class="line">    image = ((image + 1) * 127.5).astype(np.uint8)</span><br><span class="line">    image = np.concatenate([image[i * picture_num:(i + 1) * picture_num] for i in range(picture_num)], axis=2)</span><br><span class="line">    image = np.concatenate([image[i] for i in range(picture_num)], axis=0)</span><br><span class="line">    cv.imwrite(save_path, image)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    (x, y), (_, _) = keras.datasets.mnist.load_data()</span><br><span class="line">    batch_size = 256</span><br><span class="line">    epochs = 20</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    save_path = r'.\acgan'</span><br><span class="line">    if not os.path.exists(save_path):</span><br><span class="line">        os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">    x = x[..., np.newaxis].astype(np.float32) / 127.5 - 1</span><br><span class="line">    y = y[..., np.newaxis]</span><br><span class="line">    x = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(0.0002, 0.5)</span><br><span class="line">    loss = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    real_dacc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    fake_dacc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    gacc = keras.metrics.BinaryAccuracy()</span><br><span class="line"></span><br><span class="line">    model_d = discriminator(input_shape=(28, 28, 1))</span><br><span class="line">    model_d.compile(optimizer=optimizer, loss=['binary_crossentropy', 'sparse_categorical_crossentropy'])</span><br><span class="line"></span><br><span class="line">    model_g = generator(input_label_shape=(1,), input_noise_shape=(100,))</span><br><span class="line"></span><br><span class="line">    model_g.build(input_shape=[(1,), (100,)])</span><br><span class="line">    model_g.summary()</span><br><span class="line">    keras.utils.plot_model(model_g, 'ACGAN-generator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_d.build(input_shape=(28, 28, 1))</span><br><span class="line">    model_d.summary()</span><br><span class="line">    keras.utils.plot_model(model_d, 'ACGAN-discriminator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model = acgan(input_label_shape=(1,), input_noise_shape=(100,), model_g=model_g, model_d=model_d)</span><br><span class="line">    model.compile(optimizer=optimizer, loss=['binary_crossentropy', 'sparse_categorical_crossentropy'])</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=[(1,), (100,)])</span><br><span class="line">    model.summary()</span><br><span class="line">    keras.utils.plot_model(model, 'ACGAN.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        x = x.shuffle(np.random.randint(0, 10000))</span><br><span class="line">        x_db = iter(x)</span><br><span class="line"></span><br><span class="line">        for step, (real_image, real_label) in enumerate(x_db):</span><br><span class="line">            noise = np.random.normal(0, 1, (real_image.shape[0], 100)).astype(np.float32)</span><br><span class="line">            fake_label = np.random.randint(0, 10, (real_image.shape[0], 1))</span><br><span class="line"></span><br><span class="line">            fake_image = model_g([noise, fake_label])</span><br><span class="line"></span><br><span class="line">            real_dacc(np.ones((real_image.shape[0], 1)), model_d(real_image)[0])</span><br><span class="line">            fake_dacc(np.zeros((real_image.shape[0], 1)), model_d(fake_image)[0])</span><br><span class="line">            gacc(np.ones((real_image.shape[0], 1)), model([noise, fake_label])[0])</span><br><span class="line"></span><br><span class="line">            real_dloss = model_d.train_on_batch(real_image, [np.ones((real_image.shape[0], 1)), real_label])</span><br><span class="line">            fake_dloss = model_d.train_on_batch(fake_image, [np.zeros((real_image.shape[0], 1)), fake_label])</span><br><span class="line">            gloss = model.train_on_batch([noise, fake_label], [np.ones((real_image.shape[0], 1)), fake_label])</span><br><span class="line"></span><br><span class="line">            if step % 20 == 0:</span><br><span class="line">                print('epoch = {}, step = {}, real_dacc = {}, fake_dacc = {}, gacc = {}'.format(epoch, step, real_dacc.result(), fake_dacc.result(), gacc.result()))</span><br><span class="line">                real_dacc.reset_states()</span><br><span class="line">                fake_dacc.reset_states()</span><br><span class="line">                gacc.reset_states()</span><br><span class="line">                fake_data = np.random.normal(0, 1, (100, 100)).astype(np.float32)</span><br><span class="line">                fake_label = np.array(list(range(10)) * 10).reshape((-1, 1))</span><br><span class="line">                fake_image = model_g([fake_data, fake_label])</span><br><span class="line">                save_picture(fake_image.numpy(), save_path + '\\epoch{}_step{}.jpg'.format(epoch, step), 10)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Generative_adversarial/acgan_R.png" alt="acgan"></p><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Generative_adversarial/acgan_T.png" alt="acgan"></p><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>可以设置一些<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li><strong>ACGAN对于网络结构，优化器参数，网络层的一些超参数都是非常敏感的，效果不好不容易发现原因，这可能需要较多的工程实践经验</strong>。</li><li><strong>先创建判别器，然后进行compile，这样判别器就固定了，然后创建生成器时，不要训练判别器，需要将判别器的trainable改成False，此时不会影响之前固定的判别器</strong>，这个可以<strong>通过模型的_collection_collected_trainable_weights属性查看</strong>，如果该属性为空，则模型不训练，否则模型可以训练，compile之后，该属性固定，无论后面如何修改trainable，只要不重新compile，都不影响训练。</li><li><strong>ACGAN中引入了Embedding层，并且使用乘法将标签结合在输入随机数之中，这样可以避免标签数远远小于随机数的维度造成的灾难</strong>，因为CGAN中的生成器是采用Concatenate的方式将其结合，但是如果随机数为100维，而类别只有2类，则类别的影响会非常小。而且<strong>采用了卷积层的方式减少了使用全连接层的参数</strong>，<strong>使用辅助分类器对判别器输出的类别标签进行分类，更有效的完成不同类别图像的生成</strong>。</li></ol><h1 id="ACGAN小结"><a href="#ACGAN小结" class="headerlink" title="ACGAN小结"></a><font size="5" color="red">ACGAN小结</font></h1><p>  ACGAN是一种有效的生成式对抗网络，从上图可以看出<strong>ACGAN模型的参数量只有0.4M</strong>，和DCGAN参数量几乎相同，相比于CGAN，减少了参数量，而且效果有显著的提升，小伙伴们一定要掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;ACGAN&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>CGAN</title>
    <link href="https://USTCcoder.github.io/2020/05/31/generative_adversarial%20CGAN/"/>
    <id>https://USTCcoder.github.io/2020/05/31/generative_adversarial CGAN/</id>
    <published>2020-05-31T08:51:11.000Z</published>
    <updated>2020-06-06T03:15:03.987Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">CGAN</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>CGAN(Conditional Generative Adversarial Networks, 条件生成式对抗网络)</strong>:于<strong>2014年</strong>提出，<strong>引入标签变量，可以通过控制其标签变量的值，产生不同类别的图像</strong>，其网络结构和GAN基本类似，只是多了一些条件变量的处理。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/cgan.png" alt="cgan"></p><h1 id="CGAN特点"><a href="#CGAN特点" class="headerlink" title="CGAN特点"></a><font size="5" color="red">CGAN特点</font></h1><p>  <font size="3"><strong>生成器的输入有两个，一个是随机数，一个是标签数据的one-hot编码形式，利用Concatenate层将两个输入融合</strong>。</font><br>  <font size="3"><strong>判别器的输入也有两个，一个是输入图像，一个是标签数据的one-hot编码形式，首先利用Flatten将输入图像转化维一维向量，然后利用Concatenate层将两个输入融合</strong>。</font></p><h1 id="CGAN图像分析"><a href="#CGAN图像分析" class="headerlink" title="CGAN图像分析"></a><font size="5" color="red">CGAN图像分析</font></h1><p><img src="/images/Generative_adversarial/CGAN-generator.png" alt="generator"><br><img src="/images/Generative_adversarial/CGAN-discriminator.png" alt="discriminator"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generator(input_label_shape, input_noise_shape):</span><br><span class="line">    label = keras.layers.Input(input_label_shape, name='input_label')</span><br><span class="line">    noise = keras.layers.Input(input_noise_shape, name='input_noise')</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='concatenate')([noise, label])</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Dense(256, activation='relu', name='dense_relu1'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn1'),</span><br><span class="line">                keras.layers.Dense(512, activation='relu', name='dense_relu2'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn2'),</span><br><span class="line">                keras.layers.Dense(1024, activation='relu', name='dense_relu3'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn3'),</span><br><span class="line">                keras.layers.Dense(784, activation='tanh', name='dense_tanh'),</span><br><span class="line">                keras.layers.Reshape((28, 28, 1), name='reshape'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model([noise, label], x, name='CGAN-Generator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discriminator(input_image_shape, input_label_shape):</span><br><span class="line">    label = keras.layers.Input(input_label_shape, name='input_label')</span><br><span class="line">    image = keras.layers.Input(input_image_shape, name='input_image')</span><br><span class="line"></span><br><span class="line">    image_tensor = keras.layers.Flatten(name='flatten')(image)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='concatenate')([image_tensor, label])</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Dense(512, activation='relu', name='dense_relu1'),</span><br><span class="line">                keras.layers.Dense(256, activation='relu', name='dense_relu2'))(x)</span><br><span class="line"></span><br><span class="line">    conf = keras.layers.Dense(1, activation='sigmoid', name='dense_sigmoid')(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model([image, label], conf, name='CGAN-Discriminator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def cgan(input_noise_shape, input_label_shape, model_g, model_d):</span><br><span class="line">    label = keras.layers.Input(input_label_shape, name='input_label')</span><br><span class="line">    noise = keras.layers.Input(input_noise_shape, name='input_noise')</span><br><span class="line"></span><br><span class="line">    x = model_g([noise, label])</span><br><span class="line">    model_d.trainable = False</span><br><span class="line">    conf = model_d([x, label])</span><br><span class="line"></span><br><span class="line">    model = keras.Model([noise, label], conf, name='CGAN')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def save_picture(image, save_path, picture_num):</span><br><span class="line">    image = ((image + 1) * 127.5).astype(np.uint8)</span><br><span class="line">    image = np.concatenate([image[i * picture_num:(i + 1) * picture_num] for i in range(picture_num)], axis=2)</span><br><span class="line">    image = np.concatenate([image[i] for i in range(picture_num)], axis=0)</span><br><span class="line">    cv.imwrite(save_path, image)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    (x, y), (_, _) = keras.datasets.mnist.load_data()</span><br><span class="line">    batch_size = 256</span><br><span class="line">    epochs = 20</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    save_path = r'.\cgan'</span><br><span class="line">    if not os.path.exists(save_path):</span><br><span class="line">        os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">    x = x[..., np.newaxis].astype(np.float32) / 127.5 - 1</span><br><span class="line">    y = tf.one_hot(y, depth=10)</span><br><span class="line">    x = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(0.0002, 0.5)</span><br><span class="line">    loss = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    real_dacc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    fake_dacc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    gacc = keras.metrics.BinaryAccuracy()</span><br><span class="line"></span><br><span class="line">    model_d = discriminator(input_image_shape=(28, 28, 1), input_label_shape=(10,))</span><br><span class="line">    model_d.compile(optimizer=optimizer, loss=['binary_crossentropy'])</span><br><span class="line"></span><br><span class="line">    model_g = generator(input_noise_shape=(100,), input_label_shape=(10,))</span><br><span class="line"></span><br><span class="line">    model_g.build(input_shape=[(100,), (10,)])</span><br><span class="line">    model_g.summary()</span><br><span class="line">    keras.utils.plot_model(model_g, 'CGAN-generator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_d.build(input_shape=[(28, 28, 1), (10,)])</span><br><span class="line">    model_d.summary()</span><br><span class="line">    keras.utils.plot_model(model_d, 'CGAN-discriminator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model = cgan(input_noise_shape=(100,), input_label_shape=(10,), model_g=model_g, model_d=model_d)</span><br><span class="line">    model.compile(optimizer=optimizer, loss=['binary_crossentropy'])</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=[(100,), (10,)])</span><br><span class="line">    model.summary()</span><br><span class="line">    keras.utils.plot_model(model, 'CGAN.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        x = x.shuffle(np.random.randint(0, 10000))</span><br><span class="line">        x_db = iter(x)</span><br><span class="line"></span><br><span class="line">        for step, (real_image, real_label) in enumerate(x_db):</span><br><span class="line">            noise = np.random.normal(0, 1, (real_image.shape[0], 100)).astype(np.float32)</span><br><span class="line">            fake_label = tf.one_hot(np.random.randint(0, 10, (real_image.shape[0])), depth=10)</span><br><span class="line"></span><br><span class="line">            fake_image = model_g([noise, fake_label])</span><br><span class="line"></span><br><span class="line">            real_dacc(np.ones((real_image.shape[0], 1)), model_d([real_image, real_label]))</span><br><span class="line">            fake_dacc(np.zeros((real_image.shape[0], 1)), model_d([fake_image, fake_label]))</span><br><span class="line">            gacc(np.ones((real_image.shape[0], 1)), model([noise, fake_label]))</span><br><span class="line"></span><br><span class="line">            real_dloss = model_d.train_on_batch([real_image, real_label], np.ones((real_image.shape[0], 1)))</span><br><span class="line">            fake_dloss = model_d.train_on_batch([fake_image, fake_label], np.zeros((real_image.shape[0], 1)))</span><br><span class="line">            gloss = model.train_on_batch([noise, fake_label], np.ones((real_image.shape[0], 1)))</span><br><span class="line"></span><br><span class="line">            if step % 20 == 0:</span><br><span class="line">                print('epoch = {}, step = {}, real_dacc = {}, fake_dacc = {}, gacc = {}'.format(epoch, step, real_dacc.result(), fake_dacc.result(), gacc.result()))</span><br><span class="line">                real_dacc.reset_states()</span><br><span class="line">                fake_dacc.reset_states()</span><br><span class="line">                gacc.reset_states()</span><br><span class="line">                fake_data = np.random.normal(0, 1, (100, 100)).astype(np.float32)</span><br><span class="line">                fake_label = tf.one_hot(np.array(list(range(10)) * 10), depth=10)</span><br><span class="line">                fake_image = model_g([fake_data, fake_label])</span><br><span class="line">                save_picture(fake_image.numpy(), save_path + '\\epoch{}_step{}.jpg'.format(epoch, step), 10)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Generative_adversarial/cgan_R.png" alt="cgan"></p><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Generative_adversarial/cgan_T.png" alt="cgan"></p><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>可以设置一些<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li><strong>CGAN对于网络结构，优化器参数，网络层的一些超参数都是非常敏感的，效果不好不容易发现原因，这可能需要较多的工程实践经验</strong>。</li><li><strong>先创建判别器，然后进行compile，这样判别器就固定了，然后创建生成器时，不要训练判别器，需要将判别器的trainable改成False，此时不会影响之前固定的判别器</strong>，这个可以<strong>通过模型的_collection_collected_trainable_weights属性查看</strong>，如果该属性为空，则模型不训练，否则模型可以训练，compile之后，该属性固定，无论后面如何修改trainable，只要不重新compile，都不影响训练。</li><li><strong>CGAN的网络结构和GAN基本相同，因此也只适合小图像的生成</strong>，其中<strong>生成器使用Concatenate实现标签数据和输入随机数的结合，判别器使用Concatenate实现标签数据和输入图像的结合，一定要注意首先要对图像进行Flatten处理，否则会出错</strong>。</li></ol><h1 id="CGAN小结"><a href="#CGAN小结" class="headerlink" title="CGAN小结"></a><font size="5" color="red">CGAN小结</font></h1><p>  CGGAN是一种简单的生成式对抗网络，从上图可以看出<strong>CGAN模型的参数量只有2M</strong>，和普通的GAN网络差不多，<strong>通过CGAN可以实现指定类别的图像生成，不再是完全的随机数产生，因此对于实际的工程应用是有意义的</strong>，值得小伙伴们学习。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;CGAN&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>DCGAN</title>
    <link href="https://USTCcoder.github.io/2020/05/29/generative_adversarial%20DCGAN/"/>
    <id>https://USTCcoder.github.io/2020/05/29/generative_adversarial DCGAN/</id>
    <published>2020-05-29T01:11:34.000Z</published>
    <updated>2020-06-06T02:39:29.512Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">DCGAN</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>DCGAN(Deep Convolutional Generative Adversarial Networks, 深度卷积生成式对抗网络):</strong>于<strong>2016年</strong>发表于ICLR，是GAN类型网络的升级版本，其中改变的只是<strong>将GAN中的全连接层变为卷积层和上采样层，这样可以使用更少的参数实现更大像素图像的生成</strong>。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/dcgan.png" alt="Dataset"></p><h1 id="DCGAN特点"><a href="#DCGAN特点" class="headerlink" title="DCGAN特点"></a><font size="5" color="red">DCGAN特点</font></h1><p>  <font size="3"><strong>将全连接层换成了卷积层和上采样层，极大缩小参数量，可以实现大尺寸图像的生成</strong>。</font></p><h1 id="DCGAN图像分析"><a href="#DCGAN图像分析" class="headerlink" title="DCGAN图像分析"></a><font size="5" color="red">DCGAN图像分析</font></h1><p><img src="/images/Generative_adversarial/DCGAN-generator.png" alt="generator"><br><img src="/images/Generative_adversarial/DCGAN-discriminator.png" alt="discriminator"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Dense(1568, activation='relu', name='dense_relu'),</span><br><span class="line">                keras.layers.Reshape((7, 7, 32), name='reshape'),</span><br><span class="line">                keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', name='conv1'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn1'),</span><br><span class="line">                keras.layers.ReLU(name='relu1'),</span><br><span class="line">                keras.layers.UpSampling2D((2, 2), name='upsampling1'),</span><br><span class="line">                keras.layers.Conv2D(128, (3, 3), (1, 1), 'same', name='conv2'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn2'),</span><br><span class="line">                keras.layers.ReLU(name='relu2'),</span><br><span class="line">                keras.layers.UpSampling2D((2, 2), name='upsampling2'),</span><br><span class="line">                keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', name='conv3'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn3'),</span><br><span class="line">                keras.layers.ReLU(name='relu3'),</span><br><span class="line">                keras.layers.Conv2D(1, (3, 3), (1, 1), 'same', activation='tanh', name='conv_tanh'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='DCGAN-Generator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discriminator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(32, (3, 3), (2, 2), 'same', name='conv1'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn1'),</span><br><span class="line">                keras.layers.LeakyReLU(0.2, name='leakyrelu1'),</span><br><span class="line">                keras.layers.Conv2D(64, (3, 3), (2, 2), 'same', name='conv2'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn2'),</span><br><span class="line">                keras.layers.LeakyReLU(0.2, name='leakyrelu2'),</span><br><span class="line">                keras.layers.Conv2D(128, (3, 3), (2, 2), 'same', name='conv3'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn3'),</span><br><span class="line">                keras.layers.LeakyReLU(0.2, name='leakyrelu3'),</span><br><span class="line">                keras.layers.GlobalAveragePooling2D(name='global_averagepool'),</span><br><span class="line">                keras.layers.Dense(1, activation='sigmoid', name='dense_sigmoid'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='DCGAN-Discriminator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def dcgan(input_shape, model_g, model_d):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape)</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = model_g(x)</span><br><span class="line">    model_d.trainable = False</span><br><span class="line">    x = model_d(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='DCGAN')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def save_picture(image, save_path, picture_num):</span><br><span class="line">    image = ((image + 1) * 127.5).astype(np.uint8)</span><br><span class="line">    image = np.concatenate([image[i * picture_num:(i + 1) * picture_num] for i in range(picture_num)], axis=2)</span><br><span class="line">    image = np.concatenate([image[i] for i in range(picture_num)], axis=0)</span><br><span class="line">    cv.imwrite(save_path, image)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    (x, _), (_, _) = keras.datasets.mnist.load_data()</span><br><span class="line">    batch_size = 256</span><br><span class="line">    epochs = 20</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    save_path = r'.\dcgan'</span><br><span class="line">    if not os.path.exists(save_path):</span><br><span class="line">        os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">    x = x[..., np.newaxis].astype(np.float32) / 127.5 - 1</span><br><span class="line">    x = tf.data.Dataset.from_tensor_slices(x).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(0.0002, 0.5)</span><br><span class="line">    loss = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    real_dacc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    fake_dacc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    gacc = keras.metrics.BinaryAccuracy()</span><br><span class="line"></span><br><span class="line">    model_d = discriminator(input_shape=(28, 28, 1))</span><br><span class="line">    model_d.compile(optimizer=optimizer, loss='binary_crossentropy')</span><br><span class="line"></span><br><span class="line">    model_g = generator(input_shape=(100,))</span><br><span class="line"></span><br><span class="line">    model_g.build(input_shape=(100,))</span><br><span class="line">    model_g.summary()</span><br><span class="line">    keras.utils.plot_model(model_g, 'DCGAN-generator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_d.build(input_shape=(28, 28, 1))</span><br><span class="line">    model_d.summary()</span><br><span class="line">    keras.utils.plot_model(model_d, 'DCGAN-discriminator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model = dcgan(input_shape=(100,), model_g=model_g, model_d=model_d)</span><br><span class="line">    model.compile(optimizer=optimizer, loss='binary_crossentropy')</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(100,))</span><br><span class="line">    model.summary()</span><br><span class="line">    keras.utils.plot_model(model, 'DCGAN.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        x = x.shuffle(np.random.randint(0, 10000))</span><br><span class="line">        x_db = iter(x)</span><br><span class="line"></span><br><span class="line">        for step, real_image in enumerate(x_db):</span><br><span class="line">            noise = np.random.normal(0, 1, (real_image.shape[0], 100))</span><br><span class="line">            fake_image = model_g(noise)</span><br><span class="line"></span><br><span class="line">            real_dacc(np.ones((real_image.shape[0], 1)), model_d(real_image))</span><br><span class="line">            fake_dacc(np.zeros((real_image.shape[0], 1)), model_d(fake_image))</span><br><span class="line">            gacc(np.ones((real_image.shape[0], 1)), model(noise))</span><br><span class="line"></span><br><span class="line">            real_dloss = model_d.train_on_batch(real_image, np.ones((real_image.shape[0], 1)))</span><br><span class="line">            fake_dloss = model_d.train_on_batch(fake_image, np.zeros((real_image.shape[0], 1)))</span><br><span class="line">            gloss = model.train_on_batch(noise, np.ones((real_image.shape[0], 1)))</span><br><span class="line"></span><br><span class="line">            if step % 20 == 0:</span><br><span class="line">                print('epoch = {}, step = {}, real_dacc = {}, fake_dacc = {}, gacc = {}'.format(epoch, step, real_dacc.result(), fake_dacc.result(), gacc.result()))</span><br><span class="line">                real_dacc.reset_states()</span><br><span class="line">                fake_dacc.reset_states()</span><br><span class="line">                gacc.reset_states()</span><br><span class="line">                fake_data = np.random.normal(0, 1, (100, 100))</span><br><span class="line">                fake_image = model_g(fake_data)</span><br><span class="line">                save_picture(fake_image.numpy(), save_path + '\\epoch{}_step{}.jpg'.format(epoch, step), 10)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Generative_adversarial/dcgan_R.png" alt="gan"></p><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Generative_adversarial/dcgan_T.png" alt="gan"></p><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>可以设置一些<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li><strong>DCGAN对于网络结构，优化器参数，网络层的一些超参数都是非常敏感的，效果不好不容易发现原因，这可能需要较多的工程实践经验</strong>。</li><li><strong>先创建判别器，然后进行compile，这样判别器就固定了，然后创建生成器时，不要训练判别器，需要将判别器的trainable改成False，此时不会影响之前固定的判别器</strong>，这个可以<strong>通过模型的_collection_collected_trainable_weights属性查看</strong>，如果该属性为空，则模型不训练，否则模型可以训练，compile之后，该属性固定，无论后面如何修改trainable，只要不重新compile，都不影响训练。</li></ol><h1 id="DCGAN小结"><a href="#DCGAN小结" class="headerlink" title="DCGAN小结"></a><font size="5" color="red">DCGAN小结</font></h1><p>  DCGAN是一种非常简单的生成式对抗网络，从上图可以看出<strong>DCGAN模型的参数量只有0.4M，这样使得大图像的生成变得可能</strong>，DCGAN没有特别的创新点，运用了深度卷积在初代GAN上，为以后GAN的发展提供了思路。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;DCGAN&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>GAN</title>
    <link href="https://USTCcoder.github.io/2020/05/27/generative_adversarial%20GAN/"/>
    <id>https://USTCcoder.github.io/2020/05/27/generative_adversarial GAN/</id>
    <published>2020-05-27T14:48:56.000Z</published>
    <updated>2020-06-06T02:37:29.952Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">GAN</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>GAN(Generative Adversarial Networks, 生成式对抗网络):</strong>是GAN类型网络的初代版本，<strong>据说是Ian Goodfellow在2014年喝了一杯啤酒之后，在梦中产生的想法</strong>，我不禁感叹，大佬就是大佬啊，虽然现在这是最简单的生成式对抗网络模型，其效果也被很多模型超越，但是它的思想值得我们学习。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/gan.png" alt="gan"></p><h1 id="GAN特点"><a href="#GAN特点" class="headerlink" title="GAN特点"></a><font size="5" color="red">GAN特点</font></h1><p>  <font size="3"><strong>只采用了全连接层和ReLU激活函数，没有使用卷积层对图像进行处理</strong>。</font><br>  <font size="3"><strong>生成器的输出使用tanh，产生[-1, 1]的图像，判别器的输出使用sigmoid，产生真或者假的逻辑值</strong>。</font></p><h1 id="GAN图像分析"><a href="#GAN图像分析" class="headerlink" title="GAN图像分析"></a><font size="5" color="red">GAN图像分析</font></h1><p><img src="/images/Generative_adversarial/GAN-generator.png" alt="generator"><br><img src="/images/Generative_adversarial/GAN-discriminator.png" alt="discriminator"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Dense(256, activation='relu', name='dense_relu1'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn1'),</span><br><span class="line">                keras.layers.Dense(512, activation='relu', name='dense_relu2'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn2'),</span><br><span class="line">                keras.layers.Dense(1024, activation='relu', name='dense_relu3'),</span><br><span class="line">                keras.layers.BatchNormalization(momentum=0.8, name='bn3'),</span><br><span class="line">                keras.layers.Dense(784, activation='tanh', name='dense_tanh'),</span><br><span class="line">                keras.layers.Reshape((28, 28, 1)))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='GAN-Generator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def discriminator(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Flatten(name='flatten'),</span><br><span class="line">                keras.layers.Dense(512, activation='relu', name='dense_relu1'),</span><br><span class="line">                keras.layers.Dense(256, activation='relu', name='dense_relu2'),</span><br><span class="line">                keras.layers.Dense(1, activation='sigmoid', name='dense_sigmoid'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='GAN-Discriminator')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def gan(input_shape, model_g, model_d):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = model_g(x)</span><br><span class="line">    model_d.trainable = False</span><br><span class="line">    x = model_d(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='GAN')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def save_picture(image, save_path, picture_num):</span><br><span class="line">    image = ((image + 1) * 127.5).astype(np.uint8)</span><br><span class="line">    image = np.concatenate([image[i * picture_num:(i + 1) * picture_num] for i in range(picture_num)], axis=2)</span><br><span class="line">    image = np.concatenate([image[i] for i in range(picture_num)], axis=0)</span><br><span class="line">    cv.imwrite(save_path, image)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    (x, _), (_, _) = keras.datasets.mnist.load_data()</span><br><span class="line">    batch_size = 256</span><br><span class="line">    epochs = 20</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    save_path = r'.\gan'</span><br><span class="line">    if not os.path.exists(save_path):</span><br><span class="line">        os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">    x = x[..., np.newaxis].astype(np.float) / 127.5 - 1</span><br><span class="line">    x = tf.data.Dataset.from_tensor_slices(x).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(0.0002, 0.5)</span><br><span class="line">    loss = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    real_dacc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    fake_dacc = keras.metrics.BinaryAccuracy()</span><br><span class="line">    gacc = keras.metrics.BinaryAccuracy()</span><br><span class="line"></span><br><span class="line">    model_d = discriminator(input_shape=(28, 28, 1))</span><br><span class="line">    model_d.compile(optimizer=optimizer, loss='binary_crossentropy')</span><br><span class="line"></span><br><span class="line">    model_g = generator(input_shape=(100,))</span><br><span class="line"></span><br><span class="line">    model_g.build(input_shape=(100,))</span><br><span class="line">    model_g.summary()</span><br><span class="line">    keras.utils.plot_model(model_g, 'GAN-generator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model_d.build(input_shape=(28, 28, 1))</span><br><span class="line">    model_d.summary()</span><br><span class="line">    keras.utils.plot_model(model_d, 'GAN-discriminator.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    model = gan(input_shape=(100,), model_g=model_g, model_d=model_d)</span><br><span class="line">    model.compile(optimizer=optimizer, loss='binary_crossentropy')</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(100,))</span><br><span class="line">    model.summary()</span><br><span class="line">    keras.utils.plot_model(model, 'GAN.png', show_shapes=True, show_layer_names=True)</span><br><span class="line"></span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        x = x.shuffle(np.random.randint(0, 10000))</span><br><span class="line">        x_db = iter(x)</span><br><span class="line"></span><br><span class="line">        for step, real_image in enumerate(x_db):</span><br><span class="line">            noise = np.random.normal(0, 1, (real_image.shape[0], 100))</span><br><span class="line">            fake_image = model_g(noise)</span><br><span class="line"></span><br><span class="line">            real_dacc(np.ones((real_image.shape[0], 1)), model_d(real_image))</span><br><span class="line">            fake_dacc(np.zeros((real_image.shape[0], 1)), model_d(fake_image))</span><br><span class="line">            gacc(np.ones((real_image.shape[0], 1)), model(noise))</span><br><span class="line"></span><br><span class="line">            real_dloss = model_d.train_on_batch(real_image, np.ones((real_image.shape[0], 1)))</span><br><span class="line">            fake_dloss = model_d.train_on_batch(fake_image, np.zeros((real_image.shape[0], 1)))</span><br><span class="line">            gloss = model.train_on_batch(noise, np.ones((real_image.shape[0], 1)))</span><br><span class="line"></span><br><span class="line">            if step % 20 == 0:</span><br><span class="line">                print('epoch = {}, step = {}, real_dacc = {}, fake_dacc = {}, gacc = {}'.format(epoch, step, real_dacc.result(), fake_dacc.result(), gacc.result()))</span><br><span class="line">                real_dacc.reset_states()</span><br><span class="line">                fake_dacc.reset_states()</span><br><span class="line">                gacc.reset_states()</span><br><span class="line">                fake_data = np.random.normal(0, 1, (100, 100))</span><br><span class="line">                fake_image = model_g(fake_data)</span><br><span class="line">                save_picture(fake_image.numpy(), save_path + '\\epoch{}_step{}.jpg'.format(epoch, step), 10)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Generative_adversarial/gan_R.png" alt="gan"></p><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Generative_adversarial/gan_T.png" alt="gan"></p><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>可以设置一些<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li><strong>GAN对于网络结构，优化器参数，网络层的一些超参数都是非常敏感的，效果不好不容易发现原因，这可能需要较多的工程实践经验</strong>。</li><li><strong>先创建判别器，然后进行compile，这样判别器就固定了，然后创建生成器时，不要训练判别器，需要将判别器的trainable改成False，此时不会影响之前固定的判别器</strong>，这个可以<strong>通过模型的_collection_collected_trainable_weights属性查看</strong>，如果该属性为空，则模型不训练，否则模型可以训练，compile之后，该属性固定，无论后面如何修改trainable，只要不重新compile，都不影响训练。</li><li><strong>因为全都是全连接层，GAN适用于小目标的生成</strong>，如果是一个512x512x3的图像的生成，使用全连接层，那么就需要786432个神经元，上一层的神经元数目应该更多，设为1048576个，那么这两层之间全连接的参数量为八千多亿个，这是非常不现实的，而且效果也会特别差。</li></ol><h1 id="GAN小结"><a href="#GAN小结" class="headerlink" title="GAN小结"></a><font size="5" color="red">GAN小结</font></h1><p>  GAN是一种非常简单的生成式对抗网络，从上图可以看出<strong>GAN模型的参数量只有2M</strong>，虽然现在GAN网络不是最好的生成式对抗网络，但是其网络对抗思想，对后面的深度学习网络的发展有重要的影响。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;GAN&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>生成式对抗网络数据集</title>
    <link href="https://USTCcoder.github.io/2020/05/25/generative_adversarial%20Dataset/"/>
    <id>https://USTCcoder.github.io/2020/05/25/generative_adversarial Dataset/</id>
    <published>2020-05-25T07:41:43.000Z</published>
    <updated>2020-06-07T14:27:38.867Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Data Set</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Generative Adversarial Networks(GAN, 生成式对抗网络):</strong>是一类深度学习模型，也是计算机视觉的<strong>新晋成员</strong>，由<strong>Ian J. Goodfellow在2014年10月</strong>提出，<strong>短短5年多的时间，已经有成百上千种不同的GAN网络被提出，可以说GAN的提出受到了广泛的关注</strong>。现在的GAN不仅仅只是生成网络，而其应用更是越来越广泛，包括<strong>图像生成</strong>，<strong>超分辨率提升</strong>，<strong>风格迁移</strong>等等。而且GAN的使用非常有趣，可以带领我们目睹从0到1的变化趋势。<br><a id="more"></a></p><p><img src="/images/Generative_adversarial/Dataset.png" alt="Dataset"></p><h1 id="GAN的理论"><a href="#GAN的理论" class="headerlink" title="GAN的理论"></a><font size="5" color="red">GAN的理论</font></h1><p>生成式对抗网络，顾名思义，在对抗中生成图像，其<strong>灵感来源于博弈论</strong>，这里不做太多的数学公式推导，只是简单描述GAN的工作机理。既然提到对抗，当然GAN网络不是一个模型，而是两个模型，其中一个称之为<strong>Generator(生成器)</strong>，负责生成图像，另一个称之为<strong>Discriminator(判别器)</strong>，负责对生成的图像进行打分，它们两者进行对抗和博弈。</p><font color="red">假如你是一个刚学习做饭的新手，我想做出来一道精美的饭菜，这不是天方夜谭吗？但是我有一个不嫌弃我做饭难吃的好朋友小明，他也不是一个专业的评委，他每天中午在五星大酒店吃饭，然后晚上来我家吃我做的饭，是不是一脸黑人问号？不过不用纠结小明和我之间的py交易，但是小明并不会做饭，不能够亲自知道我如何操作，而是告诉我哪里欠缺。于是乎我开始了自己的尝试。第一天小明中午吃大酒店之后，晚上来我家吃饭，他很轻松的分辨出了我做的不如酒店好吃，并且告诉我盐放多了。于是乎我下一次少放点盐。第二天小明中午吃了大酒店之后，晚上来我家，也很轻松的分辨出了我做的不好吃，并且告诉我这次油放少了。于是乎我下一次多放一点油。第三天，第四天。。。一直到一年以后，我做饭的技术越来越好，小明很难从中发现问题，认为我做的和大酒店做的很像，小明越来越认真的区分我做的和酒店之间的差距，他的目标是将酒店做的都全部分成一类，将我做的菜都分成一类，而我越来越认真的对自己的厨艺进行改进，我的目标是让小明将我做的菜分成酒店那一类。就这样我们在相互学习中提升自己，5年以后，我成了一名米其林厨师，而小明成为了一名美食鉴赏专家。好了，故事讲完了，GAN的理论也结束了，我在GAN中就扮演生成器的角色，而小明则是一个判别器的角色。</font><h1 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a><font size="5" color="red">数据集介绍</font></h1><p><strong>数据集</strong>：因为GAN的任务很多，包括<strong>图像生成，包括分辨率提升，风格迁移</strong>等等，为了方便模型调试的方便，在我的博客中使用到了三个数据集。</p><p>第一个是大家的老朋友，也是入门深度学习一定会接触的mnist手写数字数据集，带我们见证如何从杂乱无章的数据生成手写数字的过程。<br>简单介绍一下mnist数据集，在tensorflow2.0中，<strong>不需要专门下载，已经给我们提供了手写数字的函数，位于tf.keras.datasets.mnist.load_data()，返回值为两个元组，第一个为训练数据和训练标签，第二个为测试数据和测试标签。训练数据大小为60000张图像，每张图像为28x28的大小，标签为[0, 9]的稀疏表示，非one-hot编码形式。测试集大小为10000张图像，大小格式和训练集相同</strong>。<br><img src="/images/Generative_adversarial/mnist.png" alt="mnist"></p><p>第二个数据集是monet2photo数据集，<strong>用来测试SRGAN, CycleGAN的算法正确性，SRGAN是一种超分辨率算法，CycleGAN是一种风格迁移算法</strong>。<br>简单介绍一下monet2photo数据集，<strong>monet指莫奈风格的画作，photo指拍摄的图像，用于将两张图片的风格进行迁移</strong>，<strong>SRGAN使用了photo图像进行分辨率的提升</strong>。其中monet训练集包括1072张图像，位于trainA文件夹中，测试集包括121张图像，位于testA文件夹中，photo训练集包括6287张图像，位于trainB文件夹中，测试集包括751张图像，位于testB文件夹中，其中图像都是jpg文件，大小都是256x256的。<br><img src="/images/Generative_adversarial/monet2photo.png" alt="monet2photo"></p><p>第三个数据集是edges2shoes数据集，<strong>用来测试DiscoGAN和pix2pix的算法正确性，其中DiscoGAN和pix2pix都是风格迁移算法，由于损失函数的计算和CycleGAN不同，因此需要的数据集也不同</strong>。<br>也简单介绍一下edges2shoes数据集，<strong>edges指鞋子的轮廓，shoes指鞋子的图像，其中用于将两张图片的风格进行迁移，特点是相同的图像，只是风格不同，两张图像存放于一个文件中，左边是风格A，右边是风格B，而monet2photo两张图像不但风格不同，而且图像本身也不同，每个图像文件只保存一张图像</strong>，其中edges2shoes训练集包括49825张图像，位于train文件夹中，测试集包括200张图像，位于val文件夹中，其中图像都是jpg文件，大小都是512x256的。<br><img src="/images/Generative_adversarial/edges.png" alt="edges2shoes"></p><h1 id="一些说明"><a href="#一些说明" class="headerlink" title="一些说明"></a><font size="5" color="red">一些说明</font></h1><ol><li>在学习的时候，小伙伴可能会遇到一些代码上的困难，如<strong>tensorflow</strong>，<strong>numpy</strong>，<strong>opencv</strong>的用法，可以查看我的深度学习框架和Python常用库相关文章，里面会有一些简单的介绍，小伙伴们可以进行学习，最好是手动敲一敲，看一看。</li><li>因为这个博客是对学习的一些总结和记录，意在和学习者探讨和交流，并且给准备入门的同学一些手把手的教学，因此关于生成式对抗网络的算法参数设计，我都是自己尝试的，不是针对于这个数据集最优的参数，大家可以<strong>根据自己的实际需要修改网络结构</strong>，但是<strong>生成式对抗网络是非常sensitive的，可能调整了一些参数以后，网络模型不收敛，得到完全超出预料的结果，这都是正常的，因此这也需要小伙伴们具有非常丰富的实战经验</strong>。</li><li>实际的工程应用中，常常还需要对数据集进行<strong>大小调整和增强</strong>，在这里为了简单起见，没有进行复杂的操作，小伙伴们应用中要记得根据自己的需要，对图像进行<strong>resize或者padding</strong>，然后<strong>旋转</strong>，<strong>对比度增强</strong>，<strong>仿射运算</strong>等等操作，增加模型的鲁棒性，并且实际中的图像不一定按照顺序命名的，因此应用中也要注意图像读取的文件名。</li><li>为了让学习者看的方便和清晰，我没有使用多个文件对程序进行封装，因为我在刚开始学习模型的时候，查看GitHub代码，一个模型可能需要好几个文件夹，每个文件夹里面又有很多的代码文件，其中很多文件互相调用。虽然这样的工程项目是非常好管理和运行的，但是给初学者一种丈二和尚摸不着头脑的感觉，对此我深有体会。所以我就使用一个.py文件来封装，因此代码可能会有几百行，但是其中的各个函数和类都有自己的名字，可以保证学习者不会被纸老虎吓住。</li><li>在生成式对抗网络学习中，我会列举出一些经典的生成式对抗网络模型，因为模型太多，并且仍在不断的更新进步之中，所以大家可以联系我，和我进行沟通和交流，或者推荐给我一些优秀的模型。</li><li>关于问题的交流，图像的数据，需要的同学可以到主页查看我的QQ或者邮箱，我会非常荣幸的提供力所能及的帮助，小伙伴加好友的时候一定要记得备注，不然我可能会忽视一些粗心的小伙伴。</li></ol><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  <strong>生成式对抗网络</strong>是计算机视觉的<strong>新晋成员</strong>，也是非常具有前景的任务之一，而且现在的网络模型的功能也越来越多，不仅仅是用于图像生成，而是运用于各种实际工程应用之中。自从深度学习的时代到来，各种神经网络结构百花齐放，很难说出最好的生成式对抗网络模型，可能一个模型适用于很多数据，但也<strong>不能说明某一个算法一定优于另一个算法</strong>，我们要做的就是尽可能多的<strong>学习各种各样的深度学习模型</strong>，然后<strong>吸取这些模型成功的原因</strong>，投入到自己的工程应用之中。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Data Set&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成式对抗网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>熵，交叉熵，相对熵的关系</title>
    <link href="https://USTCcoder.github.io/2020/05/23/deep%20learning%20entropy/"/>
    <id>https://USTCcoder.github.io/2020/05/23/deep learning entropy/</id>
    <published>2020-05-23T08:22:32.000Z</published>
    <updated>2020-09-04T01:12:51.468Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Entropy &amp; Cross Entropy &amp; Relative Entropy</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Entropy, Cross Entropy, Relative Entropy(熵，交叉熵，相对熵)</strong>:在机器学习或者深度学习的过程中，避免不了与熵接触，但是熵是什么，小伙伴们是否有很多问号？感觉是那么回事，但是又无法说清楚。<br><a id="more"></a></p><p><img src="/images/deep_learning/entropy.png" alt="entropy"></p><h1 id="Entropy-熵"><a href="#Entropy-熵" class="headerlink" title="Entropy(熵)"></a><font size="5" color="red">Entropy(熵)</font></h1><p><strong>Entropy(熵)</strong>：在<strong>计算机，通信领域指的是Information Entropy(信息熵)</strong>，信息熵<strong>代表随机变化或者系统的不确定性，熵越大则随机变量或系统的不确定性就越大</strong>，如何理解呢？我们可以<strong>根据分布，找到一个最优策略，信息熵就是使用最优策略衡量这个分布所花费的代价</strong>。用公式表示为</p><script type="math/tex; mode=display">H(p) = -\sum_{x}^{\ }{p(x)log_{2}(p(x))}</script><p>其中p为随机变化或者系统的分布函数。<br>举个例子说明，假如一个袋子里面有两个红球，一个白球，一个蓝球，取出一个球是什么颜色，问这个系统的信息熵为多少？计算步骤为</p><script type="math/tex; mode=display">H = -\frac{1}{2} \times log_{2}\frac{1}{2} -\frac{1}{4} \times log_{2}\frac{1}{4} -\frac{1}{4} \times log_{2}\frac{1}{4} = 1.5</script><p>如何理解这个结果呢？我们按照最优策略进行代价计算，最优策略是花费一次机会猜红球，猜对了概率为0.5，如果猜错了则还需要花费一次机会猜白球或者蓝球，因此总代价为0.5 x 1 + 0.5 x 2 = 1.5。<br>如果有四个球，一个红球，一个蓝球，一个白球，一个黄球，则系统的信息熵则是</p><script type="math/tex; mode=display">H = -\frac{1}{4} \times log_{2}\frac{1}{4} -\frac{1}{4} \times log_{2}\frac{1}{4} -\frac{1}{4} \times log_{2}\frac{1}{4} -\frac{1}{4} \times log_{2}\frac{1}{4} = 2</script><p>也很好理解，首先猜是否为红球或者蓝球，如果是，概率为0.5，再猜是否为红色，概率为0.25，所需要的代价为两次机会，其他的球同理，则总代价为0.25 x 2 x 4 = 2。<br>那么如果有四个球，全为红色，系统的信息熵为多少呢？</p><script type="math/tex; mode=display">H = -log_{2}1 = 0</script><p>只有红球，我根本不需要任何代价就可以消除系统的不确定性，因此信息熵为0。<br>我第一次接触信息熵时是有点懵的，如果明天一定下雨，信息量为0，明天有50%的可能下雨，信息熵最大，等于1。是不是有点怪怪的，这不是废话吗？这对我来说有啥价值？其实不是这样，信息熵和我们的直观感觉不同，直观感觉只有确定的事情才有意义，而信息熵衡量的就是系统的不确定性，因此有种反人类的感觉。</p><h1 id="Cross-Entropy-交叉熵"><a href="#Cross-Entropy-交叉熵" class="headerlink" title="Cross Entropy(交叉熵)"></a><font size="5" color="red">Cross Entropy(交叉熵)</font></h1><p><strong>Cross Entropy(交叉熵)</strong>：<strong>代表两个概率分布之间的差异性信息</strong>。如何理解呢？<strong>交叉熵就是使用预测分布的最佳策略衡量真实分布所花费的代价，这个值一定是大于等于信息熵的</strong>，用公式表示为</p><script type="math/tex; mode=display">H(p, q) = -\sum_{x}^{\ }{p(x)log_{2}(q(x))}</script><p>其中p为真实分布函数，q为预测分布函数。<br>用之前的例子说明，假如一个袋子里面有两个红球，一个白球，一个蓝球，预测结果为一个红球，两个白球，一个蓝球，问交叉熵有多少？计算步骤为</p><script type="math/tex; mode=display">H = -\frac{1}{2} \times log_{2}\frac{1}{4} -\frac{1}{4} \times log_{2}\frac{1}{2} -\frac{1}{4} \times log_{2}\frac{1}{4} = 1.75</script><p>如何理解这个结果呢？我们<strong>使用预测分布的最优策略按照真实分布的概率，进行代价计算</strong>，<strong>交叉熵越低，则预测分布越接近真实分布，当两个分布相同时，交叉熵等于信息熵，因此在分类函数常常使用交叉熵作为Loss函数</strong>。</p><h1 id="Relative-Entropy-相对熵"><a href="#Relative-Entropy-相对熵" class="headerlink" title="Relative Entropy(相对熵)"></a><font size="5" color="red">Relative Entropy(相对熵)</font></h1><p><strong>Relative Entropy(相对熵)</strong>：又被称为<strong>KL散度(Kullback-Leibler divergence)</strong>，<strong>是两个概率分布间差异的非对称性度量</strong>。如何理解呢？设两个分布分别为p和q，<strong>KL(p||q)就是按照p的最佳策略来计算p的分布所需要的代价(信息熵)与按照q的最佳策略计算p的分布所需要的代价(交叉熵)之间的差异，这个值一定是大于等于0的</strong>，用公式表示为</p><script type="math/tex; mode=display">KL(p||q) = \sum_{x}^{\ }{p(x)log_2\frac{p(x)}{q(x)}} = H(p, q) - H(p)</script><p>其中p为真实分布函数，q为预测分布函数。<br>用之前的例子说明，假如一个袋子里面有两个红球，一个白球，一个蓝球，预测结果为一个红球，两个白球，一个蓝球，问相对熵有多少？计算步骤为</p><script type="math/tex; mode=display">KL(p||q) = \frac{1}{2} \times log_2\frac{\frac{1}{2}}{\frac{1}{4}} + \frac{1}{4} \times log_2\frac{\frac{1}{4}}{\frac{1}{2}} + \frac{1}{4} \times log_2\frac{\frac{1}{4}}{\frac{1}{4}} = 0.25</script><p>如何理解这个结果呢？我们<strong>使用真实分布的最优策略按照真实分布的概率，进行代价计算，得到信息熵</strong>，上面计算的结果为1.5，我们<strong>使用预测分布的最优策略按照真实分布的概率，进行代价计算，得到交叉熵</strong>，上面计算的结果为1.75，<strong>相对熵就是两者之间的差异</strong>1.75 - 1.5 = 0.25，<strong>相对熵熵越低，则预测分布越接近真实分布，当两个分布相同时，相对熵为0</strong>，在<strong>多分类问题中，信息熵等于0，因此交叉熵等于KL散度</strong>，但是因为KL散度计算较为复杂，因此一般都使用交叉熵作为损失函数，但是<strong>KL散度在某些场景下有着交叉熵无法代替的作用，如VAE(Variational Autoencoder)</strong>。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  <strong>熵是我们学习机器学习和深度学习必须要掌握的基本知识，在模型的损失函数和评价指标中经常使用</strong>，小伙伴们一定要掌握它，否则只是听别人说什么交叉熵，KL散度之乎者也的，自己完全插不上话，相信大家看了这个博客后，一定能够对熵这个家族有更深刻的理解。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Entropy &amp; Cross Entropy &amp; Relative Entropy&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>Regularization黑科技</title>
    <link href="https://USTCcoder.github.io/2020/05/22/deep%20learning%20regularization/"/>
    <id>https://USTCcoder.github.io/2020/05/22/deep learning regularization/</id>
    <published>2020-05-22T04:26:23.000Z</published>
    <updated>2020-05-22T13:21:02.922Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Regularization</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Regularization(正则化)</strong>:简单的说就是<strong>减小测试误差的行为</strong>，我们在构建深度学习模型时，最终目的是为了让模型更好的面对测试数据，而不是训练数据。但是<strong>网络在学习的过程中很容易就出现了Overfitting(过拟合)</strong>，这就导致<strong>模型的泛化能力下降</strong>，所以需要<strong>引入一些正则化的方法，降低模型的复杂度</strong>。<br><a id="more"></a></p><p><img src="/images/deep_learning/regularization.png" alt="regularization"></p><h1 id="L1-Regularization-L1正则"><a href="#L1-Regularization-L1正则" class="headerlink" title="L1 Regularization(L1正则)"></a><font size="5" color="red">L1 Regularization(L1正则)</font></h1><script type="math/tex; mode=display">J'(\omega, b) = J(\omega, b) + \frac{\lambda}{2m} \underset{i}{\sum} {|\omega_{i}|}</script><p>其中$m$为样本个数，$\lambda$为超参数，用于控制正则化的程度。<br><strong>L1 Regularization(L1正则)</strong>：是<strong>指在目标函数的后面加上系数惩罚项，L1正则对应的惩罚项为L1范数，通过让原目标函数加上了所有权重绝对值之和</strong>来实现正则化。<br><strong>L1 Regularization的优点</strong>：可以<strong>起到特征选择的作用</strong>，因为<strong>在梯度更新的时候，L1正则的导数为1，每次都会加上或者减去一个常数，所以很容易产生权值为0的情况，会使特征变得稀疏</strong>。<br>在TensorFlow中，keras.regularizers.l1给我们提供了L1正则化的函数，在网络模型中，可以<strong>通过将正则化函数赋值给网络层的kernel_regularizer参数</strong>，达到正则化的作用。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#keras.regularizers.l1(l=0.01)：l为正则化因子，默认为0.01</span><br><span class="line">keras.layers.Dense(units, kernel_regularizer=keras.regularizers.l1(1e-4))</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="L2-Regularization-L2正则"><a href="#L2-Regularization-L2正则" class="headerlink" title="L2 Regularization(L2正则)"></a><font size="5" color="red">L2 Regularization(L2正则)</font></h1><script type="math/tex; mode=display">J'(\omega, b) = J(\omega, b) + \frac{\lambda}{2m} \underset{i}{\sum} {\omega_{i}^{2}}</script><p>其中$m$为样本个数，$\lambda$为超参数，用于控制正则化的程度。<br><strong>L2 Regularization(L2正则)</strong>：是<strong>指在目标函数的后面加上系数惩罚项，L2正则对应的惩罚项为L2范数，通过让原目标函数加上了所有权重的平方和</strong>来实现正则化。<br><strong>L2 Regularization的优点</strong>：<strong>更适合防止模型过拟合</strong>。因为<strong>在梯度更新的时候，L2正则的导数与权值成比例，因此当权值缩小后，梯度也会变小，所以使系数趋向于变小但是不为0，所以L2正则会使模型变得简单，防止过拟合</strong>。<br>在TensorFlow中，keras.regularizers.l2给我们提供了L2正则化的函数，在网络模型中，可以<strong>通过将正则化函数赋值给网络层的kernel_regularizer参数</strong>，达到正则化的作用。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#keras.regularizers.l2(l=0.01)：l为正则化因子，默认为0.01</span><br><span class="line">keras.layers.Dense(units, kernel_regularizer=keras.regularizers.l2(1e-4))</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="为什么通过L1，L2正则可以防止过拟合"><a href="#为什么通过L1，L2正则可以防止过拟合" class="headerlink" title="为什么通过L1，L2正则可以防止过拟合"></a><font size="5" color="red">为什么通过L1，L2正则可以防止过拟合</font></h1><p>首先我们讨论过拟合产生的原因，过拟合是指拟合函数要考虑到每一个点，<strong>当存在噪声时，函数值会剧烈变化，因此导数值会非常大，所以只有网络权值足够大时，才能够保证导数值很大</strong>。也就是说网络的权值较大，<strong>加入正则化后，为了达到较小的目标函数，会降低网络的参数，因此该参数就不是最优解，所以可以防止过拟合</strong>。</p><h1 id="Dropout-随机失活"><a href="#Dropout-随机失活" class="headerlink" title="Dropout(随机失活)"></a><font size="5" color="red">Dropout(随机失活)</font></h1><p><strong>Dropout(随机失活)</strong>：也是一种计算方便，功能强大的正则化方法，<strong>其原理是随机将某些神经元失活，只训练剩下的节点，每次失活的神经元都不一样，相当于每次迭代都是在训练不同的网络。其目的是降低节点之间的关联性和模型复杂度，使网络不要总是依赖于某些神经元的权值，从而达到正则化的效果</strong>。<br>在TensorFlow中，已经给我们提供了Dropout网络层，在keras.layers模块中，使用时直接在网络的某些层加上Dropout即可，非常简单。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"># rate为随机失活的比例，一般设为0.2较好。</span><br><span class="line">keras.layers.Dropout(rate)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/dropout.png" alt="dropout"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  深度学习中经常会出现过拟合的现象，其<strong>主要原因是深度学习网络参数量大，模型结构复杂，因此很容易就学习到一些数据特有的性质</strong>，因此就会产生过拟合，<strong>解决过拟合的方法不止Regularization和Dropout两种</strong>，还可以进行<strong>数据增强</strong>操作，<strong>使得数据变得多样化，防止模型对特定数据产生依赖</strong>，有关数据增强的内容，可以参考我的另一篇博客Data Augmentation(数据增强)，里面列举了一些数据增强的常用操作。<strong>EarlyStopping(早停)</strong>也可以缓解过拟合的现象，<strong>当模型在验证集上效果出现波动，甚至下降时，我们可以考虑将模型停止学习，称之为早停</strong>，有关EarlyStopping的内容，可以参考我的另一篇博客Callbacks黑科技，在里面介绍了如何在训练中自动的监测模型的指标，并以此作为是否早停的依据。关于过拟合现象，有很多种方法可以缓解，具体如何选择，需要小伙伴们多多尝试，熟能生巧。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Regularization&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>TensorFlow训练，验证，预测的三种方法</title>
    <link href="https://USTCcoder.github.io/2020/05/21/deep%20learning%20train_evaluate_predict/"/>
    <id>https://USTCcoder.github.io/2020/05/21/deep learning train_evaluate_predict/</id>
    <published>2020-05-21T13:19:10.000Z</published>
    <updated>2020-06-09T11:32:33.325Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Train &amp; Evaluate &amp; Predict</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Train, Evaluate, Predict(训练，验证，预测)</strong>:<strong>是深度学习中的基础内容，想要完成一个深度学习工程问题，训练，验证，预测是必不可少的环节</strong>，今天以LeNet-5模型为例，给入门的小伙伴们提供TensorFlow中三种常用的训练，验证，预测方法。<br><a id="more"></a></p><p><img src="/images/deep_learning/train.png" alt="train"></p><h1 id="fit，evaluate，predict-训练，验证，预测"><a href="#fit，evaluate，predict-训练，验证，预测" class="headerlink" title="fit，evaluate，predict(训练，验证，预测)"></a><font size="5" color="red">fit，evaluate，predict(训练，验证，预测)</font></h1><p><strong>通过模型的fit方法实现训练过程，evaluate方法实现验证过程，predict方法实现预测过程</strong>。这三种方法灵活性较差， 但是很方便，而且可以通过回调函数实现复杂的逻辑控制，是工程应用中经常使用的方法**。fit方法用于训练过程，较为复杂，因此需要设置很多参数，evaluate方法，往往一个epoch或者几个epoch进行一次验证，因此参数比较固定，一般来说只需要验证集数据，其余参数选择默认参数即可。predict方法更加简单，参数也比较固定，一般来说只需要测试集数据，其他参数选择默认参数即可。只有fit方法的参数需要仔细设计，其常用的参数如下：</p><ol><li>x：训练集数据。</li><li>y：训练集标签。</li><li>epochs：达到训练迭代次数epochs停止训练。</li><li>verbose：显示方式，verbose=0，不显示，verbose=1，以进度条显示，verbose=2，每轮迭代显示一次。</li><li>callbacks：回调函数，可以参考我的Callbacks黑科技博客，里面有回调函数的详细使用方法。</li><li>validation_data：验证数据集。</li><li>initial_epoch：开始训练的迭代次数，用于多次阶段性训练中，如迁移学习。<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def preprocess(x, y):</span><br><span class="line">    x = tf.cast(x, dtype=tf.float32) / 255.</span><br><span class="line">    x = tf.reshape(x, (28, 28, 1))</span><br><span class="line">    y = tf.one_hot(y, depth=10)</span><br><span class="line">    y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">    return x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    (x, y), (x_test, y_test) = keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">    batch_size = 256</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    max_epoch = 5</span><br><span class="line"></span><br><span class="line">    db = tf.data.Dataset.from_tensor_slices((x, y))</span><br><span class="line">    db = db.map(preprocess).shuffle(10000).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))</span><br><span class="line">    db_test = db_test.map(preprocess).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    # 创建模型</span><br><span class="line">    model = keras.Sequential([keras.layers.Conv2D(6, (3, 3), (1, 1), 'same', name='Conv1'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn1'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu1'),</span><br><span class="line">                              keras.layers.MaxPool2D((2, 2), (2, 2), name='Maxpool1'),</span><br><span class="line">                              keras.layers.Conv2D(16, (3, 3), (1, 1), 'same', name='Conv2'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn2'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu2'),</span><br><span class="line">                              keras.layers.MaxPool2D((2, 2), (2, 2), name='Maxpool2'),</span><br><span class="line">                              keras.layers.Conv2D(120, (3, 3), (1, 1), 'same', name='Conv3'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn3'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu3'),</span><br><span class="line">                              keras.layers.Flatten(name='Flatten'),</span><br><span class="line">                              keras.layers.Dense(84, activation='relu', name='Dense1_1'),</span><br><span class="line">                              keras.layers.Dropout(0.2, name='Dropout'),</span><br><span class="line">                              keras.layers.Dense(10, activation='softmax', name='Dense2_1')], name='Model')</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(None, 28, 28, 1))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(1e-3)</span><br><span class="line">    lossor = keras.losses.CategoricalCrossentropy()</span><br><span class="line">    metrics = keras.metrics.CategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=optimizer, loss=lossor, metrics=[metrics])</span><br><span class="line"></span><br><span class="line">    # 模型训练</span><br><span class="line">    model.fit(db, epochs=max_epoch, validation_data=db_test, verbose=2)</span><br><span class="line"></span><br><span class="line">    print('----------------------Validation----------------------')</span><br><span class="line">    model.evaluate(db_test)</span><br><span class="line"></span><br><span class="line">    it_test = iter(db_test)</span><br><span class="line">    test_x, test_y = next(it_test)</span><br><span class="line">    print('----------------------Prediction----------------------')</span><br><span class="line">    y_pred = model.predict(test_x)</span><br><span class="line">    print('loss: %.6f, categorical_accuracy: %6f' % (lossor(test_y, y_pred), metrics(test_y, y_pred)))</span><br></pre></td></tr></tbody></table></figure></li></ol><p><img src="/images/deep_learning/train1.png" alt="train1"></p><h1 id="train-on-batch，test-on-batch，predict-on-batch训练，验证和预测方法"><a href="#train-on-batch，test-on-batch，predict-on-batch训练，验证和预测方法" class="headerlink" title="train_on_batch，test_on_batch，predict_on_batch训练，验证和预测方法"></a><font size="5" color="red">train_on_batch，test_on_batch，predict_on_batch训练，验证和预测方法</font></h1><p><strong>通过模型的train_on_batch方法实现训练过程，test_on_batch方法实现验证过程，predict_on_batch方法实现预测过程</strong>，相对于fit方法更加灵活， 但是需要手写一些回调方法实现对训练过程的控制，predict_on_batch只有一个参数，只需要输入测试集即可。train_on_batch和test_on_batch常用的参数如下：</p><ol><li>x：训练集或验证集数据。</li><li>y：训练集或验证集标签。</li><li>reset_metrics：是否累积，如果为True则仅适用于该批次，为False则会跨批次累积。<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def preprocess(x, y):</span><br><span class="line">    x = tf.cast(x, dtype=tf.float32) / 255.</span><br><span class="line">    x = tf.reshape(x, (28, 28, 1))</span><br><span class="line">    y = tf.one_hot(y, depth=10)</span><br><span class="line">    y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">    return x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    (x, y), (x_test, y_test) = keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">    batch_size = 256</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    max_epoch = 5</span><br><span class="line"></span><br><span class="line">    db = tf.data.Dataset.from_tensor_slices((x, y))</span><br><span class="line">    db = db.map(preprocess).shuffle(10000).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))</span><br><span class="line">    db_test = db_test.map(preprocess).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    # 创建模型</span><br><span class="line">    model = keras.Sequential([keras.layers.Conv2D(6, (3, 3), (1, 1), 'same', name='Conv1'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn1'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu1'),</span><br><span class="line">                              keras.layers.MaxPool2D((2, 2), (2, 2), name='Maxpool1'),</span><br><span class="line">                              keras.layers.Conv2D(16, (3, 3), (1, 1), 'same', name='Conv2'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn2'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu2'),</span><br><span class="line">                              keras.layers.MaxPool2D((2, 2), (2, 2), name='Maxpool2'),</span><br><span class="line">                              keras.layers.Conv2D(120, (3, 3), (1, 1), 'same', name='Conv3'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn3'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu3'),</span><br><span class="line">                              keras.layers.Flatten(name='Flatten'),</span><br><span class="line">                              keras.layers.Dense(84, activation='relu', name='Dense1_1'),</span><br><span class="line">                              keras.layers.Dropout(0.2, name='Dropout'),</span><br><span class="line">                              keras.layers.Dense(10, activation='softmax', name='Dense2_1')], name='Model')</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(None, 28, 28, 1))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(1e-3)</span><br><span class="line">    lossor = keras.losses.CategoricalCrossentropy()</span><br><span class="line">    metrics = keras.metrics.CategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=optimizer, loss=lossor, metrics=[metrics])</span><br><span class="line"></span><br><span class="line">    # 模型训练</span><br><span class="line">    for epoch in range(max_epoch):</span><br><span class="line">        it_train = iter(db)</span><br><span class="line">        it_test = iter(db_test)</span><br><span class="line">        model.reset_metrics()</span><br><span class="line"></span><br><span class="line">        for train_x, train_y in it_train:</span><br><span class="line">            train_result = model.train_on_batch(train_x, train_y)</span><br><span class="line"></span><br><span class="line">        for val_x, val_y in it_test:</span><br><span class="line">            val_result = model.test_on_batch(val_x, val_y)</span><br><span class="line"></span><br><span class="line">        print('epoch: %d, Train loss: %.6f, Train categorical_accuracy: %6f' % (epoch, train_result[0], train_result[1]))</span><br><span class="line">        print('epoch: %d, Validation loss: %.6f, Validation categorical_accuracy: %6f' % (epoch, val_result[0], val_result[1]))</span><br><span class="line"></span><br><span class="line">    it_test = iter(db_test)</span><br><span class="line">    test_x, test_y = next(it_test)</span><br><span class="line">    y_pred = model.predict_on_batch(test_x)</span><br><span class="line">    print('----------------------Prediction----------------------')</span><br><span class="line">    print('loss: %.6f, categorical_accuracy: %6f' % (lossor(test_y, y_pred), metrics(test_y, y_pred)))</span><br></pre></td></tr></tbody></table></figure></li></ol><p><img src="/images/deep_learning/train2.png" alt="train2"></p><h1 id="自定义训练，验证，预测方法"><a href="#自定义训练，验证，预测方法" class="headerlink" title="自定义训练，验证，预测方法"></a><font size="5" color="red">自定义训练，验证，预测方法</font></h1><p><strong>自定义训练不需要对模型进行编译，直接利用优化器和损失函数，利用梯度下降法反向传播迭代参数，灵活度最高，但是难度也最大</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def preprocess(x, y):</span><br><span class="line">    x = tf.cast(x, dtype=tf.float32) / 255.</span><br><span class="line">    x = tf.reshape(x, (28, 28, 1))</span><br><span class="line">    y = tf.one_hot(y, depth=10)</span><br><span class="line">    y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">    return x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    (x, y), (x_test, y_test) = keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">    batch_size = 256</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    max_epoch = 5</span><br><span class="line"></span><br><span class="line">    db = tf.data.Dataset.from_tensor_slices((x, y))</span><br><span class="line">    db = db.map(preprocess).shuffle(10000).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))</span><br><span class="line">    db_test = db_test.map(preprocess).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    # 创建模型</span><br><span class="line">    model = keras.Sequential([keras.layers.Conv2D(6, (3, 3), (1, 1), 'same', name='Conv1'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn1'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu1'),</span><br><span class="line">                              keras.layers.MaxPool2D((2, 2), (2, 2), name='Maxpool1'),</span><br><span class="line">                              keras.layers.Conv2D(16, (3, 3), (1, 1), 'same', name='Conv2'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn2'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu2'),</span><br><span class="line">                              keras.layers.MaxPool2D((2, 2), (2, 2), name='Maxpool2'),</span><br><span class="line">                              keras.layers.Conv2D(120, (3, 3), (1, 1), 'same', name='Conv3'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn3'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu3'),</span><br><span class="line">                              keras.layers.Flatten(name='Flatten'),</span><br><span class="line">                              keras.layers.Dense(84, activation='relu', name='Dense1_1'),</span><br><span class="line">                              keras.layers.Dropout(0.2, name='Dropout'),</span><br><span class="line">                              keras.layers.Dense(10, activation='softmax', name='Dense2_1')], name='Model')</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(None, 28, 28, 1))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(1e-3)</span><br><span class="line">    lossor = keras.losses.CategoricalCrossentropy()</span><br><span class="line">    train_loss = keras.metrics.Mean()</span><br><span class="line">    train_metrics = keras.metrics.CategoricalAccuracy()</span><br><span class="line">    val_loss = keras.metrics.Mean()</span><br><span class="line">    val_metrics = keras.metrics.CategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">    for epoch in range(max_epoch):</span><br><span class="line">        it_train = iter(db)</span><br><span class="line">        it_test = iter(db_test)</span><br><span class="line">        for train_x, train_y in db:</span><br><span class="line">            with tf.GradientTape() as tape:</span><br><span class="line">                y_pred = model(train_x, training=True)</span><br><span class="line">                loss = lossor(train_y, y_pred)</span><br><span class="line">            grads = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">            optimizer.apply_gradients(zip(grads, model.trainable_variables))</span><br><span class="line">            train_loss.update_state(loss)</span><br><span class="line">            train_metrics.update_state(train_y, y_pred)</span><br><span class="line"></span><br><span class="line">        for val_x, val_y in db_test:</span><br><span class="line">            y_pred = model(val_x, training=False)</span><br><span class="line">            loss = lossor(val_y, y_pred)</span><br><span class="line">            val_loss.update_state(loss)</span><br><span class="line">            val_metrics.update_state(val_y, y_pred)</span><br><span class="line"></span><br><span class="line">        print('Epoch: %d, Train loss: %.6f, Train acc: %.6f, Valid loss: %.6f, Valid acc: %.6f' % (epoch, train_loss.result(), train_metrics.result(), val_loss.result(), val_metrics.result()))</span><br><span class="line"></span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        train_metrics.reset_states()</span><br><span class="line">        val_loss.reset_states()</span><br><span class="line">        val_metrics.reset_states()</span><br><span class="line"></span><br><span class="line">    print('----------------------Prediction----------------------')</span><br><span class="line">    it_test = iter(db_test)</span><br><span class="line">    test_x, test_y = next(db_test)</span><br><span class="line">    y_pred = model(test_x, training=False)</span><br><span class="line">    loss = lossor(test_y, y_pred)</span><br><span class="line">    print('loss: %.6f, categorical_accuracy: %6f' % (loss, val_metrics(test_y, y_pred)))</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/train3.png" alt="train3"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  这篇博客给小伙伴们介绍了三种常用的训练，验证和预测方法，主要学习第一种和第三种方法，如果为了使用方便则考虑第一种，如果为了模型更加灵活则考虑第三种，希望小伙伴们都可以熟练掌握。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Train &amp; Evaluate &amp; Predict&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>TensorFlow定义网络层</title>
    <link href="https://USTCcoder.github.io/2020/05/20/deep%20learning%20create%20layer/"/>
    <id>https://USTCcoder.github.io/2020/05/20/deep learning create layer/</id>
    <published>2020-05-20T14:34:43.000Z</published>
    <updated>2020-06-14T06:13:34.112Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Define Layer</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Define Layer(定义网络层)</strong>:<strong>是深度学习中的基础内容，想使用深度学习方法解决实际问题，首先就需要建立一个网络层</strong>，今天以LeNet-5模型为例，给入门的小伙伴们提供TensorFlow2.0两种定义网络层的方法。<br><a id="more"></a></p><p><img src="/images/deep_learning/layer.png" alt="layer"></p><h1 id="第一种方法"><a href="#第一种方法" class="headerlink" title="第一种方法"></a><font size="5" color="red">第一种方法</font></h1><p>只用TensorFlow中定义好的网络层，直接使用keras.layer下的网络层类即可，是最简单的网络层创建方法，缺点也很明显，灵活度很低。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = keras.models.Sequential([keras.layers.Input(shape=(28, 28, 1), name='input'),</span><br><span class="line">                                     keras.layers.Conv2D(6, kernel_size=(5, 5), padding='same', activation='relu', name='conv1'),</span><br><span class="line">                                     keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='maxpool1'),</span><br><span class="line">                                     keras.layers.Conv2D(16, kernel_size=(5, 5), padding='valid', activation='relu', name='conv2'),</span><br><span class="line">                                     keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='maxpool2'),</span><br><span class="line">                                     keras.layers.Flatten(name='flatten'),</span><br><span class="line">                                     keras.layers.Dense(120, activation='relu', name='dense1'),</span><br><span class="line">                                     keras.layers.Dense(84, activation='relu', name='dense2'),</span><br><span class="line">                                     keras.layers.Dense(10, activation='softmax', name='dense3')], name='LeNet-5')</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(None, 28, 28, 1))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/model1.png" alt="model1"></p><h1 id="第二种方法"><a href="#第二种方法" class="headerlink" title="第二种方法"></a><font size="5" color="red">第二种方法</font></h1><p>通过<strong>继承keras.layer.Layer来创建模型，是最灵活的方式，可以满足任何网络层的定义，但是难度也更大，必须通过重载call函数自定义调用方式</strong>。<br><strong>__init__函数，是自定义层的构造函数，可以在这里准备一些和输入尺寸无关的网络层，如定义一些参数，接收构造函数的输入等等</strong>。<br><strong>call函数，inputs是上一层网络的输出，return是自定义网络的输出，在call函数中写入本层要实现的内容</strong>。<br><strong>build函数，上面两个都是必须的函数，而build函数是根据需要来创建的，一些参数可能需要根据上一层的网络的输出来确定的，如reshape，resize等等操作，因此这些参数的定义无法在__init__中完成，所以需要在build中完成参数的定义，input_shape参数是上一层网络的输出维度。在没有定义build函数时，会默认调用一次空的build函数</strong>。<br><strong>dynamic=False参数，在调用父类的构造函数时，可以传入dynamic参数，其中默认为False，搭建静态图，如果需要动态调整参数，则需要写入dynamic=True</strong></p><p>因此当我们需要自定义一个网络层时，我们的思路为：</p><ol><li><p>根据需要<strong>判断自定义层是否可以由keras.layer提供的网络层组合而成</strong>，如CNN中大量存在Convlution+BN+ReLU层，因此我们可以定义一个层一次性完成三个步骤，而且这三个层都存在于keras.layer中，所以使用卷积层时也不必要使用build函数，直接调用keras.layers.Conv2D接口即可。</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Conv_Bn_ReLU(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_ReLU, self).__init__(name=name)</span><br><span class="line">        self.conv = keras.layers.Conv2D(filters, kernel_size, strides, padding)</span><br><span class="line">        self.bn = keras.layers.BatchNormalization()</span><br><span class="line">        self.relu = keras.layers.ReLU()</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        conv = self.conv(inputs)</span><br><span class="line">        bn = self.bn(conv)</span><br><span class="line">        output = self.relu(bn)</span><br><span class="line"></span><br><span class="line">        return output</span><br></pre></td></tr></tbody></table></figure></li><li><p>根据需要<strong>判断是否必须通过上一层输出的尺寸进行操作，如果是则需要定义build函数，如果不需要根据上一层尺寸来判断，则可以在__init__中创建</strong>。</p></li></ol><p>定义参数说明：</p><ul><li>在<strong>__init__中定义参数，可以使用self.add_weight()创建指定形状的参数</strong>，但是无法创建和输入尺寸相关的参数，<strong>也可以使用tf.Variable()创建指定形状的参数，其中括号里可以为numpy数组，也可以为EagerTensor(动态张量)</strong>。</li><li>在<strong>build中定义参数，可以使用self.add_weight()创建指定形状的参数</strong>，而且可以创建和输入尺寸相关的参数，<strong>也可以使用tf.Variable()创建指定形状的参数，但是使用Input层，build方法或者使用fit和train_on_batch训练时会传入输入参数尺寸，因为没有具体数值，因此会自动启动静态图，在静态模式下无法产生EagerTensor(动态张量)，所以不能在Variable的参数列表中传入动态张量，但是仍然可以传入numpy格式的参数。</strong>。</li><li><strong>因此推荐在<strong>init</strong>中搭建和输入不相关的参数，在build中搭建和输入相关的参数</strong>。</li></ul><p>下面是完全使用自定义网络层完成的LeNet-5网络模型。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyDense(keras.layers.Layer):</span><br><span class="line">    def __init__(self, units, name):</span><br><span class="line">        super(MyDense, self).__init__(name=name)</span><br><span class="line">        self.units = units</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        self.w = self.add_weight(name='kernel', shape=(input_shape[-1], self.units), initializer=keras.initializers.GlorotUniform())</span><br><span class="line">        self.b = self.add_weight(name='bias', shape=(self.units,), initializer=keras.initializers.Zeros())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return tf.matmul(inputs, self.w) + self.b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyConv(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(MyConv, self).__init__(name=name)</span><br><span class="line">        self.filters = filters</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">        self.strides = strides</span><br><span class="line">        self.padding = padding</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        self.w = self.add_weight(name='kernel', shape=(self.kernel_size, self.kernel_size, input_shape[-1], self.filters), initializer=keras.initializers.GlorotUniform())</span><br><span class="line">        self.b = self.add_weight(name='bias', shape=(self.filters,), initializer=keras.initializers.zeros)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return tf.nn.bias_add(tf.nn.conv2d(inputs, self.w, (1, self.strides, self.strides, 1), self.padding), self.b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyRelu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        super(MyRelu, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return tf.nn.relu(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyFlatten(keras.layers.Layer):</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        super(MyFlatten, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return tf.reshape(inputs, (-1, tf.reduce_prod(inputs.shape[1:])))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MySoftmax(keras.layers.Layer):</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        super(MySoftmax, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return tf.nn.softmax(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyMaxpool(keras.layers.Layer):</span><br><span class="line">    def __init__(self, kernel_size, strides, padding, name):</span><br><span class="line">        super(MyMaxpool, self).__init__(name=name)</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">        self.strides = strides</span><br><span class="line">        self.padding = padding</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return tf.nn.max_pool2d(inputs, (1, self.kernel_size, self.kernel_size, 1), (1, self.strides, self.strides, 1), self.padding)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = keras.models.Sequential([keras.layers.Input(shape=(28, 28, 1), name='input'),</span><br><span class="line">                                     MyConv(filters=6, kernel_size=3, strides=1, padding='SAME', name='conv1'),</span><br><span class="line">                                     MyRelu(name='relu1'),</span><br><span class="line">                                     MyMaxpool(kernel_size=2, strides=2, padding='SAME', name='maxpool1'),</span><br><span class="line">                                     MyConv(filters=16, kernel_size=3, strides=1, padding='SAME', name='conv2'),</span><br><span class="line">                                     MyRelu(name='relu2'),</span><br><span class="line">                                     MyMaxpool(kernel_size=2, strides=2, padding='SAME', name='maxpool2'),</span><br><span class="line">                                     MyConv(filters=120, kernel_size=3, strides=1, padding='SAME', name='conv3'),</span><br><span class="line">                                     MyRelu(name='relu3'),</span><br><span class="line">                                     MyFlatten(name='flatten'),</span><br><span class="line">                                     MyDense(units=84, name='dense1'),</span><br><span class="line">                                     MyRelu(name='relu4'),</span><br><span class="line">                                     MyDense(units=10, name='dense2'),</span><br><span class="line">                                     MySoftmax(name='softmax')], name='LeNet-5')</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(None, 28, 28, 1))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/layer2.png" alt="layer2"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  <strong>网络层的第二种定义方式和模型定义的第三种定义方式非常类似</strong>，在call函数中定义符合自己需要的网络层，一般的顺序是<strong>先从keras.layer中寻找自己需要的层是否已经提供，如卷积层，池化层，全连接层等等，这时不需要我们手动定义，但是如果某些层非常复杂，如SE注意力层，需要我们手动定义</strong>，希望小伙伴们可以多多练习。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Define Layer&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>TensorFlow定义模型的三种方法</title>
    <link href="https://USTCcoder.github.io/2020/05/20/deep%20learning%20create%20model/"/>
    <id>https://USTCcoder.github.io/2020/05/20/deep learning create model/</id>
    <published>2020-05-20T08:20:39.000Z</published>
    <updated>2020-06-08T05:19:36.662Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Define Model</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Define Model(定义模型)</strong>:<strong>是深度学习中的基础内容，想使用深度学习方法解决实际问题，首先就需要建立一个模型</strong>，今天以LeNet-5模型为例，给入门的小伙伴们提供TensorFlow2.0三种自定义模型的方法。<br><a id="more"></a></p><p><img src="/images/deep_learning/model.png" alt="model"></p><h1 id="第一种方法"><a href="#第一种方法" class="headerlink" title="第一种方法"></a><font size="5" color="red">第一种方法</font></h1><p>通过<strong>keras.Sequential来创建一个序列模型，也是最简单的一种模型创建方法，缺点也很明显，灵活度很低，不能共享某一层，不能有分支，不能有多个输入输出。只适合于串联模型的创建(如VGG16)，不适合并联模型的创建(如Inception-V3，ResNet50)</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = keras.models.Sequential([keras.layers.Input(shape=(28, 28, 1), name='input'),</span><br><span class="line">                                     keras.layers.Conv2D(6, kernel_size=(5, 5), padding='same', activation='relu', name='conv1'),</span><br><span class="line">                                     keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='maxpool1'),</span><br><span class="line">                                     keras.layers.Conv2D(16, kernel_size=(5, 5), padding='valid', activation='relu', name='conv2'),</span><br><span class="line">                                     keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='maxpool2'),</span><br><span class="line">                                     keras.layers.Flatten(name='flatten'),</span><br><span class="line">                                     keras.layers.Dense(120, activation='relu', name='dense1'),</span><br><span class="line">                                     keras.layers.Dense(84, activation='relu', name='dense2'),</span><br><span class="line">                                     keras.layers.Dense(10, activation='softmax', name='dense3')], name='LeNet-5')</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(None, 28, 28, 1))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/model1.png" alt="model1"></p><h1 id="第二种方法"><a href="#第二种方法" class="headerlink" title="第二种方法"></a><font size="5" color="red">第二种方法</font></h1><p><strong>通过函数API来创建一个模型，是一种灵活的方式，可以定义更加复杂的模型，灵活度很高，可以共享层，可以分支，且可以满足多个输入输出</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lenet(input_shape):</span><br><span class="line"></span><br><span class="line">    net = dict()</span><br><span class="line"></span><br><span class="line">    net['input'] = keras.layers.Input(shape=input_shape, name='input')</span><br><span class="line">    net['conv1'] = keras.layers.Conv2D(6, kernel_size=(5, 5), activation='relu', padding='same', name='conv1')(net['input'])</span><br><span class="line">    net['maxpool1'] = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='maxpool1')(net['conv1'])</span><br><span class="line">    net['conv2'] = keras.layers.Conv2D(16, kernel_size=(5, 5), activation='relu', padding='valid', name='conv2')(net['maxpool1'])</span><br><span class="line">    net['maxpool2'] = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='maxpool2')(net['conv2'])</span><br><span class="line">    net['flatten'] = keras.layers.Flatten(name='flatten')(net['maxpool2'])</span><br><span class="line">    net['dense1'] = keras.layers.Dense(120, activation='relu', name='dense1')(net['flatten'])</span><br><span class="line">    net['dense2'] = keras.layers.Dense(84, activation='relu', name='dense2')(net['dense1'])</span><br><span class="line">    net['dense3'] = keras.layers.Dense(10, activation='softmax', name='dense3')(net['dense2'])</span><br><span class="line"></span><br><span class="line">    model = keras.Model(net['input'], net['dense3'], name='LeNet-5')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = lenet((28, 28, 1))</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(None, 28, 28, 1))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/model2.png" alt="model2"></p><h1 id="第三种方法"><a href="#第三种方法" class="headerlink" title="第三种方法"></a><font size="5" color="red">第三种方法</font></h1><p>通过<strong>继承keras.Model来创建模型，是一种最灵活的方式，可以满足任何模型的定义，但是难度也更大，必须通过重载call函数自定义调用方式</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class LeNet_5(keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(LeNet_5, self).__init__(name='LeNet-5')</span><br><span class="line"></span><br><span class="line">        self.conv1 = keras.layers.Conv2D(6, kernel_size=(5, 5), padding='same', activation='relu', name='conv1')</span><br><span class="line">        self.maxpool1 = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='maxpool1')</span><br><span class="line">        self.conv2 = keras.layers.Conv2D(16, kernel_size=(5, 5), padding='valid', activation='relu', name='conv2')</span><br><span class="line">        self.maxpool2 = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='maxpool2')</span><br><span class="line">        self.flatten = keras.layers.Flatten(name='flatten')</span><br><span class="line">        self.dense1 = keras.layers.Dense(120, activation='relu', name='dense1')</span><br><span class="line">        self.dense2 = keras.layers.Dense(84, activation='relu', name='dense2')</span><br><span class="line">        self.dense3 = keras.layers.Dense(10, activation='softmax', name='dense3')</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, training=None, mask=None):</span><br><span class="line"></span><br><span class="line">        conv1 = self.conv1(inputs)</span><br><span class="line">        maxpool1 = self.maxpool1(conv1)</span><br><span class="line">        conv2 = self.conv2(maxpool1)</span><br><span class="line">        maxpool2 = self.maxpool2(conv2)</span><br><span class="line">        flatten = self.flatten(maxpool2)</span><br><span class="line">        dense1 = self.dense1(flatten)</span><br><span class="line">        dense2 = self.dense2(dense1)</span><br><span class="line">        output = self.dense3(dense2)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = LeNet_5()</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(None, 28, 28, 1))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/model3.png" alt="model3"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  <strong>模型的定义没有固定的方式</strong>，小伙伴们可以<strong>根据平时的代码习惯进行选择</strong>，但是<strong>第二种方法和第三种方法必须要掌握一种</strong>，否则很难定义复杂的网络模型，希望小伙伴们可以多多练习。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Define Model&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>TensorBoard黑科技</title>
    <link href="https://USTCcoder.github.io/2020/05/19/deep%20learning%20tensorboard/"/>
    <id>https://USTCcoder.github.io/2020/05/19/deep learning tensorboard/</id>
    <published>2020-05-19T12:39:39.000Z</published>
    <updated>2020-06-11T02:06:01.825Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">TensorBoard</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>TensorBoard</strong>:是一个<strong>可视化工具</strong>，它可以用来<strong>展示网络流图，损失函数，评价函数等等随epoch的变化过程</strong>，其工作原理是，<strong>程序给磁盘的某个目录写数据，然后监听器就可以监听到这个目录的变化，打开Web浏览器就可以从监听器中获得数据，完成实时的数据更新</strong>。今天给小伙伴们介绍TensorBoard的两种使用方法，希望小伙伴们可以认真学习，动手尝试。<br><a id="more"></a></p><p><img src="/images/deep_learning/tensorboard.png" alt="tensorboard"></p><h1 id="第一种使用方法"><a href="#第一种使用方法" class="headerlink" title="第一种使用方法"></a><font size="5" color="red">第一种使用方法</font></h1><p>以mnist数据集为例，向小伙伴们介绍TensorBoard的第一种使用方法，<strong>使用tensorflow种keras.callbacks模块下的TensorBoard类</strong>，其常用参数列举如下。</p><ol><li><strong>log_dir: 用来保存Tensorboard的日志文件等内容的位置</strong>。</li><li><strong>histogram_freq: 用来计算各个层的激活值和模型权重直方图</strong>。</li><li><strong>write_graph: 是否在TensorBoard中可视化图形(数据流图)</strong>。</li><li><strong>pdate_freq：’batch’或’epoch’或整数。使用’batch’，每批之后将损失和指标写入TensorBoard。’epoch’同理。如果使用整数，假设1000，回调将每1000个样本将指标和损失写入TensorBoard，但是向TensorBoard写入太频繁会减慢训练速度</strong>。<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lenet(input_shape):</span><br><span class="line">    input_tensor = keras.Input(shape=input_shape)</span><br><span class="line">    </span><br><span class="line">    net = dict()</span><br><span class="line">    </span><br><span class="line">    net['input'] = input_tensor</span><br><span class="line">    net['conv1_1'] = keras.layers.Conv2D(6, kernel_size=(3, 3), activation='relu', padding='same', name='conv1_1')(net['input'])</span><br><span class="line">    net['pool1'] = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool1')(net['conv1_1'])</span><br><span class="line">    net['conv2_1'] = keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', name='conv2_1')(net['pool1'])</span><br><span class="line">    net['pool2'] = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool2')(net['conv2_1'])</span><br><span class="line">    net['conv3_1'] = keras.layers.Conv2D(120, kernel_size=(3, 3), activation='relu', padding='same', name='conv3_1')(net['pool2'])</span><br><span class="line">    net['Flatten'] = keras.layers.Flatten()(net['conv3_1'])</span><br><span class="line">    net['dense1'] = keras.layers.Dense(84, activation='relu')(net['Flatten'])</span><br><span class="line">    net['dense2'] = keras.layers.Dense(num_class, activation='softmax')(net['dense1'])</span><br><span class="line">    </span><br><span class="line">    model = keras.Model(net['input'], net['dense2'])</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def preprocess(x, y):</span><br><span class="line">    x = tf.cast(x, dtype=tf.float32) / 255.</span><br><span class="line">    x = tf.reshape(x, (28, 28, 1))</span><br><span class="line">    y = tf.one_hot(y, depth=num_class)</span><br><span class="line">    y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">    return x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    num_class = 10</span><br><span class="line">    # 分离数据</span><br><span class="line">    (x, y), (x_test, y_test) = keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">    batch_size = 256</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    max_epoch = 20</span><br><span class="line">    log_dir = '.\\logs'</span><br><span class="line"></span><br><span class="line">    db = tf.data.Dataset.from_tensor_slices((x, y))</span><br><span class="line">    db = db.map(preprocess).shuffle(10000).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))</span><br><span class="line">    db_test = db_test.map(preprocess).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    # 创建模型</span><br><span class="line">    model = lenet((28, 28, 1))</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(None, 28, 28, 1))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    # 模型参数设置</span><br><span class="line">    model.compile(</span><br><span class="line">        optimizer=keras.optimizers.Adam(lr=1e-3),</span><br><span class="line">        loss=keras.losses.CategoricalCrossentropy(),</span><br><span class="line">        metrics=['acc']</span><br><span class="line">                  )</span><br><span class="line"></span><br><span class="line">    logging = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_grads=True)</span><br><span class="line"></span><br><span class="line">    # 模型训练</span><br><span class="line">    model.fit(db, epochs=max_epoch, callbacks=[logging], validation_data=db_test)</span><br></pre></td></tr></tbody></table></figure></li></ol><p>运行后<strong>日志文件会保存在py文件同级目录logs下</strong>，然后<strong>在cmd命令行中运行tensorboard --logdir=path(logs的绝对路径即可，也可以进入py文件的同级目录中，直接写tensorboard --logdir=logs)</strong>，然后会<strong>出现一个本地的Web网址，复制并在浏览器中打开</strong>即可完成可视化工作。<br><img src="/images/deep_learning/tensorboard1.png" alt="tensorboard1"></p><h1 id="第二种使用方法"><a href="#第二种使用方法" class="headerlink" title="第二种使用方法"></a><font size="5" color="red">第二种使用方法</font></h1><p>上面的方法是<strong>通过keras的高层API接口实现TensorBoard的使用</strong>，下面的方法在任何情况下均可以使用，包括在pytorch中也可以使用。其<strong>使用tf.summary.create_file_writer(log_dir)创建一个写入日志文件的对象，log_dir为要写入日志的目录。然后可以像写入文件一样使用with语句进行TensorBoard的写入</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">log_dir = '.\\logs\\mnist'</span><br><span class="line">summary_writer = tf.summary.create_file_writer(log_dir) </span><br><span class="line"></span><br><span class="line">with summary_writer.as_default():</span><br><span class="line">    # 添加文本模块</span><br><span class="line">    tf.summary.text(name, data, step=None, description=None)</span><br><span class="line">    # 添加图像模块</span><br><span class="line">    tf.summary.image(name, data, step=None, max_outputs=3, description=None)</span><br><span class="line">    # 添加标量模块</span><br><span class="line">    tf.summary.scalar(name, data, step=None, description=None)</span><br><span class="line">    # 添加直方图模块</span><br><span class="line">    tf.summary.histogram(name, data, step=None, buckets=None, description=None)</span><br><span class="line">    # 追踪计算路径，和trace_export配合使用</span><br><span class="line">    tf.summary.trace_on(graph=True, profiler=False)</span><br><span class="line">    # 路径输出，当使用自定义训练方式时，可以查看计算图，前提是要保证函数在静态图中运行，即有@tf.function修饰</span><br><span class="line">    tf.summary.trace_export(name="autograph", step=0, profiler_outdir=log_dir)</span><br></pre></td></tr></tbody></table></figure><p></p><p>运行后<strong>日志文件会保存在py文件同级目录logs下</strong>，然后<strong>在cmd命令行中运行tensorboard --logdir=path(logs的绝对路径即可，也可以进入py文件的同级目录中，直接写tensorboard --logdir=logs)</strong>，然后会<strong>出现一个本地的Web网址，复制并在浏览器中打开</strong>即可完成可视化工作。</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line">import datetime</span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def preprocess(x, y):</span><br><span class="line">    x = tf.cast(x, dtype=tf.float32) / 255.</span><br><span class="line">    x = tf.reshape(x, (28, 28, 1))</span><br><span class="line">    y = tf.one_hot(y, depth=num_class)</span><br><span class="line">    y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">    return x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lenet(input_shape):</span><br><span class="line">    input_tensor = keras.Input(shape=input_shape)</span><br><span class="line"></span><br><span class="line">    net = dict()</span><br><span class="line"></span><br><span class="line">    net['input'] = input_tensor</span><br><span class="line">    net['conv1_1'] = keras.layers.Conv2D(6, kernel_size=(3, 3), activation='relu', padding='same', name='conv1_1')(</span><br><span class="line">        net['input'])</span><br><span class="line">    net['pool1'] = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool1')(net['conv1_1'])</span><br><span class="line">    net['conv2_1'] = keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', name='conv2_1')(</span><br><span class="line">        net['pool1'])</span><br><span class="line">    net['pool2'] = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool2')(net['conv2_1'])</span><br><span class="line">    net['conv3_1'] = keras.layers.Conv2D(120, kernel_size=(3, 3), activation='relu', padding='same', name='conv3_1')(</span><br><span class="line">        net['pool2'])</span><br><span class="line">    net['Flatten'] = keras.layers.Flatten()(net['conv3_1'])</span><br><span class="line">    net['dense1'] = keras.layers.Dense(84, activation='relu')(net['Flatten'])</span><br><span class="line">    net['dense2'] = keras.layers.Dense(num_class, activation='softmax')(net['dense1'])</span><br><span class="line"></span><br><span class="line">    model = keras.Model(net['input'], net['dense2'])</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@tf.function</span><br><span class="line">def dynamic_gradient_descent(train_x, train_y):</span><br><span class="line">    y_pred = model(train_x, training=True)</span><br><span class="line">    loss = lossor(train_y, y_pred)</span><br><span class="line">    grads = tf.gradients(loss, model.trainable_variables)</span><br><span class="line"></span><br><span class="line">    return grads</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def save_picture(image, picture_num):</span><br><span class="line">    image = ((image + 1) * 127.5).astype(np.uint8)</span><br><span class="line">    image = np.concatenate([image[i * picture_num:(i + 1) * picture_num] for i in range(picture_num)], axis=2)</span><br><span class="line">    image = np.concatenate([image[i] for i in range(picture_num)], axis=0)</span><br><span class="line"></span><br><span class="line">    return image[np.newaxis, ...]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    num_class = 10</span><br><span class="line"></span><br><span class="line">    (x, y), (x_test, y_test) = keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">    batch_size = 250</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    max_epoch = 10</span><br><span class="line"></span><br><span class="line">    db = tf.data.Dataset.from_tensor_slices((x, y))</span><br><span class="line">    db = db.map(preprocess).shuffle(10000).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))</span><br><span class="line">    db_test = db_test.map(preprocess).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    # 创建模型</span><br><span class="line">    model = lenet((28, 28, 1))</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(None, 28, 28, 1))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    # 模型参数设置</span><br><span class="line">    model.compile(</span><br><span class="line">        optimizer=keras.optimizers.Adam(lr=1e-3),</span><br><span class="line">        loss=keras.losses.CategoricalCrossentropy(),</span><br><span class="line">        metrics=['acc']</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(1e-3)</span><br><span class="line">    lossor = keras.losses.CategoricalCrossentropy()</span><br><span class="line">    val_metrics = keras.metrics.CategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">    stamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")</span><br><span class="line">    log_dir = '.\\%s' % stamp</span><br><span class="line">    summary_writer = tf.summary.create_file_writer(log_dir)</span><br><span class="line"></span><br><span class="line">    tf.summary.trace_on()</span><br><span class="line"></span><br><span class="line">    for epoch in range(max_epoch):</span><br><span class="line">        it_train = iter(db)</span><br><span class="line">        for train_x, train_y in db:</span><br><span class="line">            grads = dynamic_gradient_descent(train_x, train_y)</span><br><span class="line">            optimizer.apply_gradients(zip(grads, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">        it_test = iter(db_test.shuffle(10000))</span><br><span class="line">        test_x, test_y = next(it_test)</span><br><span class="line">        y_pred = model(test_x)</span><br><span class="line">        loss = lossor(test_y, y_pred)</span><br><span class="line">        val_metrics(test_y, y_pred)</span><br><span class="line">        acc = val_metrics.result()</span><br><span class="line"></span><br><span class="line">        with summary_writer.as_default():</span><br><span class="line">            tf.summary.scalar('loss', loss, step=epoch)</span><br><span class="line">            tf.summary.scalar('acc', acc, step=epoch)</span><br><span class="line">            tf.summary.image('train_image', save_picture(test_x[:100].numpy(), 10), step=epoch)</span><br><span class="line"></span><br><span class="line">    with summary_writer.as_default():</span><br><span class="line">        tf.summary.trace_export(name="autograph", step=0)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/deep_learning/tensorboard2.png" alt="tensorboard2"><br>此时要注意，<strong>手动追踪计算图时，如果创建了多个计算图则会出错，计算图不显示。如果传入数据的类型或者维度发生了改变，则会创建新的计算图，因此会有多个计算图存在，所以显示会报错，这时可以修改类型或者维度，使其不创建新的计算图，或者给新的计算图赋予其他的step，防止同时存在于一张图上</strong>。上例中，训练集的大小为60000，如果batch设置为256，则60000/256无法整除，所以最后一个batch会额外创建计算图，在这里我将batch修改为250，这时60000/250可以整除，这样就可以成功显示计算图了。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  TensorBoard是我们使用TensorFlow的好帮手，有了可视化工具，我们可以<strong>方便的查看模型在何时出现了过拟合，可以直观的查看模型在合适已经不再有效训练，可以帮助我们更加清晰的了解自己设计的模型</strong>，因此我们需要掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;TensorBoard&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>误差，偏差，方差，噪声的关系</title>
    <link href="https://USTCcoder.github.io/2020/05/19/deep%20learning%20variance%20bias/"/>
    <id>https://USTCcoder.github.io/2020/05/19/deep learning variance bias/</id>
    <published>2020-05-19T05:59:02.000Z</published>
    <updated>2020-05-19T07:42:11.320Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Error &amp; Bias &amp; Variance &amp; Noise</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Error,Bias,Variance,Noise(误差，偏差，方差，噪声)</strong>:是机器学习中的一组重要概念，小伙伴们可能也听说过这些，但是可能不清楚它们之间到底有什么练习，今天给大家捋一捋。<br><a id="more"></a></p><p><img src="/images/deep_learning/error.png" alt="error"></p><h1 id="误差，偏差，方差，噪声的定义"><a href="#误差，偏差，方差，噪声的定义" class="headerlink" title="误差，偏差，方差，噪声的定义"></a><font size="5" color="red">误差，偏差，方差，噪声的定义</font></h1><p>Error(误差)：把<strong>学习器的实际预测输出与样本的真是输出之间的差异称为误差</strong>，一般<strong>定义为损失函数Loss</strong>，学习的<strong>主要目的就是最小化Loss</strong>。在<strong>训练数据上得到的Loss称之为training erroe(训练误差)</strong>，在<strong>新样本数据得到的Loss称之为(generalization erroe)泛化误差</strong>。显然，<strong>我们希望得到泛化误差小</strong>的学习器。<br>Bias(偏差)：<strong>度量算法的期望预测与真实结果的偏离程度</strong>，刻画了<strong>学习算法本身的拟合能力</strong>。<br>Variance(方差)：<strong>度量同样大小的不同训练集所导致学习性能的变化</strong>，刻画了<strong>数据扰动的影响</strong>。<br>Noise(噪声)：<strong>表达了当前任务上，学习器所能到达的泛化误差的下界</strong>，刻画了<strong>学习本身的难度</strong>，可以理解为数据的label本身就是不准确的，因此<strong>模型无论如何学习都不可能消除</strong>。</p><h1 id="误差，偏差，方差，噪声的关系"><a href="#误差，偏差，方差，噪声的关系" class="headerlink" title="误差，偏差，方差，噪声的关系"></a><font size="5" color="red">误差，偏差，方差，噪声的关系</font></h1><p>下面用公式具体说明它们四者之间的关系，首先定义一些符号，这里参考了周志华老师的《机器学习》，也许叫它西瓜书可能更知名一些，一个一边吃西瓜一边给你侃机器学习的大牛。令测试样本为$x$，令$y_D$为$x$的标签，$y$为$x$的真实标签，$f(x;D)$为训练集$D$上学得得模型$f$对$x$得预测输出。<br>模型得期望预测结果为，各训练集预测结果的均值，用公式表达如下式</p><script type="math/tex; mode=display">\overline{f}(x) = E[f(x;D)]</script><p>不同训练集上产生得训练方差为，各训练集预测结果减模型的期望预测结果的均值，用公式表达如下式</p><script type="math/tex; mode=display">var(x) = E[(f(x;D) - \overline{f}(x))^2]</script><p>样本中存在的噪声为，数据集中的标签$y_D$与数据$x$的真实标签$y$之间的差异，用公式表达如下式</p><script type="math/tex; mode=display">\epsilon^2 = E[(y_D - y)^2]</script><p>该系统的偏差为，期望输出与真实标记之间的差异，用公式表达如下式</p><script type="math/tex; mode=display">bias^2(x) = (\overline{f}(x) - y)^2</script><p>假设噪声的期望为0，$E[y_D - y] = 0$<br>下面对期望泛化误差进行分解</p><script type="math/tex; mode=display">\\begin{align} E[f;D] & = E[(f(x;D) - y_D)^2] \\\\ & = E[(f(x;D) - \overline{f}(x) + \overline{f}(x) - y_D)^2] \\\\ & = E[(f(x;D - \overline{f}(x))^2] + E[(\overline{f}(x) - y_D)^2] + 2E[(f(x;D) - \overline{f}(x))(\overline{f}(x) - y_D)] \\\\ & = E[(f(x;D) - \overline{f}(x))^2] + E[(\overline{f}(x) - y_D)^2] \\\\ & = E[(f(x;D) - \overline{f}(x))^2] + E[(\overline{f}(x) - y + y - y_D)^2] \\\\ & = E[(f(x;D) - \overline{f}(x))^2] + E[(\overline{f}(x) - y)^2] + E[(y - y_D)^2] + 2E[(\overline{f}(x) - y)(y - y_D)] \\\\ & = E[(f(x;D) - \overline{f}(x))^2] + (\overline{f}(x) - y)^2 + E[(y_D - y)^2] \\\\ & = var(x) + bias^2(x) + \epsilon^2 \\\\ \\end{align}</script><p>因为$ \overline{f}(x) = E[f(x;D)] \Rightarrow E[(f(x;D) - \overline{f}(x))] = 0$，因此第三行的最后一项为0<br>因为$E[y_D - y] = 0 \Rightarrow E[(\overline{f}(x) - y)(y - y_D)] = 0 $，因此第六行的最后一项为0<br>所以得到了一个结论，<strong>泛化误差可以分解为方差，偏差和噪声之和</strong>。<br><img src="/images/deep_learning/error1.png" alt="error1"></p><h1 id="偏差和方差的相互制约"><a href="#偏差和方差的相互制约" class="headerlink" title="偏差和方差的相互制约"></a><font size="5" color="red">偏差和方差的相互制约</font></h1><p><strong>随着模型复杂度，模型迭代次数的增加，方差和偏差可能会出现此消彼长的现象</strong>，训练不足，或者模型过于简单时会出现<strong>Under-fitting(欠拟合)</strong>，此时的模型<strong>误差主要来自于偏差</strong>，在训练时的<strong>表现为训练集和验证集上面的准确率都不高</strong>，说明出现了欠拟合。而训练太多，或者模型过于复杂时会出现<strong>Over-fitting(过拟合)</strong>，此时的模型<strong>误差主要来自于方差</strong>，在训练时的<strong>表现为训练集上的准确率非常高，验证集上面的准确率不高</strong>，说明出现了过拟合。<br><img src="/images/deep_learning/regularization.png" alt="underfitting_overfitting"></p><h1 id="如何降低泛化误差"><a href="#如何降低泛化误差" class="headerlink" title="如何降低泛化误差"></a><font size="5" color="red">如何降低泛化误差</font></h1><p>在这里我们<strong>不讨论如何降低噪声，在我们获得数据集时，数据集本身可能就会存在噪声，因此很难进行降低</strong>。我们<strong>主要讨论如何通过降低偏差和方差来降低泛化误差</strong>。</p><p><strong>如何降低偏差</strong>：</p><ol><li><strong>增加算法复杂度，但是要注意单纯的增加算法复杂度可能会导致方差的增加，可以结合正则项进行惩罚</strong>。</li><li><strong>进行合理的特征工程，检查是否遗漏重要特征</strong>。</li><li><strong>优化网络结构，因为偏差大意味着网络的拟合效果不好，因此可以更换网络层</strong>。</li></ol><p><strong>如何降低方差</strong>：</p><ol><li><strong>增加训练样本，样本代表性不足时方差大的首要原因，增加样本也是降低方差最简单的方法</strong>。</li><li><strong>引入正则项(L1正则，L2正则，Dropout等)</strong>。</li><li><strong>特征提取，对输入的特征进行提取，特征变少方差也会减小</strong>。</li><li><strong>降低模型复杂度，或者采用早停，减少迭代周期</strong>。</li></ol><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  虽然我们在使用机器学习时，很难直观的看出偏差和方差之间的关系，但是我们可能会对某些情况深有体会，如训练集的准确率达到95%，而验证集只有80%，这个情况小伙伴们都会一眼看出，过拟合了，但是过拟合背后的原因可能就不回去探究，其实<strong>过拟合就是一种方差过大的体现，这时我们就应该考虑增加训练样本，引入正则，降低模型复杂度或者采用早停等策略进行优化</strong>。还可能训练集和验证集的准确率都只达到50%，这个情况小伙伴们也应该很熟悉，欠拟合了，其实<strong>欠拟合就是一种偏差过大的体现，这时我们就应该考虑增加模型参数，优化网络结构</strong>。偏差和方差很多时候都是在理论分析时需要的，实际的工程问题，小伙伴们也可以不用去过多分析，只需要知道过拟合欠拟合该采取什么样的方法解决它，如果能够达到这样的水平，我觉得已经具备解决实际问题的能力了。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Error &amp; Bias &amp; Variance &amp; Noise&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>Callbacks黑科技</title>
    <link href="https://USTCcoder.github.io/2020/05/18/deep%20learning%20callbacks/"/>
    <id>https://USTCcoder.github.io/2020/05/18/deep learning callbacks/</id>
    <published>2020-05-18T02:05:21.000Z</published>
    <updated>2020-05-18T08:05:33.073Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Callbacks</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Callbacks(回调函数)</strong>:指<strong>在网络学习期间，对网络的性能，参数等进行修改，保存，显示，早停等一系列操作</strong>。不是深度学习必须使用的，但是掌握回调函数可以更好的让网络为我们服务，下面来具体了解一下有哪些常用的回调函数。<br><a id="more"></a></p><p><img src="/images/deep_learning/callbacks.png" alt="callbacks"></p><h1 id="Early-Stopping-早停"><a href="#Early-Stopping-早停" class="headerlink" title="Early Stopping(早停)"></a><font size="5" color="red">Early Stopping(早停)</font></h1><p>在深度学习任务中，<strong>早停是提升模型性能的一个有效方法</strong>，随着训练的进行，模型在训练集和测试集上的性能会逐渐提高，但是在训练一定的周期后，我们常常会发现<strong>训练集上的准确率仍在增加，而且损失函数在减小，但是测试集上的准确率和损失函数却在某个值附近发生波动，甚至会降低准确率</strong>。这种情况我们会认为发生了<strong>Overfitting(过拟合)</strong>，即模型学的太像了，把训练集中的一些特殊情况也学习进去了。举个简单的例子，我们学习认识一只老虎时，如果这只老虎是一只幼崽，我们学习的太像了，就会认为老虎就应该是那么大，当测试时，来了一只猫，我们就会当成是老虎，这就是Overfitting的简单理解。<br>想解决Overfitting，有很多种方法，最暴力的方法是<strong>增加数据集</strong>，还可以<strong>引入正则项</strong>，<strong>减小模型的参数</strong>，以及<strong>早停</strong>。早停是一种简单的解决Overfitting的方法，当模型的验证集已经发生波动时，我们就认为发生了过拟合，因此停止网络的学习，防止其学到过多训练集的特性。<br>在TensorFlow中在keras.callbacks中已经给我们提供了EarlyStopping的类，其常用参数主要有：</p><ol><li><strong>monitor：监视的值，默认为val_loss，当验证集的损失函数不下降时停止学习</strong>。</li><li><strong>min_delta: 监视值的最小变化，默认为0，即只要损失函数降低则视为有改进</strong>。</li><li><strong>patience: 没有改进的周期数，如果连续patience个周期都没有改进，则停止学习</strong>。</li><li><strong>verbose：是否在训练过程中详细显示</strong>。<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1)</span><br></pre></td></tr></tbody></table></figure></li></ol><h1 id="Check-Point-检查点"><a href="#Check-Point-检查点" class="headerlink" title="Check Point(检查点)"></a><font size="5" color="red">Check Point(检查点)</font></h1><p>为了<strong>对比各个训练周期的效果，我们可以将模型按照训练迭代周期进行保存</strong>。在TensorFlow中在keras.callbacks中已经给我们提供了ModelCheckpoint的类，其常用参数主要有：</p><ol><li><strong>filepath：保存模型文件的路径</strong>。</li><li><strong>monitor: 监视的值，默认为val_loss</strong>。</li><li><strong>verbose：是否在训练过程中详细显示</strong>。</li><li><strong>save_best_only: 是否只保存最佳模型</strong>。</li><li><strong>period：每个period个周期检查一次是否需要保存</strong>。<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">check_point = keras.callbacks.ModelCheckpoint(weight_path + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5', monitor='val_loss', verbose=1, save_best_only=True, period=3)</span><br></pre></td></tr></tbody></table></figure></li></ol><h1 id="CSV-Logger-CSV记录器"><a href="#CSV-Logger-CSV记录器" class="headerlink" title="CSV Logger(CSV记录器)"></a><font size="5" color="red">CSV Logger(CSV记录器)</font></h1><p>为了使我们能够<strong>直观方便的看到各个训练周期下模型的评价指标和损失函数</strong>，在TensorFlow中在keras.callbacks中已经给我们提供了CSVLogger的类，<strong>能够将各个训练周期的模型指标保存在csv文件中</strong>，其常用参数主要有：</p><ol><li><strong>filepath：保存csv文件的文件名</strong>。</li><li><strong>separator: 用于分隔csv文件中元素的分隔符，默认为’,’，一般不要修改</strong>。</li><li><strong>append：是否在文件后面追加，如果为True则追加，如果为False则覆盖，默认为False</strong>。<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">log_csv = keras.callbacks.CSVLogger(filename=csv_path)</span><br></pre></td></tr></tbody></table></figure></li></ol><h1 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a><font size="5" color="red">Tensorboard</font></h1><p>为了能够<strong>可视化我们的模型，以及了解数据流图的结构</strong>，在TensorFlow中在keras.callbacks中已经给我们提供了TensorBoard的类，能够在<strong>本地Web浏览器对网络参数，数据流图进行可视化</strong>，其常用参数主要有：</p><ol><li><strong>log_dir: 用来保存Tensorboard的日志文件等内容的位置</strong>。</li><li><strong>histogram_freq: 用来计算各个层的激活值和模型权重直方图</strong>。</li><li><strong>write_graph: 是否在TensorBoard中可视化图形(数据流图)</strong>。</li><li><strong>pdate_freq：batch或epoch或整数，默认为epoch。使用batch，每批之后将损失和指标写入TensorBoard。epoch同理。如果使用整数，假设1000，回调将每1000个样本将指标和损失写入TensorBoard，但是向TensorBoard写入太频繁会减慢训练速度</strong>。<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">log_board = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_grads=True)</span><br></pre></td></tr></tbody></table></figure></li></ol><p>运行后<strong>日志文件会保存在log_dir中</strong>，然后<strong>在cmd命令行中运行tensorboard —logdir=path(log_dir的绝对路径即可，也可以使用相对路径)</strong>，然后会<strong>出现一个本地的Web网址，复制并在浏览器中打开</strong>即可完成可视化工作。<br><strong>想了解更多关于Tensorboard的使用方法，可以参考我的另一篇博客TensorBoard黑科技，专门对TensorBoard的使用进行讲解，除了使用keras.callbacks模块下的类，还有一些其他的方法写入TensorBoard</strong>。</p><h1 id="Reduce-LR-学习率下降"><a href="#Reduce-LR-学习率下降" class="headerlink" title="Reduce LR(学习率下降)"></a><font size="5" color="red">Reduce LR(学习率下降)</font></h1><p>在训练过程中，往往在<strong>初始阶段使用较大的学习率，方便模型的收敛和跳出局部极小值点，而在训练后期学习率需要降低来细化我们的模型，降低损失函数，提高模型评价指标</strong>。在TensorFlow中在keras.callbacks中已经给我们提供了ReduceLROnPlateau的类，能够<strong>在训练过程中，自适应地调整学习率</strong>，其常用参数主要有：</p><ol><li><strong>monitor：监视的值，默认为val_loss</strong>。</li><li><strong>factor：学习率下降因子，new_lr = lr x factor</strong>。</li><li><strong>patience：没有改进的周期数，如果连续patience个周期都没有改进，则下降学习率</strong>。</li><li><strong>min_lr：设置最低学习率，防止学习率过低</strong>。<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=3, verbose=1)</span><br></pre></td></tr></tbody></table></figure></li></ol><p><strong>想了解更多关于学习率下降的使用方法，可以参考我的另一篇博客Learning Rate黑科技，专门对学习率下降的使用进行讲解，除了使用ReduceLROnPlateau，还有一些其他的方法可以自定义学习率下降的方式</strong>。</p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p>使用mnist手写数字分类作为实战，给小伙伴们演示如何使用回调函数。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">import datetime</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def preprocess(x, y):</span><br><span class="line">    x = tf.cast(x, dtype=tf.float32) / 255.</span><br><span class="line">    x = tf.reshape(x, (28, 28, 1))</span><br><span class="line">    y = tf.one_hot(y, depth=num_class)</span><br><span class="line">    y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">    return x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    num_class = 10</span><br><span class="line">    # 分离数据</span><br><span class="line">    (x, y), (x_test, y_test) = keras.datasets.mnist.load_data()</span><br><span class="line">    time_stamp = datetime.datetime.now().strftime("%Y-%m-%d-%H-%M-%S")</span><br><span class="line">    batch_size = 256</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    max_epoch = 10</span><br><span class="line">    weight_path = '.\\weights\\'</span><br><span class="line">    log_dir = '.\\logs\\' + time_stamp</span><br><span class="line">    csv_path = '.\\csv\\' + time_stamp + '.csv'</span><br><span class="line"></span><br><span class="line">    db = tf.data.Dataset.from_tensor_slices((x, y))</span><br><span class="line">    db = db.map(preprocess).shuffle(10000).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))</span><br><span class="line">    db_test = db_test.map(preprocess).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    # 创建模型</span><br><span class="line">    model = keras.Sequential([keras.layers.Conv2D(6, (3, 3), (1, 1), 'same', name='Conv1'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn1'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu1'),</span><br><span class="line">                              keras.layers.MaxPool2D((2, 2), (2, 2), name='Maxpool1'),</span><br><span class="line">                              keras.layers.Conv2D(16, (3, 3), (1, 1), 'same', name='Conv2'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn2'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu2'),</span><br><span class="line">                              keras.layers.MaxPool2D((2, 2), (2, 2), name='Maxpool2'),</span><br><span class="line">                              keras.layers.Conv2D(120, (3, 3), (1, 1), 'same', name='Conv3'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn3'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu3'),</span><br><span class="line">                              keras.layers.Flatten(name='Flatten'),</span><br><span class="line">                              keras.layers.Dense(84, activation='relu', name='Dense1_1'),</span><br><span class="line">                              keras.layers.Dropout(0.2, name='Dropout'),</span><br><span class="line">                              keras.layers.Dense(10, activation='softmax', name='Dense2_1')], name='Model')</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(None, 28, 28, 1))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    # 模型参数设置</span><br><span class="line">    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=keras.losses.CategoricalCrossentropy(), metrics=['acc'])</span><br><span class="line"></span><br><span class="line">    log_board = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_grads=True)</span><br><span class="line"></span><br><span class="line">    log_csv = keras.callbacks.CSVLogger(filename=csv_path)</span><br><span class="line"></span><br><span class="line">    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=3, verbose=1)</span><br><span class="line"></span><br><span class="line">    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1)</span><br><span class="line"></span><br><span class="line">    check_point = keras.callbacks.ModelCheckpoint(weight_path + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5', monitor='val_loss', verbose=1, save_best_only=True, period=3)</span><br><span class="line"></span><br><span class="line">    # 模型训练</span><br><span class="line">    model.fit(db, epochs=max_epoch, callbacks=[log_board, log_csv, reduce_lr, early_stopping, check_point], validation_data=db_test, initial_epoch=0)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/callbacks1.png" alt="callbacks1"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  <strong>回调函数是我们优化模型的重要工具，能够帮助我们更好的了解设计的模型，可以直观的看出模型需要如何改进，应该增加迭代周期还是应该早停，增加模型参数还是降低模型参数等等</strong>，要注意<strong>保存模型参数或者记录模型结果的时候，可以加入时间戳作为文件名，这样防止多次训练时产生太多文件导致分不清是哪一次训练的数据</strong>，所以小伙伴们一定要小心。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Callbacks&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>Transfer learning(迁移学习)</title>
    <link href="https://USTCcoder.github.io/2020/05/17/deep%20learning%20transfer%20learning/"/>
    <id>https://USTCcoder.github.io/2020/05/17/deep learning transfer learning/</id>
    <published>2020-05-17T09:30:43.000Z</published>
    <updated>2020-05-17T12:36:21.203Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Transfer learning</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Transfer learning(迁移学习)</strong>:<strong>是一种常见的机器学习方法</strong>，概括的说就是<strong>将一个预训练的模型重新用在另一个任务上</strong>。其在深度学习问题上是非常受欢迎的，因为<strong>深度学习很大的一个问题就是数据集不够</strong>，神经网络的参数量少说几百万，多则上亿，因此对于数据量的要求也是十分巨大的，但是实际的问题往往很难找到足够数量的数据集，<strong>除了数据增强方法外，还可以利用迁移学习的思想</strong>。以分类问题为例，前面的卷积层和池化层的目的是特征提取，后面全连接层的目的是进行分类。因此<strong>如果我们有很好的特征提取参数，那么我们就不需要浪费太多数据集在特征提取部分，我们重点训练网络的后半部分即可</strong>。<br><a id="more"></a></p><p><img src="/images/deep_learning/transfer.png" alt="transfer"></p><h1 id="迁移学习实现"><a href="#迁移学习实现" class="headerlink" title="迁移学习实现"></a><font size="5" color="red">迁移学习实现</font></h1><p>要完成迁移学习，首先<strong>两个任务要有一部分相同的网络结构，通常是前半部分的特征提取网络结构</strong>。在这里中使用TensorFlow中自带手写数字数据集mnist和时装数据集fashion_mnist为例，说明迁移学习的实现过程。<br>迁移学习步骤如下：</p><ol><li>设要解决的问题为Q1，找到与要解决的问题类似并且有较多的数据集的问题为Q2，首先设计一个解决问题Q2的网络结构，<strong>使用Q2的数据集，训练好后将模型的权重保存</strong>。</li><li>设计一个解决问题Q1的网络结构，<strong>将相同的层给予相同的名称，加载权重时，调用load_weights()函数，其中参数by_name赋值为True，即根据名称加载对应的权重，名称不相同的层则不加载权重</strong>。</li><li><strong>训练时可以使用较大的学习率，而且让model.layers[i].trainable = False，冻结前面的特征提取网络</strong>，因为相似的问题具有相似的特征提取权重，不会偏差太大，<strong>如果一开始就一起训练，则可能会使特征提取网络的权值产生较大的变化，浪费训练周期和数据集</strong>。</li><li><strong>训练一段时间后，使用较小的学习率，并将前面的特征提取网络解冻model.layers[i].trainable = True</strong>，<strong>为了更好的提取出适合于本问题的特征</strong>，再训练一定的时间，即可完成整个迁移学习过程。</li></ol><h1 id="第一部分，获取相似问题的权重"><a href="#第一部分，获取相似问题的权重" class="headerlink" title="第一部分，获取相似问题的权重"></a><font size="5" color="red">第一部分，获取相似问题的权重</font></h1><p>设计一个解决时装分类问题的网络模型，因为类别较少，因此网络模型较小，主要是为了说明迁移学习的过程。模型的参数保存在fashion_mnist.h5文件中，实际的代码如下。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">try:</span><br><span class="line">    import tensorflow.python.keras as keras</span><br><span class="line">except:</span><br><span class="line">    import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def preprocess(x, y):</span><br><span class="line">    x = tf.cast(x, dtype=tf.float32) / 255.</span><br><span class="line">    x = tf.reshape(x, (28, 28, 1))</span><br><span class="line">    y = tf.one_hot(y, depth=num_class)</span><br><span class="line">    y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">    return x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    num_class = 10</span><br><span class="line">    # 分离数据</span><br><span class="line">    (x, y), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line">    batch_size = 256</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    max_epoch = 10</span><br><span class="line"></span><br><span class="line">    db = tf.data.Dataset.from_tensor_slices((x, y))</span><br><span class="line">    db = db.map(preprocess).shuffle(10000).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))</span><br><span class="line">    db_test = db_test.map(preprocess).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    # 创建模型</span><br><span class="line">    model = keras.Sequential([keras.layers.Conv2D(16, (3, 3), (1, 1), 'same', name='Conv1'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn1'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu1'),</span><br><span class="line">                              keras.layers.MaxPool2D((2, 2), (2, 2), name='Maxpool1'),</span><br><span class="line">                              keras.layers.Conv2D(32, (3, 3), (1, 1), 'same', name='Conv2'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn2'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu2'),</span><br><span class="line">                              keras.layers.MaxPool2D((2, 2), (2, 2), name='Maxpool2'),</span><br><span class="line">                              keras.layers.Conv2D(120, (3, 3), (1, 1), 'same', name='Conv3'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn3'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu3'),</span><br><span class="line">                              keras.layers.Flatten(name='Flatten'),</span><br><span class="line">                              keras.layers.Dense(256, activation='relu', name='Dense1'),</span><br><span class="line">                              keras.layers.Dropout(0.2, name='Dropout'),</span><br><span class="line">                              keras.layers.Dense(10, activation='softmax', name='Dense2')], name='Model')</span><br><span class="line"></span><br><span class="line">    # 模型参数设置</span><br><span class="line">    model.compile(</span><br><span class="line">        optimizer=keras.optimizers.Adam(),</span><br><span class="line">        loss=keras.losses.CategoricalCrossentropy(),</span><br><span class="line">        metrics=['acc']</span><br><span class="line">                  )</span><br><span class="line"></span><br><span class="line">    # 模型训练</span><br><span class="line">    model.fit(db, epochs=max_epoch, validation_data=db_test)</span><br><span class="line"></span><br><span class="line">    model.save_weights('fashion_mnist.h5')</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/transfer1.png" alt="transfer1"></p><h1 id="第二部分，加载第一部分的权重，并且训练本问题"><a href="#第二部分，加载第一部分的权重，并且训练本问题" class="headerlink" title="第二部分，加载第一部分的权重，并且训练本问题"></a><font size="5" color="red">第二部分，加载第一部分的权重，并且训练本问题</font></h1><p>设计一个解决手写数字分类问题的网络模型，手写数字比时装分类问题简单一些，因此特征提取网络不变，<strong>只改变最后两层全连接层的参数</strong>，实际的代码如下。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def preprocess(x, y):</span><br><span class="line">    x = tf.cast(x, dtype=tf.float32) / 255.</span><br><span class="line">    x = tf.reshape(x, (28, 28, 1))</span><br><span class="line">    y = tf.one_hot(y, depth=num_class)</span><br><span class="line">    y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">    return x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    num_class = 10</span><br><span class="line">    # 分离数据</span><br><span class="line">    (x, y), (x_test, y_test) = keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">    batch_size = 256</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    max_epoch = 10</span><br><span class="line">    weight_path = '.\\weights'</span><br><span class="line"></span><br><span class="line">    db = tf.data.Dataset.from_tensor_slices((x, y))</span><br><span class="line">    db = db.map(preprocess).shuffle(10000).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))</span><br><span class="line">    db_test = db_test.map(preprocess).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    # 创建模型</span><br><span class="line">    model = keras.Sequential([keras.layers.Conv2D(16, (3, 3), (1, 1), 'same', name='Conv1'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn1'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu1'),</span><br><span class="line">                              keras.layers.MaxPool2D((2, 2), (2, 2), name='Maxpool1'),</span><br><span class="line">                              keras.layers.Conv2D(32, (3, 3), (1, 1), 'same', name='Conv2'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn2'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu2'),</span><br><span class="line">                              keras.layers.MaxPool2D((2, 2), (2, 2), name='Maxpool2'),</span><br><span class="line">                              keras.layers.Conv2D(120, (3, 3), (1, 1), 'same', name='Conv3'),</span><br><span class="line">                              keras.layers.BatchNormalization(name='Bn3'),</span><br><span class="line">                              keras.layers.ReLU(name='Relu3'),</span><br><span class="line">                              keras.layers.Flatten(name='Flatten'),</span><br><span class="line">                              keras.layers.Dense(128, activation='relu', name='Dense1_1'),</span><br><span class="line">                              keras.layers.Dropout(0.2, name='Dropout'),</span><br><span class="line">                              keras.layers.Dense(10, activation='softmax', name='Dense2_2')], name='Model')</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(None, 28, 28, 1))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    model.load_weights('.\\fashion_mnist.h5', by_name=True)</span><br><span class="line"></span><br><span class="line">    for i in range(12):</span><br><span class="line">        model.layers[i].trainable = False</span><br><span class="line"></span><br><span class="line">    # 模型参数设置</span><br><span class="line">    model.compile(</span><br><span class="line">        optimizer=keras.optimizers.Adam(1e-3),</span><br><span class="line">        loss=keras.losses.CategoricalCrossentropy(),</span><br><span class="line">        metrics=['acc']</span><br><span class="line">                  )</span><br><span class="line"></span><br><span class="line">    # 模型训练</span><br><span class="line">    model.fit(db, epochs=5, validation_data=db_test, initial_epoch=0)</span><br><span class="line"></span><br><span class="line">    for i in range(15):</span><br><span class="line">        model.layers[i].trainable = True</span><br><span class="line"></span><br><span class="line">    # 模型参数设置</span><br><span class="line">    model.compile(</span><br><span class="line">        optimizer=keras.optimizers.Adam(1e-4),</span><br><span class="line">        loss=keras.losses.CategoricalCrossentropy(),</span><br><span class="line">        metrics=['acc']</span><br><span class="line">                  )</span><br><span class="line"></span><br><span class="line">    # 模型训练</span><br><span class="line">    model.fit(db, epochs=max_epoch, validation_data=db_test, initial_epoch=5)</span><br><span class="line"></span><br><span class="line">    model.save_weights('mnist.h5')</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/transfer2.png" alt="transfer2"></p><h1 id="使用迁移学习与不使用迁移学习的对比"><a href="#使用迁移学习与不使用迁移学习的对比" class="headerlink" title="使用迁移学习与不使用迁移学习的对比"></a><font size="5" color="red">使用迁移学习与不使用迁移学习的对比</font></h1><p><img src="/images/deep_learning/transfer3.png" alt="transfer3"></p><ol><li>从上图可以明显的看出，使用了迁移学习之后，<strong>模型训练的速度大大加快了</strong>，因为利用相似问题的特征提取网络权值，只训练最后的分类网络，因此模型的收敛更加迅速。</li><li>因为mnist数据量并不是很少，而且分类任务比较简单，因此在很长时代的训练下，迁移学习的优势并不是很明显，在<strong>其他的复杂问题上，尤其是数据量较少的情况，效果非常明显</strong>。</li></ol><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  迁移学习<strong>不是一种算法，更多的是一种思想</strong>，一种将现有的信息借鉴过来的迁移思想，使得我们的<strong>少量数据集可以发挥更大的效果</strong>。因此在工程实际问题中常常使用，小伙伴们必须要掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Transfer learning&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>每个元音包含偶数次的最长子字符串(Leetcode 1371)</title>
    <link href="https://USTCcoder.github.io/2020/05/16/program%20Leetcode1371/"/>
    <id>https://USTCcoder.github.io/2020/05/16/program Leetcode1371/</id>
    <published>2020-05-16T07:00:19.000Z</published>
    <updated>2020-09-02T02:16:59.167Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode1371.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这个题目第一眼想到的方法是暴力求解，从第i个字符开始到第j个字符，使用两层循环，依次遍历，计算是否满足偶数个元音字母，算法复杂度也很好分析$O(n^2)$，但是求解的过程中浪费了大量的运算资源，如从第一个字符到最后一个字符，对每一个字符都判断了是否为元音字符，从第二个字符到最后一个字符，对每一个字符又判断了依次，因此大大增加了时间复杂度，如果直接使用暴力法，题目是很难通过的。</p><a id="more"></a><h1 id="暴力法"><a href="#暴力法" class="headerlink" title="暴力法"></a><font size="5" color="red">暴力法</font></h1><p>暴力法想通过这道题目也是有办法的，我们有一个先验知识，要求最长的字符串，那为何不能<strong>按照字符串长度来遍历</strong>呢？假如字符串长度为10，那么第一次遍历所有长度为10的字符串，第二次遍历所有长度为9的字符串，如果有满足条件的那一定是最优解，这样可以大大节约计算量，图解图下。<br><img src="/images/ALGORITHM/leetcode1371_brute.png" alt="brute"><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def findTheLongestSubstring(self, s):</span><br><span class="line">        """</span><br><span class="line">        :type s: str</span><br><span class="line">        :rtype: int</span><br><span class="line">        """</span><br><span class="line">        lens = len(s)</span><br><span class="line">        # i 代表迭代字符串长度为i的子串</span><br><span class="line">        for i in range(lens, -1, -1):</span><br><span class="line">            if i == 0:</span><br><span class="line">                return 0</span><br><span class="line">            # j 代表从第j个位置开始</span><br><span class="line">            for j in range(lens - i + 1):</span><br><span class="line">                tmp_s = s[j:j + i]</span><br><span class="line">                for k in ['a', 'e', 'i', 'o', 'u']:</span><br><span class="line">                    # 如果某个元音字符不是偶数个，则搜索下一个字符串</span><br><span class="line">                    if tmp_s.count(k) % 2 != 0:</span><br><span class="line">                        break</span><br><span class="line">                    # 如果u是偶数个，说明所有元音字符串都已经满足偶数，则i一定是最长的长度</span><br><span class="line">                    if k == 'u':</span><br><span class="line">                        return i</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="前缀和-状态压缩"><a href="#前缀和-状态压缩" class="headerlink" title="前缀和+状态压缩"></a><font size="5" color="red">前缀和+状态压缩</font></h1><p>这个思路参考官方题解，我们思考一个问题，<strong>如果中间某个字串满足条件，那么从第一个字符到该字串的前一个字符与从第一个字符到该字串的最后一个字符的状态是相同的</strong>，因此我们只需要维护一个前缀和，即可解决此问题，如下图所示。<br><img src="/images/ALGORITHM/leetcode1371_solve1.png" alt="solve"><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def findTheLongestSubstring(self, s):</span><br><span class="line">        """</span><br><span class="line">        :type s: str</span><br><span class="line">        :rtype: int</span><br><span class="line">        """</span><br><span class="line">        ans, status, n = 0, 0, len(s)</span><br><span class="line">        chars = ['a', 'e', 'i', 'o', 'u']</span><br><span class="line">        pos = [-1] * (1 &lt;&lt; 5)</span><br><span class="line">        pos[0] = 0</span><br><span class="line"></span><br><span class="line">        for i in range(n):</span><br><span class="line">            if s[i] in chars:</span><br><span class="line">                status ^= 1 &lt;&lt; chars.index(s[i])</span><br><span class="line">            if pos[status] != -1:</span><br><span class="line">                ans = max(ans, i + 1 - pos[status])</span><br><span class="line">            else:</span><br><span class="line">                pos[status] = i + 1</span><br><span class="line">        return ans</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="题目延申"><a href="#题目延申" class="headerlink" title="题目延申"></a><font size="5" color="red">题目延申</font></h1><p>思考：如果本题改为<strong>奇数次最长的子串</strong>该如何去解？<br><strong>思路还是相同的，只不过在寻找状态时要寻找完全相反的状态，如果从初始位置到某个位置的状态为(10101)，那么就去寻找(01010)这个状态最早出现的位置即可</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def findTheLongestSubstring(self, s):</span><br><span class="line">        """</span><br><span class="line">        :type s: str</span><br><span class="line">        :rtype: int</span><br><span class="line">        """</span><br><span class="line">        ans, status, n = 0, 0, len(s)</span><br><span class="line">        chars = ['a', 'e', 'i', 'o', 'u']</span><br><span class="line">        pos = [-1] * (1 &lt;&lt; 5)</span><br><span class="line">        pos[0] = 0</span><br><span class="line"></span><br><span class="line">        for i in range(n):</span><br><span class="line">            if s[i] in chars:</span><br><span class="line">                status ^= 1 &lt;&lt; chars.index(s[i])</span><br><span class="line">            if pos[31 ^ status] != -1:</span><br><span class="line">                ans = max(ans, i + 1 - pos[31 ^ status])</span><br><span class="line">            else:</span><br><span class="line">                pos[status] = i + 1</span><br><span class="line">        return ans</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  状态压缩是一种常用的技巧，尤其在状态较多的时候，如果使用字典存储状态，则需要较大的内存，如果使用二进制位的0和1来保存状态，则只需要bit量级的内存，但是前提是每个状态的定义只有两种，比如此题的奇数和偶数，如果状态数较多，一般不使用这种方法。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 1371&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>Data Augmentation(数据增强)</title>
    <link href="https://USTCcoder.github.io/2020/05/15/deep%20learning%20data_augmentation/"/>
    <id>https://USTCcoder.github.io/2020/05/15/deep learning data_augmentation/</id>
    <published>2020-05-15T13:23:26.000Z</published>
    <updated>2020-05-15T16:25:29.183Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Data Augmentation</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Data Augmentation(数据增强)</strong>:在深度学习工程实践中，必不可少的是数据集，但是如果<strong>自己采集数据集，非常的耗时，而且数量往往不够</strong>。这时需要一定的<strong>数据增强</strong>操作，来扩充自己的数据集<strong>使网络更加鲁棒</strong>。今天给小伙伴们盘点常用的数据增强操作。<br><a id="more"></a></p><p><img src="/images/deep_learning/data.png" alt="data"></p><h1 id="OpenCV和Numpy存储数据差异"><a href="#OpenCV和Numpy存储数据差异" class="headerlink" title="OpenCV和Numpy存储数据差异"></a><font size="5" color="red">OpenCV和Numpy存储数据差异</font></h1><p><strong>OpenCV</strong>：在OpenCV中，图像的存储是<strong>列在前，行在后</strong>，和我们的直观理解不同，因此使用起来，尤其在图像坐标索引时，需要特别注意，有关OpenCV常用操作，可以参考我的另一篇博客OpenCV常用库。<br><strong>Numpy</strong>：在Numpy中，图像的存储是<strong>行在前，列在后</strong>，这符合我们的理解，因为在学习二维数组时，就是按照行在前，列在后的思想，有关Numpy常用操作，可以参考我的另一篇博客Numpy常用库。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img = cv.imread('origin.png')</span><br><span class="line">img1 = cv.resize(img, (300, 400))</span><br></pre></td></tr></tbody></table></figure><p></p><p>img1的图像Numpy数组的shape为(400, 300, 3)，第一次使用时，奇怪的知识又增加了，一定要记得数据的转换。而且<strong>OpenCV读取和显示的图像默认是BGR类型</strong>的，这也容易产生错误。<br><img src="/images/deep_learning/compare.png" alt="compare"></p><h1 id="Flip-翻转"><a href="#Flip-翻转" class="headerlink" title="Flip(翻转)"></a><font size="5" color="red">Flip(翻转)</font></h1><p><strong>Flip(翻转)</strong>：对图像进行<strong>水平翻转，垂直翻转或者水平垂直同时翻转</strong>。翻转时图像的<strong>高宽不会发生变化</strong>，但是图像的<strong>坐标会发生变化</strong>。因此在目标检测等问题上，<strong>bounding-box需要调整相应的坐标</strong>。<br><img src="/images/deep_learning/flip0.png" alt="flip"><br><img src="/images/deep_learning/flip1.png" alt="flip"><br><img src="/images/deep_learning/flip-1.png" alt="flip"><br>在cv2中已经给我们提供了图像翻转的函数flip。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def imshow(img, boxes):</span><br><span class="line">    cv.namedWindow('result')</span><br><span class="line">    nums = len(img)</span><br><span class="line">    for i in range(nums):</span><br><span class="line">        for box in boxes[i]:</span><br><span class="line">            cv.rectangle(img[i], tuple(box[:2]), tuple(box[2:4]), color[box[-1] - 1], 2)</span><br><span class="line">    if nums == 4:</span><br><span class="line">        result = cv.vconcat([cv.hconcat(img[:2]), cv.hconcat(img[2:])])</span><br><span class="line">    else:</span><br><span class="line">        result = cv.hconcat(img)</span><br><span class="line">    cv.imshow('result', result)</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img = cv.imread('origin.png')</span><br><span class="line">boxes = [[24, 18, 220, 260, 1], [196, 16, 330, 244, 2]]</span><br><span class="line">color = [[0, 255, 0], [0, 0, 255]]</span><br><span class="line">output_size = (260, 360)</span><br><span class="line">input_size = tuple(img.shape[:2])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 垂直翻转</span><br><span class="line">flip0 = cv.flip(img, 0)</span><br><span class="line"># 水平翻转</span><br><span class="line">flip1 = cv.flip(img, 1)</span><br><span class="line"># 垂直水平同时翻转</span><br><span class="line">flip2 = cv.flip(img, -1)</span><br><span class="line">boxes0 = [[img.shape[0] - box[4 - i] if i % 2 else box[i] for i in range(4)] + [box[-1]] for box in boxes]</span><br><span class="line">boxes1 = [[box[i] if i % 2 else img.shape[1] - box[2 - i] for i in range(4)] + [box[-1]] for box in boxes]</span><br><span class="line">boxes2 = [[img.shape[0] - box[4 - i] if i % 2 else img.shape[1] - box[2 - i] for i in range(4)] + [box[-1]] for box in boxes]</span><br><span class="line">imshow([img, flip0, flip1, flip2], [boxes, boxes0, boxes1, boxes2])</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/flip.png" alt="flip"></p><h1 id="Rotate-旋转"><a href="#Rotate-旋转" class="headerlink" title="Rotate(旋转)"></a><font size="5" color="red">Rotate(旋转)</font></h1><p><strong>Rotate(旋转)</strong>：对图像进行旋转，和翻转不同，<strong>不但图像的坐标会发生变化，而且图像的高宽可能会发生变化</strong>。因此在目标检测等问题上，<strong>bounding-box需要调整相应的坐标</strong>。<br><img src="/images/deep_learning/rotate0.png" alt="rotate"><br><img src="/images/deep_learning/rotate1.png" alt="rotate"><br><img src="/images/deep_learning/rotate2.png" alt="rotate"><br>在cv2中已经给我们提供了图像旋转的函数rotate，因为旋转可能会改变图像的高宽，为了展示方便，我先将它们的高宽调成相等。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def imshow(img, boxes):</span><br><span class="line">    cv.namedWindow('result')</span><br><span class="line">    nums = len(img)</span><br><span class="line">    for i in range(nums):</span><br><span class="line">        for box in boxes[i]:</span><br><span class="line">            cv.rectangle(img[i], tuple(box[:2]), tuple(box[2:4]), color[box[-1] - 1], 2)</span><br><span class="line">    if nums == 4:</span><br><span class="line">        result = cv.vconcat([cv.hconcat(img[:2]), cv.hconcat(img[2:])])</span><br><span class="line">    else:</span><br><span class="line">        result = cv.hconcat(img)</span><br><span class="line">    cv.imshow('result', result)</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img = cv.imread('origin.png')</span><br><span class="line">boxes = [[24, 18, 220, 260, 1], [196, 16, 330, 244, 2]]</span><br><span class="line">color = [[0, 255, 0], [0, 0, 255]]</span><br><span class="line">output_size = (260, 360)</span><br><span class="line">input_size = tuple(img.shape[:2])</span><br><span class="line"></span><br><span class="line">new_size = (300, 300)</span><br><span class="line">resize_img = cv.resize(img, new_size)</span><br><span class="line">boxes = [[int(box[i] * resize_img.shape[0] / img.shape[0]) if i % 2 else int(box[i] * resize_img.shape[1] / img.shape[1]) for i in range(4)] + [box[-1]] for box in boxes]</span><br><span class="line"></span><br><span class="line">rotate0 = cv.rotate(resize_img, 0)</span><br><span class="line">boxes0 = [[resize_img.shape[0] - box[3], box[0], resize_img.shape[0] - box[1], box[2]] + [box[-1]] for box in boxes]</span><br><span class="line"></span><br><span class="line">rotate1 = cv.rotate(resize_img, 1)</span><br><span class="line">boxes1 = [[resize_img.shape[1] - box[2], resize_img.shape[0] - box[3], resize_img.shape[1] - box[0], resize_img.shape[0] - box[1]] + [box[-1]] for box in boxes]</span><br><span class="line"></span><br><span class="line">rotate2 = cv.rotate(resize_img, 2)</span><br><span class="line">boxes2 = [[box[1], resize_img.shape[1] - box[2], box[3], resize_img.shape[1] - box[0]] + [box[-1]] for box in boxes]</span><br><span class="line">imshow([resize_img, rotate0, rotate1, rotate2], [boxes, boxes0, boxes1, boxes2])</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/rotate.png" alt="rotate"></p><h1 id="Resize-调整大小"><a href="#Resize-调整大小" class="headerlink" title="Resize(调整大小)"></a><font size="5" color="red">Resize(调整大小)</font></h1><p><strong>Resize(调整大小)</strong>：对图像进行<strong>大小调整</strong>，这个操作是最复杂的，因为<strong>神经网络的输入是固定尺寸的图像</strong>，所以在<strong>大小调整后还要以相同尺寸输出</strong>，所以还需要加上<strong>灰框</strong>或者<strong>裁剪</strong>，而且<strong>还要考虑目标是否会被裁剪</strong>，因此在目标检测等问题上，<strong>bounding-box需要调整相应的坐标，而且需要判断bounding-box是否有效存在</strong>，在这里我设置了一个阈值，如果没有被裁剪掉的面积占总面积的0.25倍以上，则认为bounding-box是有效的，否则删除这个bounding-box。<br><img src="/images/deep_learning/resize0.png" alt="resize"><br>在cv2中已经给我们提供了图像调整大小的函数resize，为了展示方便，我将输出尺寸设置和输入相同。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def imshow(img, boxes):</span><br><span class="line">    cv.namedWindow('result')</span><br><span class="line">    nums = len(img)</span><br><span class="line">    for i in range(nums):</span><br><span class="line">        for box in boxes[i]:</span><br><span class="line">            cv.rectangle(img[i], tuple(box[:2]), tuple(box[2:4]), color[box[-1] - 1], 2)</span><br><span class="line">    if nums == 4:</span><br><span class="line">        result = cv.vconcat([cv.hconcat(img[:2]), cv.hconcat(img[2:])])</span><br><span class="line">    else:</span><br><span class="line">        result = cv.hconcat(img)</span><br><span class="line">    cv.imshow('result', result)</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img = cv.imread('origin.png')</span><br><span class="line">boxes = [[24, 18, 220, 260, 1], [196, 16, 330, 244, 2]]</span><br><span class="line">color = [[0, 255, 0], [0, 0, 255]]</span><br><span class="line">output_size = (260, 360)</span><br><span class="line">input_size = tuple(img.shape[:2])</span><br><span class="line"></span><br><span class="line">ratio_h, ratio_w = np.where(np.random.rand(2) &lt; 0.5,  np.random.uniform(0.5, 1, 2), np.random.uniform(1, 2, 2))</span><br><span class="line">new_h, new_w = int(input_size[0] * ratio_h), int(input_size[1] * ratio_w)</span><br><span class="line">scale_img = cv.resize(img, (new_w, new_h))</span><br><span class="line">valid_h = 0 if new_h &lt;= output_size[0] else np.random.randint(0, new_h - output_size[0])</span><br><span class="line">valid_w = 0 if new_w &lt;= output_size[1] else np.random.randint(0, new_w - output_size[1])</span><br><span class="line">crop_img = scale_img[valid_h:min(valid_h + output_size[0], new_h), valid_w:min(valid_w + output_size[1], new_w)]</span><br><span class="line">resize_img0 = np.ones((output_size[0], output_size[1], 3), np.uint8) * 127</span><br><span class="line">dy, dx = int(np.random.rand() * (output_size[0] - crop_img.shape[0])), int(np.random.rand() * (output_size[1] - crop_img.shape[1]))</span><br><span class="line">resize_img0[dy:dy + crop_img.shape[0], dx:dx + crop_img.shape[1]] = crop_img</span><br><span class="line">boxes0 = [[int(box[i] * ratio_h) + dy - valid_h if i % 2 else int(box[i] * ratio_w) + dx - valid_w for i in range(4)] + [box[-1]] for box in boxes]</span><br><span class="line">boxes1 = []</span><br><span class="line">for box in boxes0:</span><br><span class="line">    new_box = [min(output_size[3 - i], box[i]) if i &gt;= 2 else max(0, box[i]) for i in range(4)] + [box[-1]]</span><br><span class="line">    area = (box[2] - box[0]) * (box[3] - box[1])</span><br><span class="line">    new_area = (new_box[2] - new_box[0]) * (new_box[3] - new_box[1])</span><br><span class="line">    if new_area &gt; 0.25 * area:</span><br><span class="line">        boxes1.append(new_box)</span><br><span class="line">imshow([img, resize_img0], [boxes, boxes1])</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/resize.png" alt="resize"></p><h1 id="Noise-噪声"><a href="#Noise-噪声" class="headerlink" title="Noise(噪声)"></a><font size="5" color="red">Noise(噪声)</font></h1><p><strong>Noise(噪声)</strong>：对图像添加噪声，这个操作是最简单的，因为<strong>添加噪声，尺寸不会改变，因此bounding-box不需要修改</strong>，但是要注意，如果加入噪声，<strong>数据变为负数后，直接转化为uint8类型的数据会出错</strong>，所以<strong>需要先clip，将数据限制在[0, 255]之间</strong>。<br>在numpy中直接使用random模块，可以产生随机数，根据需要设置相应的噪声类型即可，一般高斯噪声较为常用。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def imshow(img, boxes):</span><br><span class="line">    cv.namedWindow('result')</span><br><span class="line">    nums = len(img)</span><br><span class="line">    for i in range(nums):</span><br><span class="line">        for box in boxes[i]:</span><br><span class="line">            cv.rectangle(img[i], tuple(box[:2]), tuple(box[2:4]), color[box[-1] - 1], 2)</span><br><span class="line">    if nums == 4:</span><br><span class="line">        result = cv.vconcat([cv.hconcat(img[:2]), cv.hconcat(img[2:])])</span><br><span class="line">    else:</span><br><span class="line">        result = cv.hconcat(img)</span><br><span class="line">    cv.imshow('result', result)</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img = cv.imread('origin.png')</span><br><span class="line">boxes = [[24, 18, 220, 260, 1], [196, 16, 330, 244, 2]]</span><br><span class="line">color = [[0, 255, 0], [0, 0, 255]]</span><br><span class="line">output_size = (260, 360)</span><br><span class="line">input_size = tuple(img.shape[:2])</span><br><span class="line"></span><br><span class="line">noise_img = np.clip(img + np.random.normal(0, 30, (img.shape[0], img.shape[1], 3)), 0, 255).astype(np.uint8)</span><br><span class="line">imshow([img, noise_img], [boxes, boxes])</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/noise.png" alt="noise"></p><h1 id="HSV-色调，饱和度，明度"><a href="#HSV-色调，饱和度，明度" class="headerlink" title="HSV(色调，饱和度，明度)"></a><font size="5" color="red">HSV(色调，饱和度，明度)</font></h1><p><strong>HSV(色调，饱和度，明度)</strong>：HSV彩色空间不同于我们熟知的RGB彩色空间，其使用<strong>色调，饱和度和明度代替红绿蓝三个通道</strong>。在OpenCV中，<strong>色调的范围是[0, 179]，饱和度和明度的范围都是[0, 255]</strong>，因为<strong>改变HSV的数值，尺寸不会改变，因此bounding-box不需要修改</strong><br>在cv2中直接使用cvtColor进行色彩空间转化，然后给HSV通道引入随机数即可达到改变色调，饱和度和明度的效果。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def imshow(img, boxes):</span><br><span class="line">    cv.namedWindow('result')</span><br><span class="line">    nums = len(img)</span><br><span class="line">    for i in range(nums):</span><br><span class="line">        for box in boxes[i]:</span><br><span class="line">            cv.rectangle(img[i], tuple(box[:2]), tuple(box[2:4]), color[box[-1] - 1], 2)</span><br><span class="line">    if nums == 4:</span><br><span class="line">        result = cv.vconcat([cv.hconcat(img[:2]), cv.hconcat(img[2:])])</span><br><span class="line">    else:</span><br><span class="line">        result = cv.hconcat(img)</span><br><span class="line">    cv.imshow('result', result)</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img = cv.imread('origin.png')</span><br><span class="line">boxes = [[24, 18, 220, 260, 1], [196, 16, 330, 244, 2]]</span><br><span class="line">color = [[0, 255, 0], [0, 0, 255]]</span><br><span class="line">output_size = (260, 360)</span><br><span class="line">input_size = tuple(img.shape[:2])</span><br><span class="line"></span><br><span class="line">hsv_img = cv.cvtColor(img, cv.COLOR_BGR2HSV)</span><br><span class="line">hsv_img = hsv_img * np.random.uniform(0.5, 2, 3)</span><br><span class="line">hsv_img[:, :, 0] = np.clip(hsv_img[:, :, 0], 0, 179)</span><br><span class="line">hsv_img[:, :, 1] = np.clip(hsv_img[:, :, 1], 0, 255)</span><br><span class="line">hsv_img[:, :, 2] = np.clip(hsv_img[:, :, 2], 0, 255)</span><br><span class="line">hsv_img = hsv_img.astype(np.uint8)</span><br><span class="line">bgr_img = cv.cvtColor(hsv_img, cv.COLOR_HSV2BGR)</span><br><span class="line">imshow([img, bgr_img], [boxes, boxes])</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/hsv.png" alt="hsv"></p><h1 id="整体代码"><a href="#整体代码" class="headerlink" title="整体代码"></a><font size="5" color="red">整体代码</font></h1><p>将上面五种操作结合起来，写在一个函数中，可以实现图像和bounding-box输入，增强后的图像和bounding-box输出。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def imshow(img, boxes):</span><br><span class="line">    cv.namedWindow('result')</span><br><span class="line">    nums = len(img)</span><br><span class="line">    for i in range(nums):</span><br><span class="line">        for box in boxes[i]:</span><br><span class="line">            cv.rectangle(img[i], tuple(box[:2]), tuple(box[2:4]), color[box[-1] - 1], 2)</span><br><span class="line">    if nums == 4:</span><br><span class="line">        result = cv.vconcat([cv.hconcat(img[:2]), cv.hconcat(img[2:])])</span><br><span class="line">    else:</span><br><span class="line">        result = cv.hconcat(img)</span><br><span class="line">    cv.imshow('result', result)</span><br><span class="line">    cv.waitKey(0)</span><br><span class="line">    cv.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def data_augmentation(image, bbox, output_shape, resize_scale=2, threshold=0.25, noise_scale=10, h_scale=2, s_scale=2, v_scale=2):</span><br><span class="line"></span><br><span class="line">    # 增加噪声</span><br><span class="line">    noise_img = np.clip(image + (np.random.normal(0, noise_scale, img.shape)), 0, 255).astype(np.uint8)</span><br><span class="line"></span><br><span class="line">    # 改变色调，饱和度，明度</span><br><span class="line">    hsv_img = cv.cvtColor(noise_img, cv.COLOR_BGR2HSV)</span><br><span class="line"></span><br><span class="line">    h_ratio = np.random.uniform(1 / h_scale, 1, 1) if np.random.rand() &lt; 0.5 else np.random.uniform(1, h_scale, 1)</span><br><span class="line">    s_ratio = np.random.uniform(1 / s_scale, 1, 1) if np.random.rand() &lt; 0.5 else np.random.uniform(1, s_scale, 1)</span><br><span class="line">    v_ratio = np.random.uniform(1 / v_scale, 1, 1) if np.random.rand() &lt; 0.5 else np.random.uniform(1, v_scale, 1)</span><br><span class="line"></span><br><span class="line">    hsv_img[:, :, 0] = np.clip(hsv_img[:, :, 0] * h_ratio, 0, 179)</span><br><span class="line">    hsv_img[:, :, 1] = np.clip(hsv_img[:, :, 1] * s_ratio, 0, 255)</span><br><span class="line">    hsv_img[:, :, 2] = np.clip(hsv_img[:, :, 2] * v_ratio, 0, 255)</span><br><span class="line"></span><br><span class="line">    hsv_img = hsv_img.astype(np.uint8)</span><br><span class="line"></span><br><span class="line">    bgr_img = cv.cvtColor(hsv_img, cv.COLOR_HSV2BGR)</span><br><span class="line"></span><br><span class="line">    # 旋转</span><br><span class="line">    rotate_flag = np.random.rand()</span><br><span class="line">    if rotate_flag &lt; 1 / 4:</span><br><span class="line">        rotate_img = cv.rotate(bgr_img, 0)</span><br><span class="line">        rotate_boxes = [[bgr_img.shape[0] - box[3], box[0], bgr_img.shape[0] - box[1], box[2]] + [box[-1]] for box in bbox]</span><br><span class="line">    elif rotate_flag &lt; 2 / 4:</span><br><span class="line">        rotate_img = cv.rotate(bgr_img, 1)</span><br><span class="line">        rotate_boxes = [[bgr_img.shape[1] - box[2], bgr_img.shape[0] - box[3], bgr_img.shape[1] - box[0], bgr_img.shape[0] - box[1]] + [box[-1]] for box in bbox]</span><br><span class="line">    elif rotate_flag &lt; 3 / 4:</span><br><span class="line">        rotate_img = cv.rotate(bgr_img, 2)</span><br><span class="line">        rotate_boxes = [[box[1], bgr_img.shape[1] - box[2], box[3], bgr_img.shape[1] - box[0]] + [box[-1]] for box in bbox]</span><br><span class="line">    else:</span><br><span class="line">        rotate_img = bgr_img</span><br><span class="line">        rotate_boxes = bbox</span><br><span class="line"></span><br><span class="line">    # 翻转操作</span><br><span class="line">    flip_flag = np.random.rand()</span><br><span class="line">    if flip_flag &lt; 1 / 4:</span><br><span class="line">        flip_img = cv.flip(rotate_img, 0)</span><br><span class="line">        flip_boxes = [[rotate_img.shape[0] - box[4 - i] if i % 2 else box[i] for i in range(4)] + [box[-1]] for box in rotate_boxes]</span><br><span class="line">    elif flip_flag &lt; 2 / 4:</span><br><span class="line">        flip_img = cv.flip(rotate_img, 1)</span><br><span class="line">        flip_boxes = [[box[i] if i % 2 else rotate_img.shape[1] - box[2 - i] for i in range(4)] + [box[-1]] for box in rotate_boxes]</span><br><span class="line">    elif flip_flag &lt; 3 / 4:</span><br><span class="line">        flip_img = cv.flip(rotate_img, -1)</span><br><span class="line">        flip_boxes = [[rotate_img.shape[0] - box[4 - i] if i % 2 else rotate_img.shape[1] - box[2 - i] for i in range(4)] + [box[-1]] for box in rotate_boxes]</span><br><span class="line">    else:</span><br><span class="line">        flip_img = rotate_img</span><br><span class="line">        flip_boxes = rotate_boxes</span><br><span class="line"></span><br><span class="line">    # 调整大小，并加灰框</span><br><span class="line">    ratio_h, ratio_w = np.where(np.random.rand(2) &lt; 0.5,  np.random.uniform(1 / resize_scale, 1, 2), np.random.uniform(1, resize_scale, 2))</span><br><span class="line">    new_h, new_w = int(flip_img.shape[0] * ratio_h), int(flip_img.shape[1] * ratio_w)</span><br><span class="line">    scale_img = cv.resize(flip_img, (new_w, new_h))</span><br><span class="line">    valid_h = 0 if new_h &lt;= output_shape[0] else np.random.randint(0, new_h - output_shape[0])</span><br><span class="line">    valid_w = 0 if new_w &lt;= output_shape[1] else np.random.randint(0, new_w - output_shape[1])</span><br><span class="line">    crop_img = scale_img[valid_h:min(valid_h + output_shape[0], new_h), valid_w:min(valid_w + output_shape[1], new_w)]</span><br><span class="line">    resize_img = np.ones((output_shape[0], output_shape[1], 3), np.uint8) * 127</span><br><span class="line">    dy, dx = int(np.random.rand() * (output_shape[0] - crop_img.shape[0])), int(np.random.rand() * (output_shape[1] - crop_img.shape[1]))</span><br><span class="line">    resize_img[dy:dy + crop_img.shape[0], dx:dx + crop_img.shape[1]] = crop_img</span><br><span class="line">    boxes = [[int(box[i] * ratio_h) + dy - valid_h if i % 2 else int(box[i] * ratio_w) + dx - valid_w for i in range(4)] + [box[-1]] for box in flip_boxes]</span><br><span class="line">    resize_boxes = []</span><br><span class="line">    for box in boxes:</span><br><span class="line">        new_box = [min(output_shape[3 - i], box[i]) if i &gt;= 2 else max(0, box[i]) for i in range(4)] + [box[-1]]</span><br><span class="line">        area = (box[2] - box[0]) * (box[3] - box[1])</span><br><span class="line">        new_area = (new_box[2] - new_box[0]) * (new_box[3] - new_box[1])</span><br><span class="line">        if new_area &gt; threshold * area:</span><br><span class="line">            resize_boxes.append(new_box)</span><br><span class="line"></span><br><span class="line">    return resize_img, resize_boxes</span><br><span class="line"></span><br><span class="line">img = cv.imread('origin.png')</span><br><span class="line">boxes = [[24, 18, 220, 260, 1], [196, 16, 330, 244, 2]]</span><br><span class="line">color = [[0, 255, 0], [0, 0, 255]]</span><br><span class="line">output_size = (300, 300)</span><br><span class="line">input_size = tuple(img.shape[:2])</span><br><span class="line"></span><br><span class="line">img1, bbox1 = data_augmentation(img, boxes, output_size)</span><br><span class="line">img2, bbox2 = data_augmentation(img, boxes, output_size)</span><br><span class="line">img3, bbox3 = data_augmentation(img, boxes, output_size)</span><br><span class="line">img4, bbox4 = data_augmentation(img, boxes, output_size)</span><br><span class="line">imshow([img1, img2, img3, img4], [bbox1, bbox2, bbox3, bbox4])</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/result.png" alt="result"></p><h1 id="其他数据增强操作"><a href="#其他数据增强操作" class="headerlink" title="其他数据增强操作"></a><font size="5" color="red">其他数据增强操作</font></h1><p>上面说的数据增强操作，主要是<strong>针对于有bounding-box的目标检测任务，所以数据增强会受到一定的限制</strong>。如果我们<strong>面对的问题仅仅是一个图像分类问题，那么我们就会有更多的图像增强操作</strong>，如<strong>设置图像的旋转角度，或者进行仿射操作</strong>，但是因为bounding-box的存在，<strong>如果进行任意角度的旋转或者仿射操作，bounding-box就不再是一个与坐标轴平行的矩形框，而且图像的尺寸变化也会变得非常复杂</strong>，因此通过上面5中数据增强操作已经可以满足绝大部分的需要，所以不探讨如何使用其他方法，有感兴趣的小伙伴们可以去寻找一些自己喜欢的数据增强方式。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  <strong>数据量是深度学习模型性能的重要决定因素</strong>，数据量很少，可能很好的算法也很难达到较好的效果。因此数据增强操作就变得异常重要，如何进行数据增强是小伙伴们必须要掌握的技术。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Data Augmentation&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>新21点(Leetcode 837)</title>
    <link href="https://USTCcoder.github.io/2020/05/15/program%20Leetcode837/"/>
    <id>https://USTCcoder.github.io/2020/05/15/program Leetcode837/</id>
    <published>2020-05-15T06:03:13.000Z</published>
    <updated>2020-09-02T02:18:41.203Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode837.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这个题目看上去非常复杂，题目意思是每次获得牌的点数为1-W之间，大于等于K则不再要牌，否则继续要牌，如果手中牌的点数之和小于等于N则赢，求赢的概率。</p><a id="more"></a><h1 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a><font size="5" color="red">DFS</font></h1><p>最直观的想法，深度搜索，第一张牌有w种抽法，第二张牌有w种抽法，如果最后值大于N，则不加入获胜总概率，如果最后值大于等于K小于N则加入获胜总概率。时间复杂度较高。但是利用python常用库functools中的lru_cache，可以大大节约计算量，会保存计算过的值，如dfs(20, 0.001)已经计算过了，则会建立一个字典，保存这个值，再次调用时会直接获取字典中的值，避免了重复计算。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from functools import lru_cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Solution(object):</span><br><span class="line">    def new21Game(self, N, K, W):</span><br><span class="line">        """</span><br><span class="line">        :type N: int</span><br><span class="line">        :type K: int</span><br><span class="line">        :type W: int</span><br><span class="line">        :rtype: float</span><br><span class="line">        """</span><br><span class="line">        @lru_cache(None)</span><br><span class="line">        def dfs(current_val, current_pro):</span><br><span class="line">            if current_val &gt;= K:</span><br><span class="line">                return 0 if current_val &gt; N else current_pro</span><br><span class="line">            return sum(map(lambda v: dfs(current_val + v, current_pro / W), range(1, W + 1)))</span><br><span class="line"></span><br><span class="line">        return dfs(0, 1)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="BFS"><a href="#BFS" class="headerlink" title="BFS"></a><font size="5" color="red">BFS</font></h1><p>可以使用DFS的题目一般都可以使用BFS来求解，思路也比较清晰，抽取第一张牌有W中可能，将这些可能的结果都保存在双端队列中，然后抽取第二张牌，将第二张牌所有可能的结果都保存到双端队列中，如果最后值大于N，则不加入获胜总概率，如果最后值大于等于K小于N则加入获胜总概率。时间复杂度较高。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from collections import deque</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Solution(object):</span><br><span class="line">    def new21Game(self, N, K, W):</span><br><span class="line">        """</span><br><span class="line">        :type N: int</span><br><span class="line">        :type K: int</span><br><span class="line">        :type W: int</span><br><span class="line">        :rtype: float</span><br><span class="line">        """</span><br><span class="line">        queue = deque([[0, 1]])</span><br><span class="line">        res = 0</span><br><span class="line">        while queue:</span><br><span class="line">            current_val, current_pro = queue.popleft()</span><br><span class="line">            for i in range(1, W + 1):</span><br><span class="line">                if current_val &gt; N:</span><br><span class="line">                    break</span><br><span class="line">                elif current_val &gt;= K:</span><br><span class="line">                    res += current_pro</span><br><span class="line">                    break</span><br><span class="line">                else:</span><br><span class="line">                    queue.append([current_val + i, current_pro / W])</span><br><span class="line">        return res</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="DP"><a href="#DP" class="headerlink" title="DP"></a><font size="5" color="red">DP</font></h1><p>使用上面两者暴力搜索方法当然不是最优的解法，因为其中包含了大量的重复计算，如果我们能够保存当前计算的状态，则可以大大降低时间复杂度。因此想到了DP动态规划。在这里我直接使用官方题解中的图片来阐述。<br><img src="/images/ALGORITHM/leetcode837_dp.png" alt="dp"><br>使用动态规划算法的时间复杂度为$O(min(N,K+W))$，空间复杂度为$O(K+W)$。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def new21Game(self, N, K, W):</span><br><span class="line">        """</span><br><span class="line">        :type N: int</span><br><span class="line">        :type K: int</span><br><span class="line">        :type W: int</span><br><span class="line">        :rtype: float</span><br><span class="line">        """</span><br><span class="line">        if K == 0:</span><br><span class="line">            return 1.0</span><br><span class="line">        dp = [0.0] * (K + W + 1)</span><br><span class="line">        for i in range(K, min(N, K + W - 1) + 1):</span><br><span class="line">            dp[i] = 1.0</span><br><span class="line">        dp[K - 1] = min(N - K + 1, W) / W</span><br><span class="line">        for i in range(K - 2, -1, -1):</span><br><span class="line">            dp[i] = dp[i + 1] - (dp[i + W + 1] - dp[i + 1]) / W</span><br><span class="line">        return dp[0]</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  动态规划问题是面试时最常问的算法之一，因此这道题目的关键是掌握DP算法，并且学习反向DP的思考过程。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 837&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>Metrics黑科技</title>
    <link href="https://USTCcoder.github.io/2020/05/12/deep%20learning%20metrics/"/>
    <id>https://USTCcoder.github.io/2020/05/12/deep learning metrics/</id>
    <published>2020-05-12T02:46:07.000Z</published>
    <updated>2020-09-03T15:41:17.747Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Metrics</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Metrics(评价指标)</strong>:评价指标是<strong>检验神经网络模型好坏的评定依据</strong>，也是<strong>我们要达到的最终目标</strong>，指标越好则我们的任务完成的越好。有时我们需要<strong>根据评价指标修改我们的网络模型，参数</strong>等等，就类似于考试成绩一样，我们根据成绩检验自己薄弱的地方，然后去调整和修改，神经网络也是一样，只有建立好合适的评价指标，才能真正区分网络的优劣。今天给小伙伴们介绍深度学习中常用的评价指标。<br><a id="more"></a></p><p><img src="/images/deep_learning/confuse.png" alt="confuse"></p><h1 id="Confusion-matrix-混淆矩阵"><a href="#Confusion-matrix-混淆矩阵" class="headerlink" title="Confusion matrix(混淆矩阵)"></a><font size="5" color="red">Confusion matrix(混淆矩阵)</font></h1><p>在介绍评价指标之前，首先要介绍混淆矩阵，以一个二分类问题来说，行标签代表预测值为正还是为负，列标签代表真实值为正还是为负，因此产生4中状态。</p><ol><li><strong>TP(True Positive，真正率)</strong>：真代表预测正确，正代表预测为正样本，因此含义为<strong>将正样本预测为正样本的个数</strong>。</li><li><strong>TN(True Negative，真负率)</strong>：真代表预测正确，负代表预测为负样本，因此含义为<strong>将负样本预测为负样本的个数</strong>。</li><li><strong>FP(False Positive，假正率)</strong>：假代表预测错误，正代表预测为正样本，因此含义为<strong>将负样本预测为正样本的个数</strong>。</li><li><strong>FN(False Negative，假负率)</strong>：假代表预测错误，负代表预测为负样本，因此含义为<strong>将正样本预测为负样本的个数</strong>。</li></ol><p>混淆矩阵是将所有可能的状态列举出来，然后通过表格可以计算出相应的评价指标，<strong>对于样本不平衡问题非常有效</strong>。</p><h1 id="Accuracy-准确率"><a href="#Accuracy-准确率" class="headerlink" title="Accuracy(准确率)"></a><font size="5" color="red">Accuracy(准确率)</font></h1><script type="math/tex; mode=display">acc = \frac{TP + TN}{TP + TN + FP + FN}</script><p><strong>Accuracy(准确率)</strong>：指<strong>正确预测的数量与总数的比值，准确率越高则代表预测的越准确</strong>。在TensorFlow2.0中已经给我们提供了计算Accuracy的评价函数。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"># 计算准确率，必须完全相等才算准确</span><br><span class="line">metrics = keras.metrics.Accuracy()</span><br><span class="line"></span><br><span class="line"># 计算二分类准确率，大于threshold即算为准确</span><br><span class="line">metrics = keras.metrics.BinaryAccuracy(threshold=0.5)</span><br><span class="line"></span><br><span class="line"># 计算多分类准确率</span><br><span class="line">metrics = keras.metrics.CategoricalAccuracy()</span><br><span class="line"></span><br><span class="line"># 计算TopK多分类准确率，k默认为5</span><br><span class="line">metrics = keras.metrics.TopKCategoricalAccuracy(k=5)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="Precision-精确度"><a href="#Precision-精确度" class="headerlink" title="Precision(精确度)"></a><font size="5" color="red">Precision(精确度)</font></h1><script type="math/tex; mode=display">precision = \frac{TP}{TP + FP}</script><p><strong>Precision(精确度)</strong>：又称为查准率，指<strong>正样本预测为正的数量与所有样本预测为正的数量的比值，精确度越高，代表对预测为正的样本中，正样本的比例越高</strong>。当<strong>负样本判断错误的成本非常高，正样本判断错误的成本非常低的时候，我们选择较高的精确度</strong>，保证预测为正的样本中，负样本尽可能少，减少负样本判断错误的成本。在TensorFlow2.0中已经给我们提供了计算Precision的评价函数。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"># 默认选择阈值为0.5</span><br><span class="line">metrics = keras.metrics.Precision(thresholds=None)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="Recall-召回率"><a href="#Recall-召回率" class="headerlink" title="Recall(召回率)"></a><font size="5" color="red">Recall(召回率)</font></h1><script type="math/tex; mode=display">recall = \frac{TP}{TP + FN}</script><p><strong>Recall(召回率)</strong>：又称为敏感度(Sensitivity)，查全率，指<strong>正样本预测为正的数量与所有正样本的数量的比值，召回率越高，代表所有正样本中，预测为正的样本的比例越高</strong>。当<strong>正样本判断错误的成本非常高，负样本判断错误的成本非常低的时候，我们选择较高的召回率</strong>，保证正样本尽可能被判断正确，减少正样本判断错误的成本。在TensorFlow2.0中已经给我们提供了计算Recall的评价函数。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"># 默认选择阈值为0.5</span><br><span class="line">metrics = keras.metrics.Recall(thresholds=None)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="F1-score"><a href="#F1-score" class="headerlink" title="F1-score"></a><font size="5" color="red">F1-score</font></h1><script type="math/tex; mode=display">F1-score = \frac{2}{\frac{1}{precision} + \frac{1}{recall}} = \frac{2 \times precision \times recall}{precision + recall}</script><p><strong>F1-score</strong>：<strong>精确度和召回率的分子是相同的，分母有差异，两者是此消彼长的</strong>，究竟如何选择两者之间的权重，需要结合具体问题具体分析，判断是正样本判断错误的成本高还是负样本判断错误的成本高。<strong>为了兼顾精确度和召回率，引入了两者的调和平均数F1-score作为指标</strong>。在TensorFlow2.0中没有计算F1-score的评价函数，<strong>需要自己定义计算方法</strong>。</p><ol><li><strong>继承keras.metrics.Metric类</strong></li><li><strong>在<strong>init</strong>(): 中变量要通过下面这种方式创建self.var = self.add_weight(name=name, initializer=’zeros’)</strong></li><li><strong>在update_state():中变量要通过下面这种方式更新self.var.assign_add()</strong></li><li><strong>在result()中: 返回变量</strong></li></ol><h1 id="Specificity-特异度"><a href="#Specificity-特异度" class="headerlink" title="Specificity(特异度)"></a><font size="5" color="red">Specificity(特异度)</font></h1><script type="math/tex; mode=display">specificity = \frac{TN}{FP + TN}</script><p><strong>Specificity(特异度)</strong>：指<strong>负样本预测为负的数量与所有负样本的数量的比值，特异度越高，代表所有负样本中，预测为负的样本的比例越高</strong>。在TensorFlow2.0中没有计算Specificity的评价函数，<strong>需要自己定义计算方法</strong>，步骤如下。</p><ol><li><strong>继承keras.metrics.Metric类</strong></li><li><strong>在<strong>init</strong>(): 中变量要通过下面这种方式创建self.var = self.add_weight(name=name, initializer=’zeros’)</strong></li><li><strong>在update_state():中变量要通过下面这种方式更新self.var.assign_add()</strong></li><li><strong>在result()中: 返回变量</strong></li></ol><h1 id="FPR-假正率"><a href="#FPR-假正率" class="headerlink" title="FPR(假正率)"></a><font size="5" color="red">FPR(假正率)</font></h1><script type="math/tex; mode=display">FPR = 1 - specificity = \frac{FP}{FP + TN}</script><p><strong>FPR(假正率)</strong>：指<strong>负样本预测为正的数量与所有负样本的数量的比值，假正率越低，代表所有负样本中，预测为正样本的比例越低</strong>。在TensorFlow2.0中已经给我们提供了计算假正个数的评价函数，是<strong>将数据最后一个维度计算之后，对所有结果求和</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"># 默认选择阈值为0.5，注意计算的是个数，而不是比例</span><br><span class="line">metrics = keras.metrics.FalsePositives(thresholds=None)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="ROC-Curve-Receiver-Operating-Characteristic，受试者操作特性曲线"><a href="#ROC-Curve-Receiver-Operating-Characteristic，受试者操作特性曲线" class="headerlink" title="ROC Curve(Receiver Operating Characteristic，受试者操作特性曲线)"></a><font size="5" color="red">ROC Curve(Receiver Operating Characteristic，受试者操作特性曲线)</font></h1><p><strong>ROC Curve</strong>：是一条<strong>以不同阈值下的FPR(假正率)为横坐标，不同阈值下的Recall(召回率)为纵坐标的曲线</strong>。<br><strong>ROC曲线绘制步骤</strong>：</p><ol><li><strong>从高到低，依次将每个测试样本属于正样本的概率值从大到小排序</strong>。</li><li>根据排序，<strong>从高到低依次将概率值作为阈值，当概率大于等于这个阈值时，预测为正样本，否则预测为负样本</strong>。</li><li>根据步骤2得到的结果计算FPR和Recall，<strong>遍历所有样本，得到一组(FPR, Recall)，并和阈值取值为0和1下的(FPR, Recall)结合在图像上绘制出来</strong>，就可以得到ROC曲线。</li></ol><p>ROC曲线特性：</p><ol><li>当测试集中的<strong>正负样本的分布变化的时候，ROC曲线能够保持不变</strong>，这是ROC曲线的最大优点，但是<strong>如果正负样本的成本差距较大，则很难从ROC曲线中看出结论</strong>。</li><li>ROC曲线<strong>越接近左上角代表模型的效果越好</strong>。<br><img src="/images/deep_learning/roc.png" alt="roc"></li></ol><h1 id="AUC-Area-Under-Curve，ROC曲线下的面积"><a href="#AUC-Area-Under-Curve，ROC曲线下的面积" class="headerlink" title="AUC(Area Under Curve，ROC曲线下的面积)"></a><font size="5" color="red">AUC(Area Under Curve，ROC曲线下的面积)</font></h1><p><strong>AUC</strong>：<strong>定义为ROC曲线下的面积，值域为[0, 1]</strong>，<strong>该值越大，代表ROC曲线越接近左上角，说明模型效果越好</strong>。在TensorFlow2.0中已经给我们提供了计算AUC的评价函数。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"># 默认选择计算阈值的个数为200个</span><br><span class="line">metrics = keras.metrics.AUC(num_thresholds=200)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="PR-Curve-Precision-Recall-Curve，查准率查全率曲线"><a href="#PR-Curve-Precision-Recall-Curve，查准率查全率曲线" class="headerlink" title="PR Curve(Precision Recall Curve，查准率查全率曲线)"></a><font size="5" color="red">PR Curve(Precision Recall Curve，查准率查全率曲线)</font></h1><p><strong>PR Curve</strong>：是一条<strong>以不同阈值下的Recall(查全率)为横坐标，不同阈值下的Precision(查准率)为纵坐标的曲线</strong>。<br><strong>PR曲线绘制步骤</strong>：</p><ol><li><strong>从高到低，依次将每个测试样本属于正样本的概率值从大到小排序</strong>。</li><li>根据排序，<strong>从高到低依次将概率值作为阈值，当概率大于等于这个阈值时，预测为正样本，否则预测为负样本</strong>。</li><li>根据步骤2得到的结果计算Recall和Precision，<strong>遍历所有样本，得到一组(Recall, Precision)，并和阈值取值为0和1下的(Recall, Precision)结合在图像上绘制出来</strong>，就可以得到PR曲线。</li></ol><p>PR曲线特性：</p><ol><li>PR曲线的两个指标都聚焦于正样本，因此在类别不平衡问题中主要关心正样本，在正负样本的成本差距较大情况下，PR曲线优于ROC曲线。</li><li>PR曲线<strong>越接近右上角代表模型的效果越好</strong>。<br><img src="/images/deep_learning/pr.png" alt="pr"></li></ol><h1 id="AP-Average-Precision，平均精度-和mAP-mean-Average-Precision"><a href="#AP-Average-Precision，平均精度-和mAP-mean-Average-Precision" class="headerlink" title="AP(Average Precision，平均精度)和mAP(mean Average Precision)"></a><font size="5" color="red">AP(Average Precision，平均精度)和mAP(mean Average Precision)</font></h1><p><strong>AP(平均精度)</strong>：<strong>定义为PR曲线下的面积，值域为[0, 1]</strong>，<strong>该值越大，代表PR曲线越接近右上角，说明模型效果越好</strong>。<br><strong>mAP</strong>：<strong>对每个类的AP求平均，值域为[0, 1]</strong>，<strong>该值越大，说明模型效果越好，因为目标检测算法中存在正负样本不平衡的问题，因此mAP是目标检测算法中最重要的指标之一</strong>。在TensorFlow2.0中没有计算AP的评价函数，<strong>需要自己定义计算方法</strong>，步骤如下。</p><ol><li><strong>继承keras.metrics.Metric类</strong></li><li><strong>在<strong>init</strong>(): 中变量要通过下面这种方式创建self.var = self.add_weight(name=name, initializer=’zeros’)</strong></li><li><strong>在update_state():中变量要通过下面这种方式更新self.var.assign_add()</strong></li><li><strong>在result()中: 返回变量</strong></li></ol><h1 id="IOU-Intersection-Over-Union，交并比"><a href="#IOU-Intersection-Over-Union，交并比" class="headerlink" title="IOU(Intersection Over Union，交并比)"></a><font size="5" color="red">IOU(Intersection Over Union，交并比)</font></h1><p><strong>IOU(交并比)</strong>：可以理解为算法的结果与标记结果的重合程度，算法的结果与真实物体进行<strong>交运算的结果除以进行并运算的结果</strong>。用于<strong>评估语义分割算法性能的指标是平均IOU</strong>，通过下图可以直观的看出IOU的计算方法，IOU的值越大，算法的效果越好。在TensorFlow2.0中没有计算IOU的评价函数，<strong>需要自己定义计算方法</strong>，步骤如下。</p><ol><li><strong>继承keras.metrics.Metric类</strong></li><li><strong>在<strong>init</strong>(): 中变量要通过下面这种方式创建self.var = self.add_weight(name=name, initializer=’zeros’)</strong></li><li><strong>在update_state():中变量要通过下面这种方式更新self.var.assign_add()</strong></li><li><strong>在result()中: 返回变量</strong><br><img src="/images/Semantic_segmentation/Dataset_I.png" alt="IOU"></li></ol><h1 id="MSE-Mean-Squared-Error，均方误差"><a href="#MSE-Mean-Squared-Error，均方误差" class="headerlink" title="MSE(Mean Squared Error，均方误差)"></a><font size="5" color="red">MSE(Mean Squared Error，均方误差)</font></h1><script type="math/tex; mode=display">MSE = \frac{1}{N} \sum_{i=1}^{N}{(y^{(i)} - f(x^{(i)}))^2}</script><p><strong>MSE(均方误差)</strong>：指<strong>预测结果与真实值之差平方的期望值</strong>，其中<strong>平方误差是估计值和实际值之差的平方，也称为(L2 Loss)</strong>，MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。在TensorFlow2.0中已经给我们提供了计算MSE的评价函数，是<strong>将数据最后一个维度计算之后，对所有结果求平均值</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line">metrics = keras.metrics.MeanSquaredError()</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="MAE-Mean-Absolute-Error，平均绝对误差"><a href="#MAE-Mean-Absolute-Error，平均绝对误差" class="headerlink" title="MAE(Mean Absolute Error，平均绝对误差)"></a><font size="5" color="red">MAE(Mean Absolute Error，平均绝对误差)</font></h1><script type="math/tex; mode=display">MAE = \frac{1}{N} \sum_{i=1}^{N}{|y^{(i)} - f(x^{(i)})|}</script><p><strong>MAE(平均绝对误差)</strong>：指<strong>预测结果与真实值绝对误差的平均值</strong>，其中<strong>绝对误差是估计值和实际值之间的距离，也称为(L1 Loss)</strong>，MAE能更好地反映预测值误差的实际情况。在TensorFlow2.0中已经给我们提供了计算MAE的评价函数，是<strong>将数据最后一个维度计算之后，对所有结果求平均值</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line">metrics = keras.metrics.MeanAbsoluteError()</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="Binary-Cross-Entropy-二分类交叉熵"><a href="#Binary-Cross-Entropy-二分类交叉熵" class="headerlink" title="Binary Cross Entropy(二分类交叉熵)"></a><font size="5" color="red">Binary Cross Entropy(二分类交叉熵)</font></h1><script type="math/tex; mode=display">Binary \ Cross \ Entropy =  -\frac{1}{N} \sum_{i=1}^{N}{(y^{(i)} \cdot \ln{(f(x^{(i)})}) + (1 - y^{(i)}) \cdot (1 - \ln{(f(x^{(i)})}))}</script><p><strong>Binary Cross Entropy</strong>：用来<strong>评估当前训练得到的概率分布与真实分布的差异情况</strong>，它刻画的是实际概率与期望概率的距离，也就是交叉熵的值越小，两个概率分布就越接近。在<strong>二分类中，每个数据独立计算交叉熵，和同一维度其他数据无关，允许多个1同时出现，经常配合sigmoid激活函数使用</strong>。在TensorFlow2.0中已经给我们提供了计算Binary Cross Entropy的评价函数，是<strong>将每一个数据单独计算之后，对所有数据求平均值</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line">metrics = keras.metrics.BinaryCrossentropy()</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="Categorical-Cross-Entropy-多分类交叉熵"><a href="#Categorical-Cross-Entropy-多分类交叉熵" class="headerlink" title="Categorical Cross Entropy(多分类交叉熵)"></a><font size="5" color="red">Categorical Cross Entropy(多分类交叉熵)</font></h1><script type="math/tex; mode=display">Categorical \ Cross \ Entropy =  -\frac{1}{N} \sum_{i=1}^{N}{(y^{(i)} \cdot \ln{(f(x^{(i)})))}}</script><p><strong>Categorical Cross Entropy</strong>：用来<strong>评估当前训练得到的概率分布与真实分布的差异情况</strong>，它刻画的是实际概率与期望概率的距离，也就是交叉熵的值越小，两个概率分布就越接近。在<strong>多分类中，在某个维度上计算交叉熵，在该维度上其他数据一般只有一个1出现，其他全为0，经常配合Softmax激活函数使用</strong>。在TensorFlow2.0中已经给我们提供了计算Categorical Cross Entropy的评价函数，是<strong>将数据最后一个维度计算之后，对所有结果求平均值</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line">metrics = keras.metrics.BinaryCrossentropy()</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  评价函数和损失函数类似，<strong>评价函数在一定程度上可以和损失函数相互转化</strong>，如<strong>MSE，MAE，交叉熵既可以作为评价函数，又可以作为损失函数</strong>。在选择评价函数时，我们首先要<strong>判断任务类型</strong>，是分类任务还是回归任务。<strong>分类任务可以考虑Accuracy评价函数，回归任务可以考虑MSE或者MAE评价函数</strong>。也可以<strong>根据正负样本的损失比例，来给予Precision和Recall一定的权重</strong>。在实际的工程应用之中，往往需要自己设计一个合适的评价函数，如<strong>目标检测问题，就需要设计一个mAP评价函数</strong>，<strong>目标分割问题，就需要设计一个IOU评价函数</strong>。因此小伙伴们需要<strong>多实践，多尝试</strong>，实践出真知。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Metrics&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>Learning Rate黑科技</title>
    <link href="https://USTCcoder.github.io/2020/05/11/deep%20learning%20learning_rate/"/>
    <id>https://USTCcoder.github.io/2020/05/11/deep learning learning_rate/</id>
    <published>2020-05-10T23:22:13.000Z</published>
    <updated>2020-05-18T08:52:33.115Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Learning Rate</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Learning Rate(学习率)</strong>:深度学习中有一个重要的参数为学习率，小伙伴们应该都知道，这是<strong>设置优化器时的一个必要参数</strong>，学习率<strong>指导我们在梯度下降的过程中，如何去使用损失函数的梯度调整网络的权重</strong>。因此对网络的影响是非常重要的。今天给小伙伴们盘点一下常用的学习率黑科技。<br><a id="more"></a></p><p><img src="/images/deep_learning/learning_rate.png" alt="learning_rate"></p><h1 id="学习率特点"><a href="#学习率特点" class="headerlink" title="学习率特点"></a><font size="5" color="red">学习率特点</font></h1><p><strong>大学习率优点</strong>：</p><ul><li>能够<strong>加速学习速率</strong>，更快的使Loss变小， 且<strong>容易跳出局部最优值</strong>。<br>大学习率缺点：</li><li>可能导致模型<strong>在极小值震动，使模型不精确</strong>。</li></ul><p><strong>小学习率优点：</strong></p><ul><li><strong>帮助模型收敛，有助于模型细化，提高模型精度</strong>。<br>小学习率缺点：</li><li><strong>收敛缓慢，可能被困在某个具备最优值附近</strong>。</li></ul><p>因此在深度学习工程中，<strong>既要使用大学习率加速收敛，跳出局部最优，也要使用小学习率，提高模型精度</strong>。所以我们需要在训练过程中修改学习率的大小。</p><h1 id="fixed-固定值"><a href="#fixed-固定值" class="headerlink" title="fixed(固定值)"></a><font size="5" color="red">fixed(固定值)</font></h1><script type="math/tex; mode=display">lr = \eta</script><p>其中$ \eta $为初始学习率<br>在TensorFlow2.0中已经给我们提供了自定义学习率的类LearningRateScheduler。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def scheduler(epoch):</span><br><span class="line">    lr = base_lr </span><br><span class="line">    print('\nBatch %d: setting learning rate to %s.' % (epoch + 1, lr))</span><br><span class="line">    return lr</span><br><span class="line"></span><br><span class="line">base_lr = 0.01</span><br><span class="line">max_epoch = 100</span><br><span class="line">reduce_lr = keras.callbacks.LearningRateScheduler(scheduler)</span><br><span class="line">model.fit(db, epochs=max_epoch, callbacks=[reduce_lr], validation_data=db_test)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/fixed.png" alt="fixed"></p><h1 id="step-阶梯下降"><a href="#step-阶梯下降" class="headerlink" title="step(阶梯下降)"></a><font size="5" color="red">step(阶梯下降)</font></h1><script type="math/tex; mode=display">lr = \eta * \gamma^{\left\lfloor {\frac{epoch}{step}} \right\rfloor}</script><p>其中$ \eta $为初始学习率，$ epoch $为当前迭代次数，$ \gamma $为下降比例，$ step $为下降周期<br>在TensorFlow2.0中已经给我们提供了自定义学习率的类LearningRateScheduler。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line">def scheduler(epoch):</span><br><span class="line">    lr = base_lr * gamma ** (epoch // step_size)</span><br><span class="line">    print('\nBatch %d: setting learning rate to %s.' % (epoch + 1, lr))</span><br><span class="line">    return lr</span><br><span class="line"></span><br><span class="line">base_lr = 0.01</span><br><span class="line">step_size = 10</span><br><span class="line">gamma = 0.1</span><br><span class="line">max_epoch = 100</span><br><span class="line">reduce_lr = keras.callbacks.LearningRateScheduler(scheduler)</span><br><span class="line">model.fit(db, epochs=max_epoch, callbacks=[reduce_lr], validation_data=db_test)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/step.png" alt="step"></p><h1 id="exp-指数下降"><a href="#exp-指数下降" class="headerlink" title="exp(指数下降)"></a><font size="5" color="red">exp(指数下降)</font></h1><script type="math/tex; mode=display">lr = \eta * (\gamma^{epoch})</script><p>其中$ \eta $为初始学习率，$ epoch $为当前迭代次数，$ \gamma $为指数下降底数<br>在TensorFlow2.0中已经给我们提供了自定义学习率的类LearningRateScheduler。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def scheduler(epoch):</span><br><span class="line">    lr = base_lr * gamma ** epoch</span><br><span class="line">    print('\nBatch %d: setting learning rate to %s.' % (epoch + 1, lr))</span><br><span class="line">    return lr</span><br><span class="line"></span><br><span class="line">base_lr = 0.01</span><br><span class="line">gamma = 0.9</span><br><span class="line">max_epoch = 100</span><br><span class="line">reduce_lr = keras.callbacks.LearningRateScheduler(scheduler)</span><br><span class="line">model.fit(db, epochs=max_epoch, callbacks=[reduce_lr], validation_data=db_test)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/exp.png" alt="exp"></p><h1 id="poly-多项式下降"><a href="#poly-多项式下降" class="headerlink" title="poly(多项式下降)"></a><font size="5" color="red">poly(多项式下降)</font></h1><script type="math/tex; mode=display">lr = \eta * (1 - \frac{epoch}{max\\_epoch})^\gamma</script><p>其中$ \eta $为初始学习率，$ epoch $为当前迭代次数，$ max\_epoch $为最大迭代次数，$ \gamma $为多项式的幂<br>在TensorFlow2.0中已经给我们提供了自定义学习率的类LearningRateScheduler。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def scheduler(epoch):</span><br><span class="line">    lr = base_lr * (1 - epoch / max_epoch) ** gamma</span><br><span class="line">    print('\nBatch %d: setting learning rate to %s.' % (epoch + 1, lr))</span><br><span class="line">    return lr</span><br><span class="line"></span><br><span class="line">base_lr = 0.01</span><br><span class="line">gamma = 2</span><br><span class="line">max_epoch = 100</span><br><span class="line">reduce_lr = keras.callbacks.LearningRateScheduler(scheduler)</span><br><span class="line">model.fit(db, epochs=max_epoch, callbacks=[reduce_lr], validation_data=db_test)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/poly.png" alt="poly"></p><h1 id="Cosine-Annealing-余弦退火"><a href="#Cosine-Annealing-余弦退火" class="headerlink" title="Cosine Annealing(余弦退火)"></a><font size="5" color="red">Cosine Annealing(余弦退火)</font></h1><script type="math/tex; mode=display">lr = \begin{cases} epoch \* \frac{ \eta - \alpha }{ warm\\_epoch } + \alpha & epoch \le warm\\_epoch \\\\ 0.5 \* \eta \* (1 + \cos{(pi * \frac{epoch - warm\\_epoch}{max\\_epoch - warm\\_epoch})})& epoch > warm\\_epoch \end{cases}</script><p>其中$ \alpha $为初始学习率，$ \eta $为最高学习率，$ epoch $为当前迭代次数，$ max\_epoch $为最大迭代次数，$ warm\_epoch $为开始退火的迭代次数<br>在TensorFlow2.0中已经给我们提供了自定义学习率的类LearningRateScheduler。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def scheduler(epoch):</span><br><span class="line">    lr = (base_lr - begin_lr) / warm_epoch * epoch + begin_lr if epoch &lt;= warm_epoch else 0.5 * base_lr * (1 + np.cos(np.pi * (epoch - warm_epoch) / (max_epoch - warm_epoch)))</span><br><span class="line">    print('\nBatch %d: setting learning rate to %s.' % (epoch + 1, lr))</span><br><span class="line">    return lr</span><br><span class="line"></span><br><span class="line">base_lr = 0.01</span><br><span class="line">begin_lr = 0.001</span><br><span class="line">max_epoch = 100</span><br><span class="line">warm_epoch = 10</span><br><span class="line">reduce_lr = keras.callbacks.LearningRateScheduler(scheduler)</span><br><span class="line">model.fit(db, epochs=max_epoch, callbacks=[reduce_lr], validation_data=db_test)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/cosine.png" alt="cosine"></p><h1 id="ReduceLROnPlateau"><a href="#ReduceLROnPlateau" class="headerlink" title="ReduceLROnPlateau"></a><font size="5" color="red">ReduceLROnPlateau</font></h1><p>Plateau意思为高原，从其英文表达中可以清晰的看出，当要<strong>监视的值出现平坦时，学习率会下降</strong>。<br>在TensorFlow2.0中已经给我们提供了ReduceLROnPlateau类，其常用参数有</p><ol><li><strong>monitor：要监视的值，默认为val_loss</strong>。</li><li><strong>factor：学习率下降因子，new_lr = lr x factor</strong>。</li><li><strong>patience：耐心周期，如果在patience周期内没有改善，则降低学习率</strong>。</li><li><strong>min_lr：设置最低学习率，防止学习率过低</strong>。</li></ol><p>因为ReduceLROnPlateau<strong>可以实现自适应的学习率变化</strong>，而且<strong>不需要去自定义学习率下降方法</strong>，所以<strong>大部分情况下使用ReduceLROnPlateau即可完成训练任务</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">base_lr = 0.01</span><br><span class="line">begin_lr = 0.001</span><br><span class="line">max_epoch = 100</span><br><span class="line">warm_epoch = 10</span><br><span class="line">reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0)</span><br><span class="line">model.fit(db, epochs=max_epoch, callbacks=[reduce_lr], validation_data=db_test)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="model-optimizer-lr-assign"><a href="#model-optimizer-lr-assign" class="headerlink" title="model.optimizer.lr.assign"></a><font size="5" color="red">model.optimizer.lr.assign</font></h1><p>在<strong>Model类对象中，有optimizer.lr属性，其保存着学习率的值，使用assign方法可以设置学习率</strong>。在<strong>自定义训练过程中，常常使用这种方法进行学习率动态修改</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.optimizer.lr.assign(model.optimizer.lr / 2)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  小伙伴们观察几个常见学习率函数后发现，都是<strong>呈现一个递减的函数</strong>，递减是因为<strong>模型刚开始训练，需要加速收敛，而且容易落入局部最小值，因此需要较大的学习率，随着学习的推进，我们需要对模型进行细化，使其精度提高，因此需要较小的学习率</strong>。在这个博客里列举的是一些常用的学习率下降函数，大家可以<strong>根据自己的实际问题进行调整，只要是满足递减条件，都可以用来尝试</strong>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Learning Rate&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>Optimizer黑科技</title>
    <link href="https://USTCcoder.github.io/2020/05/10/deep%20learning%20optimizer/"/>
    <id>https://USTCcoder.github.io/2020/05/10/deep learning optimizer/</id>
    <published>2020-05-10T07:37:36.000Z</published>
    <updated>2020-09-04T10:55:09.165Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Optimizer</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Optimizer(优化器)</strong>:在深度神经网络中，<strong>如果说到达Loss最小是我们的终点站，那么出行方式就是优化器，它给我们提供一种方法接近Loss最小的地方</strong>。打一个简单的比方，如果我要从安徽前往北京，北京就是我们的目标，相当于神经网络中的Loss函数，而我们选择的交通方式则是优化器，我可以选择长途汽车出行，也可以选择火车出行，也可以选择飞机出行。选择哪一种交通方式最为合适呢？这也需要根据实际问题确定，优化器也是这样，<strong>没有最好的优化器，只有最适合的优化器</strong>，这里我不写太多的公式，主要是聊一聊各个优化器的用法，特点和参数表达的意义。<br><a id="more"></a></p><p><img src="/images/deep_learning/optimizer.png" alt="optimizer"></p><h1 id="SGD-Stochastic-Gradient-Descent，随机梯度下降"><a href="#SGD-Stochastic-Gradient-Descent，随机梯度下降" class="headerlink" title="SGD(Stochastic Gradient Descent，随机梯度下降)"></a><font size="5" color="red">SGD(Stochastic Gradient Descent，随机梯度下降)</font></h1><script type="math/tex; mode=display">\begin{cases} W_{t+1}=W_{t}-\eta_{t} g_{t} \\\\ g_{t} = \Delta J_{i_{s}}(W_{t}) \end{cases}</script><p>其中$W_t$表示t时刻模型参数，$g_t$表示第t次迭代的梯度，$\eta_t$表示学习率，一般取值0.001。<br><strong>SGD(随机梯度下降)</strong>：指从一批训练样本中随机选取一个，并引入一些随机性和噪声，然后利用梯度下降法进行训练。<br><strong>SGD优点</strong>：</p><ol><li>梯度计算快，<strong>引入噪声增加鲁棒性</strong>，并且便于逃离鞍点，实验证明只要噪声不是特别大，SGD都能很好地收敛。</li><li>应用大型数据集，<strong>比标准的梯度下降算法快速很多</strong>。</li></ol><p><strong>SGD缺点</strong>：</p><ol><li><strong>SGD引入噪声，可能使得权值更新的方向不一定正确</strong>。</li><li><strong>SGD难以逃离局部最优解</strong>。<br>在TensorFlow2.0中给我们提供了SGD的类。<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.0, nesterov=False, name='SGD')</span><br></pre></td></tr></tbody></table></figure></li></ol><h1 id="Momentum-使用动量的随机梯度下降"><a href="#Momentum-使用动量的随机梯度下降" class="headerlink" title="Momentum(使用动量的随机梯度下降)"></a><font size="5" color="red">Momentum(使用动量的随机梯度下降)</font></h1><script type="math/tex; mode=display">\begin{cases} W_{t+1}=W_{t} - v_{t} \\\\ v_{t} = \alpha v_{t-1} + \eta \Delta J(W_{t}) \end{cases}</script><p>其中$W_t$表示t时刻模型参数，$v_t$表示当前积攒的速度，$\alpha$表示动力大小，一般取值0.9，$\Delta J(W_t)$表示第t次迭代的梯度，$\eta_t$表示学习率，一般取值0.001。<br><strong>Momentum(使用动量的随机梯度下降)</strong>：引入一个积攒的梯度信息动量进行加速随机梯度下降法的训练过程。<br><strong>Momentum优点</strong>：</p><ol><li><strong>加速SGD的收敛</strong>。</li><li><strong>引入动量的思想，可能会冲破局部最小值的影响</strong>。</li></ol><p><strong>Momentum缺点</strong>：</p><ul><li><strong>依赖动量可能会使下降速度越来越大，使得冲上另一个山坡</strong>。<br>在TensorFlow2.0中SGD类属性引入了动量因子，因此调用SGD即可。<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=False, name='Momentum')</span><br></pre></td></tr></tbody></table></figure></li></ul><h1 id="NAG-Nesterov-accelerated-gradient，牛顿加速梯度"><a href="#NAG-Nesterov-accelerated-gradient，牛顿加速梯度" class="headerlink" title="NAG(Nesterov accelerated gradient，牛顿加速梯度)"></a><font size="5" color="red">NAG(Nesterov accelerated gradient，牛顿加速梯度)</font></h1><script type="math/tex; mode=display">\begin{cases} W_{t+1}=W_{t} - v_{t} \\\\ v_{t} = \alpha v_{t-1} + \eta \Delta J(W_{t} - \alpha v_{t - 1}) \end{cases}</script><p>其中$W_t$表示t时刻模型参数，$v_t$表示当前积攒的速度，$\alpha$表示动力大小，一般取值0.9，$\Delta J(W_t)$表示第t次迭代的梯度，$\eta_t$表示学习率，一般取值0.001。<br><strong>NAG(牛顿加速梯度)</strong>：是Momentum的变种，引入了一个校正因子，使得Momentum不会盲目听从动量指示，能提前知道自己下一步去哪里，并且预防下坡过头而冲上另一个山坡。<br><strong>NAG优点</strong>：</p><ul><li><strong>较好的解决了Momentum中存在的速度过快的情况</strong>。</li></ul><p><strong>NAG缺点</strong>：</p><ul><li>引入了修正因子，多了一个本次梯度相对上一次梯度的变化量，因此<strong>需要再进行一次前向传播和后向传播，速度大大降低</strong>。<br>在TensorFlow2.0中SGD类属性引入了nesterov动量，因此调用SGD即可。<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True, name='NAG')</span><br></pre></td></tr></tbody></table></figure></li></ul><h1 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a><font size="5" color="red">AdaGrad</font></h1><script type="math/tex; mode=display">W_{t+1}=W_{t} - \frac{\eta_{0}}{\sqrt{\sum_{t'=1}^{t}{(g_{t',i})}} + \epsilon} \odot g_{t, i}</script><p>其中$ W<em>{t} $表示t时刻模型参数，$ g</em>{t, i} $表示第t次迭代的梯度，$ \epsilon $避免分母为0，一般取值1e-7，$ \eta_{t} $表示初始学习率，一般取值0.001。<br><strong>AdaGrad</strong>：独立适应所有参数的学习率，缩放每个参数反比于其梯度历史总和的平方根，具有大梯度的参数有较大的学习率，小梯度的参数有较小的学习率<br><strong>AdaGrad优点</strong>：</p><ol><li>对于<strong>出现较多的类别，给予较小的学习率，对于比较少的类别数据，给予较大的学习率，适合于稀疏数据或者分布不平衡的数据集</strong>。</li><li><strong>不需要人为调节学习率</strong>，它可以完成自动调节。<br><strong>AdaGrad缺点</strong>：</li></ol><ul><li><strong>随着迭代次数的增加，学习率会越来越小，最终趋近于0</strong>。<br>在TensorFlow2.0中给我们提供了AdaGrad的类<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = keras.optimizers.Adagrad(learning_rate=0.001, epsilon=1e-7)</span><br></pre></td></tr></tbody></table></figure></li></ul><h1 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a><font size="5" color="red">RMSProp</font></h1><script type="math/tex; mode=display">\begin{cases} W_{t+1}=W_{t} - \frac{\eta_0}{\sqrt{E[g^2]_t} + \epsilon} \odot g_t \\\\ E[g^2]_t = \alpha E[g^2]_{t - 1} + (1 - \alpha)g_t^2  \end{cases}</script><p>其中$ W<em>{t} $表示t时刻模型参数，$ g_t $表示第t次迭代的梯度，$ \epsilon $避免分母为0，一般取值1e-7，$ \eta</em>{t} $表示全局学习率，一般取值0.001，$ \alpha $表示动力大小，一般取值0.9，$ E[g^2]_t $表示前t次的梯度平方的均值。<br><strong>RMSProp</strong>：是对AdaGrad的改进，<strong>修改了AdaGrad的梯度累加变为加权平均</strong>。<br><strong>RMSProp优点</strong>：</p><ul><li>由于使用加权平均，可以<strong>避免AdaGrad中学习率越来越低的问题</strong>。<br>在TensorFlow2.0中给我们提供了RMSProp的类<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = keras.optimizers.RMSProp(learning_rate=0.001, rho=0.9, epsilon=1e-7)</span><br></pre></td></tr></tbody></table></figure></li></ul><h1 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a><font size="5" color="red">AdaDelta</font></h1><script type="math/tex; mode=display">\begin{cases} W_{t+1}=W_{t} + \Delta W_{t} \\\\ \Delta W_{t} = -\frac{\sqrt{\sum_{i=1}^{t-1}{\Delta W_{i}}}}{\sqrt{E[g^2]_t} + \epsilon} \\\\ E[g^2]_t = \alpha E[g^2]_{t - 1} + (1 - \alpha)g_t^2  \end{cases}</script><p>其中$ W_{t} $表示t时刻模型参数，$ g_t $表示第t次迭代的梯度，$ \epsilon $避免分母为0，一般取值1e-7，$ \alpha $表示动力大小，一般取值0.95，$ E[g^2]_t $表示前t次的梯度平方的均值。<br><strong>AdaDelta</strong>：<strong>结合AdaGrad和RMSProp两种算法每次参数的更新步长</strong>。<br><strong>AdaDelta优点</strong>：</p><ul><li><strong>不需要设置全局学习率</strong>。<br><strong>AdaDelta缺点</strong>：</li><li><strong>在模型训练的后期，模型会反复地在局部最小值附近抖动</strong>。<br>在TensorFlow2.0中给我们提供了AdaDelta的类<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 其中的learning_rate匹配论文中的精确形式，使用1.0，等效为实现与学习率无关</span><br><span class="line">optimizer = tf.keras.optimizers.AdaDelta(learning_rate=1.0, rho=0.95, epsilon=1e-7)</span><br></pre></td></tr></tbody></table></figure></li></ul><h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a><font size="5" color="red">Adam</font></h1><script type="math/tex; mode=display">\begin{cases} W_{t+1}=W_{t} - \frac{\eta}{\sqrt{\hat{v_t}} + \epsilon} \hat{m_t} \\\\ \hat{m_t} = \frac{m_t}{1-\beta_1^t}，\hat{v_t} = \frac{v_t}{1-\beta_2^t} \\\\ v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \\\\ m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t   \end{cases}</script><p>其中$ m<em>t，v_t $分别表示一阶动量和二阶动量，$ \beta_1，\beta_2 $分别表示动力大小，一般取值0.9和0.999，$ \hat{m_t}，\hat{v_t} $分别为各自的修正值，$ W</em>{t} $表示第t次迭代模型参数，$ g<em>t $表示第t次迭代的梯度，$ \epsilon $避免分母为0，一般取值1e-7，$ \eta</em>{t} $表示全局学习率，一般取值0.001。<br><strong>Adam</strong>：通过<strong>计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应学习率，同时获得AdaGrad和RMSProp的优点</strong>，在很多情况是算<strong>默认工作性能比较优秀的优化器</strong>。<br><strong>Adam优点</strong>：</p><ol><li><strong>充分利用了梯度的二阶矩</strong>。</li><li><strong>适用于不稳定的目标函数，鲁棒性强</strong>。</li><li><strong>适用于大规模的数据和参数的场景</strong>。<br>在TensorFlow2.0中给我们提供了Adam的类<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta1=0.9, beta_2=0.999, epsilon=1e-7)</span><br></pre></td></tr></tbody></table></figure></li></ol><h1 id="优化器的比较"><a href="#优化器的比较" class="headerlink" title="优化器的比较"></a><font size="5" color="red">优化器的比较</font></h1><ol><li>在<strong>下降速度</strong>上：<strong>自适应学习优化器(AdaGrad，RMSProp，AdaDelta，Adam)和动量优化器(Momentum，NAG)明显快于SGD</strong>。</li><li>在<strong>下降轨迹</strong>上：<strong>SGD和自适应学习优化器大致相同，动量优化器因为动量原因，可能会越过最低点，导致多走一段距离</strong>。</li><li>在<strong>运行效果</strong>上：<strong>自适应学习优化器和动量优化器基本不会停留在鞍点，而SGD可能会停留在鞍点</strong>。</li></ol><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  在选择优化器的时候，往往<strong>首先考虑Adam优化器</strong>，如果Adam优化器的效果不好，可以<strong>尝试RMSProp</strong>，然后再考虑其他的优化器。但是在实际的工程应用中，<strong>不同的优化器适合不同的场景，不能说明在任何情况下Adam都比其他的优化器更加优秀，选择哪种优化器应该结合具体的问题进行分析</strong>，而且要小伙伴们要记住<strong>模型效果不好可能是多方面原因造成的</strong>，可能是<strong>模型结构太小</strong>，或者<strong>参数设置不合理</strong>，或者<strong>损失函数设计偏差*等等，</strong>不能盲目地更换优化器**来提升性能。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Optimizer&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>寻找最远的关系(某大厂手撕面试题)</title>
    <link href="https://USTCcoder.github.io/2020/05/10/program%20Interview1/"/>
    <id>https://USTCcoder.github.io/2020/05/10/program Interview1/</id>
    <published>2020-05-10T07:00:19.000Z</published>
    <updated>2020-09-02T02:09:13.784Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/interview1.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这个题目思路很清晰，首先想到两种方法，DFS和BFS，但是要注意如何去解决关系中出现环的情况。</p><a id="more"></a><h1 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a><font size="5" color="red">DFS</font></h1><p>深度优先搜索，就是从那个人开始，一直往下搜索，如果找不到可以联系的人，则记录当前的关系并和已知最远关系进行比较，如果大于最远关系，则赋值给最远关系，并回溯。如果在搜索的过程中发现下一个人出现在已经搜索到的人之中，则回溯，防止出现死循环。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def dfs(current_people, current_relationship):</span><br><span class="line">    global result</span><br><span class="line"></span><br><span class="line">    for neighbor in range(1, n + 1):</span><br><span class="line">        if relationship[current_people][neighbor] == 1 and str(neighbor) not in current_relationship:</span><br><span class="line">            dfs(neighbor, current_relationship + str(neighbor))</span><br><span class="line"></span><br><span class="line">    if len(current_relationship) &gt; len(result):</span><br><span class="line">        result = current_relationship</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    n, m = [int(x) for x in line.strip().split()]</span><br><span class="line">    relationship = [[0 for _ in range(n + 1)] for _ in range(n + 1)]</span><br><span class="line">    for i in range(m):</span><br><span class="line">        a, b = [int(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">        relationship[a][b] = 1</span><br><span class="line">        relationship[b][a] = 1</span><br><span class="line">    p = int(sys.stdin.readline().strip())</span><br><span class="line">    result = ''</span><br><span class="line">    dfs(p, str(p))</span><br><span class="line">    print(result)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="BFS"><a href="#BFS" class="headerlink" title="BFS"></a><font size="5" color="red">BFS</font></h1><p>广度优先搜索，就是从那个人开始，将他可以联系到的所有人都记录下来，并称之为朋友，然后再寻找他朋友的朋友，即将和他朋友的朋友都记录下来，然后再寻找他朋友的朋友的朋友。如果再搜索时发现某个朋友已经出现在寻找的路径之中，则不继续搜索这个路径，每次迭代后关系就会增加一轮，直到最远的关系被找到为止。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    n, m = [int(x) for x in line.strip().split()]</span><br><span class="line">    relationship = [[0 for _ in range(n + 1)] for _ in range(n + 1)]</span><br><span class="line">    for i in range(m):</span><br><span class="line">        a, b = [int(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">        relationship[a][b] = 1</span><br><span class="line">        relationship[b][a] = 1</span><br><span class="line">    p = int(sys.stdin.readline().strip())</span><br><span class="line">    result = ''</span><br><span class="line">    queue = [[p, str(p)]]</span><br><span class="line">    while queue:</span><br><span class="line">        result = queue[-1]</span><br><span class="line">        current_people, current_relationship = queue.pop(0)</span><br><span class="line">        for neighbor in range(1, n + 1):</span><br><span class="line">            if relationship[current_people][neighbor] == 1 and str(neighbor) not in current_relationship:</span><br><span class="line">                queue.append([neighbor, current_relationship + str(neighbor)])</span><br><span class="line">    print(result[-1])</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  BFS和DFS是两种重要的路径搜索方法，在绝大多数情况下，两种方法可以相互转换，小伙伴们一定要掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Interview&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>排序数组(Leetcode 912)</title>
    <link href="https://USTCcoder.github.io/2020/05/10/program%20Leetcode912/"/>
    <id>https://USTCcoder.github.io/2020/05/10/program Leetcode912/</id>
    <published>2020-05-10T06:27:38.000Z</published>
    <updated>2020-09-02T02:18:50.488Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode912.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这个题目非常简单，初学语言的小伙伴们都应该可以求解。排序也是算法中最基础最核心的问题之一，今天不是讲解这一道题目，而是给小伙伴们讲解常见的十种排序算法，就按照题目的要求，升序排列。</p><a id="more"></a><h1 id="稳定排序和不稳定排序"><a href="#稳定排序和不稳定排序" class="headerlink" title="稳定排序和不稳定排序"></a><font size="5" color="red">稳定排序和不稳定排序</font></h1><p>在排序算法性能的比较中，除了时间复杂度和空间复杂度外，还有一个因素是排序是否稳。什么是稳定呢？简单来说，稳定是两个相同的数字不会因为排序算法而导致顺序调换，如1，3(1)，5，3(2)排序后为1，3(2)，3(1)，5，本来第一个3和第二个3交换了顺序，那么这个排序就是不稳定的。对于纯数字来说，稳定和不稳定没有太大意义，但是如果对于某个类，将某个属性进行排序，则就非常有必要了。举个例子，有100个人到银行排队，它们都是普通客户，这时候来了一个VIP客户，这时需要对数据进行排序，将VIP客户的优先级提高，但是如果排序是不稳定的，则后面100个人的顺序将会被打乱，这时，之前排在第一个的人肯定是不愿意的。这就是不稳定排序造成的后果。</p><h1 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a><font size="5" color="red">冒泡排序</font></h1><p>冒泡排序可能是我们最早接触的排序算法之一了，什么是冒泡排序呢？比较相邻的两个数值，如果前一个数大于后一个数，则交换两个数，就像吐泡泡一样，每一轮迭代会将最大的数放在最后，而且可以添加一个监视器，如果某一轮迭代都没有发生任何依次交换，说明这个序列已经是有序的，则可以提前停止。冒泡排序算法的时间复杂度为$O(n^2)$，最好情况为$O(n)$，最坏情况为$O(n^2)$，空间复杂度$O(1)$，稳定排序。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def bubble_sort(self, nums):</span><br><span class="line">        for i in range(len(nums) - 1):</span><br><span class="line">             # 改进后的冒泡，设置一个交换标志位</span><br><span class="line">            flag = False </span><br><span class="line">            for j in range(len(nums) - i - 1):</span><br><span class="line">                if nums[j] &gt; nums[j + 1]:</span><br><span class="line">                    nums[j], nums[j + 1] = nums[j + 1], nums[j]</span><br><span class="line">                    flag = True</span><br><span class="line">            if not flag:</span><br><span class="line">                return nums</span><br><span class="line">        return nums</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a><font size="5" color="red">选择排序</font></h1><p>选择排序思路也非常简单，每次从剩余数组中选择一个最小的放在当前位置上，选择排序算法的时间复杂度为$O(n^2)$，最好情况为$O(n^2)$，最坏情况为$O(n^2)$，空间复杂度$O(1)$，不稳定排序。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def selection_sort(self, nums):</span><br><span class="line">        for i in range(len(nums)):</span><br><span class="line">            min_idx = i</span><br><span class="line">            for j in range(i + 1, len(nums)):</span><br><span class="line">                if nums[min_idx] &gt; nums[j]:</span><br><span class="line">                    min_idx = j</span><br><span class="line"></span><br><span class="line">            nums[i], nums[min_idx] = nums[min_idx], nums[i]</span><br><span class="line">        return nums</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a><font size="5" color="red">插入排序</font></h1><p>插入排序类似于冒泡排序，插入排序保证当前位置以前的都是有序的，将当前位置从后向前通过交换的方式，插入到之前有序的位置中，插入排序算法的时间复杂度为$O(n^2)$，最好情况为$O(n)$，最坏情况为$O(n^2)$，空间复杂度$O(1)$，稳定排序。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def insertion_sort(self, nums):</span><br><span class="line">        # 第一层for表示循环插入的遍数</span><br><span class="line">        for i in range(1, len(nums)):</span><br><span class="line">            for j in range(i, 0, -1):</span><br><span class="line">                if nums[j] &lt; nums[j - 1]:</span><br><span class="line">                    nums[j], nums[j - 1] = nums[j - 1], nums[j]</span><br><span class="line">                else:</span><br><span class="line">                    break</span><br><span class="line">        return nums</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a><font size="5" color="red">快速排序</font></h1><p>快速排序是一种面试常问的排序方法，其思想是每次选择一个基准值，将小于等于基准值的放在左边，大于基准值的放在右边，然后递归左边和右边两个子序列，快速排序算法的时间复杂度为$O(nlog(n))$，最好情况为$O(nlog(n))$，最坏情况为$O(n^2)$，空间复杂度$O(log(n))$，不稳定排序。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def quick_sort(self, nums):</span><br><span class="line"></span><br><span class="line">        def quicksort(nums, begin, end):</span><br><span class="line">            if begin &lt; end:</span><br><span class="line">                base_element, head, tail = nums[begin], begin, end</span><br><span class="line">                while head &lt; tail:</span><br><span class="line">                    while head &lt; tail and nums[tail] &gt; base_element:</span><br><span class="line">                        tail -= 1</span><br><span class="line">                    while head &lt; tail and nums[head] &lt;= base_element:</span><br><span class="line">                        head += 1</span><br><span class="line">                    nums[head], nums[tail] = nums[tail], nums[head]</span><br><span class="line">                nums[tail], nums[begin] = nums[begin], nums[tail]</span><br><span class="line">                quicksort(nums, begin, head - 1)</span><br><span class="line">                quicksort(nums, head + 1, end)</span><br><span class="line">        quicksort(nums, 0, len(nums) - 1)</span><br><span class="line">        return nums</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a><font size="5" color="red">归并排序</font></h1><p>归并排序利用了一种分治的思想，先对序列进行分解，分解成长度为1的n个子序列，这n个子序列因为长度为1，故都是有序的，然后再进行合并，合并的时候就是两个有序数组进行合并，因此排序速度会大大提升，归并排序算法的时间复杂度为$O(nlog(n))$，最好情况为$O(nlog(n))$，最坏情况为$O(nlog(n))$，空间复杂度$O(n)$，稳定排序。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def merge_sort(self, nums):</span><br><span class="line"></span><br><span class="line">        def merge(nums, begin, mid, end):</span><br><span class="line">            new_list = []</span><br><span class="line">            p_1, p_2 = begin, mid + 1</span><br><span class="line">            while p_1 &lt;= mid and p_2 &lt;= end:</span><br><span class="line">                new_list, p_1, p_2 = [new_list + [nums[p_1]], p_1 + 1, p_2 + 0] if nums[p_1] &lt;= nums[p_2] else [new_list + [nums[p_2]], p_1 + 0, p_2 + 1]</span><br><span class="line">            new_list += nums[p_2:end + 1] if p_1 &gt; mid else nums[p_1:mid + 1]</span><br><span class="line">            nums[begin:end + 1] = new_list</span><br><span class="line"></span><br><span class="line">        def mergesort(nums, begin, end):</span><br><span class="line">            if begin &lt; end:</span><br><span class="line">                mergesort(nums, begin, (begin + end) // 2)</span><br><span class="line">                mergesort(nums, (begin + end) // 2 + 1, end)</span><br><span class="line">                merge(nums, begin, (begin + end) // 2, end)</span><br><span class="line"></span><br><span class="line">        mergesort(nums, 0, len(nums) - 1)</span><br><span class="line">        return nums</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a><font size="5" color="red">桶排序</font></h1><p>桶排序首先建立k个桶，每个桶有一定的范围，将原始序列分桶装入，这样排序时只要每个桶是有序的，则整体是有序的，可以降低时间复杂度。桶排序算法的时间复杂度为$O(n + k)$，最好情况为$O(n + k)$，最坏情况为$O(n^2)$，空间复杂度$O(n + k)$，稳定排序。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def bucket_sort(self, nums):</span><br><span class="line">        min_num = min(nums)</span><br><span class="line">        max_num = max(nums)</span><br><span class="line">        # 桶的大小</span><br><span class="line">        bucket_range = (max_num - min_num) / len(nums)</span><br><span class="line">        # 桶数组</span><br><span class="line">        count_list = [[] for i in range(len(nums) + 1)]</span><br><span class="line">        # 向桶数组填数</span><br><span class="line">        for i in nums:</span><br><span class="line">            count_list[int((i - min_num) // bucket_range)].append(i)</span><br><span class="line">        nums.clear()</span><br><span class="line">        # 回填，这里桶内部排序直接调用了sorted</span><br><span class="line">        for i in count_list:</span><br><span class="line">            for j in sorted(i):</span><br><span class="line">                nums.append(j)</span><br><span class="line">        return nums</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a><font size="5" color="red">计数排序</font></h1><p>计数排序的原理也很简单，用数组下标统计元素出现的个数即可，因为数组下标是升序排列的，因此输出时只需要按照数组下标顺序依次输出即可，缺点也很明显，对非整数数组进行排序不太方便。计数排序算法的时间复杂度为$O(n + k)$，最好情况为$O(n + k)$，最坏情况为$O(n + k)$，空间复杂度$O(n)$，稳定排序。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def count_sort(self, nums):</span><br><span class="line">        # 找到最大最小值</span><br><span class="line">        min_num = min(nums)</span><br><span class="line">        max_num = max(nums)</span><br><span class="line">        # 计数列表</span><br><span class="line">        count_list = [0] * (max_num - min_num + 1)</span><br><span class="line">        # 计数</span><br><span class="line">        for i in nums:</span><br><span class="line">            count_list[i - min_num] += 1</span><br><span class="line">        nums.clear()</span><br><span class="line">        # 填回</span><br><span class="line">        for ind, i in enumerate(count_list):</span><br><span class="line">            while i != 0:</span><br><span class="line">                nums.append(ind + min_num)</span><br><span class="line">                i -= 1</span><br><span class="line">        return nums</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a><font size="5" color="red">希尔排序</font></h1><p>希尔排序类似于插入排序，但是不同点是插入排序增量为1，要插入的值从后向前，一个一个比较，而希尔排序是从后向前有间隔的比较，间隔不同时间复杂度不同，一般按照总长度二分的方式设置间隔。希尔排序算法的时间复杂度为$O(n^{1.3})$，最好情况为$O(n)$，最坏情况为$O(n^2)$，空间复杂度$O(1)$，不稳定排序。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def shell_sort(self, nums):</span><br><span class="line">        lens = len(nums)</span><br><span class="line">        # 初始步长设置为总长度的一半</span><br><span class="line">        gap = lens // 2</span><br><span class="line">        while gap &gt;= 1:</span><br><span class="line">            for i in range(gap, lens):</span><br><span class="line">                j = i</span><br><span class="line">                # 在每一组里面进行直接插入排序</span><br><span class="line">                while j &gt;= gap and nums[j - gap] &gt; nums[j]:</span><br><span class="line">                    nums[j], nums[j - gap] = nums[j - gap], nums[j]</span><br><span class="line">                    j -= gap</span><br><span class="line">            # 更新步长</span><br><span class="line">            gap = gap//2</span><br><span class="line">        return nums</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a><font size="5" color="red">堆排序</font></h1><p>堆排序使用了最大堆的概念，指父节点的值大于孩子节点的值，首先建立一个最大堆，然后交换最后一个元素与堆顶元素的值，接着对最大堆进行下沉操作，如果交换进去的值小于孩子节点，则使其下沉，更新最大堆，然后继续交换得到第二大的值，并重复上面的操作即可得到升序的数组。堆排序算法的时间复杂度为$O(nlog(n))$，最好情况为$O(nlog(n))$，最坏情况为$O(nlog(n))$，空间复杂度$O(1)$，不稳定排序。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def heap_sort(self, nums):</span><br><span class="line"></span><br><span class="line">        def heapify(arr, n, i):</span><br><span class="line">            largest = i</span><br><span class="line">            left = 2 * i + 1</span><br><span class="line">            right = 2 * i + 2</span><br><span class="line">            if left &lt; n and arr[i] &lt; arr[left]:</span><br><span class="line">                largest = left</span><br><span class="line">            if right &lt; n and arr[largest] &lt; arr[right]:</span><br><span class="line">                largest = right</span><br><span class="line">            if largest != i:</span><br><span class="line">                arr[i], arr[largest] = arr[largest], arr[i]</span><br><span class="line">                heapify(arr, n, largest)</span><br><span class="line"></span><br><span class="line">        n = len(nums)</span><br><span class="line">        # 创建一个长度为n的最大堆</span><br><span class="line">        for i in range(n, -1, -1):</span><br><span class="line">            heapify(nums, n, i)</span><br><span class="line">        for i in range(n - 1, 0, -1):</span><br><span class="line">            # 将大顶堆的堆顶元素和最后一个元素交换</span><br><span class="line">            nums[i], nums[0] = nums[0], nums[i]</span><br><span class="line">            # 创建一个长度为n - 1的最大堆</span><br><span class="line">            heapify(nums, i, 0)</span><br><span class="line">        return nums</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a><font size="5" color="red">基数排序</font></h1><p>基数排序类似于桶排序和计数排序，这个排序方式先设置10个桶，按照十进制位进行排序，先排个位，将个位按照升序放入桶内，然后将桶内的数字依次取出，然后排十位，百位，依次下取直到最高位为止，最后将最高位排序后的元素从桶内依次取出即可完成升序排列，缺点和计数排序相同，对非整数数组进行排序不太方便。基数排序算法的时间复杂度为$O(nk)$，最好情况为$O(nk)$，最坏情况为$O(nk)$，空间复杂度$O(n)$，稳定排序。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def radix_sort(self, nums):</span><br><span class="line">        # 记录当前正在排哪一位，最低位为1</span><br><span class="line">        i = 0</span><br><span class="line">        j = len(str(max(nums)))</span><br><span class="line">        while i &lt; j:</span><br><span class="line">            # 初始化桶数组</span><br><span class="line">            bucket_list = [[] for _ in range(10)]</span><br><span class="line">            for x in nums:</span><br><span class="line">                # 找到位置放入桶数组</span><br><span class="line">                bucket_list[(x // (10 ** i)) % 10].append(x)</span><br><span class="line">            nums.clear()</span><br><span class="line">            # 放回原序列</span><br><span class="line">            for x in bucket_list:</span><br><span class="line">                for y in x:</span><br><span class="line">                    nums.append(y)</span><br><span class="line">            i += 1</span><br><span class="line">        return nums</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  排序问题是算法经久不衰的考点之一，这十种方法各有利弊，没有绝对的好与不好，只有不同的适用场景罢了，其中最重要的几个排序方法是冒泡排序，快速排序，归并排序，堆排序。由于其面试出题过于频繁，很多公司已经不考这个简单的算法问题，但是小伙伴们也必须要掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 912&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>Loss黑科技</title>
    <link href="https://USTCcoder.github.io/2020/05/09/deep%20learning%20loss/"/>
    <id>https://USTCcoder.github.io/2020/05/09/deep learning loss/</id>
    <published>2020-05-09T10:30:17.000Z</published>
    <updated>2020-09-04T06:40:51.147Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Loss</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Loss(损失函数)</strong>:在深度学习任务中，一个最重要的概念就是损失函数，这里不区分损失函数，代价函数和目标函数，这里指的就是神经网络中最终需要优化的函数。<strong>损失函数决定者参数更新的方向</strong>，神经网络的<strong>输出y_pred要和真实值y_true进行比较，使两者的距离越小越好，这个距离的度量就是损失函数</strong>。打一个简单的比方，损失函数对于神经网络来说就有如灯塔之于船只，可以指明前进的方向。<strong>神经网络的参数更新是根据损失函数来确定的</strong>，如果损失函数设置错误，则会产生巨大偏差，甚至南辕北辙的效果，在这篇博客中，我向大家介绍一些常用的损失函数。<br><a id="more"></a></p><p><img src="/images/deep_learning/loss.png" alt="loss"></p><h1 id="MSE-Mean-Squared-Error，均方误差"><a href="#MSE-Mean-Squared-Error，均方误差" class="headerlink" title="MSE(Mean Squared Error，均方误差)"></a><font size="5" color="red">MSE(Mean Squared Error，均方误差)</font></h1><script type="math/tex; mode=display">MSE = \frac{1}{N} \sum_{i=1}^{N}{(y^{(i)} - f(x^{(i)}))^2}</script><p><strong>MSE</strong>：指<strong>参数估计值与参数真值之差平方的期望值</strong>，其中<strong>平方误差是估计值和实际值之差的平方，也称为(L2 Loss)</strong>，MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。在TensorFlow2.0中已经给我们提供了计算MSE的损失函数。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"># 使用函数，在最后一个维度上对数据求MSE，输出维度等于输入删去最后一个维度。</span><br><span class="line">keras.losses.mean_squared_error(y_true, y_pred)</span><br><span class="line"></span><br><span class="line"># 使用类对象，在最后一个维度上对数据求MSE，然后对结果求全局平均值，输出结果只有一个数。</span><br><span class="line">loss = keras.losses.MeanSquaredError()</span><br><span class="line">loss(y_true, y_pred)</span><br><span class="line"># 等价于keras.losses.MeanSquaredError()(y_true, y_pred)</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="MAE-Mean-Absolute-Error，平均绝对误差"><a href="#MAE-Mean-Absolute-Error，平均绝对误差" class="headerlink" title="MAE(Mean Absolute Error，平均绝对误差)"></a><font size="5" color="red">MAE(Mean Absolute Error，平均绝对误差)</font></h1><script type="math/tex; mode=display">MAE = \frac{1}{N} \sum_{i=1}^{N}{|y^{(i)} - f(x^{(i)})|}</script><p><strong>MAE</strong>：指<strong>参数估计值与参数真值绝对误差的平均值</strong>，其中<strong>绝对误差是估计值和实际值之间的距离，也称为(L1 Loss)</strong>，MAE能更好地反映预测值误差的实际情况。在TensorFlow2.0中已经给我们提供了计算MAE的损失函数。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"># 使用函数，在最后一个维度上对数据求MAE，输出维度等于输入删去最后一个维度。</span><br><span class="line">keras.losses.mean_absolute_error(y_true, y_pred)</span><br><span class="line"></span><br><span class="line"># 使用类对象，在最后一个维度上对数据求MAE，然后对结果求全局平均值，输出结果只有一个数。</span><br><span class="line">loss = keras.losses.MeanAbsoluteError()</span><br><span class="line">loss(y_true, y_pred)</span><br><span class="line"># 等价于tf.reduce_mean(keras.losses.mean_absolute_error(y_true, y_pred))</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="Smooth-L1-Loss-平滑L1损失"><a href="#Smooth-L1-Loss-平滑L1损失" class="headerlink" title="Smooth L1 Loss(平滑L1损失)"></a><font size="5" color="red">Smooth L1 Loss(平滑L1损失)</font></h1><script type="math/tex; mode=display">smooth_{L_1} =  \begin{cases} 0.5x^2 & |x| < 1 \\ |x| - 0.5 & |x| \ge 1 \end{cases}</script><p><strong>MAE的优点</strong>：<strong>鲁棒性更好</strong>，如果<strong>误差大于1，MSE会将误差放大，因此对异常数据更加敏感</strong>，为了调整异常值会牺牲很多样本。<br><strong>MSE的优点</strong>：<strong>稳定性更好</strong>，如果<strong>误差小于1，MSE会将误差缩小，产生一个较小的波动，可以更加细化模型</strong>，MAE可能会跳过这个微小区域，到达另一个误差更大的地方。<br><strong>Smooth L1 Loss</strong>可以<strong>完美的结合MAE和MSE的优点，在误差大于1的情况下，不会放大误差牺牲样本，在误差小于1的情况下，还能够细化模型</strong>，因此是一种较好的损失函数，在<strong>目标检测</strong>算法中常常使用。<br><img src="/images/deep_learning/smoothl1.png" alt="smoothl1"></p><h1 id="Binary-Cross-Entropy-二分类交叉熵损失函数"><a href="#Binary-Cross-Entropy-二分类交叉熵损失函数" class="headerlink" title="Binary Cross Entropy(二分类交叉熵损失函数)"></a><font size="5" color="red">Binary Cross Entropy(二分类交叉熵损失函数)</font></h1><script type="math/tex; mode=display">Binary \ Cross \ Entropy =  -\frac{1}{N} \sum_{i=1}^{N}{(y^{(i)} \cdot \ln{(f(x^{(i)})}) + (1 - y^{(i)}) \cdot \ln{(1 - f(x^{(i)})})}</script><p><strong>Binary Cross Entropy</strong>：用来<strong>评估当前训练得到的概率分布与真实分布的差异情况</strong>，它刻画的是实际概率与期望概率的距离，也就是交叉熵的值越小，两个概率分布就越接近，减少交叉熵损失就是在提高模型的预测准确率。在<strong>二分类中，每个数据独立计算交叉熵，和同一维度其他数据无关，允许多个1同时出现，经常配合sigmoid激活函数使用</strong>。在TensorFlow2.0中已经给我们提供了计算Binary Cross Entropy的损失函数。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"># 使用函数，对每一个数据做二分类交叉熵，输入维度和输出维度相同，对预测值y_pred先计算sigmoid值，然后计算交叉熵。</span><br><span class="line">tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)</span><br><span class="line"></span><br><span class="line"># 使用函数，对每一个数据做二分类交叉熵，并在最后一个维度上计算平均值，输出维度等于输入删去最后一个维度，from_logits=False(默认)则不计算Sigmoid的值，from_logits=True则先计算Sigmoid的值，然后计算交叉熵。</span><br><span class="line">keras.losses.binary_crossentropy(y_true, y_pred, from_logits=True)</span><br><span class="line"># 等价于keras.losses.binary_crossentropy(y_true, keras.activations.sigmoid(y_pred), from_logits=False)</span><br><span class="line"># 等价于tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred), axis=-1)</span><br><span class="line"></span><br><span class="line"># 使用类对象，对每一个数据做二分类交叉熵，然后对结果求全局平均值，输出结果只有一个数。from_logits=False(默认)则不计算Sigmoid的值，from_logits=True则先计算Sigmoid的值，然后计算交叉熵。</span><br><span class="line">loss = keras.losses.BinaryCrossentropy(from_logits=True)</span><br><span class="line">loss(y_true, y_pred)</span><br><span class="line"># 等价于tf.reduce_mean(keras.losses.binary_crossentropy(y_true, y_pred, from_logits=True))</span><br><span class="line"># 等价于tf.reduce_mean(keras.losses.binary_crossentropy(y_true, keras.activations.sigmoid(y_pred), from_logits=False))</span><br><span class="line"># 等价于tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred))</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="Categorical-Cross-Entropy-多分类交叉熵损失函数"><a href="#Categorical-Cross-Entropy-多分类交叉熵损失函数" class="headerlink" title="Categorical Cross Entropy(多分类交叉熵损失函数)"></a><font size="5" color="red">Categorical Cross Entropy(多分类交叉熵损失函数)</font></h1><script type="math/tex; mode=display">Categorical \ Cross \ Entropy =  -\frac{1}{N} \sum_{i=1}^{N}{(y^{(i)} \cdot \ln{(f(x^{(i)})))}}</script><p><strong>Categorical Cross Entropy</strong>：用来<strong>评估当前训练得到的概率分布与真实分布的差异情况</strong>，它刻画的是实际概率与期望概率的距离，也就是交叉熵的值越小，两个概率分布就越接近，减少交叉熵损失就是在提高模型的预测准确率。在<strong>多分类中，在某个维度上计算交叉熵，在该维度上其他数据一般只有一个1出现，其他全为0，经常配合Softmax激活函数使用</strong>。在TensorFlow2.0中已经给我们提供了计算Categorical Cross Entropy的损失函数。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"># 使用函数，对最后一个维度做多分类交叉熵，输出维度等于输入删去最后一个维度，对预测值先求Softmax值，然后计算交叉熵。</span><br><span class="line">tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)</span><br><span class="line"></span><br><span class="line"># 使用函数，对最后一个维度做多分类交叉熵，输出维度等于输入删去最后一个维度，from_logits=False(默认)则不计算Softmax的值，from_logits=True则先计算Softmax的值，然后计算交叉熵。</span><br><span class="line">keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True)</span><br><span class="line"># 等价于keras.losses.categorical_crossentropy(y_true, keras.activations.softmax(y_pred), from_logits=False)</span><br><span class="line"># 等价于tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)</span><br><span class="line"></span><br><span class="line"># 使用类对象，对最后一个维度做多分类交叉熵，然后对结果求全局平均值，输出结果只有一个数。from_logits=False(默认)则不计算Softmax的值，from_logits=True则先计算Softmax的值，然后计算交叉熵。</span><br><span class="line">loss = keras.losses.CategoricalCrossentropy(from_logits=True)</span><br><span class="line">loss(y_true, y_pred)</span><br><span class="line"># 等价于tf.reduce_mean(keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True))</span><br><span class="line"># 等价于tf.reduce_mean(keras.losses.categorical_crossentropy(y_true, keras.activations.softmax(y_pred), from_logits=False))</span><br><span class="line"># 等价于tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="Focal-Loss-聚焦损失"><a href="#Focal-Loss-聚焦损失" class="headerlink" title="Focal Loss(聚焦损失)"></a><font size="5" color="red">Focal Loss(聚焦损失)</font></h1><script type="math/tex; mode=display">Focal \ Loss =  \begin{cases} - \alpha \cdot (1 - f(x^{(i)}))^{\gamma} \cdot \ln{(f(x^{(i)}))} & y^{(i)} = 1 \\ - (1 - \alpha) \cdot (f(x^{(i)}))^{\gamma} \cdot \ln{(1 - f(x^{(i)}))} & y^{(i)} = 0 \end{cases}</script><p><strong>Focal Loss(聚焦损失)</strong>：是<strong>何凯明于2017年提出的一种解决目标检测算法中，一步法正负样本比例严重失衡的问题</strong>。他认为一步法和两步法的表现差异<strong>主要原因是大量背景类别导致</strong>的，因此设计了一个<strong>简单密集型网络RetinaNet来训练，保证速度的同时达到了精度最优</strong>。Focal Loss中有两个重要特点，<strong>引入$ \alpha $控制正负样本的权重，引入$ \gamma $控制容易分类和难分类样本的权重</strong>。</p><p><strong>$ \alpha $</strong>：在这里称之为<strong>平衡因子</strong>，它的作用是<strong>平衡正负样本的占比</strong>，在<strong>论文中取0.25</strong>。为什么负样本多，反而占比还大呢？因为正样本比负样本更难区分，负样本为背景区域，较易区分，损失函数较小，因此占比较大。正样本难以区分，损失函数较大，因此占比较小。</p><p><strong>$ \gamma $</strong>：在这里称之为<strong>调制权重</strong>，他的作用是<strong>关注分类的难易程度</strong>，在<strong>论文中取2</strong>。一个正样本，如果预测概率越大，则$1-f(x^{(i)})$越小，那么$(1-f(x^{(i)}))^{\gamma}$就更小，说明让网络不要过多关心容易区分的问题，反之，如果预测概率较小，则$1-f(x^{(i)})$较大，那么$(1-f(x^{(i)}))^{\gamma}$就相对较大，说明让网络多关心难区分的问题。</p><p><strong>Focal Loss和Binary Cross Entropy</strong>：当参数<strong>满足$ \alpha=0.5，\gamma=0 $时Focal Loss就变成了Binary Cross Entropy</strong>，Binary Cross Entropy中一个正样本，预测结果为0.8时的损失为$- \ln{0.8}=0.223$，预测结果为0.2时的损失为$- \ln{0.2}=1.609$，两者相差8倍，而在Focal Loss中一个正样本，预测结果为0.8时的损失为$- 0.25 \times (1-0.8)^2 \times \ln{0.8}=0.00223$，预测结果为0.2时的损失为$- 0.25 \times (1-0.2)^2 \times \ln{0.2}=0.2575$，两者相差115倍，此时可以说网络更加关心预测为0.2时产生的损失，这就是Focal Loss受到广泛关注的特点。但是Tensorflow2.0没有给我们提供Focal Loss损失函数，需要我们自己设计。大家可以参考目标检测文章中RetinaNet中的相关内容，里面有Focal Loss的具体实现代码。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  在选择损失函数时，我们首先要<strong>判断任务类型</strong>，是分类任务还是回归任务。<strong>分类任务可以考虑交叉熵损失函数，回归任务可以考虑MSE或者MAE损失函数</strong>。但是在实际工程应用之中，往往不是一个简单的损失函数就能解决的，有时<strong>既用到分类损失函数，又用到回归损失函数</strong>，而且还要为两者之间设置权重系数。有时需要根据需要自己设计一些属于特殊数据集或者特殊模型的损失函数。因此需要小伙伴们多多尝试，总结经验，最终会成为一代大牛，</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Loss&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>Addition VS Concatenate</title>
    <link href="https://USTCcoder.github.io/2020/05/08/deep%20learning%20addition_vs_concatenate/"/>
    <id>https://USTCcoder.github.io/2020/05/08/deep learning addition_vs_concatenate/</id>
    <published>2020-05-08T13:25:59.000Z</published>
    <updated>2020-05-12T02:41:58.813Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Addition VS Concatenate</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>(Feature fusion)特征融合</strong>:VGG网络提出以后，给人们一种印象，深度学习越复杂，参数越多，会有越强的表达能力。但是在深度神经网络的研究过程中，发现<strong>到达一定深度后，一味地增加网络的深度并不能带来效果的提升，反而会导致网络收敛变慢</strong>。这时需要引入一些其他的方法既能提高网络的表达能力，也不会使网络收敛速度大大降低。两种特征融合方法<strong>(Addition和Concatenate)</strong>随着时代的发展产生了。相信小伙伴们也已经有所了解，在这里我系统的整理一下它们之间的区别。<br><a id="more"></a></p><p><img src="/images/deep_learning/addition.png" alt="addition"></p><h1 id="Addition"><a href="#Addition" class="headerlink" title="Addition"></a><font size="5" color="red">Addition</font></h1><p>  <strong>Addition</strong>，即<strong>残差相加</strong>，最经典的模型就是<strong>ResNet</strong>。关于ResNet的有关代码解析可以参考特征提取网络中的一篇博客<font size="4"><a href="https://ustccoder.github.io/2020/03/09/feature_extraction ResNet/">ResNet</a></font><br><img src="/images/Feature_extraction/ResNet.png" alt="ResNet"><br>优点：</p><ol><li>使用残差结构，使得深层网络包含浅层网络的信息，因此<strong>反向传播时可以通过不同的分支到达浅层网络</strong>。并且<strong>梯度中包含常数项</strong>，增加了梯度值，<strong>解决了梯度消失的问题</strong>。</li><li><strong>增加了网络的泛化能力</strong>，和VGG这种串行的网络相比，删除一层可能严重破坏网络结构，而ResNet删除一层，性能不会产生很大的退化。</li><li>浅层网络代表浅层特征，深层网络代表深层特征，浅层特征可能指代一些纹理信息，深层特征可能指代一些轮廓信息。<strong>残差结构将浅层特征与深层特征进行相加，实现特征互补，因此网络的表达能力更强</strong>。</li></ol><h1 id="Concatenate"><a href="#Concatenate" class="headerlink" title="Concatenate"></a><font size="5" color="red">Concatenate</font></h1><p>  <strong>Concatenate</strong>，即<strong>通道合并</strong>，最经典的模型就是<strong>DenseNet</strong>。关于DenseNet的有关代码解析可以参考特征提取网络中的一篇博客<font size="4"><a href="https://ustccoder.github.io/2020/03/16/feature_extraction DenseNet/">DenseNet</a></font><br><img src="/images/Feature_extraction/DenseNet.png" alt="DenseNet"><br>优点：</p><ol><li>使用通道合并，使得深层网络包含浅层网络的信息，因此<strong>反向传播时可以通过不同的分支到达浅层网络</strong>，<strong>解决了梯度消失的问题</strong>。</li><li><strong>增加了网络的泛化能力</strong>，和VGG这种串行的网络相比，删除一层可能严重破坏网络结构，而DenseNet删除一层，性能不会产生很大的退化。</li><li>浅层网络代表浅层特征，深层网络代表深层特征，浅层特征可能指代一些纹理信息，深层特征可能指代一些轮廓信息。<strong>通道合并可以理解为从不同的角度观察特征图，使重要的细节信息从多方位角度保存下来，然后与深层特征结合互补，因此网络的表达能力更强</strong>。</li></ol><h1 id="Addition与Concatenate的区别"><a href="#Addition与Concatenate的区别" class="headerlink" title="Addition与Concatenate的区别"></a><font size="5" color="red">Addition与Concatenate的区别</font></h1><ol><li>最明显的区别是<strong>数据维度的变化</strong>，<strong>Addition操作要求数据维度完全相同</strong>，并且<em>相加后的维度和之前相同<strong>，</strong>Concatenate操作要求数据可以有一个维度不相同<strong>，一般是通道数维度，即batch_size，图像的高和宽都是相同的，并且</strong>合并后的通道数是参与合并的数据通道数之和*</em>。</li><li>Addtion是在<strong>保持特征维度的情况下，增加了特征的信息量</strong>，而Concatenate是通过<strong>增加特征维度的情况下，增加了特征的信息量，但是每一维特征的信息量没有变化</strong>。</li><li>因为Addition和Concatenate在数据维度上的差异，当后面加入卷积层时，导致<strong>Addition的计算量比Concatenate小得多，更加节省参数和计算量</strong>。</li><li><strong>Concatenate将原始特征直接拼接，让网络自己学习如何融合特征，信息不会丢失，而且更加体现出多角度融合的概念</strong>，而<strong>Addition则将特征进行相加，相当于指定了一种特征融合的方式</strong>，因此可以认为<strong>Addition是Concatenate的一种特殊形式</strong>。</li></ol><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  在这个博客中简单直白的讲述了两种特征融合方法的区别，在使用中根据小伙伴们的需要，如果<strong>参数量较小，则可以使用多角度Concatenate来增加信息量，并让网络自我学习更新参数</strong>。<strong>如果参数量较大，则可以使用Addition来增加信息量，给网络指定一个方向，让网络学习更新参数</strong>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Addition VS Concatenate&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>Normalization黑科技</title>
    <link href="https://USTCcoder.github.io/2020/05/07/deep%20learning%20BN_VS_LN_VS_IN_VS_GN/"/>
    <id>https://USTCcoder.github.io/2020/05/07/deep learning BN_VS_LN_VS_IN_VS_GN/</id>
    <published>2020-05-07T09:53:31.000Z</published>
    <updated>2020-08-16T16:13:34.382Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Normalization</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Normalization(标准化)</strong>:深度神经网络模型训练困难，其中一个重要的现象就是<strong>ICS(Internal Covariate Shift，内部协变量偏移)</strong>，其中<strong>解决的方法就是Normalization</strong>，现在标准化成为深度学习<strong>必备神器</strong>，今天带小伙伴们看一看，瞧一瞧。<br><a id="more"></a></p><h1 id="ICS的解释"><a href="#ICS的解释" class="headerlink" title="ICS的解释"></a><font size="5" color="red">ICS的解释</font></h1><p><strong>ICS(Internal Covariate Shift，内部协变量偏移)</strong>：将神经网络的每一层的输入作为一个分布来看代，由于神经网络的参数是随机的，因此可能会<strong>导致相同的输入分布却得到了不同的输出分布</strong>。随着网络层数的加深，<strong>输入分布再经过多次非线性变换后，已经被改变</strong>，但是其<strong>标签还是一致的</strong>，这就有一种不协调的感觉，这可能会带来下面几种问题。</p><ol><li>在训练的过程中，<strong>网络需要不断适应新的输入数据分布，所以会大大降低学习速度</strong>。</li><li>由于<strong>参数的分布不同，所以可能导致很多数据落入饱和区，使得学习过早停止</strong>。</li><li>某些<strong>参数分布偏离太大，对其他层或者输出产生了巨大影响</strong>。</li></ol><h1 id="Normalization原理分析"><a href="#Normalization原理分析" class="headerlink" title="Normalization原理分析"></a><font size="5" color="red">Normalization原理分析</font></h1><p>  为了解决上述ICS问题，我们需要将变量分布变成相同分布的，这使我们想到了<strong>标准化操作</strong>。<br><img src="/images/deep_learning/normal.png" alt="normal"></p><ol><li>我们可以通过$ \hat{x} = \frac{x - \mu}{\sigma} $，$ \mu $是平移参数，$ \sigma $是缩放参数<strong>将数据变成符合均值为0，方差为1的标准分布</strong>。</li><li>我们再通过$ y = \gamma \cdot \hat{x} + \beta $, $ \beta $是再平移参数，$ \gamma $是再缩放参数<strong>将数据变成符合均值为$ \beta $，方差为$ {\gamma}^2 $的标准分布</strong>。</li></ol><p><font size="3" color="red">  奇怪的知识增加了？？？为什么第一步得到标准分布之后，第二步又给变走了？</font><br>  是这样的，首先为了<strong>保证模型的表达能力不因为规范化而下降</strong>，如果没有再平移和缩放，会导致输入的参数分布可能发生较大的变化，这样可能会对模型的表达能力产生影响。其次这两组参数是意义上完全不同的概念，<strong>$ \mu $和$ \sigma $受到上一层输入的影响，$ \beta $和$ \gamma $是独立的，与输入无关</strong>，是网络后来加入的，<strong>会在接下来训练过程中不断学习的，也是为了尊重神经网络的学习结果</strong>。因此这两步是有必要的。</p><h1 id="Normalization优点"><a href="#Normalization优点" class="headerlink" title="Normalization优点"></a><font size="5" color="red">Normalization优点</font></h1><ol><li><strong>解决了ICS(Internal Covariate Shift，内部协变量偏移)问题</strong>。</li><li><strong>加快学习速度，防止梯度消失现象</strong>，因为标准化后，会将数据拉回到0附近，对于Sigmoid，tanh激活函数来说，可能就会<strong>从饱和区拉回到线性区</strong>，因此可以防止梯度消失现象。</li><li><strong>减弱对初始化的依赖性</strong>，因为参数需要进行标准化，所以初始化参数时，不用限制较大。</li><li><strong>可以对抗over fitting</strong>，因为会将输入进行变化，当输入导致均值产生偏移，没关系，后面还有Normalization，会<strong>对偏移进行修正</strong>，所以会起到一些防止过拟合的作用。</li></ol><h1 id="常见的Normalization"><a href="#常见的Normalization" class="headerlink" title="常见的Normalization"></a><font size="5" color="red">常见的Normalization</font></h1><p><img src="/images/deep_learning/normalization.png" alt="normalization"><br>为了说明的清晰，我们<strong>将输入的feature map shape记为[N, H, W, C]</strong>，其中<strong>N代表batch_size，H，W代表特征图的高和宽，C代表特征图的通道数</strong>。<br>并且为了直观说明，将feature map看作一个学校，N代表年级数量，规定值为3，C代表每个年级的班级数量，规定值为6，H和W代表班级的每一排和每一列，规定值都为10。</p><h2 id="BN-Batch-Normalization，2015"><a href="#BN-Batch-Normalization，2015" class="headerlink" title="BN(Batch Normalization，2015)"></a><font color="red">BN(Batch Normalization，2015)</font></h2><p><strong>BN(Batch Normalization)</strong>：<strong>保留通道的维度C，对N，H，W做C次标准化</strong>，相当于分别按照班级将所有年级所有同学的成绩进行标准化(如一年级一班，二年级一班，三年级一班的所有同学进行标准化，然后再将一年级二班，二年级二班，三年级二班的所有同学进行标准化，直到将一年级六班，二年级六班，三年级六班的所有同学进行标准化，一共做了6次标准化)。<strong>batch_size越大，效果越好，适合固定深度的前向神经网络，如CNN，不适用于RNN</strong>。</p><h2 id="自定义BN层"><a href="#自定义BN层" class="headerlink" title="自定义BN层"></a><font color="red">自定义BN层</font></h2><p>在TensorFlow2.0中已经给我们提供了BN层的类keras.layers.BatchNormalization，使用时直接调用即可，这里也给出了自定义BN层的方法。</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class BatchNormalization(keras.layers.Layer):</span><br><span class="line">    def __init__(self, beta_initializer='zeros', gamma_initializer='ones',</span><br><span class="line">                 beta_regularizer=None, gamma_regularizer=None,</span><br><span class="line">                 beta_constraint=None, gamma_constraint=None, epsilon=1e-5,</span><br><span class="line">                 **kwargs):</span><br><span class="line">        super(BatchNormalization, self).__init__(**kwargs)</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.beta_initializer = keras.initializers.get(beta_initializer)</span><br><span class="line">        self.gamma_initializer = keras.initializers.get(gamma_initializer)</span><br><span class="line">        self.beta_regularizer = keras.regularizers.get(beta_regularizer)</span><br><span class="line">        self.gamma_regularizer = keras.regularizers.get(gamma_regularizer)</span><br><span class="line">        self.beta_constraint = keras.constraints.get(beta_constraint)</span><br><span class="line">        self.gamma_constraint = keras.constraints.get(gamma_constraint)</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        assert len(input_shape) == 4</span><br><span class="line">        self.gamma = self.add_weight(shape=(input_shape[-1],), name='gamma', initializer=self.gamma_initializer,</span><br><span class="line">                                     regularizer=self.gamma_regularizer, constraint=self.gamma_constraint)</span><br><span class="line">        self.beta = self.add_weight(shape=(input_shape[-1],), name='beta', initializer=self.beta_initializer,</span><br><span class="line">                                    regularizer=self.beta_regularizer, constraint=self.beta_constraint)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        mean, variance = tf.nn.moments(inputs, axes=[0, 1, 2], keepdims=True)</span><br><span class="line">        outputs = (inputs - mean) / tf.sqrt(variance + self.epsilon)</span><br><span class="line">        return outputs * self.gamma + self.beta</span><br><span class="line"></span><br><span class="line">    def get_config(self):</span><br><span class="line">        config = {</span><br><span class="line">            'epsilon': self.epsilon,</span><br><span class="line">            'beta_initializer': keras.initializers.serialize(self.beta_initializer),</span><br><span class="line">            'gamma_initializer': keras.initializers.serialize(self.gamma_initializer),</span><br><span class="line">            'beta_regularizer': keras.regularizers.serialize(self.beta_regularizer),</span><br><span class="line">            'gamma_regularizer': keras.regularizers.serialize(self.gamma_regularizer),</span><br><span class="line">            'beta_constraint': keras.constraints.serialize(self.beta_constraint),</span><br><span class="line">            'gamma_constraint': keras.constraints.serialize(self.gamma_constraint)</span><br><span class="line">        }</span><br><span class="line">        base_config = super(BatchNormalization, self).get_config()</span><br><span class="line"></span><br><span class="line">        return dict(list(base_config.items()) + list(config.items()))</span><br></pre></td></tr></tbody></table></figure><h2 id="LN-Layer-Normalization，2016"><a href="#LN-Layer-Normalization，2016" class="headerlink" title="LN(Layer Normalization，2016)"></a><font color="red">LN(Layer Normalization，2016)</font></h2><p><strong>LN(Layer Normalization)</strong>：<strong>保留batch_size的维度N，对H，W，C做N次标准化</strong>，相当于分别按照年级将所有班级所有同学的成绩进行标准化(如一年级一班，一年级二班直到一年级六班的所有同学进行标准化，然后再将二年级一班，二年级二班直到二年级六班的所有同学进行标准化，最后将三年级一班，三年级二班直到三年级六班的所有同学进行标准化，一共做了3次标准化)。<strong>通道数越大，效果越好，不依赖batch_size的大小，适合深度不固定的网络，如RNN，不适用于CNN</strong>。</p><h2 id="自定义LN层"><a href="#自定义LN层" class="headerlink" title="自定义LN层"></a><font color="red">自定义LN层</font></h2><p>在TensorFlow2.0中已经给我们提供了LN层的类keras.layers.LayerNormalization，使用时直接调用即可，这里也给出了自定义LN层的方法。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class LayerNormalization(keras.layers.Layer):</span><br><span class="line">    def __init__(self, beta_initializer='zeros', gamma_initializer='ones',</span><br><span class="line">                 beta_regularizer=None, gamma_regularizer=None,</span><br><span class="line">                 beta_constraint=None, gamma_constraint=None, epsilon=1e-5,</span><br><span class="line">                 **kwargs):</span><br><span class="line">        super(LayerNormalization, self).__init__(**kwargs)</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.beta_initializer = keras.initializers.get(beta_initializer)</span><br><span class="line">        self.gamma_initializer = keras.initializers.get(gamma_initializer)</span><br><span class="line">        self.beta_regularizer = keras.regularizers.get(beta_regularizer)</span><br><span class="line">        self.gamma_regularizer = keras.regularizers.get(gamma_regularizer)</span><br><span class="line">        self.beta_constraint = keras.constraints.get(beta_constraint)</span><br><span class="line">        self.gamma_constraint = keras.constraints.get(gamma_constraint)</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        assert len(input_shape) == 4</span><br><span class="line">        self.gamma = self.add_weight(shape=(input_shape[-1],), name='gamma', initializer=self.gamma_initializer,</span><br><span class="line">                                     regularizer=self.gamma_regularizer, constraint=self.gamma_constraint)</span><br><span class="line">        self.beta = self.add_weight(shape=(input_shape[-1],), name='beta', initializer=self.beta_initializer,</span><br><span class="line">                                    regularizer=self.beta_regularizer, constraint=self.beta_constraint)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        mean, variance = tf.nn.moments(inputs, axes=[1, 2, 3], keepdims=True)</span><br><span class="line">        outputs = (inputs - mean) / tf.sqrt(variance + self.epsilon)</span><br><span class="line">        return outputs * self.gamma + self.beta</span><br><span class="line"></span><br><span class="line">    def get_config(self):</span><br><span class="line">        config = {</span><br><span class="line">            'epsilon': self.epsilon,</span><br><span class="line">            'beta_initializer': keras.initializers.serialize(self.beta_initializer),</span><br><span class="line">            'gamma_initializer': keras.initializers.serialize(self.gamma_initializer),</span><br><span class="line">            'beta_regularizer': keras.regularizers.serialize(self.beta_regularizer),</span><br><span class="line">            'gamma_regularizer': keras.regularizers.serialize(self.gamma_regularizer),</span><br><span class="line">            'beta_constraint': keras.constraints.serialize(self.beta_constraint),</span><br><span class="line">            'gamma_constraint': keras.constraints.serialize(self.gamma_constraint)</span><br><span class="line">        }</span><br><span class="line">        base_config = super(LayerNormalization, self).get_config()</span><br><span class="line"></span><br><span class="line">        return dict(list(base_config.items()) + list(config.items()))</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="IN-Instance-Normalization，2017"><a href="#IN-Instance-Normalization，2017" class="headerlink" title="IN(Instance Normalization，2017)"></a><font color="red">IN(Instance Normalization，2017)</font></h2><p><strong>IN(Instance Normalization)</strong>：<strong>保留batch_size的维度N和通道的维度C，对H，W做NxC次标准化</strong>，相当于分别按照年级和班级将所有同学的成绩进行标准化(如一年级一班的所有同学进行标准化，一年级二班的所有同学进行标准化，直到一年级六班的所有同学进行标准化，然后再将二年级一班的所有同学进行标准化，二年级二班的所有同学进行标准化，直到二年级六班的所有同学进行标准化，最后将三年级一班的所有同学进行标准化，三年级二班的所有同学进行标准化，直到三年级六班的所有同学进行标准化，一共做了18次标准化)。最初<strong>用于生成式对抗网络中的风格迁移，生成结果依赖于某个图像实例，只对特征图的高和宽进行标准化，保持图像实例之间的独立</strong>。</p><h2 id="自定义IN层"><a href="#自定义IN层" class="headerlink" title="自定义IN层"></a><font color="red">自定义IN层</font></h2><p>在TensorFlow2.0中没有提供IN层的类，需要自己定义，这里也给出了自定义IN层的方法。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class InstanceNormalization(keras.layers.Layer):</span><br><span class="line">    def __init__(self, beta_initializer='zeros', gamma_initializer='ones',</span><br><span class="line">                 beta_regularizer=None, gamma_regularizer=None,</span><br><span class="line">                 beta_constraint=None, gamma_constraint=None, epsilon=1e-5,</span><br><span class="line">                 **kwargs):</span><br><span class="line">        super(InstanceNormalization, self).__init__(**kwargs)</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.beta_initializer = keras.initializers.get(beta_initializer)</span><br><span class="line">        self.gamma_initializer = keras.initializers.get(gamma_initializer)</span><br><span class="line">        self.beta_regularizer = keras.regularizers.get(beta_regularizer)</span><br><span class="line">        self.gamma_regularizer = keras.regularizers.get(gamma_regularizer)</span><br><span class="line">        self.beta_constraint = keras.constraints.get(beta_constraint)</span><br><span class="line">        self.gamma_constraint = keras.constraints.get(gamma_constraint)</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        assert len(input_shape) == 4</span><br><span class="line">        self.gamma = self.add_weight(shape=(input_shape[-1],), name='gamma', initializer=self.gamma_initializer,</span><br><span class="line">                                     regularizer=self.gamma_regularizer, constraint=self.gamma_constraint)</span><br><span class="line">        self.beta = self.add_weight(shape=(input_shape[-1],), name='beta', initializer=self.beta_initializer,</span><br><span class="line">                                    regularizer=self.beta_regularizer, constraint=self.beta_constraint)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        mean, variance = tf.nn.moments(inputs, axes=[1, 2], keepdims=True)</span><br><span class="line">        outputs = (inputs - mean) / tf.sqrt(variance + self.epsilon)</span><br><span class="line">        return outputs * self.gamma + self.beta</span><br><span class="line"></span><br><span class="line">    def get_config(self):</span><br><span class="line">        config = {</span><br><span class="line">            'epsilon': self.epsilon,</span><br><span class="line">            'beta_initializer': keras.initializers.serialize(self.beta_initializer),</span><br><span class="line">            'gamma_initializer': keras.initializers.serialize(self.gamma_initializer),</span><br><span class="line">            'beta_regularizer': keras.regularizers.serialize(self.beta_regularizer),</span><br><span class="line">            'gamma_regularizer': keras.regularizers.serialize(self.gamma_regularizer),</span><br><span class="line">            'beta_constraint': keras.constraints.serialize(self.beta_constraint),</span><br><span class="line">            'gamma_constraint': keras.constraints.serialize(self.gamma_constraint)</span><br><span class="line">        }</span><br><span class="line">        base_config = super(InstanceNormalization, self).get_config()</span><br><span class="line"></span><br><span class="line">        return dict(list(base_config.items()) + list(config.items()))</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="GN-Group-Normalization，2018"><a href="#GN-Group-Normalization，2018" class="headerlink" title="GN(Group Normalization，2018)"></a><font color="red">GN(Group Normalization，2018)</font></h2><p><strong>GN(Group Normalization)</strong>：为了解决BN中对较小batch_size效果较差的问题，将通道数C分为G组，每组有C/G个通道数，然后将这些通道数中的元素标准化，<strong>做NxC/G次标准化</strong>，如果将班级数量分为2组，相当于分别按照年级先将班级分为2组，一共分成6组，然后对所有组所有同学的成绩进行标准化(如一年级一班，一年级二班，一年级三班的所有同学进行标准化，一年级四班，一年级五班，一年级六班的所有同学进行标准化，然后再将二年级一班，二年级二班，二年级三班的所有同学进行标准化，二年级四班，二年级五班，二年级六班的所有同学进行标准化，最后将三年级一班，三年级二班，三年级三班的所有同学进行标准化，三年级四班，三年级五班，三年级六班的所有同学进行标准化，一共做了6次标准化)。分组之后，<strong>不依赖batch_size的大小，因此不会被batch_size约束</strong>。</p><h2 id="自定义GN层"><a href="#自定义GN层" class="headerlink" title="自定义GN层"></a><font color="red">自定义GN层</font></h2><p>在TensorFlow2.0中没有提供GN层的类，需要自己定义，这里也给出了自定义GN层的方法。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class GroupNormalization(keras.layers.Layer):</span><br><span class="line">    def __init__(self, group=32, beta_initializer='zeros', gamma_initializer='ones',</span><br><span class="line">                 beta_regularizer=None, gamma_regularizer=None,</span><br><span class="line">                 beta_constraint=None, gamma_constraint=None, epsilon=1e-5,</span><br><span class="line">                 **kwargs):</span><br><span class="line">        super(GroupNormalization, self).__init__(**kwargs)</span><br><span class="line">        self.group = group</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.beta_initializer = keras.initializers.get(beta_initializer)</span><br><span class="line">        self.gamma_initializer = keras.initializers.get(gamma_initializer)</span><br><span class="line">        self.beta_regularizer = keras.regularizers.get(beta_regularizer)</span><br><span class="line">        self.gamma_regularizer = keras.regularizers.get(gamma_regularizer)</span><br><span class="line">        self.beta_constraint = keras.constraints.get(beta_constraint)</span><br><span class="line">        self.gamma_constraint = keras.constraints.get(gamma_constraint)</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        assert len(input_shape) == 4</span><br><span class="line">        assert input_shape[-1] &gt;= self.group</span><br><span class="line">        assert input_shape[-1] % self.group == 0</span><br><span class="line"></span><br><span class="line">        self.gamma = self.add_weight(shape=(input_shape[-1],), name='gamma', initializer=self.gamma_initializer,</span><br><span class="line">                                     regularizer=self.gamma_regularizer, constraint=self.gamma_constraint)</span><br><span class="line">        self.beta = self.add_weight(shape=(input_shape[-1],), name='beta', initializer=self.beta_initializer,</span><br><span class="line">                                    regularizer=self.beta_regularizer, constraint=self.beta_constraint)</span><br><span class="line">        self.built = True</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        inputs = tf.reshape(inputs, shape=[-1, inputs.shape[1], inputs.shape[2], self.group, inputs.shape[-1] // self.group])</span><br><span class="line">        mean, variance = tf.nn.moments(inputs, axes=[1, 2, 3], keepdims=True)</span><br><span class="line">        outputs = (inputs - mean) / tf.sqrt(variance + self.epsilon)</span><br><span class="line">        outputs = tf.reshape(outputs, shape=[-1, inputs.shape[1], inputs.shape[2], inputs.shape[3] * inputs.shape[4]])</span><br><span class="line">        return outputs * self.gamma + self.beta</span><br><span class="line"></span><br><span class="line">    def get_config(self):</span><br><span class="line">        config = {</span><br><span class="line">            'epsilon': self.epsilon,</span><br><span class="line">            'beta_initializer': keras.initializers.serialize(self.beta_initializer),</span><br><span class="line">            'gamma_initializer': keras.initializers.serialize(self.gamma_initializer),</span><br><span class="line">            'beta_regularizer': keras.regularizers.serialize(self.beta_regularizer),</span><br><span class="line">            'gamma_regularizer': keras.regularizers.serialize(self.gamma_regularizer),</span><br><span class="line">            'beta_constraint': keras.constraints.serialize(self.beta_constraint),</span><br><span class="line">            'gamma_constraint': keras.constraints.serialize(self.gamma_constraint)</span><br><span class="line">        }</span><br><span class="line">        base_config = super(GroupNormalization, self).get_config()</span><br><span class="line">        return dict(list(base_config.items()) + list(config.items()))</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  虽然自定义标准化层看起来非常复杂，其实本质代码只有call函数中的几行而已，而且<strong>根据不同的Normalization，只需要修改求均值和方差的轴即可</strong>。<strong>Normalization是卷积神经网络的Trick(小技巧)</strong>，自从Normalization被提出以后，几乎各个网络都能看到它的身影，灵活掌握不同Normalization，是小伙伴们需要达成的目标。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Normalization&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>Upsampling黑科技</title>
    <link href="https://USTCcoder.github.io/2020/05/06/deep%20learning%20upsampling/"/>
    <id>https://USTCcoder.github.io/2020/05/06/deep learning upsampling/</id>
    <published>2020-05-06T09:47:37.000Z</published>
    <updated>2020-05-12T02:40:18.932Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Upsampling</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Upsampling(上采样)</strong>:简单的说就是放大图像，获得更大的分辨率，上采样对于卷积神经网络任务来说并不是必须的，但是在某些场合中却必须要使用，尤其是在<strong>语义分割</strong>任务中，如何选择上采样的方式可能决定着语义分割效果。可能小伙伴们对上采样不是很熟悉，但是对于下采样一定都不陌生，典型的下采样步骤就是池化。因此上采样也可被看作是<strong>池化的逆过程</strong>。<br><a id="more"></a></p><p><img src="/images/Semantic_segmentation/SegNet_U.png" alt="upsampling"></p><h1 id="Maxpooling-Indices-最大池化索引-与Upsampling-上采样-和Deconvolution-反卷积-之间的区别"><a href="#Maxpooling-Indices-最大池化索引-与Upsampling-上采样-和Deconvolution-反卷积-之间的区别" class="headerlink" title="Maxpooling-Indices(最大池化索引)与Upsampling(上采样)和Deconvolution(反卷积)之间的区别"></a><font size="5" color="red">Maxpooling-Indices(最大池化索引)与Upsampling(上采样)和Deconvolution(反卷积)之间的区别</font></h1><p>  <font size="3"><strong>Maxpooling-Indices(最大池化索引)</strong>：又称为<strong>Unpooling(反池化)</strong>，池化后<strong>记录最大值所在的位置</strong>，在反池化的过程中，给相应位置上写入值，<strong>其他位置为0</strong>。这个方法没有参数，<strong>但是这个方法并不常用，因为存在大量的稀疏数据，使模型收敛速度大大降低。</strong></font><br>  <font size="3"><strong>Upsampling(上采样)</strong>：将输入<strong>resize到设置大小</strong>，然后利用指定的插值方法<strong>对周围的值进行插值</strong>，常用<strong>最近邻插值</strong>和<strong>双线性插值</strong>。因为相邻区域的像素和特征应该是相似的，因此这个方法特别常用，<strong>既没有参数，也不会存在稀疏数据。</strong></font><br>  <font size="3"><strong>Deconvolution(反卷积)</strong>：<strong>本质是卷积</strong>，注意<strong>反卷积并不能从卷积的结果返回到卷积前的数据，只能返回到卷积前的尺寸</strong>。卷积通过设置kernel_size卷积核大小，strides步长和padding填充方式可以将图像的分辨率降低，相反的反卷积可以通过设置kernel_size卷积核大小，strides步长和padding填充方式<strong>先对数据进行填充，然后再进行卷积操作</strong>，可以将图像的分辨率增加。<strong>这个方法不推荐经常使用，因为存在大量参数，而且可能会存在棋盘格效应，可以参考<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">棋盘格可视化</a></strong>。</font></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  由于CNN经常使用池化来缩小图像尺寸，方便提取更深层次的特征，因此下采样是CNN网络重要的组成部分，但是在某些特殊场景需要<strong>对图像大小进行复原</strong>，因此上采样应运而生，所以要想系统的学习神经网络，上采样知识点是小伙伴们必不可少的。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Upsampling&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>SSD</title>
    <link href="https://USTCcoder.github.io/2020/05/05/Object%20detection%20SSD/"/>
    <id>https://USTCcoder.github.io/2020/05/05/Object detection SSD/</id>
    <published>2020-05-05T05:36:40.000Z</published>
    <updated>2020-08-11T15:03:28.671Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">SSD</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>SSD(Single Shot MultiBox Detector)</strong>:于<strong>2016年发表在ECCV</strong>上。Single Shot MultiBox Detector的字面意思为：单次多框检测器，顾名思义，属于目标检测算法中一步法的思想，而且利用到多个先验框的一种算法，是一步法的典型代表。<a id="more"></a></p><p><img src="/images/Object_detection/SSD.png" alt="SSD"></p><h1 id="SSD特点"><a href="#SSD特点" class="headerlink" title="SSD特点"></a><font size="5" color="red">SSD特点</font></h1><p>  <font size="3">特征提取网络为<strong>VGG</strong>，构建特征提取网络较为简单。</font><br>  <font size="3">针对于不同尺度的特征层设计不同大小的先验框，融合不同特征层的检测信息对先验框中是否包含物体进行分类。</font></p><h1 id="SSD图像分析"><a href="#SSD图像分析" class="headerlink" title="SSD图像分析"></a><font size="5" color="red">SSD图像分析</font></h1><p><img src="/images/Object_detection/SSD_A.png" alt="SSD"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class L2_Normalize(keras.layers.Layer):</span><br><span class="line">    def __init__(self, scale, **kwargs):</span><br><span class="line">        super(L2_Normalize, self).__init__(kwargs)</span><br><span class="line">        self.scale = scale</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        self.gamma = tf.Variable(self.scale * np.ones((input_shape[3],), dtype='float32'))</span><br><span class="line"></span><br><span class="line">    def call(self, x, mask=None):</span><br><span class="line">        output = tf.nn.l2_normalize(x, axis=3)</span><br><span class="line">        output *= self.gamma</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def ssd(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', activation='relu', name='conv1_1'),</span><br><span class="line">                keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', activation='relu', name='conv1_2'),</span><br><span class="line">                keras.layers.MaxPool2D((2, 2), (2, 2), 'same', name='maxpool1'))(x)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(128, (3, 3), (1, 1), 'same', activation='relu', name='conv2_1'),</span><br><span class="line">                keras.layers.Conv2D(128, (3, 3), (1, 1), 'same', activation='relu', name='conv2_2'),</span><br><span class="line">                keras.layers.MaxPool2D((2, 2), (2, 2), 'same', name='maxpool2'))(x)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(256, (3, 3), (1, 1), 'same', activation='relu', name='conv3_1'),</span><br><span class="line">                keras.layers.Conv2D(256, (3, 3), (1, 1), 'same', activation='relu', name='conv3_2'),</span><br><span class="line">                keras.layers.Conv2D(256, (3, 3), (1, 1), 'same', activation='relu', name='conv3_3'),</span><br><span class="line">                keras.layers.MaxPool2D((2, 2), (2, 2), 'same', name='maxpool3'))(x)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(512, (3, 3), (1, 1), 'same', activation='relu', name='conv4_1'),</span><br><span class="line">                keras.layers.Conv2D(512, (3, 3), (1, 1), 'same', activation='relu', name='conv4_2'),</span><br><span class="line">                keras.layers.Conv2D(512, (3, 3), (1, 1), 'same', activation='relu', name='conv4_3'))(x)</span><br><span class="line"></span><br><span class="line">    l2_norm = L2_Normalize(20, name='l2_norm')(x)</span><br><span class="line"></span><br><span class="line">    feature1_reg = compose(keras.layers.Conv2D(4 * 4, (3, 3), (1, 1), 'same', name='feature1_reg_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature1_reg_flatten'))(l2_norm)</span><br><span class="line">    feature1_cls = compose(keras.layers.Conv2D(4 * 21, (3, 3), (1, 1), 'same', name='feature1_cls_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature1_cls_flatten'))(l2_norm)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.MaxPool2D((2, 2), (2, 2), 'same', name='maxpool4'),</span><br><span class="line">                keras.layers.Conv2D(512, (3, 3), (1, 1), 'same', activation='relu', name='conv5_1'),</span><br><span class="line">                keras.layers.Conv2D(512, (3, 3), (1, 1), 'same', activation='relu', name='conv5_2'),</span><br><span class="line">                keras.layers.Conv2D(512, (3, 3), (1, 1), 'same', activation='relu', name='conv5_3'),</span><br><span class="line">                keras.layers.MaxPool2D((3, 3), (1, 1), 'same', name='maxpool5'),</span><br><span class="line">                keras.layers.Conv2D(1024, (3, 3), (1, 1), 'same', activation='relu', dilation_rate=(6, 6), name='conv5_4'),</span><br><span class="line">                keras.layers.Conv2D(1024, (1, 1), (1, 1), 'same', activation='relu', name='conv5_5'))(x)</span><br><span class="line"></span><br><span class="line">    feature2_reg = compose(keras.layers.Conv2D(6 * 4, (3, 3), (1, 1), 'same', name='feature2_reg_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature2_reg_flatten'))(x)</span><br><span class="line">    feature2_cls = compose(keras.layers.Conv2D(6 * 21, (3, 3), (1, 1), 'same', name='feature2_cls_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature2_cls_flatten'))(x)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(256, (1, 1), (1, 1), 'same', activation='relu', name='conv6_1'),</span><br><span class="line">                keras.layers.Conv2D(512, (3, 3), (2, 2), 'same', activation='relu', name='conv6_2'))(x)</span><br><span class="line"></span><br><span class="line">    feature3_reg = compose(keras.layers.Conv2D(6 * 4, (3, 3), (1, 1), 'same', name='feature3_reg_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature3_reg_flatten'))(x)</span><br><span class="line">    feature3_cls = compose(keras.layers.Conv2D(6 * 21, (3, 3), (1, 1), 'same', name='feature3_cls_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature3_cls_flatten'))(x)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(128, (1, 1), (1, 1), 'same', activation='relu', name='conv7_1'),</span><br><span class="line">                keras.layers.Conv2D(256, (3, 3), (2, 2), 'same', activation='relu', name='conv7_2'))(x)</span><br><span class="line"></span><br><span class="line">    feature4_reg = compose(keras.layers.Conv2D(6 * 4, (3, 3), (1, 1), 'same', name='feature4_reg_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature4_reg_flatten'))(x)</span><br><span class="line">    feature4_cls = compose(keras.layers.Conv2D(6 * 21, (3, 3), (1, 1), 'same', name='feature4_cls_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature4_cls_flatten'))(x)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(128, (1, 1), (1, 1), 'same', activation='relu', name='conv8_1'),</span><br><span class="line">                keras.layers.Conv2D(256, (3, 3), (1, 1), 'valid', activation='relu', name='conv8_2'))(x)</span><br><span class="line"></span><br><span class="line">    feature5_reg = compose(keras.layers.Conv2D(4 * 4, (3, 3), (1, 1), 'same', name='feature5_reg_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature5_reg_flatten'))(x)</span><br><span class="line">    feature5_cls = compose(keras.layers.Conv2D(4 * 21, (3, 3), (1, 1), 'same', name='feature5_cls_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature5_cls_flatten'))(x)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(128, (1, 1), (1, 1), 'same', activation='relu', name='conv9_1'),</span><br><span class="line">                keras.layers.Conv2D(256, (3, 3), (1, 1), 'valid', activation='relu', name='conv9_2'))(x)</span><br><span class="line"></span><br><span class="line">    feature6_reg = compose(keras.layers.Conv2D(4 * 4, (3, 3), (1, 1), 'same', name='feature6_reg_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature6_reg_flatten'))(x)</span><br><span class="line">    feature6_cls = compose(keras.layers.Conv2D(4 * 21, (3, 3), (1, 1), 'same', name='feature6_cls_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature6_cls_flatten'))(x)</span><br><span class="line"></span><br><span class="line">    concatenate_reg = keras.layers.Concatenate(name='concatenate_reg')([feature1_reg, feature2_reg, feature3_reg, feature4_reg, feature5_reg, feature6_reg])</span><br><span class="line">    concatenate_cls = keras.layers.Concatenate(name='concatenate_cls')([feature1_cls, feature2_cls, feature3_cls, feature4_cls, feature5_cls, feature6_cls])</span><br><span class="line"></span><br><span class="line">    reshape_reg = keras.layers.Reshape((8732, 4), name='reshape_reg')(concatenate_reg)</span><br><span class="line">    reshape_cls = keras.layers.Reshape((8732, 21), name='reshape_cls')(concatenate_cls)</span><br><span class="line"></span><br><span class="line">    softmax_cls = keras.layers.Softmax(name='softmax_cls')(reshape_cls)</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Concatenate(name='concatenate')([reshape_reg, softmax_cls])</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, output, name='SSD')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = ssd(input_shape=(300, 300, 3))</span><br><span class="line">    model.build(input_shape=(None, 300, 300, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Object_detection/SSD_R.png" alt="SSD"></p><h1 id="Shape数据集完整实战"><a href="#Shape数据集完整实战" class="headerlink" title="Shape数据集完整实战"></a><font size="5" color="red">Shape数据集完整实战</font></h1><h2 id="文件路径关系说明"><a href="#文件路径关系说明" class="headerlink" title="文件路径关系说明"></a>文件路径关系说明</h2><ul><li>project<ul><li>shape<ul><li>train_imgs(训练集图像文件夹)</li><li>annotations(训练集标签文件夹)</li><li>test_imgs(测试集图像文件夹)</li></ul></li><li>SSD_weight(模型权重文件夹)</li><li>SSD_test_result(测试集结果文件夹)</li><li>SSD.py</li></ul></li></ul><h2 id="实战步骤说明"><a href="#实战步骤说明" class="headerlink" title="实战步骤说明"></a>实战步骤说明</h2><ol><li>目标检测和语义分割是两种不同类型的工程项目，目标检测实战处理比语义分割困难的多，首先要<strong>读取真实框信息</strong>，将其保存下来，为了后面编码使用。</li><li><strong>建立先验框</strong>，根据网络结构，在不同特征层上建立不同的先验框，先验框的总个数为每个回归分类特征层的像素点个数x每个像素点上的先验框个数。以论文中的先验框为例，特征层有6个，大小分别为38x38，19x19，10x10，5x5，3x3，1x1，特征层上每个像素点的先验框个数分别为4，6，6，6，4，4。<br><img src="/images/Object_detection/SSD_P.png" alt="anchor"><script type="math/tex; mode=display">38^2 \times 4+19^2 \times 6+10^2 \times 6+5^2 \times 6+3^2 \times 4+1^2 \times 4=8732</script>故先验框总数为8732个。</li><li>根据真实框的信息，和所有先验框计算IOU，将IOU大于设定值的记录下来，作为正样本。然后进行<strong>编码</strong>，在所属类别的置信度上面置1，其他类别置信度置0，并计算正样本先验框的中心坐标与宽高和真实框的中心坐标与宽高之间的差异。输出(batch_size, num_prior, 4 + 1 + num_class)，num_prior为先验框的个数，每个先验框有4 + 1 + num_class个值，4代表中心坐标和宽高相对真实框的差异，1代表属于背景的置信度，num_class代表属于某一个类别的置信度。编码的目的是得到真实框对应的神经网络的输出应该是什么样子，然后让两者尽可能的接近。<br><strong>IOU(Intersection Over Union，交并比)</strong>：用于<strong>评估语义分割算法性能的指标是平均IOU</strong>，交并比也非常好理解，算法的结果与真实物体进行<strong>交运算的结果除以进行并运算的结果</strong>。通过下图可以直观的看出IOU的计算方法。<br><img src="/images/Semantic_segmentation/Dataset_I.png" alt="IOU"></li><li><strong>设计损失函数</strong>，因为先验框中大部分都是负样本，因此不能直接计算损失函数，首先要对<strong>正负样本进行比例调整。一般选择正负样本比例为1：3</strong>，然后使用<strong>交叉熵损失函数</strong>计算正负样本的分类损失，使用<strong>smooth L1 loss</strong>计算正样本的定位损失。</li><li>搭建神经网络，<strong>设置合适参数</strong>，进行训练。</li><li>预测时，需要根据神经网络的输出进行<strong>逆向解码(编码的反过程)</strong>，根据置信度，选择<strong>非背景置信度大于设定值的先验框作为候选框</strong>，并且该框的<strong>类别设为置信度最大索引对应的类别</strong>，如最大值的索引为2，则该预测框预测的物体类别是第二类。然后<strong>根据先验框的坐标和4个回归参数确定候选框的左上角和右下角坐标</strong>。对<strong>每一类候选框进行NMS得到预测框</strong>，并且在图像上<strong>画出预测框</strong>，并且<strong>标出置信度</strong>即可完成目标检测任务。<br><strong>NMS(Non-Maximum Suppression，非极大值抑制)</strong>：简单地说，<strong>不是最大的我不要</strong>，在目标检测中，往往图像上存在大量先验框，会导致很多附近的框都会预测出同一个物体，但是我们<strong>只保留最大的一个预测结果</strong>，这就是非极大值抑制。<br>步骤：<br>(1)<strong>从最大概率矩形框F开始</strong>，分别判断A~E与F的IOU是否大于某个设定的阈值，<strong>假设B、D与F的重叠度超过阈值，那么就扔掉B、D</strong>；并<strong>标记第一个矩形框F</strong>，是我们保留下来的。<br>(2)<strong>从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框</strong>。<br>(3)<strong>重复步骤(2)，直到所有的框都被抛弃或者保留</strong>。<br><img src="/images/Object_detection/Dataset_N.png" alt="NMS"></li></ol><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>神经网络的输出维度为(batch_size, num_prior, 4 + 1 + num_class)，<strong>此数据集为3类，因此最后一个维度是8</strong>。每个先验框有8个索引，前面4个索引代表先验框的回归参数，用来对先验框进行调整得到预测框，索引为4代表背景，索引为5代表圆形，索引为6代表三角形，索引为7代表正方形。</li><li>实际的工程应用中，常常还需要对数据集进行<strong>大小调整和增强</strong>，在这里为了简单起见，没有进行复杂的操作，小伙伴们应用中要记得根据自己的需要，对图像进行<strong>resize或者padding</strong>，然后<strong>旋转</strong>，<strong>对比度增强</strong>，<strong>仿射运算</strong>等等操作，增加模型的鲁棒性，并且实际中的图像不一定按照顺序命名的，因此应用中也要注意图像读取的文件名。</li><li>设置了<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li>使用<strong>yield</strong>关键字，产生可迭代对象，不用将所有的数据都保存下来，大大节约内存。</li><li>其中将1000个数据，分成800个训练集，100个验证集和100个测试集，小伙伴们可以自行修改。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>SSD的<strong>特征提取网络为VGG</strong>，小伙伴们可以参考特征提取网络部分内容，选择其他的网络进行特征提取，比较不同网络参数量，运行速度，最终结果之间的差异。</li><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li><strong>根据实际的图像大小，选择合适的特征层数，先验框的形状，先验框数量，以及各种阈值</strong></li><li><strong>anchor尺寸的确定</strong>，anchor一般是正方形或者长方形，每个特征层上设置最大尺寸max_size和最小尺寸min_size，如果先验框为4个，则代表两个正方形和两个长方形，一个正方形的边长为min_size，另一个为$\sqrt{max \underline{} size \times min \underline{} size}$，一个长方形的边长为$(min \underline{} size \times \sqrt2，min \underline{} size \div \sqrt2)$，另一个长方形的边长为$(min \underline{} size \div \sqrt2，min \underline{} size \times \sqrt2)$，如果先验框为6个，则添加两个长方形，将上面的$\sqrt2$改成$\sqrt3$即可。</li><li>因为这个博客是对学习的一些总结和记录，意在和学习者探讨和交流，并且给准备入门的同学一些手把手的教学，因此关于目标检测的算法参数设计，我都是自己尝试的，不是针对于这个数据集最优的参数，大家可以根据自己的实际需要修改网络结构。</li></ol><h2 id="完整实战代码"><a href="#完整实战代码" class="headerlink" title="完整实战代码"></a>完整实战代码</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br></pre></td><td class="code"><pre><span class="line">import colorsys</span><br><span class="line">import os</span><br><span class="line">import xml.etree.ElementTree as ET</span><br><span class="line">from functools import reduce</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取先验框函数</span><br><span class="line">def get_prior(layer_id):</span><br><span class="line">    layer_id = layer_id - 1</span><br><span class="line"></span><br><span class="line">    box_widths = []</span><br><span class="line">    box_heights = []</span><br><span class="line"></span><br><span class="line">    current_ratios = [1, 1]</span><br><span class="line">    for ratio in ratios[layer_id]:</span><br><span class="line">        current_ratios.extend([ratio, 1 / ratio])</span><br><span class="line"></span><br><span class="line">    for ratio in current_ratios:</span><br><span class="line">        if ratio == 1 and len(box_widths) == 0:</span><br><span class="line">            box_widths.append(min_size[layer_id])</span><br><span class="line">            box_heights.append(min_size[layer_id])</span><br><span class="line">        elif ratio == 1 and len(box_widths) &gt; 0:</span><br><span class="line">            box_widths.append((min_size[layer_id] * max_size[layer_id]) ** 0.5)</span><br><span class="line">            box_heights.append((min_size[layer_id] * max_size[layer_id]) ** 0.5)</span><br><span class="line">        elif ratio != 1:</span><br><span class="line">            box_widths.append(min_size[layer_id] * ratio ** 0.5)</span><br><span class="line">            box_heights.append(min_size[layer_id] / ratio ** 0.5)</span><br><span class="line"></span><br><span class="line">    step_x = img_size[1] / feature_map[layer_id]</span><br><span class="line">    step_y = img_size[0] / feature_map[layer_id]</span><br><span class="line">    linx = np.linspace(0.5 * step_x, img_size[1] - 0.5 * step_x, feature_map[layer_id])</span><br><span class="line">    liny = np.linspace(0.5 * step_y, img_size[0] - 0.5 * step_y, feature_map[layer_id])</span><br><span class="line"></span><br><span class="line">    centers_x, centers_y = np.meshgrid(linx, liny)</span><br><span class="line">    centers_x = centers_x.reshape(-1, 1)</span><br><span class="line">    centers_y = centers_y.reshape(-1, 1)</span><br><span class="line"></span><br><span class="line">    # 获得先验框的中心坐标</span><br><span class="line">    prior_center = np.concatenate((centers_x, centers_y), axis=1)</span><br><span class="line">    prior_center = np.tile(prior_center, (1, prior[layer_id] * 2))</span><br><span class="line"></span><br><span class="line">    prior_lt_rb = prior_center.copy()</span><br><span class="line"></span><br><span class="line">    # 获得先验框的左上右下</span><br><span class="line">    prior_lt_rb[:, ::4] -= box_widths</span><br><span class="line">    prior_lt_rb[:, 1::4] -= box_heights</span><br><span class="line">    prior_lt_rb[:, 2::4] += box_widths</span><br><span class="line">    prior_lt_rb[:, 3::4] += box_heights</span><br><span class="line"></span><br><span class="line">    # 归一化到[0, 1]</span><br><span class="line">    prior_lt_rb[:, ::2] /= img_size[1]</span><br><span class="line">    prior_lt_rb[:, 1::2] /= img_size[0]</span><br><span class="line">    prior_lt_rb = prior_lt_rb.reshape(-1, 4)</span><br><span class="line">    prior_lt_rb = np.minimum(np.maximum(prior_lt_rb, 0.0), 1.0)</span><br><span class="line"></span><br><span class="line">    prior_center_wh = np.zeros_like(prior_lt_rb)</span><br><span class="line">    # 获得先验框的宽和高</span><br><span class="line">    prior_center_wh[:, 0] = 0.5 * (prior_lt_rb[:, 2] + prior_lt_rb[:, 0])</span><br><span class="line">    prior_center_wh[:, 1] = 0.5 * (prior_lt_rb[:, 3] + prior_lt_rb[:, 1])</span><br><span class="line">    prior_center_wh[:, 2] = prior_lt_rb[:, 2] - prior_lt_rb[:, 0]</span><br><span class="line">    prior_center_wh[:, 3] = prior_lt_rb[:, 3] - prior_lt_rb[:, 1]</span><br><span class="line"></span><br><span class="line">    return prior_center_wh.astype(np.float32), prior_lt_rb.astype(np.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 从xml文件中获取bounding-box信息</span><br><span class="line">def get_bbox(image_id, bbox_path, annotations_path):</span><br><span class="line">    with open(bbox_path, 'w') as f:</span><br><span class="line">        for id in image_id:</span><br><span class="line">            # 图片路径</span><br><span class="line">            info = os.getcwd() + imgs_path[1:] + '\\' + str(id) + '.jpg'</span><br><span class="line">            in_file = open(annotations_path + '\\' + str(id) + '.xml', encoding='utf-8')</span><br><span class="line">            tree = ET.parse(in_file)</span><br><span class="line">            root = tree.getroot()</span><br><span class="line"></span><br><span class="line">            for obj in root.iter('object'):</span><br><span class="line">                difficult = obj.find('difficult').text</span><br><span class="line">                cls = obj.find('name').text</span><br><span class="line">                if cls not in classes or int(difficult) == 1:</span><br><span class="line">                    continue</span><br><span class="line">                cls_id = classes.index(cls)</span><br><span class="line">                xmlbox = obj.find('bndbox')</span><br><span class="line">                b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))</span><br><span class="line">                info += " " + ",".join([str(x) for x in b]) + ',' + str(cls_id)</span><br><span class="line">            f.writelines(info + '\n')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class L2_Normalize(keras.layers.Layer):</span><br><span class="line">    def __init__(self, scale, **kwargs):</span><br><span class="line">        super(L2_Normalize, self).__init__(kwargs)</span><br><span class="line">        self.scale = scale</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        self.gamma = tf.Variable(self.scale * np.ones((input_shape[3],), dtype='float32'))</span><br><span class="line"></span><br><span class="line">    def call(self, x, mask=None):</span><br><span class="line">        output = tf.nn.l2_normalize(x, axis=3)</span><br><span class="line">        output *= self.gamma</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def small_ssd(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(32, (3, 3), (1, 1), 'same', activation='relu', name='conv1_1'),</span><br><span class="line">                keras.layers.MaxPool2D((2, 2), (2, 2), 'same', name='maxpool1'))(x)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', activation='relu', name='conv2_1'),</span><br><span class="line">                keras.layers.MaxPool2D((2, 2), (2, 2), 'same', name='maxpool2'))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Conv2D(128, (3, 3), (1, 1), 'same', activation='relu', name='conv3_1')(x)</span><br><span class="line"></span><br><span class="line">    l2_norm = L2_Normalize(20, name='l2_norm')(x)</span><br><span class="line"></span><br><span class="line">    feature1_reg = compose(keras.layers.Conv2D(prior[0] * 4, (3, 3), (1, 1), 'same', name='feature1_reg_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature1_reg_flatten'))(l2_norm)</span><br><span class="line">    feature1_cls = compose(keras.layers.Conv2D(prior[0] * num_class, (3, 3), (1, 1), 'same', name='feature1_cls_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature1_cls_flatten'))(l2_norm)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.MaxPool2D((2, 2), (2, 2), 'same', name='maxpool3'),</span><br><span class="line">                keras.layers.Conv2D(256, (3, 3), (1, 1), 'same', activation='relu', name='conv4_1'),)(x)</span><br><span class="line"></span><br><span class="line">    feature2_reg = compose(keras.layers.Conv2D(prior[1] * 4, (3, 3), (1, 1), 'same', name='feature2_reg_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature2_reg_flatten'))(x)</span><br><span class="line">    feature2_cls = compose(keras.layers.Conv2D(prior[1] * num_class, (3, 3), (1, 1), 'same', name='feature2_cls_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature2_cls_flatten'))(x)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(512, (3, 3), (2, 2), 'valid', activation='relu', name='conv5_1'))(x)</span><br><span class="line"></span><br><span class="line">    feature3_reg = compose(keras.layers.Conv2D(prior[2] * 4, (3, 3), (1, 1), 'same', name='feature3_reg_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature3_reg_flatten'))(x)</span><br><span class="line">    feature3_cls = compose(keras.layers.Conv2D(prior[2] * num_class, (3, 3), (1, 1), 'same', name='feature3_cls_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature3_cls_flatten'))(x)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(128, (1, 1), (1, 1), 'same', activation='relu', name='conv6_1'),</span><br><span class="line">                keras.layers.Conv2D(256, (3, 3), (2, 2), 'valid', activation='relu', name='conv6_2'))(x)</span><br><span class="line"></span><br><span class="line">    feature4_reg = compose(keras.layers.Conv2D(prior[3] * 4, (3, 3), (1, 1), 'same', name='feature4_reg_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature4_reg_flatten'))(x)</span><br><span class="line">    feature4_cls = compose(keras.layers.Conv2D(prior[3] * num_class, (3, 3), (1, 1), 'same', name='feature4_cls_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature4_cls_flatten'))(x)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(128, (1, 1), (1, 1), 'same', activation='relu', name='conv7_1'),</span><br><span class="line">                keras.layers.Conv2D(256, (3, 3), (1, 1), 'valid', activation='relu', name='conv7_2'))(x)</span><br><span class="line"></span><br><span class="line">    feature5_reg = compose(keras.layers.Conv2D(prior[4] * 4, (3, 3), (1, 1), 'same', name='feature5_reg_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature5_reg_flatten'))(x)</span><br><span class="line">    feature5_cls = compose(keras.layers.Conv2D(prior[4] * num_class, (3, 3), (1, 1), 'same', name='feature5_cls_conv'),</span><br><span class="line">                           keras.layers.Flatten(name='feature5_cls_flatten'))(x)</span><br><span class="line"></span><br><span class="line">    concatenate_reg = keras.layers.Concatenate(name='concatenate_reg')([feature1_reg, feature2_reg, feature3_reg, feature4_reg, feature5_reg])</span><br><span class="line">    concatenate_cls = keras.layers.Concatenate(name='concatenate_cls')([feature1_cls, feature2_cls, feature3_cls, feature4_cls, feature5_cls])</span><br><span class="line"></span><br><span class="line">    reshape_reg = keras.layers.Reshape((num_prior, 4), name='reshape_reg')(concatenate_reg)</span><br><span class="line">    reshape_cls = keras.layers.Reshape((num_prior, num_class), name='reshape_cls')(concatenate_cls)</span><br><span class="line"></span><br><span class="line">    softmax_cls = keras.layers.Softmax(name='softmax_cls')(reshape_cls)</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Concatenate(name='concatenate')([reshape_reg, softmax_cls])</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, output, name='Small_SSD')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 计算IOU函数</span><br><span class="line">def iou(box):</span><br><span class="line">    inter_upleft = np.maximum(prior_lt_rb[:, :2], box[:2])</span><br><span class="line">    inter_botright = np.minimum(prior_lt_rb[:, 2:4], box[2:])</span><br><span class="line"></span><br><span class="line">    inter_wh = inter_botright - inter_upleft</span><br><span class="line">    inter_wh = np.maximum(inter_wh, 0)</span><br><span class="line">    inter = inter_wh[:, 0] * inter_wh[:, 1]</span><br><span class="line">    # 真实框的面积</span><br><span class="line">    area_true = (box[2] - box[0]) * (box[3] - box[1])</span><br><span class="line">    # 先验框的面积</span><br><span class="line">    area_gt = (prior_lt_rb[:, 2] - prior_lt_rb[:, 0]) * (prior_lt_rb[:, 3] - prior_lt_rb[:, 1])</span><br><span class="line">    # 计算iou</span><br><span class="line">    union = area_true + area_gt - inter</span><br><span class="line"></span><br><span class="line">    iou = inter / union</span><br><span class="line"></span><br><span class="line">    return iou</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 根据真实框bounding-box编码函数</span><br><span class="line">def encoder(box):</span><br><span class="line">    iou_val = iou(box)</span><br><span class="line">    encoded_box = np.zeros((num_prior, 5))</span><br><span class="line"></span><br><span class="line">    # 找到每一个真实框，重合程度较高的先验框</span><br><span class="line">    assign_mask = iou_val &gt; overlap_threshold</span><br><span class="line">    encoded_box[:, -1][assign_mask] = iou_val[assign_mask]</span><br><span class="line"></span><br><span class="line">    # 找到对应的先验框</span><br><span class="line">    assigned_priors = prior_center_wh[assign_mask]</span><br><span class="line"></span><br><span class="line">    # 先计算真实框的中心与长宽</span><br><span class="line">    box_center = 0.5 * (box[:2] + box[2:])</span><br><span class="line">    box_wh = box[2:] - box[:2]</span><br><span class="line"></span><br><span class="line">    # 再计算重合度较高的先验框的中心与长宽</span><br><span class="line">    assigned_priors_center = assigned_priors[:, :2]</span><br><span class="line">    assigned_priors_wh = assigned_priors[:, 2:4]</span><br><span class="line"></span><br><span class="line">    # 根据真实框求ssd应该有的预测结果</span><br><span class="line">    encoded_box[:, :2][assign_mask] = box_center - assigned_priors_center</span><br><span class="line">    encoded_box[:, :2][assign_mask] /= assigned_priors_wh</span><br><span class="line"></span><br><span class="line">    # 除以0.1</span><br><span class="line">    encoded_box[:, :2][assign_mask] /= variances[:2]</span><br><span class="line"></span><br><span class="line">    encoded_box[:, 2:4][assign_mask] = np.log(box_wh / assigned_priors_wh)</span><br><span class="line">    # 除以0.2</span><br><span class="line">    encoded_box[:, 2:4][assign_mask] /= variances[2:]</span><br><span class="line"></span><br><span class="line">    return encoded_box</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取网络输出标签数据，即作为损失函数的真实输入y_true</span><br><span class="line">def assign_boxes(boxes):</span><br><span class="line">    # 大小为num_box * (4 + num_class)，4代表4个位置回归</span><br><span class="line">    assignment = np.zeros((num_prior, 4 + num_class))</span><br><span class="line">    assignment[:, 4] = 1.0</span><br><span class="line">    if len(boxes) == 0:</span><br><span class="line">        return assignment</span><br><span class="line">    # 对每一个真实框都进行iou计算</span><br><span class="line">    encoded_boxes = np.apply_along_axis(f_encode, 1, boxes[:, :4])</span><br><span class="line">    # 每一个真实框的编码后的值，和iou</span><br><span class="line">    encoded_boxes = encoded_boxes.reshape(-1, num_prior, 5)</span><br><span class="line">    # 取重合程度最大的先验框，并且获取这个先验框的index</span><br><span class="line">    best_iou = encoded_boxes[:, :, -1].max(axis=0)</span><br><span class="line">    best_iou_idx = encoded_boxes[:, :, -1].argmax(axis=0)</span><br><span class="line">    best_iou_mask = best_iou &gt; 0</span><br><span class="line">    best_iou_idx = best_iou_idx[best_iou_mask]</span><br><span class="line"></span><br><span class="line">    # 保留重合程度最大的先验框的应该有的预测结果</span><br><span class="line">    encoded_boxes = encoded_boxes[:, best_iou_mask, :]</span><br><span class="line">    assignment[:, :4][best_iou_mask] = encoded_boxes[best_iou_idx, np.arange(len(best_iou_idx)), :4]</span><br><span class="line">    # 4代表为背景的概率，为0</span><br><span class="line">    assignment[:, 4:][best_iou_mask] = boxes[best_iou_idx, 4:]</span><br><span class="line">    # 通过assign_boxes我们就获得了，输入进来的这张图片，应该有的预测结果是什么样子的</span><br><span class="line">    return assignment</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 通过yield获取可迭代对象</span><br><span class="line">def generate_arrays_from_file(train_data, batch_size):</span><br><span class="line">    # 获取总长度</span><br><span class="line">    n = len(train_data)</span><br><span class="line">    i = 0</span><br><span class="line">    while True:</span><br><span class="line">        X_train = []</span><br><span class="line">        Y_train = []</span><br><span class="line">        # 获取一个batch_size大小的数据</span><br><span class="line">        while len(X_train) &lt; batch_size:</span><br><span class="line">            if i == 0:</span><br><span class="line">                np.random.shuffle(train_data)</span><br><span class="line">            # 从文件中读取图像</span><br><span class="line">            # train_data[i] = 2</span><br><span class="line">            img = cv.imread(imgs_path + '\\' + str(train_data[i]) + '.jpg')</span><br><span class="line">            # print(str(train_data[i]))</span><br><span class="line">            img = img / 127.5 - 1</span><br><span class="line">            info = np.array([list(map(int, x.split(','))) for x in bounding_info[train_data[i]].split()[3:]])</span><br><span class="line">            if not len(info):</span><br><span class="line">                i = (i + 1) % n</span><br><span class="line">                continue</span><br><span class="line">            box = (info[:, :4] + 1).astype(np.float32)</span><br><span class="line">            box[:, [0, 2]] = box[:, [0, 2]] / img_size[1]</span><br><span class="line">            box[:, [1, 3]] = box[:, [1, 3]] / img_size[0]</span><br><span class="line">            label = np.eye(num_class)[np.array(info[:, 4] + 1, np.int32)]</span><br><span class="line">            if ((box[:, 0] - box[:, 2]) &gt;= 0).any() or ((box[:, 1] - box[:, 3]) &gt;= 0).any():</span><br><span class="line">                i = (i + 1) % n</span><br><span class="line">                continue</span><br><span class="line">            box = np.concatenate([box, label], axis=-1)</span><br><span class="line">            X_train.append(img)</span><br><span class="line">            y = assign_boxes(box)</span><br><span class="line">            Y_train.append(y)</span><br><span class="line">            i = (i + 1) % n</span><br><span class="line">        yield tf.constant(X_train), tf.constant(Y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 定义损失函数</span><br><span class="line">class Loss:</span><br><span class="line">    def l1_smooth_loss(self, y_true, y_pred):</span><br><span class="line">        abs_loss = tf.abs(y_true - y_pred)</span><br><span class="line">        sq_loss = 0.5 * (y_true - y_pred) ** 2</span><br><span class="line">        l1_loss = tf.where(tf.less(abs_loss, 1.0), sq_loss, abs_loss - 0.5)</span><br><span class="line">        return tf.reduce_sum(l1_loss, axis=-1)</span><br><span class="line"></span><br><span class="line">    def softmax_loss(self, y_true, y_pred):</span><br><span class="line">        y_pred = tf.maximum(y_pred, 1e-7)</span><br><span class="line">        softmax_loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)</span><br><span class="line">        return softmax_loss</span><br><span class="line"></span><br><span class="line">    def compute_loss(self, y_true, y_pred):</span><br><span class="line">        # 每一张图的pos的个数，shape为batch_size</span><br><span class="line">        y_pos = 1 - y_true[:, :, 4]</span><br><span class="line">        num_pos = tf.reduce_sum(y_pos, axis=-1)</span><br><span class="line">        # 获取一定的负样本</span><br><span class="line">        num_neg = tf.minimum(neg_pos_ratio * num_pos, num_prior - num_pos)</span><br><span class="line">        # 找到了哪些值是大于0的</span><br><span class="line">        pos_num_neg_mask = tf.greater(num_neg, 0)</span><br><span class="line">        # 求平均每个图片要取多少个负样本</span><br><span class="line">        has_min = tf.cast(tf.reduce_any(pos_num_neg_mask), tf.float32)</span><br><span class="line">        num_neg = tf.concat([num_neg, [(1 - has_min) * negatives_for_hard]], axis=0)</span><br><span class="line">        num_neg_batch = tf.reduce_mean(tf.boolean_mask(num_neg, tf.greater(num_neg, 0)))</span><br><span class="line">        num_neg_batch = tf.cast(num_neg_batch, tf.int32)</span><br><span class="line"></span><br><span class="line">        # 找到实际上在该位置不应该有预测结果的框，求他们最大的置信度。</span><br><span class="line">        max_confs = tf.reduce_max(y_pred[:, :, 5:5 + num_class - 1], axis=2)</span><br><span class="line"></span><br><span class="line">        # 取top_k个置信度，作为负样本</span><br><span class="line">        _, indices = tf.nn.top_k(max_confs * y_true[:, :, 4], k=num_neg_batch)</span><br><span class="line"></span><br><span class="line">        # 找到负样本的一维索引</span><br><span class="line">        batch_idx = tf.expand_dims(tf.range(0, batch_size), 1)</span><br><span class="line">        batch_idx = tf.tile(batch_idx, (1, num_neg_batch))</span><br><span class="line">        full_indices = (tf.reshape(batch_idx, [-1]) * num_prior + tf.reshape(indices, [-1]))</span><br><span class="line"></span><br><span class="line">        y_true_pos = y_true[tf.equal(y_true[:, :, 4], 0)]</span><br><span class="line">        y_pred_pos = y_pred[tf.equal(y_true[:, :, 4], 0)]</span><br><span class="line">        y_true_neg = tf.gather(tf.reshape(y_true, (-1, 8)), axis=0, indices=full_indices)</span><br><span class="line">        y_pred_neg = tf.gather(tf.reshape(y_pred, (-1, 8)), axis=0, indices=full_indices)</span><br><span class="line"></span><br><span class="line">        y_true_valid = tf.concat([y_true_pos, y_true_neg], axis=0)</span><br><span class="line">        y_pred_valid = tf.concat([y_pred_pos, y_pred_neg], axis=0)</span><br><span class="line"></span><br><span class="line">        loc_loss = self.l1_smooth_loss(y_true_pos[:, :4], y_pred_pos[:, :4])</span><br><span class="line">        conf_loss = self.softmax_loss(y_true_valid[:, 4:], y_pred_valid[:, 4:])</span><br><span class="line"></span><br><span class="line">        return tf.reduce_mean(loc_loss) + tf.reduce_mean(conf_loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 根据网络预测解码函数，获得候选框</span><br><span class="line">def decoder(loc):</span><br><span class="line">    # 获得先验框的中心与宽高</span><br><span class="line">    prior_center_x = prior_center_wh[:, 0]</span><br><span class="line">    prior_center_y = prior_center_wh[:, 1]</span><br><span class="line">    prior_width = prior_center_wh[:, 2]</span><br><span class="line">    prior_height = prior_center_wh[:, 3]</span><br><span class="line"></span><br><span class="line">    # 获得真实框的中心与宽高</span><br><span class="line">    decode_bbox_center_x = loc[:, 0] * prior_width * variances[0] + prior_center_x</span><br><span class="line">    decode_bbox_center_y = loc[:, 1] * prior_height * variances[1] + prior_center_y</span><br><span class="line">    decode_bbox_width = np.exp(loc[:, 2] * variances[2]) * prior_width</span><br><span class="line">    decode_bbox_height = np.exp(loc[:, 3] * variances[3]) * prior_height</span><br><span class="line"></span><br><span class="line">    # 获取真实框的左上角与右下角</span><br><span class="line">    decode_bbox_xmin = decode_bbox_center_x - 0.5 * decode_bbox_width</span><br><span class="line">    decode_bbox_ymin = decode_bbox_center_y - 0.5 * decode_bbox_height</span><br><span class="line">    decode_bbox_xmax = decode_bbox_center_x + 0.5 * decode_bbox_width</span><br><span class="line">    decode_bbox_ymax = decode_bbox_center_y + 0.5 * decode_bbox_height</span><br><span class="line"></span><br><span class="line">    # 真实框的左上角与右下角进行堆叠</span><br><span class="line">    decode_bbox = np.concatenate((decode_bbox_xmin[:, np.newaxis], decode_bbox_ymin[:, np.newaxis], decode_bbox_xmax[:, np.newaxis], decode_bbox_ymax[:, np.newaxis]), axis=-1)</span><br><span class="line">    # 防止超出0与1</span><br><span class="line">    decode_bbox = np.minimum(np.maximum(decode_bbox, 0.0), 1.0)</span><br><span class="line">    return decode_bbox</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 将候选框进行非极大值抑制，获得最终的预测框</span><br><span class="line">def detection_out(pred):</span><br><span class="line">    # 回归网络预测结果</span><br><span class="line">    mbox_loc = pred[:, :4]</span><br><span class="line">    # 分类网络预测结果</span><br><span class="line">    mbox_conf = pred[:, 4:]</span><br><span class="line">    results = []</span><br><span class="line">    # 对每一个图像进行处理</span><br><span class="line">    decode_bbox = decoder(mbox_loc)</span><br><span class="line">    for c in range(1, num_class):</span><br><span class="line">        c_confs = mbox_conf[:, c]</span><br><span class="line">        c_confs_mask = c_confs &gt; confidence_threshold</span><br><span class="line">        if len(c_confs[c_confs_mask]) &gt; 0:</span><br><span class="line">            # 取出得分高于confidence_threshold的框</span><br><span class="line">            boxes_to_process = decode_bbox[c_confs_mask]</span><br><span class="line">            confs_to_process = c_confs[c_confs_mask]</span><br><span class="line">            # 进行iou的非极大抑制</span><br><span class="line">            idx = tf.image.non_max_suppression(boxes_to_process, confs_to_process, max_output_size=keep_top_k, iou_threshold=nms_thresh)</span><br><span class="line">            idx = idx.numpy()</span><br><span class="line">            # 取出在非极大抑制中效果较好的内容</span><br><span class="line">            box = boxes_to_process[idx]</span><br><span class="line">            confs = confs_to_process[idx][:, np.newaxis]</span><br><span class="line">            # 将label、置信度、框的位置进行堆叠。</span><br><span class="line">            labels = c * np.ones((len(idx), 1))</span><br><span class="line">            c_pred = np.concatenate((labels, confs, box), axis=1)</span><br><span class="line">            # 添加进result里</span><br><span class="line">            results.extend(c_pred)</span><br><span class="line">    if len(results) &gt; 0:</span><br><span class="line">        # 按照置信度进行排序</span><br><span class="line">        results = np.array(results)</span><br><span class="line">        arg = np.argsort(results[:, 1])[::-1][:keep_top_k]</span><br><span class="line">        results = results[arg]</span><br><span class="line">    return results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 将图像进行预测并画框</span><br><span class="line">def detect_image(filename):</span><br><span class="line"></span><br><span class="line">    test_img = cv.imread(filename)</span><br><span class="line">    preds = tf.squeeze(model.predict(tf.constant([test_img / 127.5 - 1])), axis=0).numpy()</span><br><span class="line"></span><br><span class="line">    # 将预测结果进行解码</span><br><span class="line">    results = detection_out(preds)</span><br><span class="line"></span><br><span class="line">    if len(results) &lt;= 0:</span><br><span class="line">        return test_img</span><br><span class="line">    print(filename)</span><br><span class="line">    # 筛选出其中得分高于confidence的框</span><br><span class="line">    det_label = results[:, 0]</span><br><span class="line">    det_conf = results[:, 1]</span><br><span class="line">    det_xmin, det_ymin, det_xmax, det_ymax = results[:, 2], results[:, 3], results[:, 4], results[:, 5]</span><br><span class="line">    indices = [index for index, conf in enumerate(det_conf) if conf &gt;= confidence_threshold]</span><br><span class="line">    top_conf = det_conf[indices]</span><br><span class="line">    top_label_indices = det_label[indices].tolist()</span><br><span class="line">    top_xmin = np.expand_dims(det_xmin[indices], -1) * img_size[1]</span><br><span class="line">    top_ymin = np.expand_dims(det_ymin[indices], -1) * img_size[0]</span><br><span class="line">    top_xmax = np.expand_dims(det_xmax[indices], -1) * img_size[1]</span><br><span class="line">    top_ymax = np.expand_dims(det_ymax[indices], -1) * img_size[0]</span><br><span class="line">    boxes = np.concatenate([top_xmin, top_ymin, top_xmax, top_ymax], axis=-1)</span><br><span class="line"></span><br><span class="line">    font = cv.FONT_HERSHEY_SIMPLEX</span><br><span class="line"></span><br><span class="line">    for i, c in enumerate(top_label_indices):</span><br><span class="line">        cls = int(c) - 1</span><br><span class="line">        predicted_class = classes[cls]</span><br><span class="line">        score = top_conf[i]</span><br><span class="line"></span><br><span class="line">        left, top, right, bottom = boxes[i]</span><br><span class="line">        left = left - expand</span><br><span class="line">        top = top - expand</span><br><span class="line">        right = right + expand</span><br><span class="line">        bottom = bottom + expand</span><br><span class="line"></span><br><span class="line">        left = max(0, np.floor(left + 0.5).astype('int32'))</span><br><span class="line">        top = max(0, np.floor(top + 0.5).astype('int32'))</span><br><span class="line">        right = min(img_size[1], np.floor(right + 0.5).astype('int32'))</span><br><span class="line">        bottom = min(img_size[0], np.floor(bottom + 0.5).astype('int32'))</span><br><span class="line"></span><br><span class="line">        # 画框</span><br><span class="line">        label = '{} {:.2f}'.format(predicted_class, score)</span><br><span class="line"></span><br><span class="line">        cv.rectangle(test_img, (left, top), (right, bottom), colors[cls], 1)</span><br><span class="line">        cv.putText(test_img, label, (left, top - int(label_size * 10)), font, label_size, colors[cls], 1)</span><br><span class="line">    return test_img</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    # 包括背景的类别数目</span><br><span class="line">    num_class = 4</span><br><span class="line">    train_data = list(range(800))</span><br><span class="line">    validation_data = list(range(800, 900))</span><br><span class="line">    test_data = range(900, 1000)</span><br><span class="line">    epochs = 100</span><br><span class="line">    batch_size = 8</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    img_size = (128, 128)</span><br><span class="line">    classes = ["circle", "triangle", "square"]</span><br><span class="line">    # 每个特征图上每个像素先验框的个数</span><br><span class="line">    prior = [4, 4, 4, 4, 4]</span><br><span class="line">    # 特征图的大小</span><br><span class="line">    feature_map = [32, 16, 7, 3, 1]</span><br><span class="line">    # 特征图上anchor的最小尺寸</span><br><span class="line">    min_size = [4, 8, 16, 32, 64]</span><br><span class="line">    # 特征图上anchor的最大尺寸</span><br><span class="line">    max_size = [8, 16, 32, 64, 80]</span><br><span class="line">    # anchor的长宽比</span><br><span class="line">    ratios = [[2], [2], [2], [2], [2]]</span><br><span class="line">    # 先验框的个数</span><br><span class="line">    num_prior = sum([prior[x] * feature_map[x] ** 2 for x in range(len(prior))])</span><br><span class="line">    # 先验框与预测框的解码方差</span><br><span class="line">    variances = [0.1, 0.1, 0.2, 0.2]</span><br><span class="line">    # 获取所有先验框</span><br><span class="line">    prior_center_wh = []</span><br><span class="line">    prior_lt_rb = []</span><br><span class="line">    for i in range(len(prior)):</span><br><span class="line">        c_wh, tl_br = get_prior(i + 1)</span><br><span class="line">        prior_center_wh.append(c_wh)</span><br><span class="line">        prior_lt_rb.append(tl_br)</span><br><span class="line">    prior_center_wh = np.vstack(prior_center_wh)</span><br><span class="line">    prior_lt_rb = np.vstack(prior_lt_rb)</span><br><span class="line"></span><br><span class="line">    # IOU超过阈值的视为正样本</span><br><span class="line">    overlap_threshold = 0.5</span><br><span class="line">    # 负样本与正样本的比例</span><br><span class="line">    neg_pos_ratio = 3</span><br><span class="line">    # 回归损失函数的比例</span><br><span class="line">    alpha = 1</span><br><span class="line">    # 如果图像中不存在正样本，则指定最低负样本个数</span><br><span class="line">    negatives_for_hard = 10</span><br><span class="line">    # 编码函数</span><br><span class="line">    f_encode = encoder</span><br><span class="line">    # 画框设置不同的颜色</span><br><span class="line">    hsv_tuples = [(x / (num_class - 1), 1., 1.) for x in range(num_class - 1)]</span><br><span class="line">    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))</span><br><span class="line">    colors = list(map(lambda x: (int(x[1] * 255), int(x[2] * 255), int(x[0] * 255)), colors))</span><br><span class="line">    # 设置图像检测最多的框数目</span><br><span class="line">    keep_top_k = 5</span><br><span class="line">    # 设置检测置信度，大于该值认为有物体</span><br><span class="line">    confidence_threshold = 0.5</span><br><span class="line">    # 非极大值抑制阈值，重叠度不得大于该值</span><br><span class="line">    nms_thresh = 0.5</span><br><span class="line">    # 预测框不要紧贴物体，向外扩展像素大小</span><br><span class="line">    expand = 5</span><br><span class="line">    # 标签大小</span><br><span class="line">    label_size = 0.3</span><br><span class="line"></span><br><span class="line">    imgs_path = r'.\shape\train_imgs'</span><br><span class="line">    annotations_path = r'.\shape\annotations'</span><br><span class="line">    test_path = r'.\shape\test_imgs'</span><br><span class="line">    save_path = r'.\SSD_test_result'</span><br><span class="line">    weight_path = r'.\SSD_weight'</span><br><span class="line">    bbox_path = r'.\shape\bbox.txt'</span><br><span class="line"></span><br><span class="line">    # 将xml存储的bbox转换为bbox.txt文件，内容为file_path + bbox + class_id</span><br><span class="line">    if 'bbox.txt' not in os.listdir(r'.\shape'):</span><br><span class="line">        get_bbox(train_data + validation_data, bbox_path, annotations_path)</span><br><span class="line"></span><br><span class="line">    with open(bbox_path, 'r') as f:</span><br><span class="line">        bounding_info = f.readlines()</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        os.mkdir(save_path)</span><br><span class="line">    except FileExistsError:</span><br><span class="line">        print(save_path + 'has been exist')</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        os.mkdir(weight_path)</span><br><span class="line">    except FileExistsError:</span><br><span class="line">        print(weight_path + 'has been exist')</span><br><span class="line"></span><br><span class="line">    model = small_ssd(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(batch_size, img_size[0], img_size[1], 3))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    optimizor = keras.optimizers.Adam(lr=1e-4)</span><br><span class="line">    lossor = Loss().compute_loss</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=optimizor, loss=lossor)</span><br><span class="line"></span><br><span class="line">    # 保存的方式，3世代保存一次</span><br><span class="line">    checkpoint_period = keras.callbacks.ModelCheckpoint(</span><br><span class="line">        weight_path + '\\' + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        save_weights_only=True,</span><br><span class="line">        save_best_only=True,</span><br><span class="line">        period=3</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 学习率下降的方式，val_loss3次不下降就下降学习率继续训练</span><br><span class="line">    reduce_lr = keras.callbacks.ReduceLROnPlateau(</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        factor=0.5,</span><br><span class="line">        patience=3,</span><br><span class="line">        verbose=1</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 是否需要早停，当val_loss一直不下降的时候意味着模型基本训练完毕，可以停止</span><br><span class="line">    early_stopping = keras.callbacks.EarlyStopping(</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        min_delta=0,</span><br><span class="line">        patience=10,</span><br><span class="line">        verbose=1</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    model.fit_generator(generate_arrays_from_file(train_data, batch_size),</span><br><span class="line">                        steps_per_epoch=max(1, len(train_data) // batch_size),</span><br><span class="line">                        validation_data=generate_arrays_from_file(validation_data, batch_size),</span><br><span class="line">                        validation_steps=max(1, len(validation_data) // batch_size),</span><br><span class="line">                        epochs=epochs,</span><br><span class="line">                        callbacks=[checkpoint_period, reduce_lr, early_stopping])</span><br><span class="line"></span><br><span class="line">    for name in test_data:</span><br><span class="line">        test_img_path = test_path + '\\' + str(name) + '.jpg'</span><br><span class="line">        save_img_path = save_path + '\\' + str(name) + '.png'</span><br><span class="line">        test_img = detect_image(test_img_path)</span><br><span class="line">        cv.imwrite(save_img_path, test_img)</span><br></pre></td></tr></tbody></table></figure><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Object_detection/SSD_T.png" alt="SSD"></p><h1 id="SSD小结"><a href="#SSD小结" class="headerlink" title="SSD小结"></a><font size="5" color="red">SSD小结</font></h1><p>  SSD是一种简单的目标检测网络，从上图可以看出SSD模型的参数量只有26M，由于其<strong>结构简单，效果稳定</strong>，因此很多场合仍然使用SSD作为目标检测算法。SSD作为一步法目标检测的元老级模型，是小伙伴们需要掌握的一个模型。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;SSD&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>Convolution黑科技</title>
    <link href="https://USTCcoder.github.io/2020/05/04/deep%20learning%20convolution/"/>
    <id>https://USTCcoder.github.io/2020/05/04/deep learning convolution/</id>
    <published>2020-05-04T04:25:58.000Z</published>
    <updated>2020-05-12T02:39:12.973Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Convolution</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Convolution</strong>:在这个博客中，我们谈论的卷积并不是实际意义中的卷积，而是神经网络中的卷积。小伙伴们可能会有疑问，两个卷积有区别吗？学过信号处理或者图像处理的小伙伴们应该很熟悉，卷积是要<strong>首先将核翻转180°</strong>，然后再应用于信号或者图像上，而<strong>相关则不需要翻转</strong>。因此<strong>神经网络中的卷积实际上是一种相关操作</strong>。<br><a id="more"></a></p><p><img src="/images/deep_learning/convolution.png" alt="convolution"></p><h1 id="Receptive-Field感受野"><a href="#Receptive-Field感受野" class="headerlink" title="Receptive Field感受野"></a><font size="5" color="red">Receptive Field感受野</font></h1><p>  <font size="3">在二维卷积中不得不提到一个重要名词：<strong>感受野</strong>，在这里我只是作为科普，说一说<strong>感受野和二维卷积的关系</strong>，不从生命科学的角度具体描述感受野和神经元的关系，感兴趣的小伙伴可以去网上搜索。</font><br>  <font size="3">想象一下，当我们看一场足球比赛，或者看一场精彩的电影时，我们的注意力集中于某个点，比如足球的运动轨迹，电影中任务的细节描写。我们的眼睛只是关注一个像素吗？答案是否定的，我们<strong>关注的是周围了一部分区域</strong>，这个<strong>区域可以称之为感受野</strong>。当我们看这个人的眼神细节时，我们还会注意到面部的动作，而很难注意到耳朵或者其他部位的变化。而卷积操作也是相同，对于某个中心点求卷积，只是计算这个点周围的值，而不去计算距离很远的像素点。意在<strong>让计算机根据人类的视觉行为做出类似的判断</strong>。</font><br><img src="/images/deep_learning/field.png" alt="field"></p><h1 id="CNN卷积神经网络"><a href="#CNN卷积神经网络" class="headerlink" title="CNN卷积神经网络"></a><font size="5" color="red">CNN卷积神经网络</font></h1><p>  <font size="3">CNN是目前深度学习领域中非常具有代表性的神经网络之一，在<strong>图像分析和处理领域</strong>取得了众多突破性的进展，包括<strong>图像识别</strong>，<strong>语义分割</strong>，<strong>目标检测</strong>等等。</font><br>  <font size="3">关于卷积的计算过程，小伙伴们应该都比较了解，通过最上面的图也可以直观的看出。随着CNN的发展，尤其是2012年AlexNet网络在ImageNet上大放异彩以后，卷积神经网络持续火爆。渐渐的一些黑科技卷积也被陆续发现。这个博客目的是向大家介绍各种卷积之间的差异。</font><br><img src="/images/deep_learning/cnn.png" alt="cnn"></p><h1 id="Depthwise-Convolution"><a href="#Depthwise-Convolution" class="headerlink" title="Depthwise Convolution"></a><font size="5" color="red">Depthwise Convolution</font></h1><p><img src="/images/deep_learning/depthwise.png" alt="depthwise"><br>  <font size="3"><strong>Depthwise Convolution(深度卷积)：在</strong>每一个通道上单独进行卷积**</font><br>  <font size="3">参数<strong>depth_multiplier默认为1</strong>，代表每个通道数进行一次单独卷积，<strong>输出的通道数和输入通道数相等</strong>，设置<strong>depth_multiplier=n</strong>，则代表每个通道数进行n次单独卷积，<strong>输出通道数是输入通道数的n倍</strong>。</font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>，主要用于<strong>轻量级深度学习网络</strong>，在<strong>MobileNet，EfficientNet，ShuffleNet</strong>网络中都有大量使用。如果一个8x8x1024的特征图，经过5x5的卷积核后变为8x8x1024的图像，经过普通卷积的参数量为1024x(1024x5x5+1)=26215424，而深度卷积参数量为1024x(1x5x5+1)=26624，参数量缩小了约1024倍。</font></p><h1 id="Pointwise-Convolution"><a href="#Pointwise-Convolution" class="headerlink" title="Pointwise Convolution"></a><font size="5" color="red">Pointwise Convolution</font></h1><p><img src="/images/deep_learning/pointwise.png" alt="pointwise"><br>  <font size="3"><strong>Pointwise Convolution(点卷积)</strong>：很好理解，卷积核的大小为1x1，小伙伴们可能产生疑问？1x1卷积有什么作用呢？</font></p><ol><li><strong>改变通道数</strong>，可以实现<strong>升维</strong>或者<strong>降维</strong>，在<strong>ResNet，MobileNet</strong>网络中有重要作用。</li><li><strong>增加非线性关系</strong>，在保持特征图尺度的前提下，可以<strong>利用非线性激活函数增加网络深度</strong>。</li><li><strong>实现跨通道信息交互</strong>，往往和Depthwise Convolution结合使用。</li></ol><h1 id="Separable-Convolution"><a href="#Separable-Convolution" class="headerlink" title="Separable Convolution"></a><font size="5" color="red">Separable Convolution</font></h1><p><img src="/images/Feature_extraction/Xception_D.png" alt="Xception"><br>  <font size="3"><strong>Separable Convolution(深度可分离卷积)</strong>：是上面两个卷积合二为一的卷积操作。</font><br>  <font size="3"><strong>第一步：DepthwiseConv，对每一个通道进行卷积</strong></font><br>  <font size="3"><strong>第二步：PointwiseConv，对第一步得到的结果进行1x1卷积，实现通道融合</strong></font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>，并且可以<strong>调整为任意合适的通道数</strong>，在<strong>Xception，MobileNet，EfficientNet，ShuffleNet</strong>网络中有大量使用。第一步的<strong>目的是减少参数量</strong>，第二步是<strong>调整通道数</strong>，因此将两个卷积操作结合，组成深度可分离卷积。</font></p><h1 id="Spatial-Separable-Convolution"><a href="#Spatial-Separable-Convolution" class="headerlink" title="Spatial Separable Convolution"></a><font size="5" color="red">Spatial Separable Convolution</font></h1><p><img src="/images/deep_learning/spatial.png" alt="spatial"><br>  <font size="3"><strong>Spatial Separable Convolution(空间可分离卷积)</strong>：将3x3的卷积分解为3x1的卷积核1x3的卷积，将7x7的卷积分解为7x1的卷积核1x7的卷积.。</font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>，在<strong>Inception类型</strong>的网络中有大量使用。如果一个64x64x256的特征图，经过7x7的卷积核后变为64x64x256的图像，经过普通卷积的参数量为256x(256x7x7+1)=3211520，而空间可分离卷积参数量为2x256x(256x7x1+1)=918016，参数量缩小了约3.5倍。</font></p><h1 id="Atrous-Convolution"><a href="#Atrous-Convolution" class="headerlink" title="Atrous Convolution"></a><font size="5" color="red">Atrous Convolution</font></h1><p><img src="/images/Semantic_segmentation/PSPNet_D.png" alt="PSPNet"><br>  <font size="3"><strong>Atrous Convolution(空洞卷积)</strong>：又称<strong>膨胀卷积(Dilated Convolution)</strong>，在卷积层引入了一个<strong>膨胀率(dilation rate)</strong>参数，定义了卷积核的间隔数量，普通卷积的卷积核dilation rate=1。</font><br>  <font size="3">优点：<strong>扩大感受野</strong>，相邻的像素点可能存在大量冗余信息，扩大感受野可能会获取多尺度信息，这在视觉任务上非常重要，且<strong>不需要引入额外参数</strong>，如果增加分辨率或者采用大尺寸的卷积核则会大大增加模型的参数量，在<strong>PSPNet，DeepLab-V3+</strong>网络中有大量使用。</font><br>  <font size="3">缺点：由于空洞卷积的<strong>计算方式类似于棋盘格式，因此可能产生棋盘格效应，可以参考<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">棋盘格可视化</a>。如果膨胀率太大卷积结果之间没有相关性，可能会丢失局部信息。</strong></font></p><h1 id="Group-Convolution"><a href="#Group-Convolution" class="headerlink" title="Group Convolution"></a><font size="5" color="red">Group Convolution</font></h1><p><img src="/images/Feature_extraction/ShuffleNet_V2_G.png" alt="ShuffleNet_V2"><br>  <font size="3"><strong>Group Convolution(分组卷积)</strong>：<strong>传统卷积是采用一种卷积全连接的思想</strong>，特征图中的每一个像素点都结合了图像中所有通道的信息。而分组卷积特征图像<strong>每一个像素点只利用到一部分原始图像的通道</strong>。</font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>，在<strong>ResNeXt，ShuffleNet-V2</strong>网络中有大量使用。如果一个64x64x256的图像，经过5x5的卷积核后变为64x64x256的图像，经过普通卷积的参数量为256x(256x5x5+1)=1638656，而分成32组的分组卷积的参数量为256x(8*5x5+1)=51456，参数量缩小了约32倍，当组数变成通道数时，则类似于Depthwise Convolution深度卷积</font></p><h1 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a><font size="5" color="red">Deconvolution</font></h1><p><img src="/images/deep_learning/deconvolution.png" alt="deconvolution"><br>  <font size="3"><strong>Deconvolution(反卷积)</strong>：<strong>本质是卷积</strong>，注意<strong>反卷积并不能从卷积的结果返回到卷积前的数据，只能返回到卷积前的尺寸</strong>。卷积通过设置kernel_size卷积核大小，strides步长和padding填充方式可以将图像的分辨率降低，相反的反卷积可以通过设置kernel_size卷积核大小，strides步长和padding填充方式<strong>先对数据进行填充，然后再进行卷积操作</strong>，可以将图像的分辨率增加。<strong>这个方法不推荐经常使用，因为存在大量参数，而且可能会存在棋盘格效应，可以参考<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">棋盘格可视化</a></strong>。</font></p><h1 id="Squeeze-and-Excitation"><a href="#Squeeze-and-Excitation" class="headerlink" title="Squeeze-and-Excitation"></a><font size="5" color="red">Squeeze-and-Excitation</font></h1><p><img src="/images/Feature_extraction/SENet_S.png" alt="SENet"><br>  <font size="3"><strong>Squeeze-and-Excitation</strong>：又称为<strong>特征重标定卷积</strong>，或者<strong>注意力机制</strong>。具体来说，就是通过<strong>学习的方式来自动获取到每个特征通道的重要程度</strong>，然后依照这个重要程度去<strong>提升有用的特征并抑制对当前任务用处不大的特征</strong>,在<strong>SENet，MobileNet-V3，EfficientNet</strong>网络中有大量使用。</font><br>  <font size="3">首先是 <strong>Squeeze操作</strong>，先<strong>进行全局池化，具有全局的感受野</strong>，并且输出的维度和输入的特征通道数相匹配，它表征着在特征通道上响应的全局分布。</font><br>  <font size="3">然后是<strong>Excitation操作</strong>，<strong>通过全连接层为每个特征通道生成权重，建立通道间的相关性</strong>，<strong>输出的权重看做是进过特征选择后的每个特征通道的重要性</strong>，然后通过<strong>乘法逐通道加权到先前的特征上</strong>，完成在通道维度上的对原始特征的重标定。</font></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  卷积操作是CNN的核心，因此在学习时常常会和它们打交道，因此系统的学习各种卷积的优缺点以及利用场景，对今后的学习工作是非常有帮助的，希望小伙伴们都可以学习和掌握。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Convolution&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>Activation黑科技</title>
    <link href="https://USTCcoder.github.io/2020/05/02/deep%20learning%20activation/"/>
    <id>https://USTCcoder.github.io/2020/05/02/deep learning activation/</id>
    <published>2020-05-02T02:48:16.000Z</published>
    <updated>2020-09-04T15:38:44.541Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Activation</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Activation(激活函数)</strong>:在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数。如果不用激活函数每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这样会导致网络的逼近能力大大降低，所以需要引入非线性函数作为激活函数，这样可以提高神经网络的表达能力，可以逼近任意函数，不再是输入的线性组合。<br><a id="more"></a></p><p><img src="/images/deep_learning/activation.png" alt="activation"></p><h1 id="Sigmoid激活函数"><a href="#Sigmoid激活函数" class="headerlink" title="Sigmoid激活函数"></a><font size="5" color="red">Sigmoid激活函数</font></h1><script type="math/tex; mode=display">f(z) = \frac{1}{1+e^{-z}}</script><p>在TensorFlow2.0中的实现<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot(x, y):</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line">x = np.linspace(-10, 10, 201)</span><br><span class="line">y = keras.activations.sigmoid(x)</span><br><span class="line">plot(x, y)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/sigmoid.png" alt="sigmoid"></p><h1 id="tanh激活函数"><a href="#tanh激活函数" class="headerlink" title="tanh激活函数"></a><font size="5" color="red">tanh激活函数</font></h1><script type="math/tex; mode=display">f(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}</script><p>在TensorFlow2.0中的实现<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot(x, y):</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line">x = np.linspace(-10, 10, 201)</span><br><span class="line">y = keras.activations.tanh(x)</span><br><span class="line">plot(x, y)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/tanh.png" alt="tanh"></p><h1 id="ReLU激活函数"><a href="#ReLU激活函数" class="headerlink" title="ReLU激活函数"></a><font size="5" color="red">ReLU激活函数</font></h1><script type="math/tex; mode=display">f(x) = \max(0, x)</script><p>在TensorFlow2.0中的实现<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot(x, y):</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line">x = np.linspace(-10, 10, 201)</span><br><span class="line">y = keras.activations.relu(x)</span><br><span class="line">plot(x, y)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/relu.png" alt="relu"></p><h1 id="Leaky-ReLU激活函数"><a href="#Leaky-ReLU激活函数" class="headerlink" title="Leaky-ReLU激活函数"></a><font size="5" color="red">Leaky-ReLU激活函数</font></h1><script type="math/tex; mode=display">f(x) = \max(\alpha x, x), \alpha=0.01</script><p>在TensorFlow2.0中的实现<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot(x, y):</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line">x = np.linspace(-10, 10, 201)</span><br><span class="line">y = keras.activations.relu(x, alpha=0.01)</span><br><span class="line">plot(x, y)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/leaky_relu.png" alt="leaky_relu"></p><h1 id="PReLU激活函数"><a href="#PReLU激活函数" class="headerlink" title="PReLU激活函数"></a><font size="5" color="red">PReLU激活函数</font></h1><script type="math/tex; mode=display">f(x) = \max(\alpha x, x)</script><p><strong>PReLU和Leaky-ReLU的表达式是相同的</strong>，区别在于<strong>Leaky-ReLU中的<script type="math/tex">\alpha</script>是预先设定的</strong>，而<strong>PReLU中的参数是根据数据，通过网络自身学习的</strong>。TensorFlow2.0中直接提供了PReLU层<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">keras.layers.layers.PReLU()</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/leaky_relu.png" alt="prelu"></p><h1 id="ReLU6激活函数"><a href="#ReLU6激活函数" class="headerlink" title="ReLU6激活函数"></a><font size="5" color="red">ReLU6激活函数</font></h1><script type="math/tex; mode=display">f(x) = \min(6, \max(0, x))</script><p>在TensorFlow2.0中的实现<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot(x, y):</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line">x = np.linspace(-10, 10, 201)</span><br><span class="line">y = keras.activations.relu(x, max_value=6)</span><br><span class="line">plot(x, y)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/relu6.png" alt="relu6"></p><h1 id="ELU激活函数"><a href="#ELU激活函数" class="headerlink" title="ELU激活函数"></a><font size="5" color="red">ELU激活函数</font></h1><script type="math/tex; mode=display">f(x) = \begin{cases}  x &  x > 0 \\\\ \alpha(e^{x} - 1) & x \le 0 \end{cases}, \alpha=1</script><p>在TensorFlow2.0中的实现<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot(x, y):</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line">x = np.linspace(-10, 10, 201)</span><br><span class="line">y = keras.activations.elu(x, alpha=1)</span><br><span class="line">plot(x, y)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/elu.png" alt="elu"></p><h1 id="SELU激活函数"><a href="#SELU激活函数" class="headerlink" title="SELU激活函数"></a><font size="5" color="red">SELU激活函数</font></h1><script type="math/tex; mode=display">f(x) = \lambda \begin{cases}  x &  x > 0 \\\\ \alpha(e^{x} - 1) & x \le 0 \end{cases}</script><script type="math/tex; mode=display">\begin{cases} \lambda=1.0507009873554804934193349852946 \\\\ \alpha=1.6732632423543772848170429916717 \end{cases}</script><p>在TensorFlow2.0中的实现，在SELU函数中，经过大量论证后，<strong>两个参数都为定值，因此不需要设置参数</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot(x, y):</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line">x = np.linspace(-10, 10, 201)</span><br><span class="line">y = keras.activations.selu(x)</span><br><span class="line">plot(x, y)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/selu.png" alt="selu"></p><h1 id="Mish激活函数"><a href="#Mish激活函数" class="headerlink" title="Mish激活函数"></a><font size="5" color="red">Mish激活函数</font></h1><script type="math/tex; mode=display">f(x) = x * tanh(\ln{(1+e^x)})</script><p>综合了tanh和Softplus的优点，类似于ReLU函数，但是对负值有轻微的梯度，其平滑的特点可能允许更好的信息深入神经网络，几乎在所有的问题上都有很好的表现，在TensorFlow2.0中的实现。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot(x, y):</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line">x = np.linspace(-10, 10, 201)</span><br><span class="line">y = x * keras.activations.tanh(keras.activations.softplus(x))</span><br><span class="line">plot(x, y)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/mish.png" alt="mish"></p><h1 id="Softplus激活函数"><a href="#Softplus激活函数" class="headerlink" title="Softplus激活函数"></a><font size="5" color="red">Softplus激活函数</font></h1><script type="math/tex; mode=display">f(x) = \ln{(1+e^x)}</script><p>在TensorFlow2.0中的实现<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot(x, y):</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line">x = np.linspace(-10, 10, 201)</span><br><span class="line">y = keras.activations.softplus(x)</span><br><span class="line">plot(x, y)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/softplus.png" alt="softplus"></p><h1 id="Swish激活函数"><a href="#Swish激活函数" class="headerlink" title="Swish激活函数"></a><font size="5" color="red">Swish激活函数</font></h1><script type="math/tex; mode=display">f(x) = x * sigmoid(x)</script><p>在TensorFlow2.0中的实现<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot(x, y):</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line">x = np.linspace(-10, 10, 201)</span><br><span class="line">y = x * keras.activations.sigmoid(x)</span><br><span class="line">plot(x, y)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/swish.png" alt="swish"></p><h1 id="H-Swish激活函数"><a href="#H-Swish激活函数" class="headerlink" title="H-Swish激活函数"></a><font size="5" color="red">H-Swish激活函数</font></h1><script type="math/tex; mode=display">f(x) = x * \frac{ReLU6(x + 3)}{6}</script><p>在TensorFlow2.0中的实现<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow.keras as keras</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot(x, y):</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line">x = np.linspace(-10, 10, 201)</span><br><span class="line">y = x * keras.activations.relu(x + 3, max_value=6) / 6</span><br><span class="line">plot(x, y)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/h_swish.png" alt="h_swish"></p><h1 id="Softmax激活函数"><a href="#Softmax激活函数" class="headerlink" title="Softmax激活函数"></a><font size="5" color="red">Softmax激活函数</font></h1><script type="math/tex; mode=display">\sigma_{i}(z) = \frac{e^{z_i}}{\sum_{j=1}^{m}{e^{z_j}}}</script><p>在TensorFlow2.0中的实现，和其他的激活函数不同，<strong>Softmax激活函数需要指定一个维度，而且使用时也必须先转化为大于一维的tensor形式，numpy格式的数据无法直接使用</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = tf.constant([[10, 1, 1, 1]], tf.float32)</span><br><span class="line">y = keras.activations.softmax(x)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/deep_learning/softmax.png" alt="softmax"></p><h1 id="优缺点比较"><a href="#优缺点比较" class="headerlink" title="优缺点比较"></a><font size="5" color="red">优缺点比较</font></h1><p><strong>Sigmoid</strong>函数优点：Sigmoid函数处处连续，处处可导。且<strong>能够控制数值的幅度</strong>，不会产生很大的变化，可以<strong>作为二分类任务的输出</strong>，而ReLU类型的激活函数对大于0的值几乎没有约束。<br><strong>Sigmoid</strong>函数缺点：饱和区的神经元会产生<strong>梯度消失</strong>现象，使得<strong>学习速度大大下降</strong>，并且指数函数计算耗时。<br><strong>tanh</strong>函数优点：tanh函数和Sigmoid类似，但是可以发现<strong>tanh的导数的值域为(0, 1]，而Sigmoid的导数的值域为(0, 0.25]</strong>，因此相当于<strong>延迟了饱和周期</strong>。<br><strong>tanh</strong>函数缺点：tanh函数和Sigmoid类似，也具有<strong>梯度消失</strong>问题，和指数计算耗时问题。<br><strong>ReLU</strong>类函数优点：ReLU类型函数(<strong>ReLU6，Leaky_ReLU，PReLU，SELU，ELU，Mish，Softplus</strong>)会使一部分神经元为0或者非常小，使得网络具有<strong>稀疏性</strong>，<strong>减少了参数的相互依赖关系</strong>，<strong>缓解了过拟合</strong>，而且ReLU函数及其导数的计算非常简单。<br><strong>ReLU</strong>类函数缺点：可能<strong>存在神经元坏死</strong>现象，在x&lt;0的时候，梯度为0，可能会使这个神经元<strong>很难再被激活</strong>，且ReLU函数不能控制参数的幅度，可能会产生<strong>梯度爆炸</strong>现象。<br><strong>Swish</strong>类函数优点：Swish函数是<strong>介于ReLU函数和Sigmoid函数之间的一种平滑函数</strong>，<strong>具有两者的优点</strong>，不会像Sigmoid函数一样产生饱和区，也不会像ReLU函数一样存在坏死神经元。<br><strong>Swish类</strong>函数缺点：Swish函数也<strong>具有两者的缺点</strong>，类似于Sigmoid函数计算耗时，类似于ReLU函数难以控制参数幅度，但整体表现较好。<br><strong>Softmax</strong>函数特点：Softmax函数和其他的激活函数不同，Softmax主要用于<strong>多分类任务</strong>中，如<strong>图像分割</strong>，<strong>目标检测</strong>，需要判断某一个像素或者某一个预测框属于哪一个类别。<strong>Softmax将输入归一化到[0, 1]之间，并且保证和为1</strong>，使人能够联想到概率的条件，也是属于[0, 1]，并且和为1。加上指数的作用是<strong>增加样本之间的差距</strong>，如果输入为90个1和1个10，则直接归一化的结果为90个0.01和1个0.1，如果10是对应的类别，即使已经分类的较好，仍然会使得误差较大。加上指数运算后，归一化的结果为90个0.000122，1个0.989，这样误差就会较小，更加接近于真实的情况。</p><h1 id="激活函数的选择"><a href="#激活函数的选择" class="headerlink" title="激活函数的选择"></a><font size="5" color="red">激活函数的选择</font></h1><ol><li>首先<strong>判断任务类型</strong>，是分类任务，回归任务，还是作为隐藏层非线性单元，如果是<strong>多分类任务则考虑Softmax激活函数</strong>，如果是<strong>二分类任务则考虑Sigmoid，tanh激活函数</strong>，如果是<strong>回归任务则可以考虑不加激活函数，因为激活函数可能会对回归的数据产生限制</strong>，如果是<strong>隐藏层非线性单元则考虑Sigmoid，tanh，ReLU，Swish</strong>等等。</li><li>如果是隐藏层非线性单元，<strong>首先尝试ReLU激活函数</strong>，如果<strong>ReLU效果欠佳则考虑ReLU变种激活函数</strong>，ReLU6，Leaky_ReLU，SELU，ELU，Mish，Softplus等等</li><li>如果效果不好，<strong>再考虑Swish类函数和Sigmoid，tanh函数</strong>，但是如果<strong>发现梯度消失问题，则避免使用Sigmoid和tanh函数</strong>。</li><li>如果都不好用，则<strong>考虑是否网络结构，超参数，损失函数设计出现问题</strong>。</li></ol><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  深度学习工程问题是一类非常复杂的问题，往往需要<strong>网络结构</strong>，<strong>超参数</strong>，<strong>损失函数</strong>，<strong>激活函数相互配合</strong>工作，可能某个结构或者某个参数适合某个激活函数，而另外的结构适合其他激活函数。因此需要小伙伴们在实际的工程任务中慢慢摸索，多多尝试。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Activation&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常用技巧" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
    
    
  </entry>
  
  <entry>
    <title>目标检测数据集</title>
    <link href="https://USTCcoder.github.io/2020/04/30/Object%20detection%20Dataset/"/>
    <id>https://USTCcoder.github.io/2020/04/30/Object detection Dataset/</id>
    <published>2020-04-30T06:05:07.000Z</published>
    <updated>2020-05-09T09:17:48.193Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Data Set</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>目标检测:</strong>是计算机视觉的<strong>基础任务</strong>，近几年来，目标检测算法取得了很大的突破，主流趋势是两种，一种是<strong>one-stage</strong>算法，以<strong>SSD，YOLO</strong>为代表，另一类是<strong>two-stage</strong>算法，以<strong>Faster R-CNN</strong>为代表。广泛应用于生活之中，包括<strong>人脸检测</strong>，<strong>自动驾驶</strong>等等方面，在近期的疫情之中也发挥了巨大的作用，在火车站，地铁口都应用到了人脸检测方法，检测到人脸后利用红外对体温进行测量，因此能<strong>手动搭建一些目标检测网络</strong>，对今后的学习工作都是非常有帮助的。<br><a id="more"></a></p><p><img src="/images/Object_detection/Dataset.png" alt="Dataset"></p><h1 id="一步法和两步法的区别"><a href="#一步法和两步法的区别" class="headerlink" title="一步法和两步法的区别"></a><font size="5" color="red">一步法和两步法的区别</font></h1><p>一步法：在原图像上面铺设大量锚点框(anchor)，然后在<strong>特征提取的时候对锚点框进行一次回归和分类</strong>，得到最终的检测结果<br>两步法：在原图像上面铺设大量锚点框(anchor)，然后<strong>先利用一个网络对锚点框进行一次分类和回归(粗筛选)</strong>，得到<strong>建议框</strong>，然后再<strong>对建议框进行一次回归和分类</strong>得到最终的检测结果。<br>优缺点：经过上面的描述，容易看出，一步法的优点是<strong>效率高</strong>，只需要一步即可完成最终检测，没有耗时的第二步，但是缺点也很明显，因为没有粗筛选，导致<strong>正负样本极端不平衡</strong>，因此<strong>检测精度略低于两步法</strong>。两步法虽然<strong>检测速度慢</strong>，但是检测<strong>精度略高于一步法</strong>。随着网络的发展，硬件水平的提高，两类算法都在不断的进步之中，速度和精度都可以取得较好的结果。</p><h1 id="数据集以及IOU，NMS介绍"><a href="#数据集以及IOU，NMS介绍" class="headerlink" title="数据集以及IOU，NMS介绍"></a><font size="5" color="red">数据集以及IOU，NMS介绍</font></h1><p><strong>数据集</strong>：为了方便模型调试的方便，我的博客中介绍的数据集是一种简单的Shape数据集，只有1000个训练样本，为了加快训练速度，数据集的大小我也调整为128x128，这个数据集只有三类物体，分别是圆形，三角形和正方形，图像数据为jpg文件，标签数据为xml文件，其中记录了物体出现的左上角和右下角坐标。<br><strong>IOU(Intersection Over Union，交并比)</strong>：用于<strong>评估语义分割算法性能的指标是平均IOU</strong>，交并比也非常好理解，算法的结果与真实物体进行<strong>交运算的结果除以进行并运算的结果</strong>。通过下图可以直观的看出IOU的计算方法。<br><img src="/images/Semantic_segmentation/Dataset_I.png" alt="IOU"><br><strong>NMS(Non-Maximum Suppression，非极大值抑制)</strong>：简单地说，<strong>不是最大的我不要</strong>，在目标检测中，往往图像上存在大量先验框，会导致很多附近的框都会预测出同一个物体，但是我们<strong>只保留最大的一个预测结果</strong>，这就是非极大值抑制。<br>步骤：<br>(1)<strong>从最大概率矩形框F开始</strong>，分别判断A~E与F的IOU是否大于某个设定的阈值，<strong>假设B、D与F的重叠度超过阈值，那么就扔掉B、D</strong>；并<strong>标记第一个矩形框F</strong>，是我们保留下来的。<br>(2)<strong>从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框</strong>。<br>(3)<strong>重复步骤(2)，直到所有的框都被抛弃或者保留</strong>。<br><img src="/images/Object_detection/Dataset_N.png" alt="NMS"></p><h1 id="一些说明"><a href="#一些说明" class="headerlink" title="一些说明"></a><font size="5" color="red">一些说明</font></h1><ol><li>在学习的时候，小伙伴可能会遇到一些代码上的困难，如<strong>tensorflow</strong>，<strong>numpy</strong>，<strong>opencv</strong>的用法，可以查看我的深度学习框架和Python常用库相关文章，里面会有一些简单的介绍，小伙伴们可以进行学习，最好是手动敲一敲，看一看。</li><li>因为这个博客是对学习的一些总结和记录，意在和学习者探讨和交流，并且给准备入门的同学一些手把手的教学，因此关于目标检测的算法参数设计，我都是自己尝试的，不是针对于这个数据集最优的参数，大家可以根据自己的实际需要修改网络结构。</li><li>实际的工程应用中，常常还需要对数据集进行<strong>大小调整和增强</strong>，在这里为了简单起见，没有进行复杂的操作，小伙伴们应用中要记得根据自己的需要，对图像进行<strong>resize或者padding</strong>，然后<strong>旋转</strong>，<strong>对比度增强</strong>，<strong>仿射运算</strong>等等操作，增加模型的鲁棒性，并且实际中的图像不一定按照顺序命名的，因此应用中也要注意图像读取的文件名。</li><li>为了让学习者看的方便和清晰，我没有使用多个文件对程序进行封装，因为我在刚开始学习模型的时候，查看GitHub代码，一个模型可能需要好几个文件夹，每个文件夹里面又有很多的代码文件，其中很多文件互相调用。虽然这样的工程项目是非常好管理和运行的，但是给初学者一种丈二和尚摸不着头脑的感觉，对此我深有体会。所以我就使用一个.py文件来封装，因此代码可能会有几百行，但是其中的各个函数和类都有自己的名字，可以保证学习者不会被纸老虎吓住。</li><li>在目标检测学习中，我会列举出一些经典的目标检测模型，因为模型太多，并且仍在不断的更新进步之中，所以大家可以联系我，和我进行沟通和交流，或者推荐给我一些优秀的模型。</li><li>关于问题的交流，图像的数据，需要的同学可以到主页查看我的QQ或者邮箱，我会非常荣幸的提供力所能及的帮助，小伙伴加好友的时候一定要记得备注，不然我可能会忽视一些粗心的小伙伴。</li></ol><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  目标检测是计算机视觉的<strong>基础任务</strong>，也是非常重要的任务之一，自从深度学习的时代到来，各种神经网络结构百花齐放，很难说出最好的目标检测方法，可能一个方法适用于很多数据，但也<strong>不能说明某一个算法一定优于另一个算法</strong>，我们要做的就是尽可能多的<strong>学习各种各样的深度学习模型</strong>，然后<strong>吸取这些模型成功的原因</strong>，投入到自己的工程应用之中。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Data Set&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>DeepLab-V3+</title>
    <link href="https://USTCcoder.github.io/2020/04/27/Semantic_segmentation%20DeepLab_V3+/"/>
    <id>https://USTCcoder.github.io/2020/04/27/Semantic_segmentation DeepLab_V3+/</id>
    <published>2020-04-27T13:10:21.000Z</published>
    <updated>2020-06-18T16:01:23.849Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">DeepLab-V3+</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>DeepLab-V3+</strong>:于<strong>2018年发表在CVPR</strong>上，应用<strong>改进的Xception作为特征提取网络</strong>，并将<strong>深度可分离卷积</strong>与<strong>ASPP(Atrous Spatial Pyramid Pooling，空洞空间卷积池化金字塔)</strong>结合，大量缩小模型参数，被认为是现在<strong>语义分割模型的新高峰</strong>。<a id="more"></a></p><p><img src="/images/Semantic_segmentation/DeepLab_V3+.png" alt="DeepLab-V3+"></p><h1 id="DeepLab-V3-特点"><a href="#DeepLab-V3-特点" class="headerlink" title="DeepLab-V3+特点"></a><font size="5" color="red">DeepLab-V3+特点</font></h1><p>  <font size="3">改进了Xception，将<strong>Middle Flow从8层变为16层</strong>，加深网络层数，且将<strong>池化层替换为深度可分离卷积层</strong>，并且在<strong>3x3深度可分离卷积层后添加BN层和ReLU层</strong>。</font><br>  <font size="3">使用ASPP结构，其中<strong>设置不同的dilation_rate提取不同尺度的特征</strong>。</font><br>  <font size="3">从<strong>Xception浅层网络中提取出一个分支</strong>，作为浅层特征，和ASPP结构产生的深层特征进行融合</font></p><h1 id="空洞卷积-atrous-convolutions-和普通卷积之间的区别"><a href="#空洞卷积-atrous-convolutions-和普通卷积之间的区别" class="headerlink" title="空洞卷积(atrous convolutions)和普通卷积之间的区别"></a><font size="5" color="red">空洞卷积(atrous convolutions)和普通卷积之间的区别</font></h1><p>  <font size="3"><strong>空洞卷积(atrous convolutions)又称膨胀卷积(dilated convolutions)</strong>，在卷积层引入了一个<strong>膨胀率(dilation rate)</strong>参数，定义了卷积核的间隔数量，普通卷积的卷积核dilation rate=1</font><br>  <font size="3">优点：<strong>扩大感受野</strong>，相邻的像素点可能存在大量冗余信息，扩大感受野可能会获取多尺度信息，这在视觉任务上非常重要，且<strong>不需要引入额外参数</strong>，如果增加分辨率或者采用大尺寸的卷积核则会大大增加模型的参数量。</font><br>  <font size="3">缺点：由于空洞卷积的<strong>计算方式类似于棋盘格式，因此可能产生棋盘格效应，可以参考<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">棋盘格可视化</a>。如果膨胀率太大卷积结果之间没有相关性，可能会丢失局部信息。</strong></font><br><img src="/images/Semantic_segmentation/PSPNet_D.png" alt="PSPNet"></p><h1 id="DeepLab-V3-图像分析"><a href="#DeepLab-V3-图像分析" class="headerlink" title="DeepLab-V3+图像分析"></a><font size="5" color="red">DeepLab-V3+图像分析</font></h1><p><img src="/images/Semantic_segmentation/DeepLab_V3+_A.png" alt="DeepLab-V3+"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_ReLU(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, name, dilation_rate=(1, 1)):</span><br><span class="line">        super(Conv_Bn_ReLU, self).__init__(name=name)</span><br><span class="line">        self.blocks = keras.Sequential()</span><br><span class="line">        self.blocks.add(keras.layers.Conv2D(filters, kernel_size, strides, padding='same', dilation_rate=dilation_rate))</span><br><span class="line">        self.blocks.add(keras.layers.BatchNormalization())</span><br><span class="line">        if name.find('relu') != -1:</span><br><span class="line">            self.blocks.add(keras.layers.ReLU())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        output = self.blocks(inputs)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Depthwiseconv_Bn_ReLU(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, name, dilation_rate=(1, 1)):</span><br><span class="line">        super(Depthwiseconv_Bn_ReLU, self).__init__(name=name)</span><br><span class="line">        self.blocks = keras.Sequential()</span><br><span class="line">        self.blocks.add(keras.layers.DepthwiseConv2D(kernel_size, strides, padding='same', dilation_rate=dilation_rate))</span><br><span class="line">        self.blocks.add(keras.layers.BatchNormalization())</span><br><span class="line">        self.blocks.add(keras.layers.ReLU())</span><br><span class="line">        self.blocks.add(Conv_Bn_ReLU(filters, (1, 1), (1, 1), name=name))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        output = self.blocks(inputs)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def block(x, filters, strides, name, dilation_rate=(1, 1)):</span><br><span class="line">    x1 = Depthwiseconv_Bn_ReLU(filters[0], (3, 3), (1, 1), name='{}_depthwiseconv_bn_relu1'.format(name), dilation_rate=dilation_rate)(x)</span><br><span class="line">    x2 = Depthwiseconv_Bn_ReLU(filters[1], (3, 3), (1, 1), name='{}_depthwiseconv_bn_relu2'.format(name), dilation_rate=dilation_rate)(x1)</span><br><span class="line">    x3 = Depthwiseconv_Bn_ReLU(filters[2], (3, 3), strides, name='{}_depthwiseconv_bn3'.format(name), dilation_rate=dilation_rate)(x2)</span><br><span class="line">    if name.find('sum') != -1:</span><br><span class="line">        output = keras.layers.Add(name='{}_add'.format(name))([x3, x])</span><br><span class="line">    elif name.find('none') != -1:</span><br><span class="line">        output = x3</span><br><span class="line">    else:</span><br><span class="line">        short_cut = compose(keras.layers.Conv2D(filters[-1], (1, 1), strides=strides, name='{}_shortcut_conv'.format(name)),</span><br><span class="line">                            keras.layers.BatchNormalization(name='{}_shortcut_bn'.format(name)))(x)</span><br><span class="line">        output = keras.layers.Add(name='{}_add'.format(name))([x3, short_cut])</span><br><span class="line">    output = keras.layers.ReLU(name='{}_relu'.format(name))(output)</span><br><span class="line">    if name.find('skip') != -1:</span><br><span class="line">        output = output, x1</span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def aspp(x, filters, dilation_rate, name):</span><br><span class="line">    x1 = compose(keras.layers.GlobalAveragePooling2D(name='{}_part1_globalaveragepool'.format(name)),</span><br><span class="line">                 keras.layers.Reshape((1, 1, x.shape[-1]), name='{}_part1_reshape'.format(name)),</span><br><span class="line">                 Conv_Bn_ReLU(filters, (1, 1), (1, 1), name='{}_part1_conv_bn_relu'.format(name)))(x)</span><br><span class="line">    x1 = tf.image.resize(x1, (x.shape[1], x.shape[2]), name='{}_part1_reshape'.format(name))</span><br><span class="line">    x2 = Conv_Bn_ReLU(filters, (1, 1), (1, 1), name='{}_part2_conv_bn_relu'.format(name))(x)</span><br><span class="line">    x3 = Depthwiseconv_Bn_ReLU(filters, (3, 3), (1, 1), name='{}_part3_depthwiseconv_bn_relu'.format(name), dilation_rate=dilation_rate[0])(x)</span><br><span class="line">    x4 = Depthwiseconv_Bn_ReLU(filters, (3, 3), (1, 1), name='{}_part4_depthwiseconv_bn_relu'.format(name), dilation_rate=dilation_rate[1])(x)</span><br><span class="line">    x5 = Depthwiseconv_Bn_ReLU(filters, (3, 3), (1, 1), name='{}_part5_depthwiseconv_bn_relu'.format(name), dilation_rate=dilation_rate[2])(x)</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x1, x2, x3, x4, x5])</span><br><span class="line"></span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def deeplab_v3_plus(input_shape):</span><br><span class="line"></span><br><span class="line">    input_tensor = keras.layers.Input(shape=input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_ReLU(32, (3, 3), (2, 2), name='conv_bn_relu1'),</span><br><span class="line">                Conv_Bn_ReLU(64, (3, 3), (1, 1), name='conv_bn_relu2'))(x)</span><br><span class="line"></span><br><span class="line">    x = block(x, [128, 128, 128], (2, 2), name='entryflow1_conv')</span><br><span class="line">    x, skip = block(x, [256, 256, 256], (2, 2), name='entryflow2_skip_conv')</span><br><span class="line">    x = block(x, [728, 728, 728], (2, 2), name='entryflow3_conv')</span><br><span class="line"></span><br><span class="line">    for i in range(16):</span><br><span class="line">        x = block(x, [728, 728, 728], (1, 1), name='middleflow{}_sum'.format(i + 1))</span><br><span class="line"></span><br><span class="line">    x = block(x, [728, 1024, 1024], (1, 1), name='exitflow1_conv')</span><br><span class="line">    x = block(x, [1536, 1536, 2048], (1, 1), name='exitflow2_none', dilation_rate=(2, 2))</span><br><span class="line"></span><br><span class="line">    x = aspp(x, 256, [6, 12, 18], name='aspp')</span><br><span class="line">    x = compose(Conv_Bn_ReLU(256, (1, 1), (1, 1), name='conv_bn_relu3'),</span><br><span class="line">                keras.layers.Dropout(0.1, name='dropout'))(x)</span><br><span class="line">    x = tf.image.resize(x, (input_shape[0] // 4, input_shape[1] // 4), name='resize1')</span><br><span class="line">    skip = Conv_Bn_ReLU(48, (1, 1), (1, 1), name='skip_conv_bn_relu')(skip)</span><br><span class="line">    concatenate = keras.layers.Concatenate(name='concatenate')([x, skip])</span><br><span class="line"></span><br><span class="line">    output = compose(Depthwiseconv_Bn_ReLU(256, (3, 3), (1, 1), name='depthwiseconv_bn_relu4'),</span><br><span class="line">                     Depthwiseconv_Bn_ReLU(256, (3, 3), (1, 1), name='depthwiseconv_bn_relu5'),</span><br><span class="line">                     keras.layers.Conv2D(21, (1, 1), (1, 1), 'same', name='conv6'))(concatenate)</span><br><span class="line"></span><br><span class="line">    output = tf.image.resize(output, (input_shape[0], input_shape[1]), name='resize2')</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Softmax(name='softmax')(output)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, output, name='DeepLab-V3+')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = deeplab_v3_plus(input_shape=(512, 512, 3))</span><br><span class="line">    model.build(input_shape=(None, 512, 512, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Semantic_segmentation/DeepLab_V3+_R.png" alt="DeepLab-V3+"></p><h1 id="Shape数据集完整实战"><a href="#Shape数据集完整实战" class="headerlink" title="Shape数据集完整实战"></a><font size="5" color="red">Shape数据集完整实战</font></h1><h2 id="文件路径关系说明"><a href="#文件路径关系说明" class="headerlink" title="文件路径关系说明"></a>文件路径关系说明</h2><ul><li>project<ul><li>shape<ul><li>train_imgs(训练集图像文件夹)</li><li>train_mask(训练集掩模文件夹)</li><li>test_imgs(测试集图像文件夹)</li></ul></li><li>DeepLab-V3+_weight(模型权重文件夹)</li><li>DeepLab-V3+_test_result(测试集结果文件夹)</li><li>DeepLab-V3+.py</li></ul></li></ul><h2 id="实战步骤说明"><a href="#实战步骤说明" class="headerlink" title="实战步骤说明"></a>实战步骤说明</h2><ol><li>语义分割实战运行较为简单，因为它的输入的训练数据为图像，输入的标签数据也是图像，首先<strong>要对输入的标签数据进行编码，转换为类别信息</strong>，要和网络的输出维度相匹配，从(batch_size, height, width, 1)转换为(batch_size, height, width, num_class + 1)，<strong>某个像素点为哪一个类别，则在该通道上置1，其余通道置0</strong>。即神经网络的输入大小为(batch_size, height, width, 3)，输出大小为(batch_size, height, width, num_class + 1)。</li><li>设计损失函数，简单情况设置交叉熵损失函数即可达到较好效果。</li><li>搭建神经网络，<strong>设置合适参数</strong>，进行训练。</li><li>预测时，需要根据神经网络的输出进行<strong>逆向解码(编码的反过程)</strong>，寻找<strong>每一个像素点，哪一个通道上值最大则归为哪一个类别</strong>，即可完成实战的过程。</li></ol><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>设置的图像<strong>类别数为实际类别数+1</strong>，1代表背景类别，<strong>此数据集为3类，最后的通道数为4，每一个通道预测一类物体</strong>。在通道方向求Softmax，并且求出最大的索引，索引为0则代表背景，索引为1则代表圆形，索引为2则代表三角形，索引为3则代表正方形。</li><li>ASPP模块中的膨胀率，可以<strong>根据需要进行调整</strong>，论文中膨胀率分别为6， 12， 18，在这个简单数据集中，输入尺寸为16，因此我选择的膨胀率是2，4，8。</li><li>设置了<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li>使用<strong>yield</strong>关键字，产生可迭代对象，不用将所有的数据都保存下来，大大节约内存。</li><li>其中将1000个数据，分成800个训练集，100个验证集和100个测试集，小伙伴们可以自行修改。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>DeepLab-V3+的<strong>特征提取网络为改进的Xception</strong>，论文中也比较了ResNet特征提取网络的性能。实战中我选择的是Middle Flow为8的Xception，小伙伴们可以参考特征提取网络部分内容，选择其他的网络进行特征提取，比较不同网络参数量，运行速度，最终结果之间的差异。</li><li>关于特征提取网络的输出，论文中给出原尺寸缩小16倍的和缩小8倍的，两者结构差异<strong>仅在缩小16倍的Entry Flow中有一个模块strides=(2, 2)</strong>，性能差异不大，但是缩小8倍的网络参数量大大增加。在实战中，数据集的尺寸较小，因此我选择了缩小8倍的网络参数，小伙伴们在使用时可以酌情修改。</li><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>实际的工程应用中，常常还需要对数据集进行<strong>大小调整和增强</strong>，在这里为了简单起见，没有进行复杂的操作，小伙伴们应用中要记得根据自己的需要，对图像进行<strong>resize或者padding</strong>，然后<strong>旋转</strong>，<strong>对比度增强</strong>，<strong>仿射运算</strong>等等操作，增加模型的鲁棒性，并且实际中的图像不一定按照顺序命名的，因此应用中也要注意图像读取的文件名。</li></ol><h2 id="完整实战代码"><a href="#完整实战代码" class="headerlink" title="完整实战代码"></a>完整实战代码</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from functools import reduce</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_ReLU(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, name, dilation_rate=(1, 1)):</span><br><span class="line">        super(Conv_Bn_ReLU, self).__init__(name=name)</span><br><span class="line">        self.blocks = keras.Sequential()</span><br><span class="line">        self.blocks.add(keras.layers.Conv2D(filters, kernel_size, strides, padding='same', dilation_rate=dilation_rate))</span><br><span class="line">        self.blocks.add(keras.layers.BatchNormalization())</span><br><span class="line">        if name.find('relu') != -1:</span><br><span class="line">            self.blocks.add(keras.layers.ReLU())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        output = self.blocks(inputs)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Depthwiseconv_Bn_ReLU(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, name, dilation_rate=(1, 1)):</span><br><span class="line">        super(Depthwiseconv_Bn_ReLU, self).__init__(name=name)</span><br><span class="line">        self.blocks = keras.Sequential()</span><br><span class="line">        self.blocks.add(keras.layers.DepthwiseConv2D(kernel_size, strides, padding='same', dilation_rate=dilation_rate))</span><br><span class="line">        self.blocks.add(keras.layers.BatchNormalization())</span><br><span class="line">        self.blocks.add(keras.layers.ReLU())</span><br><span class="line">        self.blocks.add(Conv_Bn_ReLU(filters, (1, 1), (1, 1), name=name))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        output = self.blocks(inputs)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def block(x, filters, strides, name, dilation_rate=(1, 1)):</span><br><span class="line">    x1 = Depthwiseconv_Bn_ReLU(filters[0], (3, 3), (1, 1), name='{}_depthwiseconv_bn_relu1'.format(name), dilation_rate=dilation_rate)(x)</span><br><span class="line">    x2 = Depthwiseconv_Bn_ReLU(filters[1], (3, 3), (1, 1), name='{}_depthwiseconv_bn_relu2'.format(name), dilation_rate=dilation_rate)(x1)</span><br><span class="line">    x3 = Depthwiseconv_Bn_ReLU(filters[2], (3, 3), strides, name='{}_depthwiseconv_bn3'.format(name), dilation_rate=dilation_rate)(x2)</span><br><span class="line">    if name.find('sum') != -1:</span><br><span class="line">        output = keras.layers.Add(name='{}_add'.format(name))([x3, x])</span><br><span class="line">    elif name.find('none') != -1:</span><br><span class="line">        output = x3</span><br><span class="line">    else:</span><br><span class="line">        short_cut = compose(keras.layers.Conv2D(filters[-1], (1, 1), strides=strides, name='{}_shortcut_conv'.format(name)),</span><br><span class="line">                            keras.layers.BatchNormalization(name='{}_shortcut_bn'.format(name)))(x)</span><br><span class="line">        output = keras.layers.Add(name='{}_add'.format(name))([x3, short_cut])</span><br><span class="line">    output = keras.layers.ReLU(name='{}_relu'.format(name))(output)</span><br><span class="line">    if name.find('skip') != -1:</span><br><span class="line">        output = output, x1</span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def aspp(x, filters, dilation_rate, name):</span><br><span class="line">    x1 = compose(keras.layers.GlobalAveragePooling2D(name='{}_part1_globalaveragepool'.format(name)),</span><br><span class="line">                 keras.layers.Reshape((1, 1, x.shape[-1]), name='{}_part1_reshape'.format(name)),</span><br><span class="line">                 Conv_Bn_ReLU(filters, (1, 1), (1, 1), name='{}_part1_conv_bn_relu'.format(name)))(x)</span><br><span class="line">    x1 = tf.image.resize(x1, (x.shape[1], x.shape[2]), name='{}_part1_reshape'.format(name))</span><br><span class="line">    x2 = Conv_Bn_ReLU(filters, (1, 1), (1, 1), name='{}_part2_conv_bn_relu'.format(name))(x)</span><br><span class="line">    x3 = Depthwiseconv_Bn_ReLU(filters, (3, 3), (1, 1), name='{}_part3_depthwiseconv_bn_relu'.format(name), dilation_rate=dilation_rate[0])(x)</span><br><span class="line">    x4 = Depthwiseconv_Bn_ReLU(filters, (3, 3), (1, 1), name='{}_part4_depthwiseconv_bn_relu'.format(name), dilation_rate=dilation_rate[1])(x)</span><br><span class="line">    x5 = Depthwiseconv_Bn_ReLU(filters, (3, 3), (1, 1), name='{}_part5_depthwiseconv_bn_relu'.format(name), dilation_rate=dilation_rate[2])(x)</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x1, x2, x3, x4, x5])</span><br><span class="line"></span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def small_deeplab_v3_plus(input_shape):</span><br><span class="line"></span><br><span class="line">    input_tensor = keras.layers.Input(shape=input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_ReLU(32, (3, 3), (2, 2), name='conv_bn_relu'))(x)</span><br><span class="line"></span><br><span class="line">    x = block(x, [128, 128, 128], (2, 2), name='entryflow1_conv')</span><br><span class="line">    x, skip = block(x, [256, 256, 728], (2, 2), name='entryflow2_skip_conv')</span><br><span class="line"></span><br><span class="line">    for i in range(8):</span><br><span class="line">        x = block(x, [728, 728, 728], (1, 1), name='middleflow{}_sum'.format(i + 1))</span><br><span class="line"></span><br><span class="line">    x = block(x, [728, 1024, 1024], (1, 1), name='exitflow1_conv')</span><br><span class="line">    x = block(x, [1536, 1536, 2048], (1, 1), name='exitflow2_none', dilation_rate=(2, 2))</span><br><span class="line"></span><br><span class="line">    x = aspp(x, 256, [2, 4, 8], name='aspp')</span><br><span class="line">    x = compose(Conv_Bn_ReLU(256, (1, 1), (1, 1), name='conv_bn_relu3'),</span><br><span class="line">                keras.layers.Dropout(0.1, name='dropout'))(x)</span><br><span class="line">    x = tf.image.resize(x, (input_shape[0] // 4, input_shape[1] // 4), name='resize1')</span><br><span class="line">    skip = Conv_Bn_ReLU(24, (1, 1), (1, 1), name='skip_conv_bn_relu')(skip)</span><br><span class="line">    concatenate = keras.layers.Concatenate(name='concatenate')([x, skip])</span><br><span class="line"></span><br><span class="line">    output = compose(Depthwiseconv_Bn_ReLU(256, (3, 3), (1, 1), name='depthwiseconv_bn_relu4'),</span><br><span class="line">                     Depthwiseconv_Bn_ReLU(256, (3, 3), (1, 1), name='depthwiseconv_bn_relu5'),</span><br><span class="line">                     keras.layers.Conv2D(num_class, (1, 1), (1, 1), 'same', name='conv6'))(concatenate)</span><br><span class="line"></span><br><span class="line">    output = tf.image.resize(output, (input_shape[0], input_shape[1]), name='resize2')</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Softmax(name='softmax')(output)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, output, name='Small_DeepLab-V3+')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generate_arrays_from_file(train_data, batch_size):</span><br><span class="line">    # 获取总长度</span><br><span class="line">    n = len(train_data)</span><br><span class="line">    i = 0</span><br><span class="line">    while 1:</span><br><span class="line">        X_train = []</span><br><span class="line">        Y_train = []</span><br><span class="line">        # 获取一个batch_size大小的数据</span><br><span class="line">        for _ in range(batch_size):</span><br><span class="line">            if i == 0:</span><br><span class="line">                np.random.shuffle(train_data)</span><br><span class="line">            # 从文件中读取图像</span><br><span class="line">            img = cv.imread(imgs_path + '\\' + str(train_data[i]) + '.jpg')</span><br><span class="line">            img = img / 127.5 - 1</span><br><span class="line">            X_train.append(img)</span><br><span class="line"></span><br><span class="line">            # 从文件中读取图像</span><br><span class="line">            img = cv.imread(mask_path + '\\' + str(train_data[i]) + '.png')</span><br><span class="line">            seg_labels = np.zeros((img_size[0], img_size[1], num_class))</span><br><span class="line">            for c in range(num_class):</span><br><span class="line">                seg_labels[:, :, c] = (img[:, :, 0] == c).astype(int)</span><br><span class="line">            Y_train.append(seg_labels)</span><br><span class="line"></span><br><span class="line">            # 读完一个周期后重新开始</span><br><span class="line">            i = (i + 1) % n</span><br><span class="line">        yield tf.constant(X_train), tf.constant(Y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    # 包括背景</span><br><span class="line">    num_class = 4</span><br><span class="line">    train_data = list(range(800))</span><br><span class="line">    validation_data = list(range(800, 900))</span><br><span class="line">    test_data = range(900, 1000)</span><br><span class="line">    epochs = 50</span><br><span class="line">    batch_size = 16</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    img_size = (128, 128)</span><br><span class="line">    colors = [[0, 0, 0], [0, 0, 128], [0, 128, 0], [128, 0, 0]]</span><br><span class="line"></span><br><span class="line">    mask_path = r'.\shape\train_mask'</span><br><span class="line">    imgs_path = r'.\shape\train_imgs'</span><br><span class="line">    test_path = r'.\shape\test_imgs'</span><br><span class="line">    save_path = r'.\DeepLab_V3_Plus_test_result'</span><br><span class="line">    weight_path = r'.\DeepLab_V3_Plus_weight'</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        os.mkdir(save_path)</span><br><span class="line">    except FileExistsError:</span><br><span class="line">        print(save_path + 'has been exist')</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        os.mkdir(weight_path)</span><br><span class="line">    except FileExistsError:</span><br><span class="line">        print(weight_path + 'has been exist')</span><br><span class="line"></span><br><span class="line">    model = small_deeplab_v3_plus(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line"></span><br><span class="line">    model.build(input_shape=(None, img_size[0], img_size[1], 3))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    optimizor = keras.optimizers.Adam(lr=1e-3)</span><br><span class="line">    lossor = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=optimizor, loss=lossor, metrics=['accuracy'])</span><br><span class="line"></span><br><span class="line">    # 保存的方式，3世代保存一次</span><br><span class="line">    checkpoint_period = keras.callbacks.ModelCheckpoint(</span><br><span class="line">        weight_path + '\\' + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        save_weights_only=True,</span><br><span class="line">        save_best_only=True,</span><br><span class="line">        period=3</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 学习率下降的方式，val_loss3次不下降就下降学习率继续训练</span><br><span class="line">    reduce_lr = keras.callbacks.ReduceLROnPlateau(</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        factor=0.5,</span><br><span class="line">        patience=3,</span><br><span class="line">        verbose=1</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 是否需要早停，当val_loss一直不下降的时候意味着模型基本训练完毕，可以停止</span><br><span class="line">    early_stopping = keras.callbacks.EarlyStopping(</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        min_delta=0,</span><br><span class="line">        patience=10,</span><br><span class="line">        verbose=1</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    model.fit_generator(generate_arrays_from_file(train_data, batch_size),</span><br><span class="line">                        steps_per_epoch=max(1, len(train_data) // batch_size),</span><br><span class="line">                        validation_data=generate_arrays_from_file(validation_data, batch_size),</span><br><span class="line">                        validation_steps=max(1, len(validation_data) // batch_size),</span><br><span class="line">                        epochs=epochs,</span><br><span class="line">                        callbacks=[checkpoint_period, reduce_lr, early_stopping])</span><br><span class="line"></span><br><span class="line">    for name in test_data:</span><br><span class="line">        test_img_path = test_path + '\\' + str(name) + '.jpg'</span><br><span class="line">        save_img_path = save_path + '\\' + str(name) + '.png'</span><br><span class="line">        test_img = cv.imread(test_img_path)</span><br><span class="line">        test_img = tf.constant([test_img / 127.5 - 1])</span><br><span class="line">        test_mask = model.predict(test_img)</span><br><span class="line">        test_mask = np.reshape(test_mask, (img_size[0], img_size[1], num_class))</span><br><span class="line">        test_mask = np.argmax(test_mask, axis=-1)</span><br><span class="line">        seg_img = np.zeros((img_size[0], img_size[1], 3))</span><br><span class="line">        for c in range(num_class):</span><br><span class="line">            seg_img[:, :, 0] += ((test_mask == c) * (colors[c][0]))</span><br><span class="line">            seg_img[:, :, 1] += ((test_mask == c) * (colors[c][1]))</span><br><span class="line">            seg_img[:, :, 2] += ((test_mask == c) * (colors[c][2]))</span><br><span class="line">        seg_img = seg_img.astype(np.uint8)</span><br><span class="line">        cv.imwrite(save_img_path, seg_img)</span><br></pre></td></tr></tbody></table></figure><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Semantic_segmentation/DeepLab_V3+_T.png" alt="DeepLab-V3+"></p><h1 id="DeepLab-V3-小结"><a href="#DeepLab-V3-小结" class="headerlink" title="DeepLab-V3+小结"></a><font size="5" color="red">DeepLab-V3+小结</font></h1><p>  DeepLab-V3+是一种高效的语义分割网络，从上图可以看出DeepLab-V3+模型的参数量只有41M，就目前来说，DeepLab-V3+是<strong>语义分割公认的最高峰</strong>，主要来源于<strong>ASPP网络结构和深浅层特征融合</strong>，是小伙伴们需要掌握的一个模型。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;DeepLab-V3+&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="语义分割网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>PSPNet</title>
    <link href="https://USTCcoder.github.io/2020/04/24/Semantic_segmentation%20PSPNet/"/>
    <id>https://USTCcoder.github.io/2020/04/24/Semantic_segmentation PSPNet/</id>
    <published>2020-04-24T08:09:28.000Z</published>
    <updated>2020-06-18T16:01:09.892Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">PSPNet</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>PSPNet</strong>:由香港中文大学和商汤科技提出，获得<strong>2016年ImageNet场景解析挑战的冠军</strong>，于<strong>2017发表在CVPR</strong>，通过使用<strong>金字塔池化模块</strong>完成图像分割，是一种高效的语义分割模型。<a id="more"></a></p><p><img src="/images/Semantic_segmentation/PSPNet.png" alt="PSPNet"></p><h1 id="PSPNet特点"><a href="#PSPNet特点" class="headerlink" title="PSPNet特点"></a><font size="5" color="red">PSPNet特点</font></h1><p>  <font size="3">特征提取网络选择施加<strong>空洞卷积(atrous convolutions)的ResNet</strong>，并且选择<strong>AL(auxiliary loss, 辅助损失)</strong>对ResNet进行训练，通常在某一层后接着几个转换层和全连接层，最后分类预测，并且给予<strong>损失小于1的权重</strong>，完成辅助损失，目的是<strong>缓解深度神经网络中梯度消失的问题。</strong></font><br>  <font size="3">使用<strong>金字塔池化模块聚合信息，根据不同内核的池化层，获取不同尺度的图像信息</strong>，然后再Concatenate，完成信息的融合。</font></p><h1 id="空洞卷积-atrous-convolutions-和普通卷积之间的区别"><a href="#空洞卷积-atrous-convolutions-和普通卷积之间的区别" class="headerlink" title="空洞卷积(atrous convolutions)和普通卷积之间的区别"></a><font size="5" color="red">空洞卷积(atrous convolutions)和普通卷积之间的区别</font></h1><p>  <font size="3"><strong>空洞卷积(atrous convolutions)又称膨胀卷积(dilated convolutions)</strong>，在卷积层引入了一个<strong>膨胀率(dilation rate)</strong>参数，定义了卷积核的间隔数量，普通卷积的卷积核dilation rate=1</font><br>  <font size="3">优点：<strong>扩大感受野</strong>，相邻的像素点可能存在大量冗余信息，扩大感受野可能会获取多尺度信息，这在视觉任务上非常重要，且<strong>不需要引入额外参数</strong>，如果增加分辨率或者采用大尺寸的卷积核则会大大增加模型的参数量。</font><br>  <font size="3">缺点：由于空洞卷积的<strong>计算方式类似于棋盘格式，因此可能产生棋盘格效应，可以参考<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">棋盘格可视化</a>。如果膨胀率太大卷积结果之间没有相关性，可能会丢失局部信息。</strong></font><br><img src="/images/Semantic_segmentation/PSPNet_D.png" alt="PSPNet"></p><h1 id="PSPNet图像分析"><a href="#PSPNet图像分析" class="headerlink" title="PSPNet图像分析"></a><font size="5" color="red">PSPNet图像分析</font></h1><p><img src="/images/Semantic_segmentation/PSPNet_A.png" alt="PSPNet"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_ReLU(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, name, dilation_rate=(1, 1)):</span><br><span class="line">        super(Conv_Bn_ReLU, self).__init__(name=name)</span><br><span class="line">        self.blocks = keras.Sequential()</span><br><span class="line">        self.blocks.add(keras.layers.Conv2D(filters, kernel_size, strides, padding='same', dilation_rate=dilation_rate))</span><br><span class="line">        self.blocks.add(keras.layers.BatchNormalization())</span><br><span class="line">        self.blocks.add(keras.layers.ReLU())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        output = self.blocks(inputs)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def res_block(x, filters, strides, name, dilation_rate=(1, 1)):</span><br><span class="line">    shortcut = x</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_ReLU(filters // 4, (1, 1), (1, 1), name='{}_conv_bn_relu1'.format(name)),</span><br><span class="line">                Conv_Bn_ReLU(filters // 4, (3, 3), strides, name='{}_conv_bn_relu2'.format(name), dilation_rate=dilation_rate),</span><br><span class="line">                keras.layers.Conv2D(filters, (1, 1), name='{}_conv3'.format(name)),</span><br><span class="line">                keras.layers.BatchNormalization(name='{}_bn3'.format(name)))(x)</span><br><span class="line">    if name.find('conv_block') != -1:</span><br><span class="line">        shortcut = keras.layers.Conv2D(filters, (1, 1), strides, name='{}_shortcut_conv'.format(name))(shortcut)</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Add(name='{}_add'.format(name))([x, shortcut])</span><br><span class="line">    output = keras.layers.ReLU(name='{}_relu'.format(name))(output)</span><br><span class="line"></span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def psp_block(x, name):</span><br><span class="line"></span><br><span class="line">    p1 = compose(keras.layers.MaxPool2D((60, 60), name='{}_part1_maxpool'.format(name)),</span><br><span class="line">                 Conv_Bn_ReLU(512, (1, 1), (1, 1), name='{}_part1_conv_bn_relu'.format(name)))(x)</span><br><span class="line">    p2 = compose(keras.layers.MaxPool2D((30, 30), name='{}_part2_maxpool'.format(name)),</span><br><span class="line">                 Conv_Bn_ReLU(512, (1, 1), (1, 1), name='{}_part2_conv_bn_relu'.format(name)))(x)</span><br><span class="line">    p3 = compose(keras.layers.MaxPool2D((20, 20), name='{}_part3_maxpool'.format(name)),</span><br><span class="line">                 Conv_Bn_ReLU(512, (1, 1), (1, 1), name='{}_part3_conv_bn_relu'.format(name)))(x)</span><br><span class="line">    p4 = compose(keras.layers.MaxPool2D((10, 10), name='{}_part4_maxpool'.format(name)),</span><br><span class="line">                 Conv_Bn_ReLU(512, (1, 1), (1, 1), name='{}_part4_conv_bn_relu'.format(name)))(x)</span><br><span class="line">    input_size = (x.shape[1], x.shape[2])</span><br><span class="line">    p1 = tf.image.resize(p1, input_size, name='{}_resize1'.format(name))</span><br><span class="line">    p2 = tf.image.resize(p2, input_size, name='{}_resize2'.format(name))</span><br><span class="line">    p3 = tf.image.resize(p3, input_size, name='{}_resize3'.format(name))</span><br><span class="line">    p4 = tf.image.resize(p4, input_size, name='{}_resize4'.format(name))</span><br><span class="line">    output = keras.layers.Concatenate(name='{}_concatenate'.format(name))([p1, p2, p3, p4, x])</span><br><span class="line"></span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def pspnet(input_shape):</span><br><span class="line"></span><br><span class="line">    input_tensor = keras.layers.Input(shape=input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_ReLU(64, (3, 3), (2, 2), name='conv_bn_relu1'),</span><br><span class="line">                Conv_Bn_ReLU(64, (3, 3), (1, 1), name='conv_bn_relu2'),</span><br><span class="line">                Conv_Bn_ReLU(128, (3, 3), (1, 1), name='conv_bn_relu3'),</span><br><span class="line">                keras.layers.MaxPool2D((3, 3), (2, 2), 'same', name='maxpool1'))(x)</span><br><span class="line"></span><br><span class="line">    filters = [256, 512, 1024, 2048]</span><br><span class="line">    strides = [(1, 1), (2, 2), (1, 1), (1, 1)]</span><br><span class="line">    dilation_rate = [(1, 1), (1, 1), (2, 2), (4, 4)]</span><br><span class="line">    times = [2, 3, 22, 2]</span><br><span class="line">    for i in range(len(filters)):</span><br><span class="line">        x = res_block(x, filters[i], strides=strides[i], name='conv_block{}'.format(i + 1))</span><br><span class="line">        for j in range(times[i]):</span><br><span class="line">            x = res_block(x, filters[i], strides=(1, 1), name='identity_block{}_{}'.format(i + 1, j + 1), dilation_rate=dilation_rate[i])</span><br><span class="line"></span><br><span class="line">    x = psp_block(x, name='psp_block')</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_ReLU(512, (1, 1), (1, 1), name='conv_bn_relu4'),</span><br><span class="line">                keras.layers.Dropout(0.1, name='dropout'),</span><br><span class="line">                keras.layers.Conv2D(21, (1, 1), (1, 1), 'same', name='conv5'))(x)</span><br><span class="line">    x = tf.image.resize(x, (input_shape[0], input_shape[1]), name='resize')</span><br><span class="line">    output = keras.layers.Softmax(name='softmax')(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, output, name='PSPNet')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = pspnet(input_shape=(473, 473, 3))</span><br><span class="line">    model.build(input_shape=(None, 473, 473, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Semantic_segmentation/PSPNet_R.png" alt="PSPNet"></p><h1 id="Shape数据集完整实战"><a href="#Shape数据集完整实战" class="headerlink" title="Shape数据集完整实战"></a><font size="5" color="red">Shape数据集完整实战</font></h1><h2 id="文件路径关系说明"><a href="#文件路径关系说明" class="headerlink" title="文件路径关系说明"></a>文件路径关系说明</h2><ul><li>project<ul><li>shape<ul><li>train_imgs(训练集图像文件夹)</li><li>train_mask(训练集掩模文件夹)</li><li>test_imgs(测试集图像文件夹)</li></ul></li><li>PSPNet_weight(模型权重文件夹)</li><li>PSPNet_test_result(测试集结果文件夹)</li><li>PSPNet.py</li></ul></li></ul><h2 id="实战步骤说明"><a href="#实战步骤说明" class="headerlink" title="实战步骤说明"></a>实战步骤说明</h2><ol><li>语义分割实战运行较为简单，因为它的输入的训练数据为图像，输入的标签数据也是图像，首先<strong>要对输入的标签数据进行编码，转换为类别信息</strong>，要和网络的输出维度相匹配，从(batch_size, height, width, 1)转换为(batch_size, height, width, num_class + 1)，<strong>某个像素点为哪一个类别，则在该通道上置1，其余通道置0</strong>。即神经网络的输入大小为(batch_size, height, width, 3)，输出大小为(batch_size, height, width, num_class + 1)。</li><li>设计损失函数，简单情况设置交叉熵损失函数即可达到较好效果。</li><li>搭建神经网络，<strong>设置合适参数</strong>，进行训练。</li><li>预测时，需要根据神经网络的输出进行<strong>逆向解码(编码的反过程)</strong>，寻找<strong>每一个像素点，哪一个通道上值最大则归为哪一个类别</strong>，即可完成实战的过程。</li></ol><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>设置的图像<strong>类别数为实际类别数+1</strong>，1代表背景类别，<strong>此数据集为3类，最后的通道数为4，每一个通道预测一类物体</strong>。在通道方向求Softmax，并且求出最大的索引，索引为0则代表背景，索引为1则代表圆形，索引为2则代表三角形，索引为3则代表正方形。</li><li>在PSPNet中只使用ResNet101的最后一层，<strong>可以借鉴UNet的思想，可以使用多层输出，实现多尺度特征融合。</strong></li><li>设置了<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li>使用<strong>yield</strong>关键字，产生可迭代对象，不用将所有的数据都保存下来，大大节约内存。</li><li>其中将1000个数据，分成800个训练集，100个验证集和100个测试集，小伙伴们可以自行修改。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>金字塔池化模块中的池化核，可以<strong>根据需要进行调整</strong>，论文中金字塔池化模块的输入尺寸为60x60，因此可以进行60x60，30x30，20x20，10x10的池化核，在这个简单数据集中，输入尺寸为8，因此我选择的是8x8，4x4，2x2，1x1的池化核。</li><li>PSPNet的<strong>特征提取网络为ResNet101</strong>，实战中我选择的是ResNet50，小伙伴们可以参考特征提取网络部分内容，选择其他的网络进行特征提取，比较不同网络参数量，运行速度，最终结果之间的差异。</li><li>在论文中提到的AL(辅助损失)是在构建ResNet101特征提取网络时使用的，在这里我们为了简单起见，直接使用ResNet50。</li><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>实际的工程应用中，常常还需要对数据集进行<strong>大小调整和增强</strong>，在这里为了简单起见，没有进行复杂的操作，小伙伴们应用中要记得根据自己的需要，对图像进行<strong>resize或者padding</strong>，然后<strong>旋转</strong>，<strong>对比度增强</strong>，<strong>仿射运算</strong>等等操作，增加模型的鲁棒性，并且实际中的图像不一定按照顺序命名的，因此应用中也要注意图像读取的文件名。</li></ol><h2 id="完整实战代码"><a href="#完整实战代码" class="headerlink" title="完整实战代码"></a>完整实战代码</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from functools import reduce</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_ReLU(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, name, dilation_rate=(1, 1)):</span><br><span class="line">        super(Conv_Bn_ReLU, self).__init__(name=name)</span><br><span class="line">        self.blocks = keras.Sequential()</span><br><span class="line">        self.blocks.add(keras.layers.Conv2D(filters, kernel_size, strides, padding='same', dilation_rate=dilation_rate))</span><br><span class="line">        self.blocks.add(keras.layers.BatchNormalization())</span><br><span class="line">        self.blocks.add(keras.layers.ReLU())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        output = self.blocks(inputs)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def res_block(x, filters, strides, name, dilation_rate=(1, 1)):</span><br><span class="line">    shortcut = x</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_ReLU(filters // 4, (1, 1), (1, 1), name='{}_conv_bn_relu1'.format(name)),</span><br><span class="line">                Conv_Bn_ReLU(filters // 4, (3, 3), strides, name='{}_conv_bn_relu2'.format(name), dilation_rate=dilation_rate),</span><br><span class="line">                keras.layers.Conv2D(filters, (1, 1), name='{}_conv3'.format(name)),</span><br><span class="line">                keras.layers.BatchNormalization(name='{}_bn3'.format(name)))(x)</span><br><span class="line">    if name.find('conv_block') != -1:</span><br><span class="line">        shortcut = keras.layers.Conv2D(filters, (1, 1), strides, name='{}_shortcut_conv'.format(name))(shortcut)</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Add(name='{}_add'.format(name))([x, shortcut])</span><br><span class="line">    output = keras.layers.ReLU(name='{}_relu'.format(name))(output)</span><br><span class="line"></span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def psp_block(x, name):</span><br><span class="line"></span><br><span class="line">    p1 = compose(keras.layers.MaxPool2D((x.shape[1], x.shape[2]), name='{}_part1_maxpool'.format(name)),</span><br><span class="line">                 Conv_Bn_ReLU(x.shape[-1] // 4, (1, 1), (1, 1), name='{}_part1_conv_bn_relu'.format(name)))(x)</span><br><span class="line">    p2 = compose(keras.layers.MaxPool2D((x.shape[1] // 2, x.shape[2] // 2), name='{}_part2_maxpool'.format(name)),</span><br><span class="line">                 Conv_Bn_ReLU(x.shape[-1] // 4, (1, 1), (1, 1), name='{}_part2_conv_bn_relu'.format(name)))(x)</span><br><span class="line">    p3 = compose(keras.layers.MaxPool2D((x.shape[1] // 4, x.shape[2] // 4), name='{}_part3_maxpool'.format(name)),</span><br><span class="line">                 Conv_Bn_ReLU(x.shape[-1] // 4, (1, 1), (1, 1), name='{}_part3_conv_bn_relu'.format(name)))(x)</span><br><span class="line">    p4 = compose(keras.layers.MaxPool2D((x.shape[1] // 8, x.shape[2] // 8), name='{}_part4_maxpool'.format(name)),</span><br><span class="line">                 Conv_Bn_ReLU(x.shape[-1] // 4, (1, 1), (1, 1), name='{}_part4_conv_bn_relu'.format(name)))(x)</span><br><span class="line">    input_size = (x.shape[1], x.shape[2])</span><br><span class="line">    p1 = tf.image.resize(p1, input_size, name='{}_resize1'.format(name))</span><br><span class="line">    p2 = tf.image.resize(p2, input_size, name='{}_resize2'.format(name))</span><br><span class="line">    p3 = tf.image.resize(p3, input_size, name='{}_resize3'.format(name))</span><br><span class="line">    p4 = tf.image.resize(p4, input_size, name='{}_resize4'.format(name))</span><br><span class="line">    output = keras.layers.Concatenate(name='{}_concatenate'.format(name))([p1, p2, p3, p4, x])</span><br><span class="line"></span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def small_pspnet(input_shape):</span><br><span class="line"></span><br><span class="line">    input_tensor = keras.layers.Input(shape=input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_ReLU(32, (3, 3), (2, 2), name='conv_bn_relu1'),</span><br><span class="line">                Conv_Bn_ReLU(32, (3, 3), (1, 1), name='conv_bn_relu2'),</span><br><span class="line">                Conv_Bn_ReLU(64, (3, 3), (1, 1), name='conv_bn_relu3'),</span><br><span class="line">                keras.layers.MaxPool2D((3, 3), (2, 2), 'same', name='maxpool1'))(x)</span><br><span class="line"></span><br><span class="line">    x1 = res_block(x, 128, strides=(1, 1), name='conv_block1')</span><br><span class="line">    for j in range(2):</span><br><span class="line">        x1 = res_block(x1, 128, strides=(1, 1), name='identity_block1_{}'.format(j + 1), dilation_rate=(1, 1))</span><br><span class="line"></span><br><span class="line">    x2 = res_block(x1, 256, strides=(2, 2), name='conv_block2')</span><br><span class="line">    for j in range(2):</span><br><span class="line">        x2 = res_block(x2, 256, strides=(1, 1), name='identity_block2_{}'.format(j + 1), dilation_rate=(1, 1))</span><br><span class="line"></span><br><span class="line">    x3 = res_block(x2, 512, strides=(2, 2), name='conv_block3')</span><br><span class="line">    for j in range(2):</span><br><span class="line">        x3 = res_block(x3, 512, strides=(1, 1), name='identity_block3_{}'.format(j + 1), dilation_rate=(1, 1))</span><br><span class="line"></span><br><span class="line">    x4 = res_block(x3, 1024, strides=(1, 1), name='conv_block4')</span><br><span class="line">    for j in range(2):</span><br><span class="line">        x4 = res_block(x4, 1024, strides=(1, 1), name='identity_block4_{}'.format(j + 1), dilation_rate=(2, 2))</span><br><span class="line"></span><br><span class="line">    psp4 = psp_block(x4, name='psp_block4')</span><br><span class="line">    upsampling4 = keras.layers.UpSampling2D(name='upsampling4')(psp4)</span><br><span class="line">    y2 = keras.layers.Concatenate(name='concatenate4')([x2, upsampling4])</span><br><span class="line">    y2 = Conv_Bn_ReLU(512, (1, 1), (1, 1), name='conv_bn_relu4')(y2)</span><br><span class="line">    psp2 = psp_block(y2, name='psp_block2')</span><br><span class="line"></span><br><span class="line">    y = compose(Conv_Bn_ReLU(128, (1, 1), (1, 1), name='conv_bn_relu5'),</span><br><span class="line">                keras.layers.Dropout(0.1, name='dropout'),</span><br><span class="line">                keras.layers.Conv2D(num_class, (1, 1), (1, 1), 'same', name='conv6'))(psp2)</span><br><span class="line">    y = tf.image.resize(y, (input_shape[0], input_shape[1]), name='resize')</span><br><span class="line">    output = keras.layers.Softmax(name='softmax')(y)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, output, name='Small_PSPNet')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generate_arrays_from_file(train_data, batch_size):</span><br><span class="line">    # 获取总长度</span><br><span class="line">    n = len(train_data)</span><br><span class="line">    i = 0</span><br><span class="line">    while 1:</span><br><span class="line">        X_train = []</span><br><span class="line">        Y_train = []</span><br><span class="line">        # 获取一个batch_size大小的数据</span><br><span class="line">        for _ in range(batch_size):</span><br><span class="line">            if i == 0:</span><br><span class="line">                np.random.shuffle(train_data)</span><br><span class="line">            # 从文件中读取图像</span><br><span class="line">            img = cv.imread(imgs_path + '\\' + str(train_data[i]) + '.jpg')</span><br><span class="line">            img = img / 127.5 - 1</span><br><span class="line">            X_train.append(img)</span><br><span class="line"></span><br><span class="line">            # 从文件中读取图像</span><br><span class="line">            img = cv.imread(mask_path + '\\' + str(train_data[i]) + '.png')</span><br><span class="line">            seg_labels = np.zeros((img_size[0], img_size[1], num_class))</span><br><span class="line">            for c in range(num_class):</span><br><span class="line">                seg_labels[:, :, c] = (img[:, :, 0] == c).astype(int)</span><br><span class="line">            Y_train.append(seg_labels)</span><br><span class="line"></span><br><span class="line">            # 读完一个周期后重新开始</span><br><span class="line">            i = (i + 1) % n</span><br><span class="line">        yield tf.constant(X_train), tf.constant(Y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    # 包括背景</span><br><span class="line">    num_class = 4</span><br><span class="line">    train_data = list(range(800))</span><br><span class="line">    validation_data = list(range(800, 900))</span><br><span class="line">    test_data = range(900, 1000)</span><br><span class="line">    epochs = 50</span><br><span class="line">    batch_size = 16</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    img_size = (128, 128)</span><br><span class="line">    colors = [[0, 0, 0], [0, 0, 128], [0, 128, 0], [128, 0, 0]]</span><br><span class="line"></span><br><span class="line">    mask_path = r'.\shape\train_mask'</span><br><span class="line">    imgs_path = r'.\shape\train_imgs'</span><br><span class="line">    test_path = r'.\shape\test_imgs'</span><br><span class="line">    save_path = r'.\PSPNet_test_result'</span><br><span class="line">    weight_path = r'.\PSPNet_weight'</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        os.mkdir(save_path)</span><br><span class="line">    except FileExistsError:</span><br><span class="line">        print(save_path + 'has been exist')</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        os.mkdir(weight_path)</span><br><span class="line">    except FileExistsError:</span><br><span class="line">        print(weight_path + 'has been exist')</span><br><span class="line"></span><br><span class="line">    model = small_pspnet(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line">    model.build(input_shape=(None, img_size[0], img_size[1], 3))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    optimizor = keras.optimizers.Adam(lr=1e-3)</span><br><span class="line">    lossor = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=optimizor, loss=lossor, metrics=['accuracy'])</span><br><span class="line"></span><br><span class="line">    # 保存的方式，3世代保存一次</span><br><span class="line">    checkpoint_period = keras.callbacks.ModelCheckpoint(</span><br><span class="line">        weight_path + '\\' + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        save_weights_only=True,</span><br><span class="line">        save_best_only=True,</span><br><span class="line">        period=3</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 学习率下降的方式，val_loss3次不下降就下降学习率继续训练</span><br><span class="line">    reduce_lr = keras.callbacks.ReduceLROnPlateau(</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        factor=0.5,</span><br><span class="line">        patience=3,</span><br><span class="line">        verbose=1</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 是否需要早停，当val_loss一直不下降的时候意味着模型基本训练完毕，可以停止</span><br><span class="line">    early_stopping = keras.callbacks.EarlyStopping(</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        min_delta=0,</span><br><span class="line">        patience=10,</span><br><span class="line">        verbose=1</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    model.fit_generator(generate_arrays_from_file(train_data, batch_size),</span><br><span class="line">                        steps_per_epoch=max(1, len(train_data) // batch_size),</span><br><span class="line">                        validation_data=generate_arrays_from_file(validation_data, batch_size),</span><br><span class="line">                        validation_steps=max(1, len(validation_data) // batch_size),</span><br><span class="line">                        epochs=epochs,</span><br><span class="line">                        callbacks=[checkpoint_period, reduce_lr, early_stopping])</span><br><span class="line"></span><br><span class="line">    for name in test_data:</span><br><span class="line">        test_img_path = test_path + '\\' + str(name) + '.jpg'</span><br><span class="line">        save_img_path = save_path + '\\' + str(name) + '.png'</span><br><span class="line">        test_img = cv.imread(test_img_path)</span><br><span class="line">        test_img = tf.constant([test_img / 127.5 - 1])</span><br><span class="line">        test_mask = model.predict(test_img)</span><br><span class="line">        test_mask = np.reshape(test_mask, (img_size[0], img_size[1], num_class))</span><br><span class="line">        test_mask = np.argmax(test_mask, axis=-1)</span><br><span class="line">        seg_img = np.zeros((img_size[0], img_size[1], 3))</span><br><span class="line">        for c in range(num_class):</span><br><span class="line">            seg_img[:, :, 0] += ((test_mask == c) * (colors[c][0]))</span><br><span class="line">            seg_img[:, :, 1] += ((test_mask == c) * (colors[c][1]))</span><br><span class="line">            seg_img[:, :, 2] += ((test_mask == c) * (colors[c][2]))</span><br><span class="line">        seg_img = seg_img.astype(np.uint8)</span><br><span class="line">        cv.imwrite(save_img_path, seg_img)</span><br></pre></td></tr></tbody></table></figure><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Semantic_segmentation/PSPNet_T.png" alt="PSPNet"></p><h1 id="PSPNet小结"><a href="#PSPNet小结" class="headerlink" title="PSPNet小结"></a><font size="5" color="red">PSPNet小结</font></h1><p>  PSPNet是一种高效的语义分割网络，从上图可以看出PSPNet模型的参数量有49M，PSPNet不同于SegNet和UNet，没有很对称的编码解码结构，在编码过程中，使用<strong>不同尺寸金字塔池化核完成对不同尺寸特征的融合</strong>，在解码过程中，直接使用<strong>简单的resize完成对图像信息的恢复</strong>，对后面的深度学习网络的发展有重要的影响。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;PSPNet&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="语义分割网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>UNet</title>
    <link href="https://USTCcoder.github.io/2020/04/21/Semantic_segmentation%20UNet/"/>
    <id>https://USTCcoder.github.io/2020/04/21/Semantic_segmentation UNet/</id>
    <published>2020-04-21T07:13:21.000Z</published>
    <updated>2020-06-18T16:01:52.035Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">UNet</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>UNet</strong>:于<strong>2015年发表于MICCA</strong>，设计的就是应用于<strong>医学图像</strong>的分割，由于医学影响本身的性质，语义较为简单，结构较为固定，数据量较少且具有多模态的性质，根据CT灌注方法不同，具有不同的模态。UNet实现了使用<strong>少量数据集进行大尺寸图像</strong>的有效算法，因为<strong>结构类似U型，故称之为UNet。</strong><a id="more"></a></p><p><img src="/images/Semantic_segmentation/UNet.png" alt="UNet"></p><h1 id="UNet特点"><a href="#UNet特点" class="headerlink" title="UNet特点"></a><font size="5" color="red">UNet特点</font></h1><p>  <font size="3">网络结构简单，易于实现</font><br>  <font size="3">使用<strong>Over-tile</strong>策略，因为医学图像处理的图像尺寸较大，我们针对于某一区域进行分割时，可以<strong>获取周围更大尺寸的信息作为上下文</strong>，在卷积时只使用有效部分，这样<strong>防止padding=same时添加无效信息</strong>。因此图像的尺寸会缩小，在网络中需要<strong>对浅层特征进行Crop</strong>之后才可以与深层特征进行Concatenate。</font><br>  <font size="3">使用<strong>随机弹性变形对数据进行增强</strong>，增加模型的鲁棒性。</font><br>  <font size="3">使用<strong>加权Loss</strong>，对于某一点到边界的距离呈高斯关系的权重，距离边界越近权重越大，距离越远权重越小。</font></p><h1 id="UNet图像分析"><a href="#UNet图像分析" class="headerlink" title="UNet图像分析"></a><font size="5" color="red">UNet图像分析</font></h1><p><img src="/images/Semantic_segmentation/UNet_A.png" alt="UNet"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def unet(input_shape):</span><br><span class="line"></span><br><span class="line">    input_tensor = keras.layers.Input(shape=input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x1 = compose(keras.layers.Conv2D(64, (3, 3), (1, 1), 'valid', activation='relu', name='encoder_conv1_1'),</span><br><span class="line">                 keras.layers.Conv2D(64, (3, 3), (1, 1), 'valid', activation='relu', name='encoder_conv1_2'))(x)</span><br><span class="line">    x2 = compose(keras.layers.MaxPool2D((2, 2), name='encoder_maxpool1'),</span><br><span class="line">                 keras.layers.Conv2D(128, (3, 3), (1, 1), 'valid', activation='relu', name='encoder_conv2_1'),</span><br><span class="line">                 keras.layers.Conv2D(128, (3, 3), (1, 1), 'valid', activation='relu', name='encoder_conv2_2'))(x1)</span><br><span class="line">    x3 = compose(keras.layers.MaxPool2D((2, 2), name='encoder_maxpool2'),</span><br><span class="line">                 keras.layers.Conv2D(256, (3, 3), (1, 1), 'valid', activation='relu', name='encoder_conv3_1'),</span><br><span class="line">                 keras.layers.Conv2D(256, (3, 3), (1, 1), 'valid', activation='relu', name='encoder_conv3_2'))(x2)</span><br><span class="line">    x4 = compose(keras.layers.MaxPool2D((2, 2), name='encoder_maxpool3'),</span><br><span class="line">                 keras.layers.Conv2D(512, (3, 3), (1, 1), 'valid', activation='relu', name='encoder_conv4_1'),</span><br><span class="line">                 keras.layers.Conv2D(512, (3, 3), (1, 1), 'valid', activation='relu', name='encoder_conv4_2'))(x3)</span><br><span class="line">    x5 = compose(keras.layers.MaxPool2D((2, 2), name='encoder_maxpool4'),</span><br><span class="line">                 keras.layers.Conv2D(1024, (3, 3), (1, 1), 'valid', activation='relu', name='encoder_conv5_1'),</span><br><span class="line">                 keras.layers.Conv2D(1024, (3, 3), (1, 1), 'valid', activation='relu', name='encoder_conv5_2'))(x4)</span><br><span class="line"></span><br><span class="line">    crop1 = keras.layers.Cropping2D(((88, 88), (88, 88)), name='crop1')(x1)</span><br><span class="line">    crop2 = keras.layers.Cropping2D(((40, 40), (40, 40)), name='crop2')(x2)</span><br><span class="line">    crop3 = keras.layers.Cropping2D(((16, 16), (16, 16)), name='crop3')(x3)</span><br><span class="line">    crop4 = keras.layers.Cropping2D(((4, 4), (4, 4)), name='crop4')(x4)</span><br><span class="line"></span><br><span class="line">    y4 = keras.layers.UpSampling2D((2, 2), name='decoder_upsampling4')(x5)</span><br><span class="line">    concatenate4 = keras.layers.Concatenate(name='decoder_concatenate4')([crop4, y4])</span><br><span class="line">    y3 = compose(keras.layers.Conv2D(512, (3, 3), (1, 1), 'valid', activation='relu', name='decoder_conv4_1'),</span><br><span class="line">                 keras.layers.Conv2D(512, (3, 3), (1, 1), 'valid', activation='relu', name='decoder_conv4_2'),</span><br><span class="line">                 keras.layers.UpSampling2D((2, 2), name='decoder_upsampling3'))(concatenate4)</span><br><span class="line">    concatenate3 = keras.layers.Concatenate(name='decoder_concatenate3')([crop3, y3])</span><br><span class="line">    y2 = compose(keras.layers.Conv2D(256, (3, 3), (1, 1), 'valid', activation='relu', name='decoder_conv3_1'),</span><br><span class="line">                 keras.layers.Conv2D(256, (3, 3), (1, 1), 'valid', activation='relu', name='decoder_conv3_2'),</span><br><span class="line">                 keras.layers.UpSampling2D((2, 2), name='decoder_upsampling2'))(concatenate3)</span><br><span class="line">    concatenate4 = keras.layers.Concatenate(name='decoder_concatenate2')([crop2, y2])</span><br><span class="line">    y1 = compose(keras.layers.Conv2D(128, (3, 3), (1, 1), 'valid', activation='relu', name='decoder_conv2_1'),</span><br><span class="line">                 keras.layers.Conv2D(128, (3, 3), (1, 1), 'valid', activation='relu', name='decoder_conv2_2'),</span><br><span class="line">                 keras.layers.UpSampling2D((2, 2), name='decoder_upsampling1'))(concatenate4)</span><br><span class="line">    concatenate4 = keras.layers.Concatenate(name='decoder_concatenate1')([crop1, y1])</span><br><span class="line">    y0 = compose(keras.layers.Conv2D(64, (3, 3), (1, 1), 'valid', activation='relu', name='decoder_conv1_1'),</span><br><span class="line">                 keras.layers.Conv2D(64, (3, 3), (1, 1), 'valid', activation='relu', name='decoder_conv1_2'))(concatenate4)</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Conv2D(21, (1, 1), (1, 1), 'same', activation='softmax', name='conv_softmax')(y0)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, output, name='UNet')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = unet(input_shape=(572, 572, 3))</span><br><span class="line">    model.build(input_shape=(None, 572, 572, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Semantic_segmentation/UNet_R.png" alt="UNet"></p><h1 id="Shape数据集完整实战"><a href="#Shape数据集完整实战" class="headerlink" title="Shape数据集完整实战"></a><font size="5" color="red">Shape数据集完整实战</font></h1><h2 id="文件路径关系说明"><a href="#文件路径关系说明" class="headerlink" title="文件路径关系说明"></a>文件路径关系说明</h2><ul><li>project<ul><li>shape<ul><li>train_imgs(训练集图像文件夹)</li><li>train_mask(训练集掩模文件夹)</li><li>test_imgs(测试集图像文件夹)</li></ul></li><li>UNet_weight(模型权重文件夹)</li><li>UNet_test_result(测试集结果文件夹)</li><li>UNet.py</li></ul></li></ul><h2 id="实战步骤说明"><a href="#实战步骤说明" class="headerlink" title="实战步骤说明"></a>实战步骤说明</h2><ol><li>语义分割实战运行较为简单，因为它的输入的训练数据为图像，输入的标签数据也是图像，首先<strong>要对输入的标签数据进行编码，转换为类别信息</strong>，要和网络的输出维度相匹配，从(batch_size, height, width, 1)转换为(batch_size, height, width, num_class + 1)，<strong>某个像素点为哪一个类别，则在该通道上置1，其余通道置0</strong>。即神经网络的输入大小为(batch_size, height, width, 3)，输出大小为(batch_size, height, width, num_class + 1)。</li><li>设计损失函数，简单情况设置交叉熵损失函数即可达到较好效果。</li><li>搭建神经网络，<strong>设置合适参数</strong>，进行训练。</li><li>预测时，需要根据神经网络的输出进行<strong>逆向解码(编码的反过程)</strong>，寻找<strong>每一个像素点，哪一个通道上值最大则归为哪一个类别</strong>，即可完成实战的过程。</li></ol><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>设置的图像<strong>类别数为实际类别数+1</strong>，1代表背景类别，<strong>此数据集为3类，最后的通道数为4，每一个通道预测一类物体</strong>。在通道方向求Softmax，并且求出最大的索引，索引为0则代表背景，索引为1则代表圆形，索引为2则代表三角形，索引为3则代表正方形。</li><li>实际中用到的图像的尺寸一般都不是特别大，因此<strong>不需要将图像进行Crop，所以卷积的padding修改为same</strong>。</li><li>损失函数使用交叉熵即可，使用加权Loss，计算量较大，而且需要计算边缘操作。</li><li>设置了<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li>使用<strong>yield</strong>关键字，产生可迭代对象，不用将所有的数据都保存下来，大大节约内存。</li><li>其中将1000个数据，分成800个训练集，100个验证集和100个测试集，小伙伴们可以自行修改。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>UNet的<strong>特征提取网络类似于VGG</strong>，小伙伴们可以参考特征提取网络部分内容，选择其他的网络进行特征提取，比较不同网络参数量，运行速度，最终结果之间的差异。</li><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>实际的工程应用中，常常还需要对数据集进行<strong>大小调整和增强</strong>，在这里为了简单起见，没有进行复杂的操作，小伙伴们应用中要记得根据自己的需要，对图像进行<strong>resize或者padding</strong>，然后<strong>旋转</strong>，<strong>对比度增强</strong>，<strong>仿射运算</strong>等等操作，增加模型的鲁棒性，并且实际中的图像不一定按照顺序命名的，因此应用中也要注意图像读取的文件名。</li></ol><h2 id="完整实战代码"><a href="#完整实战代码" class="headerlink" title="完整实战代码"></a>完整实战代码</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from functools import reduce</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def small_unet(input_shape):</span><br><span class="line"></span><br><span class="line">    input_tensor = keras.layers.Input(shape=input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x1 = compose(keras.layers.Conv2D(32, (3, 3), (1, 1), 'same', activation='relu', name='encoder_conv1_1'),</span><br><span class="line">                 keras.layers.Conv2D(32, (3, 3), (1, 1), 'same', activation='relu', name='encoder_conv1_2'))(x)</span><br><span class="line">    x2 = compose(keras.layers.MaxPool2D((2, 2), name='encoder_maxpool1'),</span><br><span class="line">                 keras.layers.Conv2D(32, (3, 3), (1, 1), 'same', activation='relu', name='encoder_conv2_1'),</span><br><span class="line">                 keras.layers.Conv2D(32, (3, 3), (1, 1), 'same', activation='relu', name='encoder_conv2_2'))(x1)</span><br><span class="line">    x3 = compose(keras.layers.MaxPool2D((2, 2), name='encoder_maxpool2'),</span><br><span class="line">                 keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', activation='relu', name='encoder_conv3_1'),</span><br><span class="line">                 keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', activation='relu', name='encoder_conv3_2'))(x2)</span><br><span class="line">    x4 = compose(keras.layers.MaxPool2D((2, 2), name='encoder_maxpool3'),</span><br><span class="line">                 keras.layers.Conv2D(128, (3, 3), (1, 1), 'same', activation='relu', name='encoder_conv4_1'),</span><br><span class="line">                 keras.layers.Conv2D(128, (3, 3), (1, 1), 'same', activation='relu', name='encoder_conv4_2'))(x3)</span><br><span class="line">    x5 = compose(keras.layers.MaxPool2D((2, 2), name='encoder_maxpool4'),</span><br><span class="line">                 keras.layers.Conv2D(256, (3, 3), (1, 1), 'same', activation='relu', name='encoder_conv5_1'),</span><br><span class="line">                 keras.layers.Conv2D(256, (3, 3), (1, 1), 'same', activation='relu', name='encoder_conv5_2'))(x4)</span><br><span class="line"></span><br><span class="line">    y4 = keras.layers.UpSampling2D((2, 2), name='decoder_upsampling4')(x5)</span><br><span class="line">    concatenate4 = keras.layers.Concatenate(name='decoder_concatenate4')([x4, y4])</span><br><span class="line">    y3 = compose(keras.layers.Conv2D(128, (3, 3), (1, 1), 'same', activation='relu', name='decoder_conv4_1'),</span><br><span class="line">                 keras.layers.Conv2D(128, (3, 3), (1, 1), 'same', activation='relu', name='decoder_conv4_2'),</span><br><span class="line">                 keras.layers.UpSampling2D((2, 2), name='decoder_upsampling3'))(concatenate4)</span><br><span class="line">    concatenate3 = keras.layers.Concatenate(name='decoder_concatenate3')([x3, y3])</span><br><span class="line">    y2 = compose(keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', activation='relu', name='decoder_conv3_1'),</span><br><span class="line">                 keras.layers.Conv2D(64, (3, 3), (1, 1), 'same', activation='relu', name='decoder_conv3_2'),</span><br><span class="line">                 keras.layers.UpSampling2D((2, 2), name='decoder_upsampling2'))(concatenate3)</span><br><span class="line">    concatenate4 = keras.layers.Concatenate(name='decoder_concatenate2')([x2, y2])</span><br><span class="line">    y1 = compose(keras.layers.Conv2D(32, (3, 3), (1, 1), 'same', activation='relu', name='decoder_conv2_1'),</span><br><span class="line">                 keras.layers.Conv2D(32, (3, 3), (1, 1), 'same', activation='relu', name='decoder_conv2_2'),</span><br><span class="line">                 keras.layers.UpSampling2D((2, 2), name='decoder_upsampling1'))(concatenate4)</span><br><span class="line">    concatenate4 = keras.layers.Concatenate(name='decoder_concatenate1')([x1, y1])</span><br><span class="line">    y0 = compose(keras.layers.Conv2D(32, (3, 3), (1, 1), 'same', activation='relu', name='decoder_conv1_1'),</span><br><span class="line">                 keras.layers.Conv2D(32, (3, 3), (1, 1), 'same', activation='relu', name='decoder_conv1_2'))(concatenate4)</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Conv2D(num_class, (1, 1), (1, 1), 'same', activation='softmax', name='conv_softmax')(y0)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, output, name='Small_UNet')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generate_arrays_from_file(train_data, batch_size):</span><br><span class="line">    # 获取总长度</span><br><span class="line">    n = len(train_data)</span><br><span class="line">    i = 0</span><br><span class="line">    while 1:</span><br><span class="line">        X_train = []</span><br><span class="line">        Y_train = []</span><br><span class="line">        # 获取一个batch_size大小的数据</span><br><span class="line">        for _ in range(batch_size):</span><br><span class="line">            if i == 0:</span><br><span class="line">                np.random.shuffle(train_data)</span><br><span class="line">            # 从文件中读取图像</span><br><span class="line">            img = cv.imread(imgs_path + '\\' + str(train_data[i]) + '.jpg')</span><br><span class="line">            img = img / 127.5 - 1</span><br><span class="line">            X_train.append(img)</span><br><span class="line"></span><br><span class="line">            # 从文件中读取图像</span><br><span class="line">            img = cv.imread(mask_path + '\\' + str(train_data[i]) + '.png')</span><br><span class="line">            seg_labels = np.zeros((img_size[0], img_size[1], num_class))</span><br><span class="line">            for c in range(num_class):</span><br><span class="line">                seg_labels[:, :, c] = (img[:, :, 0] == c).astype(int)</span><br><span class="line">            Y_train.append(seg_labels)</span><br><span class="line"></span><br><span class="line">            # 读完一个周期后重新开始</span><br><span class="line">            i = (i + 1) % n</span><br><span class="line">        yield tf.constant(X_train), tf.constant(Y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    # 包括背景</span><br><span class="line">    num_class = 4 </span><br><span class="line">    train_data = list(range(800))</span><br><span class="line">    validation_data = list(range(800, 900))</span><br><span class="line">    test_data = range(900, 1000)</span><br><span class="line">    epochs = 50</span><br><span class="line">    batch_size = 16</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    img_size = (128, 128)</span><br><span class="line">    colors = [[0, 0, 0], [0, 0, 128], [0, 128, 0], [128, 0, 0]]</span><br><span class="line"></span><br><span class="line">    mask_path = r'.\shape\train_mask'</span><br><span class="line">    imgs_path = r'.\shape\train_imgs'</span><br><span class="line">    test_path = r'.\shape\test_imgs'</span><br><span class="line">    save_path = r'.\UNet_test_result'</span><br><span class="line">    weight_path = r'.\UNet_weight'</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        os.mkdir(save_path)</span><br><span class="line">    except FileExistsError:</span><br><span class="line">        print(save_path + 'has been exist')</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        os.mkdir(weight_path)</span><br><span class="line">    except FileExistsError:</span><br><span class="line">        print(weight_path + 'has been exist')</span><br><span class="line"></span><br><span class="line">    model = small_unet(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line">    model.build(input_shape=(None, img_size[0], img_size[1], 3))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    optimizor = keras.optimizers.Adam(lr=1e-3)</span><br><span class="line">    lossor = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=optimizor, loss=lossor, metrics=['accuracy'])</span><br><span class="line"></span><br><span class="line">    # 保存的方式，3世代保存一次</span><br><span class="line">    checkpoint_period = keras.callbacks.ModelCheckpoint(</span><br><span class="line">        weight_path + '\\' + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        save_weights_only=True,</span><br><span class="line">        save_best_only=True,</span><br><span class="line">        period=3</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 学习率下降的方式，val_loss3次不下降就下降学习率继续训练</span><br><span class="line">    reduce_lr = keras.callbacks.ReduceLROnPlateau(</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        factor=0.5,</span><br><span class="line">        patience=3,</span><br><span class="line">        verbose=1</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 是否需要早停，当val_loss一直不下降的时候意味着模型基本训练完毕，可以停止</span><br><span class="line">    early_stopping = keras.callbacks.EarlyStopping(</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        min_delta=0,</span><br><span class="line">        patience=10,</span><br><span class="line">        verbose=1</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    model.fit_generator(generate_arrays_from_file(train_data, batch_size),</span><br><span class="line">                        steps_per_epoch=max(1, len(train_data) // batch_size),</span><br><span class="line">                        validation_data=generate_arrays_from_file(validation_data, batch_size),</span><br><span class="line">                        validation_steps=max(1, len(validation_data) // batch_size),</span><br><span class="line">                        epochs=epochs,</span><br><span class="line">                        callbacks=[checkpoint_period, reduce_lr, early_stopping])</span><br><span class="line"></span><br><span class="line">    for name in test_data:</span><br><span class="line">        test_img_path = test_path + '\\' + str(name) + '.jpg'</span><br><span class="line">        save_img_path = save_path + '\\' + str(name) + '.png'</span><br><span class="line">        test_img = cv.imread(test_img_path)</span><br><span class="line">        test_img = tf.constant([test_img / 127.5 - 1])</span><br><span class="line">        test_mask = model.predict(test_img)</span><br><span class="line">        test_mask = np.reshape(test_mask, (img_size[0], img_size[1], num_class))</span><br><span class="line">        test_mask = np.argmax(test_mask, axis=-1)</span><br><span class="line">        seg_img = np.zeros((img_size[0], img_size[1], 3))</span><br><span class="line">        for c in range(num_class):</span><br><span class="line">            seg_img[:, :, 0] += ((test_mask == c) * (colors[c][0]))</span><br><span class="line">            seg_img[:, :, 1] += ((test_mask == c) * (colors[c][1]))</span><br><span class="line">            seg_img[:, :, 2] += ((test_mask == c) * (colors[c][2]))</span><br><span class="line">        seg_img = seg_img.astype(np.uint8)</span><br><span class="line">        cv.imwrite(save_img_path, seg_img)</span><br></pre></td></tr></tbody></table></figure><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Semantic_segmentation/UNet_T.png" alt="UNet"></p><h1 id="UNet小结"><a href="#UNet小结" class="headerlink" title="UNet小结"></a><font size="5" color="red">UNet小结</font></h1><p>  UNet是一种简单的语义分割网络，在输入图像尺寸为572x572时，参数量为31M。因为其<strong>padding方式使其图像尺寸缩小，适合于大尺寸图像的分割</strong>，并且采用<strong>加权损失函数</strong>和优秀的<strong>图像增强</strong>操作，使得其在医学图像处理中有良好的表现。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;UNet&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="语义分割网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>SegNet</title>
    <link href="https://USTCcoder.github.io/2020/04/18/Semantic_segmentation%20SegNet/"/>
    <id>https://USTCcoder.github.io/2020/04/18/Semantic_segmentation SegNet/</id>
    <published>2020-04-18T10:07:25.000Z</published>
    <updated>2020-06-18T16:01:35.776Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">SegNet</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>SegNet</strong>:由剑桥大学提出，2015年被提交到CVPR，但是最后没有发表，反而在<strong>2017年发表在TPAMI</strong>上。是一种简单高效的语义分割模型。<a id="more"></a></p><p><img src="/images/Semantic_segmentation/SegNet.png" alt="SegNet"></p><h1 id="SegNet特点"><a href="#SegNet特点" class="headerlink" title="SegNet特点"></a><font size="5" color="red">SegNet特点</font></h1><p>  <font size="3">网络分为两个部分，<strong>编码(Encoder)</strong>和<strong>解码(Decoder)</strong>，结构简单，网络易于实现</font><br>  <font size="3">使用了<strong>Maxpooling-Indices(最大池化索引)</strong>来进行图像分辨率的提高，而不是采用上采样或者反卷积。</font></p><h1 id="Maxpooling-Indices-最大池化索引-与Upsampling-上采样-和Deconvolution-反卷积-之间的区别"><a href="#Maxpooling-Indices-最大池化索引-与Upsampling-上采样-和Deconvolution-反卷积-之间的区别" class="headerlink" title="Maxpooling-Indices(最大池化索引)与Upsampling(上采样)和Deconvolution(反卷积)之间的区别"></a><font size="5" color="red">Maxpooling-Indices(最大池化索引)与Upsampling(上采样)和Deconvolution(反卷积)之间的区别</font></h1><p>  <font size="3"><strong>Maxpooling-Indices(最大池化索引)</strong>：又称为<strong>Unpooling(反池化)</strong>，池化后<strong>记录最大值所在的位置</strong>，在反池化的过程中，给相应位置上写入值，<strong>其他位置为0</strong>。这个方法没有参数，<strong>但是这个方法并不常用，因为存在大量的稀疏数据，使模型收敛速度大大降低。</strong></font><br>  <font size="3"><strong>Upsampling(上采样)</strong>：将输入<strong>resize到设置大小</strong>，然后利用指定的插值方法<strong>对周围的值进行插值</strong>，常用<strong>最近邻插值</strong>和<strong>双线性插值</strong>。因为相邻区域的像素和特征应该是相似的，因此这个方法特别常用，<strong>既没有参数，也不会存在稀疏数据。</strong></font><br>  <font size="3"><strong>Deconvolution(反卷积)</strong>：<strong>本质是卷积</strong>，注意<strong>反卷积并不能从卷积的结果返回到卷积前的数据，只能返回到卷积前的尺寸</strong>。卷积通过设置kernel_size卷积核大小，strides步长和padding填充方式可以将图像的分辨率降低，相反的反卷积可以通过设置kernel_size卷积核大小，strides步长和padding填充方式<strong>先对数据进行填充，然后再进行卷积操作</strong>，可以将图像的分辨率增加。<strong>这个方法不推荐经常使用，因为存在大量参数，而且可能会存在棋盘格效应，可以参考<a href="https://distill.pub/2016/deconv-checkerboard/" target="_blank" rel="noopener">棋盘格可视化</a></strong>。</font><br><img src="/images/Semantic_segmentation/SegNet_U.png" alt="SegNet"></p><h1 id="SegNet图像分析"><a href="#SegNet图像分析" class="headerlink" title="SegNet图像分析"></a><font size="5" color="red">SegNet图像分析</font></h1><p><img src="/images/Semantic_segmentation/SegNet_A.png" alt="SegNet"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Indices_Maxpool(keras.layers.Layer):</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        super(Indices_Maxpool, self).__init__(name=name)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        val, index = inputs</span><br><span class="line">        input_size = index.shape</span><br><span class="line">        output_size = [x * 2 if i == 0 or i == 1 else x for i, x in enumerate(input_size[1:])]</span><br><span class="line">        output = tf.reshape(tf.scatter_nd(tf.reshape(index, (-1, 1)), tf.reshape(val, (-1,)), (batch_size * np.prod(output_size),)), [-1] + output_size)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Convs(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, name):</span><br><span class="line">        super(Convs, self).__init__(name=name)</span><br><span class="line">        self.blocks = keras.Sequential()</span><br><span class="line">        self.blocks.add(keras.layers.Conv2D(filters, (3, 3), (1, 1), padding='same'))</span><br><span class="line">        self.blocks.add(keras.layers.BatchNormalization())</span><br><span class="line">        self.blocks.add(keras.layers.ReLU())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        output = self.blocks(inputs)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def segnet(input_shape):</span><br><span class="line"></span><br><span class="line">    input_tensor = keras.layers.Input(shape=input_shape, batch_size=batch_size, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x1 = compose(Convs(64, name='encoder_conv1_1'),</span><br><span class="line">                 Convs(64, name='encoder_conv1_2'))(x)</span><br><span class="line">    val_1, index_1 = tf.nn.max_pool_with_argmax(x1, (2, 2), (2, 2), 'VALID', name='maxpool1')</span><br><span class="line">    x2 = compose(Convs(128, name='encoder_conv2_1'),</span><br><span class="line">                 Convs(128, name='encoder_conv2_2'))(val_1)</span><br><span class="line">    val_2, index_2 = tf.nn.max_pool_with_argmax(x2, (2, 2), (2, 2), 'VALID', name='maxpool2')</span><br><span class="line">    x3 = compose(Convs(256, name='encoder_conv3_1'),</span><br><span class="line">                 Convs(256, name='encoder_conv3_2'),</span><br><span class="line">                 Convs(256, name='encoder_conv3_3'))(val_2)</span><br><span class="line">    val_3, index_3 = tf.nn.max_pool_with_argmax(x3, (2, 2), (2, 2), 'VALID', name='maxpool3')</span><br><span class="line">    x4 = compose(Convs(512, name='encoder_conv4_1'),</span><br><span class="line">                 Convs(512, name='encoder_conv4_2'),</span><br><span class="line">                 Convs(512, name='encoder_conv4_3'))(val_3)</span><br><span class="line">    val_4, index_4 = tf.nn.max_pool_with_argmax(x4, (2, 2), (2, 2), 'VALID', name='maxpool4')</span><br><span class="line">    x5 = compose(Convs(512, name='encoder_conv5_1'),</span><br><span class="line">                 Convs(512, name='encoder_conv5_2'),</span><br><span class="line">                 Convs(512, name='encoder_conv5_3'))(val_4)</span><br><span class="line">    val_5, index_5 = tf.nn.max_pool_with_argmax(x5, (2, 2), (2, 2), 'VALID', name='maxpool5')</span><br><span class="line"></span><br><span class="line">    indices_maxpool5 = Indices_Maxpool(name='indices_maxpool5')([val_5, index_5])</span><br><span class="line">    y5 = compose(Convs(512, name='decoder_conv5_1'),</span><br><span class="line">                 Convs(512, name='decoder_conv5_2'),</span><br><span class="line">                 Convs(512, name='decoder_conv5_3'))(indices_maxpool5)</span><br><span class="line">    indices_maxpool4 = Indices_Maxpool(name='indices_maxpool4')([y5, index_4])</span><br><span class="line">    y4 = compose(Convs(512, name='decoder_conv4_1'),</span><br><span class="line">                 Convs(512, name='decoder_conv4_2'),</span><br><span class="line">                 Convs(256, name='decoder_conv4_3'))(indices_maxpool4)</span><br><span class="line">    indices_maxpool3 = Indices_Maxpool(name='indices_maxpool3')([y4, index_3])</span><br><span class="line">    y3 = compose(Convs(256, name='decoder_conv3_1'),</span><br><span class="line">                 Convs(256, name='decoder_conv3_2'),</span><br><span class="line">                 Convs(128, name='decoder_conv3_3'))(indices_maxpool3)</span><br><span class="line">    indices_maxpool2 = Indices_Maxpool(name='indices_maxpool2')([y3, index_2])</span><br><span class="line">    y2 = compose(Convs(128, name='decoder_conv2_1'),</span><br><span class="line">                 Convs(64, name='decoder_conv2_2'))(indices_maxpool2)</span><br><span class="line">    indices_maxpool1 = Indices_Maxpool(name='indices_maxpool1')([y2, index_1])</span><br><span class="line">    y1 = compose(Convs(64, name='decoder_conv1_1'),</span><br><span class="line">                 Convs(21, name='decoder_conv1_2'))(indices_maxpool1)</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Softmax(name='softmax')(y1)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, output, name='SegNet')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    batch_size = 16</span><br><span class="line">    model = segnet(input_shape=(512, 512, 3))</span><br><span class="line">    model.build(input_shape=(512, 512, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Semantic_segmentation/SegNet_R.png" alt="SegNet"></p><h1 id="Shape数据集完整实战"><a href="#Shape数据集完整实战" class="headerlink" title="Shape数据集完整实战"></a><font size="5" color="red">Shape数据集完整实战</font></h1><h2 id="文件路径关系说明"><a href="#文件路径关系说明" class="headerlink" title="文件路径关系说明"></a>文件路径关系说明</h2><ul><li>project<ul><li>shape<ul><li>train_imgs(训练集图像文件夹)</li><li>train_mask(训练集掩模文件夹)</li><li>test_imgs(测试集图像文件夹)</li></ul></li><li>SegNet_weight(模型权重文件夹)</li><li>SegNet_test_result(测试集结果文件夹)</li><li>SegNet.py</li></ul></li></ul><h2 id="实战步骤说明"><a href="#实战步骤说明" class="headerlink" title="实战步骤说明"></a>实战步骤说明</h2><ol><li>语义分割实战运行较为简单，因为它的输入的训练数据为图像，输入的标签数据也是图像，首先<strong>要对输入的标签数据进行编码，转换为类别信息</strong>，要和网络的输出维度相匹配，从(batch_size, height, width, 1)转换为(batch_size, height, width, num_class + 1)，<strong>某个像素点为哪一个类别，则在该通道上置1，其余通道置0</strong>。即神经网络的输入大小为(batch_size, height, width, 3)，输出大小为(batch_size, height, width, num_class + 1)。</li><li>设计损失函数，简单情况设置交叉熵损失函数即可达到较好效果。</li><li>搭建神经网络，<strong>设置合适参数</strong>，进行训练。</li><li>预测时，需要根据神经网络的输出进行<strong>逆向解码(编码的反过程)</strong>，寻找<strong>每一个像素点，哪一个通道上值最大则归为哪一个类别</strong>，即可完成实战的过程。</li></ol><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><ol><li>设置的图像<strong>类别数为实际类别数+1</strong>，1代表背景类别，<strong>此数据集为3类，最后的通道数为4，每一个通道预测一类物体</strong>。在通道方向求Softmax，并且求出最大的索引，索引为0则代表背景，索引为1则代表圆形，索引为2则代表三角形，索引为3则代表正方形。</li><li><strong>最大池化收敛速度较慢，因此换成上采样，不但可以使模型更加简单，而且可以加快网络的收敛速度。</strong></li><li>设置了<strong>权重的保存方式</strong>，<strong>学习率的下降方式</strong>和<strong>早停方式</strong>。</li><li>使用<strong>yield</strong>关键字，产生可迭代对象，不用将所有的数据都保存下来，大大节约内存。</li><li>其中将1000个数据，分成800个训练集，100个验证集和100个测试集，小伙伴们可以自行修改。</li><li>注意其中的一些维度变换和<strong>numpy</strong>，<strong>tensorflow</strong>常用操作，否则在阅读代码时可能会产生一些困难。</li><li>SegNet的<strong>特征提取网络(编码网络)类似于VGG</strong>，小伙伴们可以参考特征提取网络部分内容，选择其他的网络进行特征提取，比较不同网络参数量，运行速度，最终结果之间的差异。</li><li>图像输入可以先将其<strong>归一化到0-1之间或者-1-1之间</strong>，因为网络的参数一般都比较小，所以归一化后计算方便，收敛较快。</li><li>实际的工程应用中，常常还需要对数据集进行<strong>大小调整和增强</strong>，在这里为了简单起见，没有进行复杂的操作，小伙伴们应用中要记得根据自己的需要，对图像进行<strong>resize或者padding</strong>，然后<strong>旋转</strong>，<strong>对比度增强</strong>，<strong>仿射运算</strong>等等操作，增加模型的鲁棒性，并且实际中的图像不一定按照顺序命名的，因此应用中也要注意图像读取的文件名。</li></ol><h2 id="完整实战代码"><a href="#完整实战代码" class="headerlink" title="完整实战代码"></a>完整实战代码</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from functools import reduce</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2 as cv</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Convs(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, name):</span><br><span class="line">        super(Convs, self).__init__(name=name)</span><br><span class="line">        self.blocks = keras.Sequential()</span><br><span class="line">        self.blocks.add(keras.layers.Conv2D(filters, (3, 3), (1, 1), padding='same'))</span><br><span class="line">        self.blocks.add(keras.layers.BatchNormalization())</span><br><span class="line">        self.blocks.add(keras.layers.ReLU())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        output = self.blocks(inputs)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def small_segnet(input_shape):</span><br><span class="line"></span><br><span class="line">    input_tensor = keras.layers.Input(shape=input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x1 = compose(Convs(32, name='encoder_conv1_1'),</span><br><span class="line">                 Convs(32, name='encoder_conv1_2'),</span><br><span class="line">                 keras.layers.MaxPool2D((2, 2), name='encoder_maxpool1'))(x)</span><br><span class="line">    x2 = compose(Convs(64, name='encoder_conv2_1'),</span><br><span class="line">                 Convs(64, name='encoder_conv2_2'),</span><br><span class="line">                 keras.layers.MaxPool2D((2, 2), name='encoder_maxpool2'))(x1)</span><br><span class="line">    x3 = compose(Convs(128, name='encoder_conv3_1'),</span><br><span class="line">                 Convs(128, name='encoder_conv3_2'),</span><br><span class="line">                 keras.layers.MaxPool2D((2, 2), name='encoder_maxpool3'))(x2)</span><br><span class="line">    x4 = compose(Convs(256, name='encoder_conv4_1'),</span><br><span class="line">                 Convs(256, name='encoder_conv4_2'),</span><br><span class="line">                 keras.layers.MaxPool2D((2, 2), name='encoder_maxpool4'))(x3)</span><br><span class="line"></span><br><span class="line">    y4 = compose(Convs(256, name='decoder_conv4_1'),</span><br><span class="line">                 Convs(256, name='decoder_conv4_2'),</span><br><span class="line">                 keras.layers.UpSampling2D((2, 2), name='decoder_upsampling4'))(x4)</span><br><span class="line">    y3 = compose(Convs(128, name='decoder_conv3_1'),</span><br><span class="line">                 Convs(128, name='decoder_conv3_2'),</span><br><span class="line">                 keras.layers.UpSampling2D((2, 2), name='decoder_upsampling3'))(y4)</span><br><span class="line">    y2 = compose(Convs(64, name='decoder_conv2_1'),</span><br><span class="line">                 Convs(64, name='decoder_conv2_2'),</span><br><span class="line">                 keras.layers.UpSampling2D((2, 2), name='decoder_upsampling2'))(y3)</span><br><span class="line">    y1 = compose(Convs(32, name='decoder_conv1_1'),</span><br><span class="line">                 Convs(32, name='decoder_conv1_2'),</span><br><span class="line">                 keras.layers.UpSampling2D((2, 2), name='decoder_upsampling1'))(y2)</span><br><span class="line"></span><br><span class="line">    output = keras.layers.Conv2D(num_class, (3, 3), (1, 1), 'same', activation='softmax', name='conv_softmax')(y1)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, output, name='Small_SegNet')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generate_arrays_from_file(train_data, batch_size):</span><br><span class="line">    # 获取总长度</span><br><span class="line">    n = len(train_data)</span><br><span class="line">    i = 0</span><br><span class="line">    while 1:</span><br><span class="line">        X_train = []</span><br><span class="line">        Y_train = []</span><br><span class="line">        # 获取一个batch_size大小的数据</span><br><span class="line">        for _ in range(batch_size):</span><br><span class="line">            if i == 0:</span><br><span class="line">                np.random.shuffle(train_data)</span><br><span class="line">            # 从文件中读取图像</span><br><span class="line">            img = cv.imread(imgs_path + '\\' + str(train_data[i]) + '.jpg')</span><br><span class="line">            img = img / 127.5 - 1</span><br><span class="line">            X_train.append(img)</span><br><span class="line"></span><br><span class="line">            # 从文件中读取图像</span><br><span class="line">            img = cv.imread(mask_path + '\\' + str(train_data[i]) + '.png')</span><br><span class="line">            seg_labels = np.zeros((img_size[0], img_size[1], num_class))</span><br><span class="line">            for c in range(num_class):</span><br><span class="line">                seg_labels[:, :, c] = (img[:, :, 0] == c).astype(int)</span><br><span class="line">            Y_train.append(seg_labels)</span><br><span class="line"></span><br><span class="line">            # 读完一个周期后重新开始</span><br><span class="line">            i = (i + 1) % n</span><br><span class="line">        yield tf.constant(X_train), tf.constant(Y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    # 包括背景</span><br><span class="line">    num_class = 4</span><br><span class="line">    train_data = list(range(800))</span><br><span class="line">    validation_data = list(range(800, 900))</span><br><span class="line">    test_data = range(900, 1000)</span><br><span class="line">    epochs = 50</span><br><span class="line">    batch_size = 16</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    img_size = (128, 128)</span><br><span class="line">    colors = [[0, 0, 0], [0, 0, 128], [0, 128, 0], [128, 0, 0]]</span><br><span class="line"></span><br><span class="line">    mask_path = r'.\shape\train_mask'</span><br><span class="line">    imgs_path = r'.\shape\train_imgs'</span><br><span class="line">    test_path = r'.\shape\test_imgs'</span><br><span class="line">    save_path = r'.\SegNet_test_result'</span><br><span class="line">    weight_path = r'.\SegNet_weight'</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        os.mkdir(save_path)</span><br><span class="line">    except FileExistsError:</span><br><span class="line">        print(save_path + 'has been exist')</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        os.mkdir(weight_path)</span><br><span class="line">    except FileExistsError:</span><br><span class="line">        print(weight_path + 'has been exist')</span><br><span class="line"></span><br><span class="line">    model = small_segnet(input_shape=(img_size[0], img_size[1], 3))</span><br><span class="line">    model.build(input_shape=(None, img_size[0], img_size[1], 3))</span><br><span class="line">    model.summary()</span><br><span class="line"></span><br><span class="line">    optimizor = keras.optimizers.Adam(lr=1e-3)</span><br><span class="line">    lossor = keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=optimizor, loss=lossor, metrics=['accuracy'])</span><br><span class="line"></span><br><span class="line">    # 保存的方式，3世代保存一次</span><br><span class="line">    checkpoint_period = keras.callbacks.ModelCheckpoint(</span><br><span class="line">        weight_path + '\\' + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        save_weights_only=True,</span><br><span class="line">        save_best_only=True,</span><br><span class="line">        period=3</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 学习率下降的方式，val_loss3次不下降就下降学习率继续训练</span><br><span class="line">    reduce_lr = keras.callbacks.ReduceLROnPlateau(</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        factor=0.5,</span><br><span class="line">        patience=3,</span><br><span class="line">        verbose=1</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 是否需要早停，当val_loss一直不下降的时候意味着模型基本训练完毕，可以停止</span><br><span class="line">    early_stopping = keras.callbacks.EarlyStopping(</span><br><span class="line">        monitor='val_loss',</span><br><span class="line">        min_delta=0,</span><br><span class="line">        patience=10,</span><br><span class="line">        verbose=1</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    model.fit_generator(generate_arrays_from_file(train_data, batch_size),</span><br><span class="line">                        steps_per_epoch=max(1, len(train_data) // batch_size),</span><br><span class="line">                        validation_data=generate_arrays_from_file(validation_data, batch_size),</span><br><span class="line">                        validation_steps=max(1, len(validation_data) // batch_size),</span><br><span class="line">                        epochs=epochs,</span><br><span class="line">                        callbacks=[checkpoint_period, reduce_lr, early_stopping])</span><br><span class="line"></span><br><span class="line">    for name in test_data:</span><br><span class="line">        test_img_path = test_path + '\\' + str(name) + '.jpg'</span><br><span class="line">        save_img_path = save_path + '\\' + str(name) + '.png'</span><br><span class="line">        test_img = cv.imread(test_img_path)</span><br><span class="line">        test_img = tf.constant([test_img / 127.5 - 1])</span><br><span class="line">        test_mask = model.predict(test_img)</span><br><span class="line">        test_mask = np.reshape(test_mask, (img_size[0], img_size[1], num_class))</span><br><span class="line">        test_mask = np.argmax(test_mask, axis=-1)</span><br><span class="line">        seg_img = np.zeros((img_size[0], img_size[1], 3))</span><br><span class="line">        for c in range(num_class):</span><br><span class="line">            seg_img[:, :, 0] += ((test_mask == c) * (colors[c][0]))</span><br><span class="line">            seg_img[:, :, 1] += ((test_mask == c) * (colors[c][1]))</span><br><span class="line">            seg_img[:, :, 2] += ((test_mask == c) * (colors[c][2]))</span><br><span class="line">        seg_img = seg_img.astype(np.uint8)</span><br><span class="line">        cv.imwrite(save_img_path, seg_img)</span><br></pre></td></tr></tbody></table></figure><h2 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h2><p><img src="/images/Semantic_segmentation/SegNet_T.png" alt="SegNet"></p><h1 id="SegNet小结"><a href="#SegNet小结" class="headerlink" title="SegNet小结"></a><font size="5" color="red">SegNet小结</font></h1><p>  SegNet是一种简单的语义分割网络，从上图可以看出SegNet模型的参数量只有29M，虽然现在SegNet网络不是最好的语义分割网络，但是其编码解码结构的思想，对后面的深度学习网络的发展有重要的影响。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;SegNet&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="语义分割网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>岛屿数量(Leetcode 200)</title>
    <link href="https://USTCcoder.github.io/2020/04/17/program%20Leetcode200/"/>
    <id>https://USTCcoder.github.io/2020/04/17/program Leetcode200/</id>
    <published>2020-04-17T11:53:59.000Z</published>
    <updated>2020-09-02T02:18:06.738Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode200.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   岛屿数量问题是一个经典的问题，与朋友圈问题(Leetcode 547)类似，只不过朋友圈问题中的矩阵是对称的，而岛屿数量中的矩阵是非对称的。此题可以使用DFS，BFS和并查集进行求解。</p><a id="more"></a><h1 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a><font size="5" color="red">DFS</font></h1><p>DFS是先找到一块岛屿，然后对这个岛屿进行深度优先搜索，已知沿着某一个路径走下去，如果没用路可以走则回溯。直到所有的路径都走完，说明将这个岛屿探索完毕，将其剔除，从剩余的地图上继续寻找，直到所有的岛屿都找到，则算法结束，有关DFS的知识可以参考我的另一篇博客深度优先搜索(Depth-First-Search)。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def numIslands(self, grid):</span><br><span class="line">        row = len(grid)</span><br><span class="line">        if row == 0:</span><br><span class="line">            return 0</span><br><span class="line">        col = len(grid[0])</span><br><span class="line"></span><br><span class="line">        def dfs(x, y):</span><br><span class="line">            grid[x][y] = 0</span><br><span class="line">            for new_x, new_y in [(x - 1, y), (x + 1, y), (x, y - 1), (x, y + 1)]:</span><br><span class="line">                if 0 &lt;= new_x &lt; row and 0 &lt;= new_y &lt; col and grid[new_x][new_y] == '1':</span><br><span class="line">                    dfs(new_x, new_y)</span><br><span class="line"></span><br><span class="line">        num_islands = 0</span><br><span class="line">        for r in range(row):</span><br><span class="line">            for c in range(col):</span><br><span class="line">                if grid[r][c] == "1":</span><br><span class="line">                    num_islands += 1</span><br><span class="line">                    dfs(r, c)</span><br><span class="line">        return num_islands</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="BFS"><a href="#BFS" class="headerlink" title="BFS"></a><font size="5" color="red">BFS</font></h1><p>BFS是先找到一块岛屿，然后对这个岛屿进行广度优先搜索，以某个点为中心，先寻找距离该点为1的所有陆地，然后再寻找与该点距离为2的所有陆地，依次进性，直到将与该点连通的所有陆地都寻找完毕，说明将这个岛屿探索完毕，将其剔除，从剩余的地图上继续寻找，直到所有的岛屿都找到，则算法结束，有关BFS的知识可以参考我的另一篇博客广度优先搜索(Breadth-First-Search)。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">from collections import deque</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Solution:</span><br><span class="line">    def numIslands(self, grid):</span><br><span class="line">        row = len(grid)</span><br><span class="line">        if row == 0:</span><br><span class="line">            return 0</span><br><span class="line">        col = len(grid[0])</span><br><span class="line"></span><br><span class="line">        num_islands = 0</span><br><span class="line">        for r in range(row):</span><br><span class="line">            for c in range(col):</span><br><span class="line">                if grid[r][c] == "1":</span><br><span class="line">                    num_islands += 1</span><br><span class="line">                    grid[r][c] = "0"</span><br><span class="line">                    neighbors = deque([(r, c)])</span><br><span class="line">                    while neighbors:</span><br><span class="line">                        i, j = neighbors.popleft()</span><br><span class="line">                        for x, y in [(i - 1, j), (i + 1, j), (i, j - 1), (i, j + 1)]:</span><br><span class="line">                            if 0 &lt;= x &lt; row and 0 &lt;= y &lt; col and grid[x][y] == "1":</span><br><span class="line">                                neighbors.append((x, y))</span><br><span class="line">                                grid[x][y] = "0"</span><br><span class="line"></span><br><span class="line">        return num_islands</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a><font size="5" color="red">并查集</font></h1><p>并查集算法不是很常用，但是思路很清晰，初始设每个陆地都是一个岛屿，比较两个相邻的岛屿是否为同一个岛屿，如果不是同一个岛屿则合并这两个岛屿，并将岛屿数量减1，最后剩余的岛屿数量就是本题的解。难点在于如何判断两个相邻岛屿为同一个岛屿，而且如何合并两个岛屿？每一个岛屿设定一个首都即可，如果两个岛屿的首都相同，则在同一个岛屿上，否则不在同一个岛屿，合并时直接选择某一个首都最为总首都即可实现岛屿合并。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">class UnionFind:</span><br><span class="line">    def __init__(self, grid):</span><br><span class="line">        m, n = len(grid), len(grid[0])</span><br><span class="line">        self.count = 0</span><br><span class="line">        self.parent = [-1] * (m * n)</span><br><span class="line">        self.rank = [0] * (m * n)</span><br><span class="line">        for i in range(m):</span><br><span class="line">            for j in range(n):</span><br><span class="line">                if grid[i][j] == "1":</span><br><span class="line">                    self.parent[i * n + j] = i * n + j</span><br><span class="line">                    self.count += 1</span><br><span class="line"></span><br><span class="line">    def find(self, i):</span><br><span class="line">        if self.parent[i] != i:</span><br><span class="line">            self.parent[i] = self.find(self.parent[i])</span><br><span class="line">        return self.parent[i]</span><br><span class="line"></span><br><span class="line">    def union(self, x, y):</span><br><span class="line">        rootx = self.find(x)</span><br><span class="line">        rooty = self.find(y)</span><br><span class="line">        if rootx != rooty:</span><br><span class="line">            self.parent[rooty] = rootx</span><br><span class="line">            self.count -= 1</span><br><span class="line"></span><br><span class="line">    def getCount(self):</span><br><span class="line">        return self.count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Solution:</span><br><span class="line">    def numIslands(self, grid):</span><br><span class="line">        nr = len(grid)</span><br><span class="line">        if nr == 0:</span><br><span class="line">            return 0</span><br><span class="line">        nc = len(grid[0])</span><br><span class="line"></span><br><span class="line">        uf = UnionFind(grid)</span><br><span class="line">        for r in range(nr):</span><br><span class="line">            for c in range(nc):</span><br><span class="line">                if grid[r][c] == "1":</span><br><span class="line">                    grid[r][c] = "0"</span><br><span class="line">                    for x, y in [(r - 1, c), (r + 1, c), (r, c - 1), (r, c + 1)]:</span><br><span class="line">                        if 0 &lt;= x &lt; nr and 0 &lt;= y &lt; nc and grid[x][y] == "1":</span><br><span class="line">                            uf.union(r * nc + c, x * nc + y)</span><br><span class="line"></span><br><span class="line">        return uf.getCount()</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  朋友圈问题是一类经典的面试问题，因此小伙伴们务必掌握基本的DFS和BFS算法，至于并查集问题，大家理解即可，因为并查集实现也没用DFS和BFS容易，而且时间复杂度也并没有提升，只需要小伙伴们能够说出并查集思路。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 200&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>语义分割数据集</title>
    <link href="https://USTCcoder.github.io/2020/04/15/Semantic_segmentation%20Dataset/"/>
    <id>https://USTCcoder.github.io/2020/04/15/Semantic_segmentation Dataset/</id>
    <published>2020-04-15T13:02:25.000Z</published>
    <updated>2020-05-09T09:18:03.226Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Data Set</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>语义分割:</strong>是计算机视觉的<strong>基础任务</strong>，在语义分割中我们需要将视觉输入分为不同的<strong>可解释类别</strong>，和聚类分割方法不同点在于此，其类别在真实世界中是有意义的，而聚类分割是可以将物体分成若干部分，但是每一部分不一定是有语义的，在<strong>自动驾驶</strong>，<strong>图像搜索</strong>等等领域都是非常重要的。<br><a id="more"></a></p><p><img src="/images/Semantic_segmentation/Dataset.png" alt="Dataset"></p><h1 id="数据集以及IOU介绍"><a href="#数据集以及IOU介绍" class="headerlink" title="数据集以及IOU介绍"></a><font size="5" color="red">数据集以及IOU介绍</font></h1><p><strong>数据集</strong>：为了方便模型调试的方便，我的博客中介绍的数据集是一种简单的Shape数据集，只有1000个训练样本，为了加快训练速度，数据集的大小我也调整为128x128，这个数据集只有三类物体，分别是圆形，三角形和正方形，图像数据为jpg文件，掩模数据为png文件。<br><strong>mask图像</strong>：mask图像给初学者第一眼看上去是懵逼的，当然包括我也是，这不是全黑的图像吗？这有何意义呢？掩模图像的保存是使用8位二进制数，因此它的值为0-255，称之为灰度值。每一个类别用一个数替代，为了使用方便则按顺序<strong>使用1，2，3来分别代表圆形，三角形和正方形，其中背景用0表示</strong>。在图像中，0为黑色，255为白色，灰度值越接近0，则图像越黑，越接近255则图像越白。因此在这个三类问题中，图像灰度最大为3，当然看起来是黑色的。不信可以对mask乘85则可以看到颜色。<br><strong>IOU(Intersection Over Union，交并比)</strong>：用于<strong>评估语义分割算法性能的指标是平均IOU</strong>，交并比也非常好理解，算法的结果与真实物体进行<strong>交运算的结果除以进行并运算的结果</strong>。通过下图可以直观的看出IOU的计算方法。<br><img src="/images/Semantic_segmentation/Dataset_I.png" alt="IOU"></p><h1 id="一些说明"><a href="#一些说明" class="headerlink" title="一些说明"></a><font size="5" color="red">一些说明</font></h1><ol><li>在学习的时候，小伙伴可能会遇到一些代码上的困难，如<strong>tensorflow</strong>，<strong>numpy</strong>，<strong>opencv</strong>的用法，可以查看我的深度学习框架和Python常用库相关文章，里面会有一些简单的介绍，小伙伴们可以进行学习，最好是手动敲一敲，看一看。</li><li>因为这个博客是对学习的一些总结和记录，意在和学习者探讨和交流，并且给准备入门的同学一些手把手的教学，因此关于图像分割的算法参数设计，我都是自己尝试的，不是针对于这个数据集最优的参数，大家可以<strong>根据自己的实际需要修改网络结构</strong>。</li><li>实际的工程应用中，常常还需要对数据集进行<strong>大小调整和增强</strong>，在这里为了简单起见，没有进行复杂的操作，小伙伴们应用中要记得根据自己的需要，对图像进行<strong>resize或者padding</strong>，然后<strong>旋转</strong>，<strong>对比度增强</strong>，<strong>仿射运算</strong>等等操作，增加模型的鲁棒性，并且实际中的图像不一定按照顺序命名的，因此应用中也要注意图像读取的文件名。</li><li>为了让学习者看的方便和清晰，我没有使用多个文件对程序进行封装，因为我在刚开始学习模型的时候，查看GitHub代码，一个模型可能需要好几个文件夹，每个文件夹里面又有很多的代码文件，其中很多文件互相调用。虽然这样的工程项目是非常好管理和运行的，但是给初学者一种丈二和尚摸不着头脑的感觉，对此我深有体会。所以我就使用一个.py文件来封装，因此代码可能会有几百行，但是其中的各个函数和类都有自己的名字，可以保证学习者不会被纸老虎吓住。</li><li>在语义分割学习中，我会列举出一些经典的语义分割模型，因为模型太多，并且仍在不断的更新进步之中，所以大家可以联系我，和我进行沟通和交流，或者推荐给我一些优秀的模型。</li><li>关于问题的交流，图像的数据，需要的同学可以到主页查看我的QQ或者邮箱，我会非常荣幸的提供力所能及的帮助，小伙伴加好友的时候一定要记得备注，不然我可能会忽视一些粗心的小伙伴。</li></ol><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  <strong>语义分割</strong>是计算机视觉的<strong>基础任务</strong>，也是非常重要的任务之一，自从深度学习的时代到来，各种神经网络结构百花齐放，很难说出最好的语义分割方法，可能一个方法适用于很多数据，但也<strong>不能说明某一个算法一定优于另一个算法</strong>，我们要做的就是尽可能多的<strong>学习各种各样的深度学习模型</strong>，然后<strong>吸取这些模型成功的原因</strong>，投入到自己的工程应用之中。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Data Set&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="语义分割网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>GhostNet</title>
    <link href="https://USTCcoder.github.io/2020/04/08/feature_extraction%20GhostNet/"/>
    <id>https://USTCcoder.github.io/2020/04/08/feature_extraction GhostNet/</id>
    <published>2020-04-08T09:21:16.000Z</published>
    <updated>2020-05-28T05:28:13.479Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">GhostNet</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>GhostNet:</strong>来自<strong>华为诺亚方舟实验室</strong>，于<strong>2020年被CVPR</strong>接受，借鉴了大量优秀神经网络的特点，提出了一种新型的神经网络架构。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/GhostNet.png" alt="GhostNet"></p><h1 id="GhostNet特点"><a href="#GhostNet特点" class="headerlink" title="GhostNet特点"></a><font size="5" color="red">GhostNet特点</font></h1><p>  <font size="3">在Ghost Module中引入<strong>瓶颈结构</strong>和<strong>GroupConv分组卷积</strong></font><br>  <font size="3">在Ghost Bottleneck中引入<strong>DepthwiseConv深度可分离卷积</strong>和<strong>Squeeze-and-Excitation模块</strong></font></p><h1 id="Group-Convolution"><a href="#Group-Convolution" class="headerlink" title="Group Convolution"></a><font size="5" color="red">Group Convolution</font></h1><p><img src="/images/Feature_extraction/ShuffleNet_V2_G.png" alt="ShuffleNet_V2"><br>  <font size="3"><strong>Group Convolution(分组卷积)</strong>：<strong>传统卷积是采用一种卷积全连接的思想</strong>，特征图中的每一个像素点都结合了图像中所有通道的信息。而分组卷积特征图像<strong>每一个像素点只利用到一部分原始图像的通道</strong>。</font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>。如果一个64x64x256的图像，经过5x5的卷积核后变为64x64x256的图像，经过普通卷积的参数量为256x(256x5x5+1)=1638656，而分成32组的分组卷积的参数量为256x(8*5x5+1)=51456，参数量缩小了约32倍，当组数变成通道数时，则类似于Depthwise Convolution深度卷积</font></p><h1 id="Depthwise-Convolution"><a href="#Depthwise-Convolution" class="headerlink" title="Depthwise Convolution"></a><font size="5" color="red">Depthwise Convolution</font></h1><p><img src="/images/deep_learning/depthwise.png" alt="depthwise"><br>  <font size="3"><strong>Depthwise Convolution(深度卷积)：在</strong>每一个通道上单独进行卷积**</font><br>  <font size="3">参数<strong>depth_multiplier默认为1</strong>，代表每个通道数进行一次单独卷积，<strong>输出的通道数和输入通道数相等</strong>，设置<strong>depth_multiplier=n</strong>，则代表每个通道数进行n次单独卷积，<strong>输出通道数是输入通道数的n倍</strong>。</font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>。如果一个8x8x1024的特征图，经过5x5的卷积核后变为8x8x1024的图像，经过普通卷积的参数量为1024x(1024x5x5+1)=26215424，而深度卷积参数量为1024x(1x5x5+1)=26624，参数量缩小了约1024倍。</font></p><h1 id="Squeeze-and-Excitation"><a href="#Squeeze-and-Excitation" class="headerlink" title="Squeeze-and-Excitation"></a><font size="5" color="red">Squeeze-and-Excitation</font></h1><p><img src="/images/Feature_extraction/SENet_S.png" alt="SENet"><br>  <font size="3"><strong>Squeeze-and-Excitation</strong>：又称为<strong>特征重标定卷积</strong>，或者<strong>注意力机制</strong>。具体来说，就是通过<strong>学习的方式来自动获取到每个特征通道的重要程度</strong>，然后依照这个重要程度去<strong>提升有用的特征并抑制对当前任务用处不大的特征</strong>。</font><br>  <font size="3">首先是 <strong>Squeeze操作</strong>，先<strong>进行全局池化，具有全局的感受野</strong>，并且输出的维度和输入的特征通道数相匹配，它表征着在特征通道上响应的全局分布。</font><br>  <font size="3">然后是<strong>Excitation操作</strong>，<strong>通过全连接层为每个特征通道生成权重，建立通道间的相关性</strong>，<strong>输出的权重看做是进过特征选择后的每个特征通道的重要性</strong>，然后通过<strong>乘法逐通道加权到先前的特征上</strong>，完成在通道维度上的对原始特征的重标定。</font></p><h1 id="GhostNet图像分析"><a href="#GhostNet图像分析" class="headerlink" title="GhostNet图像分析"></a><font size="5" color="red">GhostNet图像分析</font></h1><p><img src="/images/Feature_extraction/GhostNet_A.png" alt="GhostNet"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_Relu, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential()</span><br><span class="line">        if name.find('depthwise') == -1:</span><br><span class="line">            self.block.add(keras.layers.Conv2D(filters, kernel_size, strides, padding=padding))</span><br><span class="line">        else:</span><br><span class="line">            self.block.add(keras.layers.DepthwiseConv2D(kernel_size, strides, padding=padding))</span><br><span class="line">        self.block.add(keras.layers.BatchNormalization())</span><br><span class="line">        if name.find('relu') != -1:</span><br><span class="line">            self.block.add(keras.layers.ReLU())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.block(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def ghost_module(x, out_channel, relu, name):</span><br><span class="line">    shortcut = Conv_Bn_Relu(out_channel // 2, (1, 1), (1, 1), 'same', name='{}_conv_bn'.format(name) + '_relu' if relu else '{}_conv_bn'.format(name))(x)</span><br><span class="line">    x = Conv_Bn_Relu(None, (3, 3), (1, 1), 'same', name='{}_depthwiseconv_bn'.format(name) + '_relu' if relu else '{}_depthwiseconv_bn'.format(name))(shortcut)</span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def se_block(x, filters, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = compose(keras.layers.GlobalAveragePooling2D(name='{}_global_averagepool'.format(name)),</span><br><span class="line">                keras.layers.Dense(filters // 4, name='{}_dense1'.format(name)),</span><br><span class="line">                keras.layers.ReLU(name='{}_relu'.format(name)),</span><br><span class="line">                keras.layers.Dense(filters, name='{}_dense2'.format(name)),</span><br><span class="line">                keras.layers.Activation('sigmoid', name='{}_sigmoid'.format(name)),</span><br><span class="line">                keras.layers.Reshape((1, 1, filters), name='{}_reshape'.format(name)))(x)</span><br><span class="line">    x = keras.layers.Multiply(name='{}_multiply'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def ghost_bneck(x, out_channel, exp_channel, kernel_size, strides, se, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = ghost_module(x, exp_channel, relu=True, name='{}_module1'.format(name))</span><br><span class="line">    if strides == (2, 2):</span><br><span class="line">        x = Conv_Bn_Relu(None, kernel_size, strides, 'same', name='{}_depthwiseconv_bn_relu'.format(name))(x)</span><br><span class="line">    if se:</span><br><span class="line">        x = se_block(x, exp_channel, name='{}_se_block'.format(name))</span><br><span class="line">    x = ghost_module(x, out_channel, relu=False, name='{}_module2'.format(name))</span><br><span class="line">    if shortcut.shape[-1] == out_channel and strides == (1, 1):</span><br><span class="line">        x = keras.layers.Add(name='{}_add'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def ghostnet(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = Conv_Bn_Relu(16, (3, 3), (2, 2), 'same', name='conv_bn_relu1')(x)</span><br><span class="line"></span><br><span class="line">    x = ghost_bneck(x, out_channel=16, exp_channel=16, kernel_size=(3, 3), strides=(1, 1), se=False, name='bneck1_1')</span><br><span class="line">    x = ghost_bneck(x, out_channel=24, exp_channel=48, kernel_size=(3, 3), strides=(2, 2), se=False, name='bneck1_2')</span><br><span class="line"></span><br><span class="line">    x = ghost_bneck(x, out_channel=24, exp_channel=72, kernel_size=(3, 3), strides=(1, 1), se=False, name='bneck2_1')</span><br><span class="line">    x = ghost_bneck(x, out_channel=40, exp_channel=72, kernel_size=(5, 5), strides=(2, 2), se=True, name='bneck2_2')</span><br><span class="line"></span><br><span class="line">    x = ghost_bneck(x, out_channel=40, exp_channel=120, kernel_size=(5, 5), strides=(1, 1), se=True, name='bneck3_1')</span><br><span class="line">    x = ghost_bneck(x, out_channel=80, exp_channel=240, kernel_size=(3, 3), strides=(2, 2), se=False, name='bneck3_2')</span><br><span class="line"></span><br><span class="line">    x = ghost_bneck(x, out_channel=80, exp_channel=200, kernel_size=(3, 3), strides=(1, 1), se=False, name='bneck4_1')</span><br><span class="line">    x = ghost_bneck(x, out_channel=80, exp_channel=184, kernel_size=(3, 3), strides=(1, 1), se=False, name='bneck4_2')</span><br><span class="line">    x = ghost_bneck(x, out_channel=80, exp_channel=184, kernel_size=(3, 3), strides=(1, 1), se=False, name='bneck4_3')</span><br><span class="line">    x = ghost_bneck(x, out_channel=112, exp_channel=480, kernel_size=(3, 3), strides=(1, 1), se=True, name='bneck4_4')</span><br><span class="line">    x = ghost_bneck(x, out_channel=112, exp_channel=672, kernel_size=(3, 3), strides=(1, 1), se=True, name='bneck4_5')</span><br><span class="line">    x = ghost_bneck(x, out_channel=160, exp_channel=672, kernel_size=(5, 5), strides=(2, 2), se=True, name='bneck4_6')</span><br><span class="line"></span><br><span class="line">    x = ghost_bneck(x, out_channel=160, exp_channel=960, kernel_size=(5, 5), strides=(1, 1), se=False, name='bneck5_1')</span><br><span class="line">    x = ghost_bneck(x, out_channel=160, exp_channel=960, kernel_size=(5, 5), strides=(1, 1), se=True, name='bneck5_2')</span><br><span class="line">    x = ghost_bneck(x, out_channel=160, exp_channel=960, kernel_size=(5, 5), strides=(1, 1), se=False, name='bneck5_3')</span><br><span class="line">    x = ghost_bneck(x, out_channel=160, exp_channel=960, kernel_size=(5, 5), strides=(1, 1), se=True, name='bneck5_4')</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_Relu(960, (1, 1), (1, 1), 'same', name='conv_bn_relu2'),</span><br><span class="line">                keras.layers.AveragePooling2D((7, 7), (7, 7), name='averagepool'),</span><br><span class="line">                Conv_Bn_Relu(1280, (1, 1), (1, 1), 'same', name='conv_bn_relu3'),</span><br><span class="line">                keras.layers.Conv2D(1000, (1, 1), (1, 1), 'same', activation='softmax', name='conv'),</span><br><span class="line">                keras.layers.Reshape((1000,), name='reshape'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='GhostNet')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = ghostnet(input_shape=(224, 224, 3))</span><br><span class="line">    model.build(input_shape=(None, 224, 224, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/GhostNet_R.png" alt="GhostNet"></p><h1 id="GhostNet小结"><a href="#GhostNet小结" class="headerlink" title="GhostNet小结"></a><font size="5" color="red">GhostNet小结</font></h1><p>  GhostNet是一种复杂的轻量级深度学习网络，参数量为5M，其借鉴了大量优秀的深度学习网络的精髓，如<strong>MobileNet的深度可分离卷积</strong>思想，<strong>AlexNet的分组卷积</strong>思想，<strong>SENet的注意力机制</strong>，因此获得了较好的效果。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;GhostNet&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>ShuffleNet-V2</title>
    <link href="https://USTCcoder.github.io/2020/04/02/feature_extraction%20ShuffleNet_V2/"/>
    <id>https://USTCcoder.github.io/2020/04/02/feature_extraction ShuffleNet_V2/</id>
    <published>2020-04-02T07:13:17.000Z</published>
    <updated>2020-05-28T02:57:16.307Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">ShuffleNet-V2</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>ShuffleNet-V2</strong>:<strong>2018年发表于ECCV</strong>，是一种高效的轻量级深度学习模型，在<strong>同等复杂度下，ShuffleNet-V2比ShuffleNet和MobileNet更准确</strong>。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/ShuffleNet_V2.png" alt="ShuffleNet_V2"></p><h1 id="ShuffleNet-V2特点"><a href="#ShuffleNet-V2特点" class="headerlink" title="ShuffleNet-V2特点"></a><font size="5" color="red">ShuffleNet-V2特点</font></h1><p>  <font size="3"><strong>借鉴了AlexNet分组卷积的概念</strong>，引入了<strong>通道分离</strong>和<strong>通道洗牌</strong>，在<strong>减少参数量的同时，增加了通道之间的联系</strong>，并且<strong>对最后的结果进行通道合并，完成特征融合</strong>。</font><br>  <font size="3">在Block中使用深度可分离卷积思想，减少模型参数量</font></p><h1 id="Group-Convolution"><a href="#Group-Convolution" class="headerlink" title="Group Convolution"></a><font size="5" color="red">Group Convolution</font></h1><p><img src="/images/Feature_extraction/ShuffleNet_V2_G.png" alt="ShuffleNet_V2"><br>  <font size="3"><strong>Group Convolution(分组卷积)</strong>：<strong>传统卷积是采用一种卷积全连接的思想</strong>，特征图中的每一个像素点都结合了图像中所有通道的信息。而分组卷积特征图像<strong>每一个像素点只利用到一部分原始图像的通道</strong>。</font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>。如果一个64x64x256的图像，经过5x5的卷积核后变为64x64x256的图像，经过普通卷积的参数量为256x(256x5x5+1)=1638656，而分成32组的分组卷积的参数量为256x(8*5x5+1)=51456，参数量缩小了约32倍，当组数变成通道数时，则类似于Depthwise Convolution深度卷积</font></p><h1 id="Separable-Convolution"><a href="#Separable-Convolution" class="headerlink" title="Separable Convolution"></a><font size="5" color="red">Separable Convolution</font></h1><p><img src="/images/Feature_extraction/Xception_D.png" alt="Xception"><br>  <font size="3"><strong>Separable Convolution(深度可分离卷积)</strong>：是上面两个卷积合二为一的卷积操作。</font><br>  <font size="3"><strong>第一步：DepthwiseConv，对每一个通道进行卷积</strong></font><br>  <font size="3"><strong>第二步：PointwiseConv，对第一步得到的结果进行1x1卷积，实现通道融合</strong></font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>，并且可以<strong>调整为任意合适的通道数</strong>。第一步的<strong>目的是减少参数量</strong>，第二步是<strong>调整通道数</strong>，因此将两个卷积操作结合，组成深度可分离卷积。</font></p><h1 id="不同尺寸ShuffleNet-V2网络结构"><a href="#不同尺寸ShuffleNet-V2网络结构" class="headerlink" title="不同尺寸ShuffleNet-V2网络结构"></a><font size="5" color="red">不同尺寸ShuffleNet-V2网络结构</font></h1><p><img src="/images/Feature_extraction/ShuffleNet_V2_C.png" alt="ShuffleNet_V2"></p><h1 id="ShuffleNet-V2图像分析"><a href="#ShuffleNet-V2图像分析" class="headerlink" title="ShuffleNet-V2图像分析"></a><font size="5" color="red">ShuffleNet-V2图像分析</font></h1><p><img src="/images/Feature_extraction/ShuffleNet_V2_A.png" alt="ShuffleNet_V2"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Channel_Split(keras.layers.Layer):</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        super(Channel_Split, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return inputs[..., :inputs.shape[-1] // 2], inputs[..., inputs.shape[-1] // 2:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Channel_Shuffle(keras.layers.Layer):</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        super(Channel_Shuffle, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        channel = inputs.shape[-1]</span><br><span class="line">        output = compose(keras.layers.Reshape((inputs.shape[1], inputs.shape[2], 2, channel // 2)),</span><br><span class="line">                           keras.layers.Permute([1, 2, 4, 3]),</span><br><span class="line">                           keras.layers.Reshape((inputs.shape[1], inputs.shape[2], channel)))(inputs)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_Relu, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential()</span><br><span class="line">        if name.find('depthwise') == -1:</span><br><span class="line">            self.block.add(keras.layers.Conv2D(filters, kernel_size, strides, padding=padding))</span><br><span class="line">        else:</span><br><span class="line">            self.block.add(keras.layers.DepthwiseConv2D(kernel_size, strides, padding=padding))</span><br><span class="line">        self.block.add(keras.layers.BatchNormalization())</span><br><span class="line">        if name.find('relu') != -1:</span><br><span class="line">            self.block.add(keras.layers.ReLU())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.block(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def block(x, out_channel, strides, name):</span><br><span class="line">    if strides == (1, 1):</span><br><span class="line">        split1, split2 = Channel_Split(name='{}_channel_split'.format(name))(x)</span><br><span class="line">    else:</span><br><span class="line">        split1 = split2 = x</span><br><span class="line"></span><br><span class="line">    split1 = compose(Conv_Bn_Relu(out_channel // 2, (1, 1), (1, 1), 'same', name='{}_part1_conv_bn_relu1'.format(name)),</span><br><span class="line">                    Conv_Bn_Relu(None, (3, 3), strides, 'same', name='{}_part1_depthwiseconv_bn'.format(name)),</span><br><span class="line">                    Conv_Bn_Relu(out_channel // 2, (1, 1), (1, 1), 'same', name='{}_part1_conv_bn_relu2'.format(name)))(split1)</span><br><span class="line"></span><br><span class="line">    if strides == (2, 2):</span><br><span class="line">        split2 = compose(Conv_Bn_Relu(None, (3, 3), strides, 'same', name='{}_part2_depthwiseconv_bn'.format(name)),</span><br><span class="line">                         Conv_Bn_Relu(out_channel // 2, (1, 1), (1, 1), 'same', name='{}_part2_conv_bn_relu'.format(name)))(split2)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([split1, split2])</span><br><span class="line">    x = Channel_Shuffle(name='{}_channel_shuffle'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def add_block(x, filters, n, name):</span><br><span class="line">    x = block(x, filters, (2, 2), name='{}_1'.format(name))</span><br><span class="line">    for i in range(n):</span><br><span class="line">        x = block(x, filters, (1, 1), name='{}_{}'.format(name, i + 2))</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def shufflenet_v2(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(24, (3, 3), (2, 2), padding='same', activation='relu', name='conv_relu1'),</span><br><span class="line">                keras.layers.MaxPool2D((3, 3), (2, 2), padding='same', name='maxpool'))(x)</span><br><span class="line"></span><br><span class="line">    x = add_block(x, 116, 3, name='block1')</span><br><span class="line">    x = add_block(x, 232, 7, name='block2')</span><br><span class="line">    x = add_block(x, 464, 3, name='block3')</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(1024, (1, 1), (1, 1), padding='same', activation='relu', name='conv_relu2'),</span><br><span class="line">                keras.layers.GlobalAveragePooling2D(name='global_averagepool'),</span><br><span class="line">                keras.layers.Dense(1000, activation='softmax', name='dense'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='ShuffleNet-V2')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = shufflenet_v2(input_shape=(224, 224, 3))</span><br><span class="line">    model.build(input_shape=(None, 224, 224, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/ShuffleNet_V2_R.png" alt="ShuffleNet_V2"></p><h1 id="ShuffleNet-V2小结"><a href="#ShuffleNet-V2小结" class="headerlink" title="ShuffleNet-V2小结"></a><font size="5" color="red">ShuffleNet-V2小结</font></h1><p>  ShuffleNet-V2是一种有效的轻量级深度学习网络，参数量只有2M，其从AlexNet中借鉴了<strong>分组卷积</strong>的思想，并且运用了<strong>通道分离</strong>和<strong>洗牌</strong>的思想，不但可以大大降低模型参数量，并且可以<strong>提高模型的鲁棒性</strong>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;ShuffleNet-V2&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>EfficientNet</title>
    <link href="https://USTCcoder.github.io/2020/03/31/feature_extraction%20EfficientNet/"/>
    <id>https://USTCcoder.github.io/2020/03/31/feature_extraction EfficientNet/</id>
    <published>2020-03-31T13:04:38.000Z</published>
    <updated>2020-05-28T02:28:51.783Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">EfficientNet</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>EfficientNet:</strong>是<strong>谷歌公司于2019年提出</strong>的高效神经网络，故得名为EfficientNet，<strong>大幅度的缩小了参数的同时提高了预测准确度</strong>。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/EfficientNet.png" alt="EfficientNet"></p><h1 id="EfficientNet特点"><a href="#EfficientNet特点" class="headerlink" title="EfficientNet特点"></a><font size="5" color="red">EfficientNet特点</font></h1><p>  <font size="3">和MobileNet_V3类似，在Block中<strong>先进行1x1卷积提升通道数</strong>，然后<strong>进行DepthwiseConv深度卷积</strong>减少参数量，并且在<strong>Block中引入残差结构和Squeeze-and-Excitation模块</strong></font><br>  <font size="3">建立<strong>多个网络深度，网络宽度，图像分辨率不同的模型</strong>，从<strong>三个方面拓展网络性能</strong></font></p><h1 id="Depthwise-Convolution"><a href="#Depthwise-Convolution" class="headerlink" title="Depthwise Convolution"></a><font size="5" color="red">Depthwise Convolution</font></h1><p><img src="/images/deep_learning/depthwise.png" alt="depthwise"><br>  <font size="3"><strong>Depthwise Convolution(深度卷积)：在</strong>每一个通道上单独进行卷积**</font><br>  <font size="3">参数<strong>depth_multiplier默认为1</strong>，代表每个通道数进行一次单独卷积，<strong>输出的通道数和输入通道数相等</strong>，设置<strong>depth_multiplier=n</strong>，则代表每个通道数进行n次单独卷积，<strong>输出通道数是输入通道数的n倍</strong>。</font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>。如果一个8x8x1024的特征图，经过5x5的卷积核后变为8x8x1024的图像，经过普通卷积的参数量为1024x(1024x5x5+1)=26215424，而深度卷积参数量为1024x(1x5x5+1)=26624，参数量缩小了约1024倍。</font></p><h1 id="Squeeze-and-Excitation"><a href="#Squeeze-and-Excitation" class="headerlink" title="Squeeze-and-Excitation"></a><font size="5" color="red">Squeeze-and-Excitation</font></h1><p><img src="/images/Feature_extraction/SENet_S.png" alt="SENet"><br>  <font size="3"><strong>Squeeze-and-Excitation</strong>：又称为<strong>特征重标定卷积</strong>，或者<strong>注意力机制</strong>。具体来说，就是通过<strong>学习的方式来自动获取到每个特征通道的重要程度</strong>，然后依照这个重要程度去<strong>提升有用的特征并抑制对当前任务用处不大的特征</strong>。</font><br>  <font size="3">首先是 <strong>Squeeze操作</strong>，先<strong>进行全局池化，具有全局的感受野</strong>，并且输出的维度和输入的特征通道数相匹配，它表征着在特征通道上响应的全局分布。</font><br>  <font size="3">然后是<strong>Excitation操作</strong>，<strong>通过全连接层为每个特征通道生成权重，建立通道间的相关性</strong>，<strong>输出的权重看做是进过特征选择后的每个特征通道的重要性</strong>，然后通过<strong>乘法逐通道加权到先前的特征上</strong>，完成在通道维度上的对原始特征的重标定。</font></p><h1 id="EfficientNet基模型B0图像分析"><a href="#EfficientNet基模型B0图像分析" class="headerlink" title="EfficientNet基模型B0图像分析"></a><font size="5" color="red">EfficientNet基模型B0图像分析</font></h1><p><img src="/images/Feature_extraction/EfficientNet_A.png" alt="EfficientNet"></p><h1 id="基模型B0的TensorFlow2-0实现"><a href="#基模型B0的TensorFlow2-0实现" class="headerlink" title="基模型B0的TensorFlow2.0实现"></a><font size="4">基模型B0的TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Swish(keras.layers.Layer):</span><br><span class="line">    def __init__(self, name='swish'):</span><br><span class="line">        super(Swish, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return inputs * keras.activations.sigmoid(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def se_block(x, down_filters, up_filters, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = compose(keras.layers.GlobalAveragePooling2D(name='{}_global_averagepool'.format(name)),</span><br><span class="line">                keras.layers.Reshape((1, 1, up_filters), name='{}_reshape'.format(name)),</span><br><span class="line">                keras.layers.Conv2D(down_filters, (1, 1), activation=Swish(name='{}_swish'.format(name)), name='{}_conv1'.format(name)),</span><br><span class="line">                keras.layers.Conv2D(up_filters, (1, 1), activation='sigmoid', name='{}_conv2'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Multiply(name='{}_multiply'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Swish(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_Swish, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential()</span><br><span class="line">        if name.find('depthwise') == -1:</span><br><span class="line">            self.block.add(keras.layers.Conv2D(filters, kernel_size, strides, padding=padding))</span><br><span class="line">        else:</span><br><span class="line">            self.block.add(keras.layers.DepthwiseConv2D(kernel_size, strides, padding=padding))</span><br><span class="line">        self.block.add(keras.layers.BatchNormalization())</span><br><span class="line">        if name.find('swish') != -1:</span><br><span class="line">            self.block.add(Swish())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.block(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def block(x, in_channel, out_channel, times1, times2, kernel_size, strides, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = compose(Conv_Bn_Swish(in_channel * times1, (1, 1), (1, 1), 'same', name='{}_conv_bn_swish'.format(name)),</span><br><span class="line">                Conv_Bn_Swish(None, kernel_size, strides, 'same', name='{}_depthwiseconv_bn_swish'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x = se_block(x, in_channel // times2, in_channel * times1, name='{}_se_block'.format(name))</span><br><span class="line"></span><br><span class="line">    x = Conv_Bn_Swish(out_channel, (1, 1), (1, 1), 'same', name='{}_conv_bn'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    if in_channel == out_channel and strides == (1, 1):</span><br><span class="line">        x = keras.layers.Add(name='{}_add'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def efficientnet(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.ZeroPadding2D(name='zeropadding'),</span><br><span class="line">                Conv_Bn_Swish(32, (3, 3), (2, 2), 'valid', name='conv_bn_swish1'))(x)</span><br><span class="line"></span><br><span class="line">    x = block(x, in_channel=32, out_channel=16, times1=1, times2=4, kernel_size=(3, 3), strides=(1, 1), name='block1')</span><br><span class="line"></span><br><span class="line">    x = block(x, in_channel=16, out_channel=24, times1=6, times2=4, kernel_size=(3, 3), strides=(2, 2), name='block2_1')</span><br><span class="line">    x = block(x, in_channel=24, out_channel=24, times1=6, times2=4, kernel_size=(3, 3), strides=(1, 1), name='block2_2')</span><br><span class="line"></span><br><span class="line">    x = block(x, in_channel=24, out_channel=40, times1=6, times2=4, kernel_size=(5, 5), strides=(2, 2), name='block3_1')</span><br><span class="line">    x = block(x, in_channel=40, out_channel=40, times1=6, times2=4, kernel_size=(5, 5), strides=(1, 1), name='block3_2')</span><br><span class="line"></span><br><span class="line">    x = block(x, in_channel=40, out_channel=80, times1=6, times2=4, kernel_size=(3, 3), strides=(2, 2), name='block4_1')</span><br><span class="line">    x = block(x, in_channel=80, out_channel=80, times1=6, times2=4, kernel_size=(3, 3), strides=(1, 1), name='block4_2')</span><br><span class="line">    x = block(x, in_channel=80, out_channel=80, times1=6, times2=4, kernel_size=(3, 3), strides=(1, 1), name='block4_3')</span><br><span class="line"></span><br><span class="line">    x = block(x, in_channel=80, out_channel=112, times1=6, times2=4, kernel_size=(5, 5), strides=(1, 1), name='block5_1')</span><br><span class="line">    x = block(x, in_channel=112, out_channel=112, times1=6, times2=4, kernel_size=(5, 5), strides=(1, 1), name='block5_2')</span><br><span class="line">    x = block(x, in_channel=112, out_channel=112, times1=6, times2=4, kernel_size=(5, 5), strides=(1, 1), name='block5_3')</span><br><span class="line"></span><br><span class="line">    x = block(x, in_channel=112, out_channel=192, times1=6, times2=4, kernel_size=(5, 5), strides=(2, 2), name='block6_1')</span><br><span class="line">    x = block(x, in_channel=192, out_channel=192, times1=6, times2=4, kernel_size=(5, 5), strides=(1, 1), name='block6_2')</span><br><span class="line">    x = block(x, in_channel=192, out_channel=192, times1=6, times2=4, kernel_size=(5, 5), strides=(1, 1), name='block6_3')</span><br><span class="line">    x = block(x, in_channel=192, out_channel=192, times1=6, times2=4, kernel_size=(5, 5), strides=(1, 1), name='block6_4')</span><br><span class="line"></span><br><span class="line">    x = block(x, in_channel=192, out_channel=320, times1=6, times2=4, kernel_size=(3, 3), strides=(1, 1), name='block7')</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_Swish(1280, (1, 1), (1, 1), 'same', name='conv_bn_swish2'),</span><br><span class="line">                keras.layers.GlobalAveragePooling2D(name='global_averagepool'),</span><br><span class="line">                keras.layers.Dropout(0.2, name='dropout'),</span><br><span class="line">                keras.layers.Dense(1000, activation='softmax', name='dense'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='EfficientNet')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = efficientnet(input_shape=(224, 224, 3))</span><br><span class="line">    model.build(input_shape=(None, 224, 224, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/EfficientNet_R.png" alt="EfficientNet"></p><h1 id="EfficientNet小结"><a href="#EfficientNet小结" class="headerlink" title="EfficientNet小结"></a><font size="5" color="red">EfficientNet小结</font></h1><p>  EfficientNet是一种复杂的深度学习网络，从上图可以看出EfficientNet基模型B0的参数量有5M，其考虑<strong>网络深度</strong>，<strong>网络宽度</strong>，<strong>图像分辨率</strong>等因素的思想值得我们学习。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;EfficientNet&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>EffNet</title>
    <link href="https://USTCcoder.github.io/2020/03/28/feature_extraction%20EffNet/"/>
    <id>https://USTCcoder.github.io/2020/03/28/feature_extraction EffNet/</id>
    <published>2020-03-28T12:58:51.000Z</published>
    <updated>2020-05-28T02:26:23.772Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">EffNet</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>EffNet</strong>:<strong>2018年发表于ICIP</strong>，提出一种<strong>新颖的卷积块设计</strong>，能够显著<strong>减轻计算负担</strong>，且性能远胜当前的最好的模型(对比MobileNet,ShuffleNet)。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/EffNet.png" alt="EffNet"></p><h1 id="EffNet特点"><a href="#EffNet特点" class="headerlink" title="EffNet特点"></a><font size="5" color="red">EffNet特点</font></h1><p>  <font size="3">借鉴了<strong>Depthwise Convolution(深度卷积)</strong>的思想，并且将Inception中<strong>spatial separable convolutions(空间可分离卷积)的思想推广到池化分解</strong>。采用<strong>深度可分离卷积</strong>代替<strong>传统的卷积</strong>，并且<strong>卷积核采用1x3和3x1代替3x3</strong>，两次卷积核之间<strong>插入一维池化核</strong>，可以进一步减少参数量。</font></p><h1 id="Depthwise-Convolution"><a href="#Depthwise-Convolution" class="headerlink" title="Depthwise Convolution"></a><font size="5" color="red">Depthwise Convolution</font></h1><p><img src="/images/deep_learning/depthwise.png" alt="depthwise"><br>  <font size="3"><strong>Depthwise Convolution(深度卷积)：在</strong>每一个通道上单独进行卷积**</font><br>  <font size="3">参数<strong>depth_multiplier默认为1</strong>，代表每个通道数进行一次单独卷积，<strong>输出的通道数和输入通道数相等</strong>，设置<strong>depth_multiplier=n</strong>，则代表每个通道数进行n次单独卷积，<strong>输出通道数是输入通道数的n倍</strong>。</font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>。如果一个8x8x1024的特征图，经过5x5的卷积核后变为8x8x1024的图像，经过普通卷积的参数量为1024x(1024x5x5+1)=26215424，而深度卷积参数量为1024x(1x5x5+1)=26624，参数量缩小了约1024倍。</font></p><h1 id="Spatial-Separable-Convolution"><a href="#Spatial-Separable-Convolution" class="headerlink" title="Spatial Separable Convolution"></a><font size="5" color="red">Spatial Separable Convolution</font></h1><p><img src="/images/deep_learning/spatial.png" alt="spatial"><br>  <font size="3"><strong>Spatial Separable Convolution(空间可分离卷积)</strong>：将3x3的卷积分解为3x1的卷积核1x3的卷积，将7x7的卷积分解为7x1的卷积核1x7的卷积.。</font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>。如果一个64x64x256的特征图，经过7x7的卷积核后变为64x64x256的图像，经过普通卷积的参数量为256x(256x7x7+1)=3211520，而空间可分离卷积参数量为2x256x(256x7x1+1)=918016，参数量缩小了约3.5倍。</font></p><h1 id="EffNet图像分析"><a href="#EffNet图像分析" class="headerlink" title="EffNet图像分析"></a><font size="5" color="red">EffNet图像分析</font></h1><p><img src="/images/Feature_extraction/EffNet_A.png" alt="EffNet"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_LeakyRelu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_LeakyRelu, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential()</span><br><span class="line">        if name.find('depthwise') == -1:</span><br><span class="line">            self.block.add(keras.layers.Conv2D(filters, kernel_size, strides, padding=padding))</span><br><span class="line">        else:</span><br><span class="line">            self.block.add(keras.layers.DepthwiseConv2D(kernel_size, strides, padding=padding))</span><br><span class="line">        self.block.add(keras.layers.BatchNormalization())</span><br><span class="line">        self.block.add(keras.layers.LeakyReLU())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.block(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def block(inchannel, outchannel, name):</span><br><span class="line"></span><br><span class="line">    return compose(Conv_Bn_LeakyRelu(inchannel, (1, 1), (1, 1), 'same', name='{}_conv_bn_leakyrelu1'.format(name)),</span><br><span class="line">                   Conv_Bn_LeakyRelu(None, (1, 3), (1, 1), 'same', name='{}_depthwiseconv_bn_leakyrelu1'.format(name)),</span><br><span class="line">                   keras.layers.MaxPool2D((1, 2), (1, 2), name='{}_maxpool'.format(name)),</span><br><span class="line">                   Conv_Bn_LeakyRelu(None, (3, 1), (1, 1), 'same', name='{}_depthwiseconv_bn_leakyrelu2'.format(name)),</span><br><span class="line">                   Conv_Bn_LeakyRelu(outchannel, (1, 1), (2, 1), 'same', name='{}_conv_bn_leakyrelu2'.format(name)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def effnet(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(block(32, 64, name='block1'),</span><br><span class="line">                block(64, 128, name='block2'),</span><br><span class="line">                block(128, 256, name='block3'))(x)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Flatten(name='flatten'),</span><br><span class="line">                keras.layers.Dense(10, activation='softmax', name='dense'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='EffNet')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = effnet(input_shape=(32, 32, 3))</span><br><span class="line">    model.build(input_shape=(None, 32, 32, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/EffNet_R.png" alt="EffNet"></p><h1 id="EffNet小结"><a href="#EffNet小结" class="headerlink" title="EffNet小结"></a><font size="5" color="red">EffNet小结</font></h1><p>  EffNet给我们<strong>提供了一种一维池化的思路</strong>，虽然论文中以Cifar10作为数据集，参数量无法和其他模型进行直接的对比，但是模型的使用效果却优于MobileNet和ShuffleNet。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;EffNet&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>MobileNet-V3</title>
    <link href="https://USTCcoder.github.io/2020/03/26/feature_extraction%20MobileNet_V3/"/>
    <id>https://USTCcoder.github.io/2020/03/26/feature_extraction MobileNet_V3/</id>
    <published>2020-03-26T07:07:42.000Z</published>
    <updated>2020-05-27T15:45:44.420Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">MobileNet-V3</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>MobileNet-V3:</strong>是Google继MobileNet-V2之后的又一力作，于<strong>2019年提出</strong>，效果较MobileNet-V2有所提升。MobileNet-V3提供了<strong>两个版本</strong>，分别为<strong>MobileNet-V3-Large</strong>以及<strong>MobileNet-V3-Small</strong>，分别适用于对资源不同要求的情况。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/MobileNet_V3.png" alt="MobileNet_V3"></p><h1 id="MobileNet-V3特点"><a href="#MobileNet-V3特点" class="headerlink" title="MobileNet-V3特点"></a><font size="5" color="red">MobileNet-V3特点</font></h1><p>  <font size="3"><strong>保留MobileNet-V2的SeparableConv深度可分离结构和残差结构</strong></font><br>  <font size="3"><strong>引入SE结构，具有轻量级的注意力模型</strong></font><br>  <font size="3"><strong>对MobileNet-V2的头部结构进行优化</strong>，MobileNet-V2中第二层得到的特征图大小为112x112x32，而在MobileNet-V3中，只需要112x112x16即可保证精度，并且提升运行速度。</font><br>  <font size="3"><strong>对MobileNet-V2的尾部结构进行优化</strong>，MobileNet-V2中对7x7的特征图进行1x1的卷积提升通道数，然后再进行全局平均池化，而在MobileNet-V3中，先对7x7的特征图进行全局平均池化，然后再进行1x1的卷积提升通道数，节约了49倍的参数量。</font><br>  <font size="3">激活函数使用<strong>h-swish</strong>和<strong>ReLU6</strong>并存的方式，加快了运行的速度</font></p><h1 id="Separable-Convolution"><a href="#Separable-Convolution" class="headerlink" title="Separable Convolution"></a><font size="5" color="red">Separable Convolution</font></h1><p><img src="/images/Feature_extraction/Xception_D.png" alt="Xception"><br>  <font size="3"><strong>Separable Convolution(深度可分离卷积)</strong>：是上面两个卷积合二为一的卷积操作。</font><br>  <font size="3"><strong>第一步：DepthwiseConv，对每一个通道进行卷积</strong></font><br>  <font size="3"><strong>第二步：PointwiseConv，对第一步得到的结果进行1x1卷积，实现通道融合</strong></font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>，并且可以<strong>调整为任意合适的通道数</strong>。第一步的<strong>目的是减少参数量</strong>，第二步是<strong>调整通道数</strong>，因此将两个卷积操作结合，组成深度可分离卷积。</font></p><h1 id="Squeeze-and-Excitation"><a href="#Squeeze-and-Excitation" class="headerlink" title="Squeeze-and-Excitation"></a><font size="5" color="red">Squeeze-and-Excitation</font></h1><p><img src="/images/Feature_extraction/SENet_S.png" alt="SENet"><br>  <font size="3"><strong>Squeeze-and-Excitation</strong>：又称为<strong>特征重标定卷积</strong>，或者<strong>注意力机制</strong>。具体来说，就是通过<strong>学习的方式来自动获取到每个特征通道的重要程度</strong>，然后依照这个重要程度去<strong>提升有用的特征并抑制对当前任务用处不大的特征</strong>。</font><br>  <font size="3">首先是 <strong>Squeeze操作</strong>，先<strong>进行全局池化，具有全局的感受野</strong>，并且输出的维度和输入的特征通道数相匹配，它表征着在特征通道上响应的全局分布。</font><br>  <font size="3">然后是<strong>Excitation操作</strong>，<strong>通过全连接层为每个特征通道生成权重，建立通道间的相关性</strong>，<strong>输出的权重看做是进过特征选择后的每个特征通道的重要性</strong>，然后通过<strong>乘法逐通道加权到先前的特征上</strong>，完成在通道维度上的对原始特征的重标定。</font></p><h1 id="不同尺寸MobileNet-V3网络结构"><a href="#不同尺寸MobileNet-V3网络结构" class="headerlink" title="不同尺寸MobileNet-V3网络结构"></a><font size="5" color="red">不同尺寸MobileNet-V3网络结构</font></h1><p><img src="/images/Feature_extraction/MobileNet_V3_C.png" alt="MobileNet_V3"></p><h1 id="MobileNet-V3图像分析"><a href="#MobileNet-V3图像分析" class="headerlink" title="MobileNet-V3图像分析"></a><font size="5" color="red">MobileNet-V3图像分析</font></h1><p><img src="/images/Feature_extraction/MobileNet_V3_A.png" alt="MobileNet_V3"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class H_Swish(keras.layers.Layer):</span><br><span class="line">    def __init__(self, name='h_swish'):</span><br><span class="line">        super(H_Swish, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return inputs * keras.activations.relu(inputs + 3, max_value=6) / 6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def se_block(x, filters, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = compose(keras.layers.GlobalAveragePooling2D(name='{}_global_averagepool'.format(name)),</span><br><span class="line">                keras.layers.Dense(filters // 4, name='{}_dense1'.format(name)),</span><br><span class="line">                keras.layers.ReLU(6, name='{}_relu6'.format(name)),</span><br><span class="line">                keras.layers.Dense(filters, name='{}_dense2'.format(name)),</span><br><span class="line">                H_Swish(name='{}_h_swish'.format(name)),</span><br><span class="line">                keras.layers.Reshape((1, 1, filters), name='{}_reshape'.format(name)))(x)</span><br><span class="line">    x = keras.layers.Multiply(name='{}_multiply'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def bneck(x, filters, up_dim, kernel_size, strides, squeeze, activation, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = compose(Conv_Bn_Relu6(up_dim, (1, 1), (1, 1), 'same', name='{}_conv_bn_{}'.format(name, activation)),</span><br><span class="line">                Conv_Bn_Relu6(None, kernel_size, strides, 'same', name='{}_depthwiseconv_bn_{}'.format(name, activation)))(x)</span><br><span class="line"></span><br><span class="line">    if squeeze:</span><br><span class="line">        x = se_block(x, up_dim, name='{}_se_block'.format(name))</span><br><span class="line"></span><br><span class="line">    x = Conv_Bn_Relu6(filters, (1, 1), (1, 1), 'same', name='{}_conv_bn'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    if shortcut.shape[-1] == filters and strides == (1, 1):</span><br><span class="line">        x = keras.layers.Add(name='{}_add'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu6(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_Relu6, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential()</span><br><span class="line">        if name.find('depthwise') == -1:</span><br><span class="line">            self.block.add(keras.layers.Conv2D(filters, kernel_size, strides, padding=padding))</span><br><span class="line">        else:</span><br><span class="line">            self.block.add(keras.layers.DepthwiseConv2D(kernel_size, strides, padding=padding))</span><br><span class="line">        self.block.add(keras.layers.BatchNormalization())</span><br><span class="line">        if name.find('h_swish') != -1:</span><br><span class="line">            self.block.add(H_Swish())</span><br><span class="line">        elif name.find('relu6') != -1:</span><br><span class="line">            self.block.add(keras.layers.ReLU(6))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.block(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def mobilenet_v3(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = Conv_Bn_Relu6(16, (3, 3), (2, 2), 'same', name='conv_bn_h_swish1')(x)</span><br><span class="line"></span><br><span class="line">    x = bneck(x, 16, 16, (3, 3), (1, 1), squeeze=False, activation='relu6', name='bneck1')</span><br><span class="line"></span><br><span class="line">    x = bneck(x, 24, 64, (3, 3), (2, 2), squeeze=False, activation='relu6', name='bneck2_1')</span><br><span class="line">    x = bneck(x, 24, 72, (3, 3), (1, 1), squeeze=False, activation='relu6', name='bneck2_2')</span><br><span class="line"></span><br><span class="line">    x = bneck(x, 40, 72, (5, 5), (2, 2), squeeze=True, activation='relu6', name='bneck3_1')</span><br><span class="line">    x = bneck(x, 40, 120, (5, 5), (1, 1), squeeze=True, activation='relu6', name='bneck3_2')</span><br><span class="line">    x = bneck(x, 40, 120, (5, 5), (1, 1), squeeze=True, activation='relu6', name='bneck3_3')</span><br><span class="line"></span><br><span class="line">    x = bneck(x, 80, 240, (3, 3), (2, 2), squeeze=False, activation='h_swish', name='bneck4_1')</span><br><span class="line">    x = bneck(x, 80, 200, (3, 3), (1, 1), squeeze=False, activation='h_swish', name='bneck4_2')</span><br><span class="line">    x = bneck(x, 80, 184, (3, 3), (1, 1), squeeze=False, activation='h_swish', name='bneck4_3')</span><br><span class="line">    x = bneck(x, 80, 184, (3, 3), (1, 1), squeeze=False, activation='h_swish', name='bneck4_4')</span><br><span class="line"></span><br><span class="line">    x = bneck(x, 112, 480, (3, 3), (1, 1), squeeze=True, activation='h_swish', name='bneck5_1')</span><br><span class="line">    x = bneck(x, 112, 672, (3, 3), (1, 1), squeeze=True, activation='h_swish', name='bneck5_2')</span><br><span class="line"></span><br><span class="line">    x = bneck(x, 160, 672, (5, 5), (2, 2), squeeze=True, activation='h_swish', name='bneck6_1')</span><br><span class="line">    x = bneck(x, 160, 960, (5, 5), (1, 1), squeeze=True, activation='h_swish', name='bneck6_2')</span><br><span class="line">    x = bneck(x, 160, 960, (5, 5), (1, 1), squeeze=True, activation='h_swish', name='bneck6_3')</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_Relu6(960, (1, 1), (1, 1), 'same', name='conv_bn_h_swish2'),</span><br><span class="line">                keras.layers.GlobalAveragePooling2D(name='global_averagepool'),</span><br><span class="line">                keras.layers.Reshape((1, 1, 960), name='reshape1'),</span><br><span class="line">                Conv_Bn_Relu6(1280, (1, 1), (1, 1), 'same', name='conv_bn_h_swish3'),</span><br><span class="line">                keras.layers.Conv2D(1000, (1, 1), activation='softmax', name='conv'),</span><br><span class="line">                keras.layers.Reshape((1000,), name='reshape2'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='MobileNet-V3')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = mobilenet_v3(input_shape=(224, 224, 3))</span><br><span class="line">    model.build(input_shape=(None, 224, 224, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/MobileNet_V3_R.png" alt="MobileNet_V3"></p><h1 id="MobileNet-V3小结"><a href="#MobileNet-V3小结" class="headerlink" title="MobileNet-V3小结"></a><font size="5" color="red">MobileNet-V3小结</font></h1><p>  MobileNet-V3是一种复杂的轻量级深度学习网络，从上图可以看出MobileNet-V3模型的参数量为5M，其在MobileNet-V2的基础上加入了大量黑科技，因此获得了更好的效果。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;MobileNet-V3&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>MobileNet-V2</title>
    <link href="https://USTCcoder.github.io/2020/03/22/feature_extraction%20MobileNet_V2/"/>
    <id>https://USTCcoder.github.io/2020/03/22/feature_extraction MobileNet_V2/</id>
    <published>2020-03-22T12:04:27.000Z</published>
    <updated>2020-05-27T14:26:35.311Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">MobileNet-V2</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>MobileNet-V2:</strong>是MobileNet的升级版本，<strong>Google公司于2018年</strong>提出，是一个结构简单，参数量较少的轻量级深度学习网络模型。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/MobileNet_V2.png" alt="MobileNet_V2"></p><h1 id="MobileNet-V2特点"><a href="#MobileNet-V2特点" class="headerlink" title="MobileNet-V2特点"></a><font size="5" color="red">MobileNet-V2特点</font></h1><p>  <font size="3"><strong>保留MobileNet的SeparableConv深度可分离卷积结构，并且在深度可分离卷积前通过1x1卷积提升通道数。</strong></font><br>  <font size="3">引入了<strong>反残差结构</strong>，借鉴了ResNet的思想，联系了不同尺度的特征</font><br>  <font size="3">引入了<strong>线性瓶颈结构</strong>，在残差相加前不使用ReLU6激活函数，避免破坏特征</font></p><h1 id="Separable-Convolution"><a href="#Separable-Convolution" class="headerlink" title="Separable Convolution"></a><font size="5" color="red">Separable Convolution</font></h1><p><img src="/images/Feature_extraction/Xception_D.png" alt="Xception"><br>  <font size="3"><strong>Separable Convolution(深度可分离卷积)</strong>：是上面两个卷积合二为一的卷积操作。</font><br>  <font size="3"><strong>第一步：DepthwiseConv，对每一个通道进行卷积</strong></font><br>  <font size="3"><strong>第二步：PointwiseConv，对第一步得到的结果进行1x1卷积，实现通道融合</strong></font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>，并且可以<strong>调整为任意合适的通道数</strong>。第一步的<strong>目的是减少参数量</strong>，第二步是<strong>调整通道数</strong>，因此将两个卷积操作结合，组成深度可分离卷积。</font></p><h1 id="MobileNet-V2图像分析"><a href="#MobileNet-V2图像分析" class="headerlink" title="MobileNet-V2图像分析"></a><font size="5" color="red">MobileNet-V2图像分析</font></h1><p><img src="/images/Feature_extraction/MobileNet_V2_A.png" alt="MobileNet_V2"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu6(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_Relu6, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential()</span><br><span class="line">        if name.find('depthwise') == -1:</span><br><span class="line">            self.block.add(keras.layers.Conv2D(filters, kernel_size, strides, padding=padding))</span><br><span class="line">        else:</span><br><span class="line">            self.block.add(keras.layers.DepthwiseConv2D(kernel_size, strides, padding=padding))</span><br><span class="line">        self.block.add(keras.layers.BatchNormalization())</span><br><span class="line">        if name.find('relu') != -1:</span><br><span class="line">            self.block.add(keras.layers.ReLU(6))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.block(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def block(x, filters, t, strides, name):</span><br><span class="line">    shortcut = x</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_Relu6(t * filters, (1, 1), (1, 1), 'same', name='{}_conv_bn_relu6'.format(name)),</span><br><span class="line">                Conv_Bn_Relu6(None, (3, 3), strides, 'same', name='{}_depthwiseconv_bn_relu6'.format(name)),</span><br><span class="line">                Conv_Bn_Relu6(filters, (1, 1), (1, 1), 'same', name='{}_conv_bn'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    if shortcut.shape[-1] == filters and strides == (1, 1):</span><br><span class="line">        x = keras.layers.Add(name='{}_add'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def add_block(x, filters, t, strides, n, name):</span><br><span class="line">    x = block(x, filters, t, strides, name='{}_1'.format(name))</span><br><span class="line">    for i in range(n - 1):</span><br><span class="line">        x = block(x, filters, t, (1, 1), name='{}_{}'.format(name, i + 2))</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def mobilenet_v2(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.ZeroPadding2D((1, 1), name='zeropadding'),</span><br><span class="line">                Conv_Bn_Relu6(32, (3, 3), (2, 2), 'valid', name='conv1'))(x)</span><br><span class="line"></span><br><span class="line">    x = add_block(x, 16, 1, (1, 1), 1, 'block1')</span><br><span class="line">    x = add_block(x, 24, 6, (2, 2), 2, 'block2')</span><br><span class="line">    x = add_block(x, 32, 6, (2, 2), 3, 'block3')</span><br><span class="line">    x = add_block(x, 64, 6, (2, 2), 4, 'block4')</span><br><span class="line">    x = add_block(x, 96, 6, (1, 1), 3, 'block5')</span><br><span class="line">    x = add_block(x, 160, 6, (2, 2), 3, 'block6')</span><br><span class="line">    x = add_block(x, 320, 6, (1, 1), 1, 'block7')</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_Relu6(1280, (1, 1), (1, 1), 'same', name='conv2'),</span><br><span class="line">                keras.layers.GlobalAveragePooling2D(name='global_averagepool'),</span><br><span class="line">                keras.layers.Dense(1000, activation='softmax', name='dense'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='MobileNet-V2')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = mobilenet_v2(input_shape=(224, 224, 3))</span><br><span class="line">    model.build(input_shape=(None, 224, 224, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/MobileNet_V2_R.png" alt="MobileNet_V2"></p><h1 id="MobileNet-V2小结"><a href="#MobileNet-V2小结" class="headerlink" title="MobileNet-V2小结"></a><font size="5" color="red">MobileNet-V2小结</font></h1><p>  MobileNet-V2是一种高效的轻量级深度学习网络，<strong>MobileNet-V2模型的参数量和MobileNet几乎相同</strong>，都是4M的参数量，因为其<strong>结合了不同尺度的特征</strong>，因此效果优于MobileNet。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;MobileNet-V2&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>Turing Reward in 2019</title>
    <link href="https://USTCcoder.github.io/2020/03/22/TuringReward-2019/"/>
    <id>https://USTCcoder.github.io/2020/03/22/TuringReward-2019/</id>
    <published>2020-03-22T08:46:39.000Z</published>
    <updated>2020-06-30T15:44:50.933Z</updated>
    
    <content type="html"><![CDATA[<p>  2020 年 3 月 18 日，ACM 宣布，计算机图形学领域的两位大牛 Patrick M. Hanrahan和 Edwin E. Catmull 获得了 2018 年的图灵奖，以表彰他们对3D计算机图形学的贡献，以及这些技术对电影制作和计算机生成图像(CGI)等应用的革命性影响。</p><a id="more"></a><p>他们通过概念创新从根本上影响了计算机图形学的领域，推动了3D动画电影的发展，而且对于视频游戏行业的发展，如VR(Virtual Reality，虚拟现实)，AR(Augmented Reality，增强现实)也起到了重要的作用，不仅如此，他们在GPU上的见解对人工智能领域产生了巨大的推动作用。</p><p>让我们来看看两位大佬所作出的主要贡献：<br><strong>艾德·卡姆尔(Edwin Catmull)</strong><br><img src="/images/TURING/ed.png" alt="Edwin Catmull"><br>   计算机科学家，迪士尼动画工作室主席，皮克斯动画工作室主席，1954年生，Catmull 于 1974 年在犹他大学获得计算机科学博士学位。他的导师包括计算机图形学之父，1988年ACM图灵奖获得者Ivan Sutherland。在他的博士学位论文中，Catmull引入两种突破性的技术来显示曲面补丁而不是多边形：用于管理计算机图形学中图像深度坐标的 Z 缓冲(Z-buffering)；以及将二维表面纹理映射在三维对象上的纹理映射(texture mapping)。</p><p>   在犹他大学期间，Catmull 还创建了一种通过指定一个粗糙多边形网格来表示一个平滑曲面的新方法。Catmull 的技术在发展真实感图形和消除 “锯齿”(原始计算机图形学的一个特征，图形周围的粗糙边缘)方面发挥了重要作用。</p><p>   离开犹他大学后， Catmull 成立了纽约理工学院(NYIT)计算机图形实验室，这是美国最早的专用计算机图形实验室之一。在那个时候，Catmull 就梦想着制作一部计算机动画电影。</p><p>   1979 年，《星球大战》的导演 George Lucas 聘请了 Catmull，使得他离自己的动画电影梦又近了一步。在 Lucas 电影公司(LucasFilm)，Catmull 及其同事继续开发 3D 计算机图形动画的创新技术，而当时这个行业仍然被传统的 2D 技术所主导。</p><p>   1986 年，史蒂夫乔布斯(Steve Jobs)收购了卢卡斯电影公司(LucasFilm)的计算机动画部门，并将其更名为皮克斯，Catmull 担任总裁。</p><p><strong>Patrick M. Hanrahan</strong><br><img src="/images/TURING/han.png" alt="Hanrahan"><br>  **Hanrahan 于 1985 年获得了威斯康星大学麦迪逊分校的生物物理学博士学位，加入皮克斯动画工作室之前，还曾在NYIT的计算机图形学实验室短暂工作过。</p><p>   在皮克斯动画工作室期间，Hanrahan 是一种新型图形系统的首席架构师，该技术可以使用真实的材料属性和光线来渲染曲线形状。此系统(后来称为 RenderMan)的一个关键思想是着色器(用于着色 CGI 图像)。RenderMan 的功能将光反射行为与几何形状分开，并计算形状上各点的颜色、透明度和纹理。RenderMan 系统还结合了 Catmull 早前在该领域做出的贡献的 Z 缓冲和细分曲面创新。</p><p>   在皮克斯工作期间， Hanrahan 还开发了体绘图(volume rendering)技术，该技术使 CGI 艺术家可以渲染 3D 数据集的 2D 投影，例如抽烟。</p><p>   在 Hanrahan 最常被引用的论文中，Hanrahan 与合著者 Marc Levoy 一起介绍了一种光场渲染技术，这种方法可通过从任意点生成新视图而没有深度信息或特征匹配，从而使观看者感觉它们正在穿越场景。Hanrahan 还开发了使用次表面散射来描绘皮肤和头发的技术，并使用蒙特卡洛射线追踪技术来渲染复杂的照明效果(所谓的全局照明或 GI)。</p><p>   1990 年，Hanrahan 在一篇开创性论文中发表了他的 RenderMan 研究。此时，距离计算机硬件发展到可以使用 Hanrahan 的 RenderMan 系统制作完整的 3D 计算机动画电影《玩具总动员》，还有五年的时间。</p><p><img src="/images/TURING/afan.png" alt="Avatar"> </p><p>   在 Catmull 的领导下，皮克斯使用 RenderMan 技术制作了一系列成功的电影。皮克斯还将 RenderMan 授权给其他电影公司，在过去 47 部获得奥斯卡最佳视觉效果提名的电影中，有 44 部使用了该软件，其中包括《阿凡达》、《泰坦尼克号》、《美女与野兽》、《指环王》三部曲以及《星球大战前传》等。</p><p>   1989 年，Hanrahan 在离开皮克斯之后进入普林斯顿大学和斯坦福大学担任学术职务。从 20 世纪 90 年代开始，他和他的研究团队扩展了 RenderMan 着色语言，使其可以在更强大的 GPU 上实时工作。Hanrahan 和他的学生开发的 GPUs 编程语言，也引起了商业版本(包括 OpenGL 阴影语言)的开发，并彻底改变了视频游戏的编写。</p><p>   GPUs 上广泛使用的各种着色语言，最终要求 GPUs 硬件设计人员开发更灵活的体系结构。这些架构又反过来使 GPUs 可以用于各种计算环境，包括为高性能计算应用程序运行算法，以及为人工智能应用程序在海量数据集上训练机器学习算法。特别一提的是，Hanrahan 和他的学生还开发了一种用于 GPU 的语言：Brook，并最终催生了NVIDIA的 CUDA。</p><p><img src="/images/TURING/cuda.png" alt="cuda"> </p><p>   Catmull 在皮克斯呆了 30 多年，皮克斯后来也成为迪斯尼动画工作室的子公司。在他的领导下，实验室的数十名研究人员发明并发布了对计算机动画电影和计算机图形产生重大贡献的基础技术，包括图像合成(image compositing)、运动模糊(motion blur)、布料模拟(cloth simulation)等。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;DEEP LEARNING&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="Computer Science" scheme="https://USTCcoder.github.io/categories/Computer-Science/"/>
    
      <category term="Turing Reward" scheme="https://USTCcoder.github.io/categories/Computer-Science/Turing-Reward/"/>
    
    
  </entry>
  
  <entry>
    <title>MobileNet</title>
    <link href="https://USTCcoder.github.io/2020/03/20/feature_extraction%20MobileNet/"/>
    <id>https://USTCcoder.github.io/2020/03/20/feature_extraction MobileNet/</id>
    <published>2020-03-20T10:57:32.000Z</published>
    <updated>2020-05-27T13:04:16.887Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">MobileNet</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>MobileNet:</strong>是<strong>2017年Google针对手机等嵌入式设备提出的一种轻量级的深层神经网络</strong>，故称之为MobileNet，由于其网络结构简单，现在仍然具有很好的适用性。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/MobileNet.png" alt="MobileNet"></p><h1 id="MobileNet特点"><a href="#MobileNet特点" class="headerlink" title="MobileNet特点"></a><font size="5" color="red">MobileNet特点</font></h1><p>  <font size="3"><strong>结构和VGG类似，只是将标准卷积换成SeparableConv深度可分离卷积</strong></font><br>  <font size="3">引入<strong>ReLU6</strong>代替ReLU，使得大于0的数值也具有非线性</font></p><h1 id="Separable-Convolution"><a href="#Separable-Convolution" class="headerlink" title="Separable Convolution"></a><font size="5" color="red">Separable Convolution</font></h1><p><img src="/images/Feature_extraction/Xception_D.png" alt="Xception"><br>  <font size="3"><strong>Separable Convolution(深度可分离卷积)</strong>：是上面两个卷积合二为一的卷积操作。</font><br>  <font size="3"><strong>第一步：DepthwiseConv，对每一个通道进行卷积</strong></font><br>  <font size="3"><strong>第二步：PointwiseConv，对第一步得到的结果进行1x1卷积，实现通道融合</strong></font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>，并且可以<strong>调整为任意合适的通道数</strong>。第一步的<strong>目的是减少参数量</strong>，第二步是<strong>调整通道数</strong>，因此将两个卷积操作结合，组成深度可分离卷积。</font></p><h1 id="MobileNet图像分析"><a href="#MobileNet图像分析" class="headerlink" title="MobileNet图像分析"></a><font size="5" color="red">MobileNet图像分析</font></h1><p><img src="/images/Feature_extraction/MobileNet_A.png" alt="MobileNet"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu6(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_Relu6, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential()</span><br><span class="line">        if name.find('depthwise') == -1:</span><br><span class="line">            self.block.add(keras.layers.Conv2D(filters, kernel_size, strides, padding=padding))</span><br><span class="line">        else:</span><br><span class="line">            self.block.add(keras.layers.DepthwiseConv2D(kernel_size, strides, padding=padding))</span><br><span class="line">        self.block.add(keras.layers.BatchNormalization())</span><br><span class="line">        self.block.add(keras.layers.ReLU(6))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.block(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def block(filters, strides, name):</span><br><span class="line"></span><br><span class="line">    return compose(Conv_Bn_Relu6(None, (3, 3), strides, 'same', name='{}_depthwiseconv_bn_relu6'.format(name)),</span><br><span class="line">                   Conv_Bn_Relu6(filters, (1, 1), (1, 1), 'same', name='{}_conv_bn_relu6'.format(name)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def mobilenet(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = Conv_Bn_Relu6(32, (3, 3), (2, 2), 'same', name='conv_bn_relu6')(x)</span><br><span class="line"></span><br><span class="line">    filters = [64, 128, 128, 256, 256, 512, 512, 512, 512, 512, 512, 1024, 1024]</span><br><span class="line">    strides = [(1, 1), (2, 2), (1, 1), (2, 2), (1, 1), (2, 2), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 2), (1, 1)]</span><br><span class="line">    for i in range(len(filters)):</span><br><span class="line">        x = block(filters[i], strides[i], name='block{}'.format(i + 1))(x)</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.GlobalAveragePooling2D(name='global_averagepool'),</span><br><span class="line">                keras.layers.Reshape((1, 1, 1024), name='reshape1'),</span><br><span class="line">                keras.layers.Dropout(0.2, name='dropout'),</span><br><span class="line">                keras.layers.Conv2D(1000, (1, 1), name='conv'),</span><br><span class="line">                keras.layers.Reshape((1000, ), name='reshape2'),</span><br><span class="line">                keras.layers.Softmax(name='softmax'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='MobileNet')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = mobilenet(input_shape=(224, 224, 3))</span><br><span class="line">    model.build(input_shape=(None, 224, 224, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/MobileNet_R.png" alt="MobileNet"></p><h1 id="MobileNet小结"><a href="#MobileNet小结" class="headerlink" title="MobileNet小结"></a><font size="5" color="red">MobileNet小结</font></h1><p>  MobileNet是一种简单的轻量级深度学习网络，从上图可以看出MobileNet模型的参数量只有4M，在移动设备中有良好表现，有时也可<strong>作为替代VGG的特征提取网络</strong>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;MobileNet&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>SqueezeNet</title>
    <link href="https://USTCcoder.github.io/2020/03/18/feature_extraction%20SqueezeNet/"/>
    <id>https://USTCcoder.github.io/2020/03/18/feature_extraction SqueezeNet/</id>
    <published>2020-03-18T05:09:52.000Z</published>
    <updated>2020-05-27T12:38:01.304Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">SqueezeNet</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>SqueezeNet:</strong>是一种轻量级深度神经网络模型，在<strong>2017年发表于ICLR</strong>，作者来自Berkeley和Stanford，其<strong>只用1/50的参数量，可以达到与AlexNet相同的精度</strong>，其核心结构为<strong>Fire Module</strong>。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/SqueezeNet.png" alt="SqueezeNet"></p><h1 id="SqueezeNet特点"><a href="#SqueezeNet特点" class="headerlink" title="SqueezeNet特点"></a><font size="5" color="red">SqueezeNet特点</font></h1><p>  <font size="3">引入<strong>Fire Module</strong>，根据<strong>降维</strong>思想，先通过1x1的卷积核对参数量进行压缩，然后采用了<strong>Inception</strong>的思想，进行多路融合。</font></p><h1 id="SqueezeNet图像分析"><a href="#SqueezeNet图像分析" class="headerlink" title="SqueezeNet图像分析"></a><font size="5" color="red">SqueezeNet图像分析</font></h1><p><img src="/images/Feature_extraction/SqueezeNet_A.png" alt="SqueezeNet"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fire_block(x, s1, e1, e3, name):</span><br><span class="line">    x = keras.layers.Conv2D(s1, (1, 1), activation='relu', name='{}_conv'.format(name))(x)</span><br><span class="line">    x1 = keras.layers.Conv2D(e1, (1, 1), activation='relu', name='{}_part1_conv'.format(name))(x)</span><br><span class="line">    x2 = keras.layers.Conv2D(e3, (3, 3), padding='same', activation='relu', name='{}_part2_conv'.format(name))(x)</span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x1, x2])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def squeezenet(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Conv2D(96, (7, 7), (2, 2), padding='same', activation='relu', name='conv1'),</span><br><span class="line">                keras.layers.MaxPool2D((3, 3), (2, 2), name='maxpool'))(x)</span><br><span class="line"></span><br><span class="line">    x = fire_block(x, 16, 64, 64, 'fire_block1_1')</span><br><span class="line">    x = fire_block(x, 16, 64, 64, 'fire_block1_2')</span><br><span class="line"></span><br><span class="line">    x = fire_block(x, 32, 128, 128, 'fire_block2_1')</span><br><span class="line">    x = keras.layers.MaxPool2D((3, 3), (2, 2), name='fire_block2_maxpool')(x)</span><br><span class="line">    x = fire_block(x, 32, 128, 128, 'fire_block2_2')</span><br><span class="line"></span><br><span class="line">    x = fire_block(x, 48, 192, 192, 'fire_block3_1')</span><br><span class="line">    x = fire_block(x, 48, 192, 192, 'fire_block3_2')</span><br><span class="line"></span><br><span class="line">    x = fire_block(x, 64, 256, 256, 'fire_block4_1')</span><br><span class="line">    x = keras.layers.MaxPool2D((3, 3), (2, 2), name='fire_block4_maxpool')(x)</span><br><span class="line">    x = fire_block(x, 64, 256, 256, 'fire_block4_2')</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.Dropout(0.5, name='dropout'),</span><br><span class="line">                keras.layers.Conv2D(1000, (1, 1), activation='relu', name='conv2'),</span><br><span class="line">                keras.layers.GlobalAveragePooling2D(name='global_averagepool'),</span><br><span class="line">                keras.layers.Softmax(name='softmax'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='SqueezeNet')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = squeezenet(input_shape=(224, 224, 3))</span><br><span class="line">    model.build(input_shape=(None, 224, 224, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/SqueezeNet_R.png" alt="SqueezeNet"></p><h1 id="SqueezeNet小结"><a href="#SqueezeNet小结" class="headerlink" title="SqueezeNet小结"></a><font size="5" color="red">SqueezeNet小结</font></h1><p>  SqueezeNet是一种简单的轻量级深度学习网络，从上图可以看出SqueezeNet模型的参数量只有1M，因此在某些特殊场合中能够发挥出很好的效果。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;SqueezeNet&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>SENet</title>
    <link href="https://USTCcoder.github.io/2020/03/17/feature_extraction%20SENet/"/>
    <id>https://USTCcoder.github.io/2020/03/17/feature_extraction SENet/</id>
    <published>2020-03-17T07:09:54.000Z</published>
    <updated>2020-05-27T15:02:21.748Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">SENet</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>SENet:</strong>是<strong>2017年发表在CVPR</strong>上的一个模型，也是<strong>最后一届ImageNet 2017竞赛分类任务的冠军</strong>，其创新点是<strong>引入了注意力Squeeze-and-Excitation (SE)模块</strong>。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/SENet.png" alt="SENet"></p><h1 id="SENet特点"><a href="#SENet特点" class="headerlink" title="SENet特点"></a><font size="5" color="red">SENet特点</font></h1><p>  <font size="3">引入了<strong>注意力Squeeze-and-Excitation(SE)模块</strong></font><br>  <font size="3">由于SE模块简单有效，因此可以<strong>很容易的和其他模型耦合</strong>，和ResNet耦合变成SE-ResNet，和Inception-V3耦合变成SE-Inception-V3等等</font></p><h1 id="Squeeze-and-Excitation"><a href="#Squeeze-and-Excitation" class="headerlink" title="Squeeze-and-Excitation"></a><font size="5" color="red">Squeeze-and-Excitation</font></h1><p><img src="/images/Feature_extraction/SENet_S.png" alt="SENet"><br>  <font size="3"><strong>Squeeze-and-Excitation</strong>：又称为<strong>特征重标定卷积</strong>，或者<strong>注意力机制</strong>。具体来说，就是通过<strong>学习的方式来自动获取到每个特征通道的重要程度</strong>，然后依照这个重要程度去<strong>提升有用的特征并抑制对当前任务用处不大的特征</strong>。</font><br>  <font size="3">首先是 <strong>Squeeze操作</strong>，先<strong>进行全局池化，具有全局的感受野</strong>，并且输出的维度和输入的特征通道数相匹配，它表征着在特征通道上响应的全局分布。</font><br>  <font size="3">然后是<strong>Excitation操作</strong>，<strong>通过全连接层为每个特征通道生成权重，建立通道间的相关性</strong>，<strong>输出的权重看做是进过特征选择后的每个特征通道的重要性</strong>，然后通过<strong>乘法逐通道加权到先前的特征上</strong>，完成在通道维度上的对原始特征的重标定。</font></p><h1 id="SE-ResNet50图像分析"><a href="#SE-ResNet50图像分析" class="headerlink" title="SE-ResNet50图像分析"></a><font size="5" color="red">SE-ResNet50图像分析</font></h1><p><img src="/images/Feature_extraction/SENet_A.png" alt="SENet"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_Relu, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.conv = keras.layers.Conv2D(filters, kernel_size, strides, padding)</span><br><span class="line">        self.bn = keras.layers.BatchNormalization()</span><br><span class="line">        self.relu = keras.layers.ReLU()</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        conv = self.conv(inputs)</span><br><span class="line">        bn = self.bn(conv)</span><br><span class="line">        output = self.relu(bn)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.conv = keras.layers.Conv2D(filters, kernel_size, strides, padding)</span><br><span class="line">        self.bn = keras.layers.BatchNormalization()</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        conv = self.conv(inputs)</span><br><span class="line">        output = self.bn(conv)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def res_block(x, filters, strides, type, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = compose(Conv_Bn_Relu(filters // 4, (1, 1), (1, 1), padding='same', name='{}{}_conv_bn_relu1'.format(type, name)),</span><br><span class="line">                Conv_Bn_Relu(filters // 4, (3, 3), strides, padding='same', name='{}{}_conv_bn_relu2'.format(type, name)),</span><br><span class="line">                Conv_Bn(filters, (1, 1), (1, 1), padding='same', name='{}{}_conv_bn3'.format(type, name)))(x)</span><br><span class="line">    x = se_block(x, filters, name='{}{}_se'.format(type, name))</span><br><span class="line">    if type == 'conv_block':</span><br><span class="line">        shortcut = keras.layers.Conv2D(filters, (1, 1), strides, name='{}{}_shortcut'.format(type, name))(shortcut)</span><br><span class="line">    x = keras.layers.Add(name='{}{}_add'.format(type, name))([x, shortcut])</span><br><span class="line">    x = keras.layers.ReLU(name='{}{}_relu3'.format(type, name))(x)</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def se_block(x, filters, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = compose(keras.layers.GlobalAveragePooling2D(name='{}_global_averagepool'.format(name)),</span><br><span class="line">                keras.layers.Dense(filters // 16, name='{}_dense1'.format(name)),</span><br><span class="line">                keras.layers.ReLU(name='{}_relu'.format(name)),</span><br><span class="line">                keras.layers.Dense(filters, name='{}_dense2'.format(name)),</span><br><span class="line">                keras.layers.Activation('sigmoid', name='{}_sigmoid'.format(name)),</span><br><span class="line">                keras.layers.Reshape((1, 1, filters), name='{}_reshape'.format(name)))(x)</span><br><span class="line">    x = keras.layers.Multiply(name='{}_multiply'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def se_resnet50(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line">    x = compose(keras.layers.ZeroPadding2D((3, 3), name='zeropadding'),</span><br><span class="line">                Conv_Bn_Relu(64, (7, 7), (2, 2), padding='valid', name='conv_bn_relu'),</span><br><span class="line">                keras.layers.MaxPool2D((3, 3), (2, 2), padding='same', name='maxpool'))(x)</span><br><span class="line">    filters = [256, 512, 1024, 2048]</span><br><span class="line">    strides = [(1, 1), (2, 2), (2, 2), (2, 2)]</span><br><span class="line">    times = [3, 4, 6, 3]</span><br><span class="line">    for i in range(len(times)):</span><br><span class="line">        x = res_block(x, filters[i], strides[i], 'conv_block', i + 1)</span><br><span class="line">        for j in range(times[i] - 1):</span><br><span class="line">            x = res_block(x, filters[i], (1, 1), 'identity_block{}_'.format(i + 1), j + 1)</span><br><span class="line">    x = compose(keras.layers.GlobalAveragePooling2D(name='global_averagepool'),</span><br><span class="line">                keras.layers.Dense(1000, activation='softmax', name='dense'))(x)</span><br><span class="line">    model = keras.Model(input_tensor, x, name='SE_ResNet50')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = se_resnet50(input_shape=(224, 224, 3))</span><br><span class="line">    model.build(input_shape=(None, 224, 224, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/SENet_R.png" alt="SENet"></p><h1 id="SENet小结"><a href="#SENet小结" class="headerlink" title="SENet小结"></a><font size="5" color="red">SENet小结</font></h1><p>  SENet是一种非常好的思路，其模型参数需要根据选择的耦合模型确定，如果耦合模型为SE-ResNet50，则参数量为28M，其<strong>注意力机制非常有效</strong>，为<strong>MobileNet-V3</strong>的发展，<strong>EfficientNet</strong>，<strong>GhostNet</strong>等网络起到了推动作用。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;SENet&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>DenseNet</title>
    <link href="https://USTCcoder.github.io/2020/03/16/feature_extraction%20DenseNet/"/>
    <id>https://USTCcoder.github.io/2020/03/16/feature_extraction DenseNet/</id>
    <published>2020-03-16T10:13:23.000Z</published>
    <updated>2020-05-27T05:10:41.754Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">DenseNet</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>DenseNet:</strong>作为<strong>CVPR2017年的Best Paper</strong>，DenseNet<strong>脱离了加深网络层数(ResNet)和加宽网络结构(Inception)来提升网络性能的定式思维</strong>，通过<strong>特征重用和旁路</strong>,既<strong>大幅度减少了网络的参数量</strong>,又在一定程度上<strong>缓解了梯度消失问题</strong>的产生。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/DenseNet.png" alt="DenseNet"></p><h1 id="DenseNet特点"><a href="#DenseNet特点" class="headerlink" title="DenseNet特点"></a><font size="5" color="red">DenseNet特点</font></h1><p>  <font size="3"><strong>同样深度的DenseNet所需的参数量相比ResNet大幅减少</strong></font><br>  <font size="3">Dense Block类似于ResNet中的Identity Block，Transition Block类似于ResNet中的Conv Block</font><br>  <font size="3">结构简单，<strong>综合了不同尺度的感受野</strong>，提升网络性能</font></p><h1 id="不同尺寸DenseNet网络结构"><a href="#不同尺寸DenseNet网络结构" class="headerlink" title="不同尺寸DenseNet网络结构"></a><font size="5" color="red">不同尺寸DenseNet网络结构</font></h1><p><img src="/images/Feature_extraction/DenseNet_C.png" alt="DenseNet"></p><h1 id="DenseNet121图像分析"><a href="#DenseNet121图像分析" class="headerlink" title="DenseNet121图像分析"></a><font size="5" color="red">DenseNet121图像分析</font></h1><p><img src="/images/Feature_extraction/DenseNet_A.png" alt="DenseNet"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_Relu, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.conv = keras.layers.Conv2D(filters, kernel_size, strides, padding)</span><br><span class="line">        self.bn = keras.layers.BatchNormalization()</span><br><span class="line">        self.relu = keras.layers.ReLU()</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        conv = self.conv(inputs)</span><br><span class="line">        bn = self.bn(conv)</span><br><span class="line">        output = self.relu(bn)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def dense_block(x, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = compose(Conv_Bn_Relu(128, (1, 1), (1, 1), 'same', name='{}_conv_bn_relu1'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(32, (3, 3), (1, 1), 'same', name='{}_conv_bn_relu2'.format(name)))(x)</span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def transition_block(x, filters, name):</span><br><span class="line">    x = compose(Conv_Bn_Relu(filters, (1, 1), (1, 1), 'same', name='{}_conv_bn_relu'.format(name)),</span><br><span class="line">                keras.layers.AveragePooling2D((2, 2), (2, 2), name='{}_averagepool'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def densenet121(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.ZeroPadding2D((3, 3), name='zeropadding1'),</span><br><span class="line">                Conv_Bn_Relu(64, (7, 7), (2, 2), 'valid', name='conv_bn_relu'),</span><br><span class="line">                keras.layers.ZeroPadding2D((1, 1), name='zeropadding2'),</span><br><span class="line">                keras.layers.MaxPool2D((3, 3), (2, 2), name='Max_Pooling'))(x)</span><br><span class="line"></span><br><span class="line">    for i in range(6):</span><br><span class="line">        x = dense_block(x, name='dense_block1_{}'.format(i + 1))</span><br><span class="line">    x = transition_block(x, 128, name='transition_block1')</span><br><span class="line"></span><br><span class="line">    for i in range(12):</span><br><span class="line">        x = dense_block(x, name='dense_block2_{}'.format(i + 1))</span><br><span class="line">    x = transition_block(x, 256, name='transition_block2')</span><br><span class="line"></span><br><span class="line">    for i in range(24):</span><br><span class="line">        x = dense_block(x, name='dense_block3_{}'.format(i + 1))</span><br><span class="line">    x = transition_block(x, 512, name='transition_block3')</span><br><span class="line"></span><br><span class="line">    for i in range(16):</span><br><span class="line">        x = dense_block(x, name='dense_block4_{}'.format(i + 1))</span><br><span class="line"></span><br><span class="line">    x = compose(keras.layers.GlobalAveragePooling2D(name='global_averagepool'),</span><br><span class="line">                keras.layers.Dense(1000, activation='softmax', name='dense'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='DenseNet121')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = densenet121(input_shape=(224, 224, 3))</span><br><span class="line">    model.build(input_shape=(None, 224, 224, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/DenseNet_R.png" alt="DenseNet"></p><h1 id="DenseNet小结"><a href="#DenseNet小结" class="headerlink" title="DenseNet小结"></a><font size="5" color="red">DenseNet小结</font></h1><p>  DenseNet是一种简单的深度学习网络，也是一种非常有效的特征提取模型。从上图可以看出<strong>DenseNet121模型的参数量只有8M，甚至是ResNet50的参数量的三分之一</strong>，因此实际任务中可以使用DenseNet作为特征提取网络，<strong>既高效又节约内存和计算量</strong>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;DenseNet&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>实现 strStr()(Leetcode 28)</title>
    <link href="https://USTCcoder.github.io/2020/03/15/program%20Leetcode28/"/>
    <id>https://USTCcoder.github.io/2020/03/15/program Leetcode28/</id>
    <published>2020-03-15T04:35:46.000Z</published>
    <updated>2020-09-02T02:17:44.310Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode28.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   这道题目简单来说就是一个字符串匹配问题，看起来很简单，但是想要降低时间复杂度较为困难。可以通过暴力法或者KMP方法求解。</p><a id="more"></a><h1 id="暴力法"><a href="#暴力法" class="headerlink" title="暴力法"></a><font size="5" color="red">暴力法</font></h1><p>暴力法很好理解，长度已经给定，穷举所有可能的子串，并且与模板字符串比较，如果相等则输出出现的位置，否则输出-1即可，时间复杂度为O((N-L)L)，空间复杂度为O(1)，其中N为原始字符串长度，L为模板字符串长度。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def strStr(self, haystack, needle):</span><br><span class="line">        """</span><br><span class="line">        :haystack: str</span><br><span class="line">        :needle: str</span><br><span class="line">        :rtype: int</span><br><span class="line">        """</span><br><span class="line">        lens = len(needle)</span><br><span class="line">        for i in range(len(haystack) - lens + 1):</span><br><span class="line">            if haystack[i:i + lens] == needle:</span><br><span class="line">                return i</span><br><span class="line">        return -1</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="KMP"><a href="#KMP" class="headerlink" title="KMP"></a><font size="5" color="red">KMP</font></h1><p>KMP算法是一种专门解决字符串匹配的算法，因为暴力法浪费了太多的比较次数，尤其是前面的字符都匹配正确后，最后一个字符匹配失败，又要从头开始匹配，效率太低，而KMP算法可以完成O(N)的时间复杂度和O(L)的空间复杂度。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def strStr(self, haystack, needle):</span><br><span class="line">        """</span><br><span class="line">        :haystack: str</span><br><span class="line">        :needle: str</span><br><span class="line">        :rtype: int</span><br><span class="line">        """</span><br><span class="line"></span><br><span class="line">        def getNext(pattern):</span><br><span class="line">            next = [-1] * len(pattern)</span><br><span class="line">            i, j = 0, -1</span><br><span class="line">            while i &lt; len(pattern) - 1:</span><br><span class="line">                if j == -1 or pattern[i] == pattern[j]:</span><br><span class="line">                    i += 1</span><br><span class="line">                    j += 1</span><br><span class="line">                    next[i] = j</span><br><span class="line">                else:</span><br><span class="line">                    j = next[j]</span><br><span class="line">            return next</span><br><span class="line"></span><br><span class="line">        next = getNext(needle)</span><br><span class="line">        i, j = 0, 0</span><br><span class="line">        while i &lt; len(haystack) and j &lt; len(needle):</span><br><span class="line">            if j == -1 or haystack[i] == needle[j]:</span><br><span class="line">                i += 1</span><br><span class="line">                j += 1</span><br><span class="line">            else:</span><br><span class="line">                j = next[j]</span><br><span class="line">        if j &gt;= len(needle):</span><br><span class="line">            return i - len(needle)</span><br><span class="line">        else:</span><br><span class="line">            return -1</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  字符串匹配问题是一个经典的问题，大多数学校或者数据结构算法课中都作为例子进行讲解，但是很难讲的非常透彻，因为这个问题太抽象，在这里我只能用文字进行描述，所以描述的可能更不清晰，小伙伴们以要理解我举得例子，尤其是j = Next[j]这一步，可以说是KMP问题的精髓，看懂了这一步基本上就可以明白算法具体在做什么了。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 28&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>Xception</title>
    <link href="https://USTCcoder.github.io/2020/03/14/feature_extraction%20Xception/"/>
    <id>https://USTCcoder.github.io/2020/03/14/feature_extraction Xception/</id>
    <published>2020-03-14T08:08:58.000Z</published>
    <updated>2020-05-27T04:43:51.157Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Xception</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Xception:</strong>是<strong>谷歌公司对Inception-V3的改进</strong>，被<strong>CVPR2017年收录</strong>，是一个很好的图像特征提取模型。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/Xception.png" alt="Xception"></p><h1 id="Xception特点"><a href="#Xception特点" class="headerlink" title="Xception特点"></a><font size="5" color="red">Xception特点</font></h1><p>  <font size="3">除了Inception的特点以外，采用了<strong>SeparableConv(深度可分离卷积)代替Inception中的(Conv)卷积操作</strong>，大大节约了参数量</font></p><h1 id="Separable-Convolution"><a href="#Separable-Convolution" class="headerlink" title="Separable Convolution"></a><font size="5" color="red">Separable Convolution</font></h1><p><img src="/images/Feature_extraction/Xception_D.png" alt="Xception"><br>  <font size="3"><strong>Separable Convolution(深度可分离卷积)</strong>：是上面两个卷积合二为一的卷积操作。</font><br>  <font size="3"><strong>第一步：DepthwiseConv，对每一个通道进行卷积</strong></font><br>  <font size="3"><strong>第二步：PointwiseConv，对第一步得到的结果进行1x1卷积，实现通道融合</strong></font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>，并且可以<strong>调整为任意合适的通道数</strong>，在<strong>Xception，MobileNet，EfficientNet，ShuffleNet</strong>网络中有大量使用。第一步的<strong>目的是减少参数量</strong>，第二步是<strong>调整通道数</strong>，因此将两个卷积操作结合，组成深度可分离卷积。</font></p><h1 id="Xception图像分析"><a href="#Xception图像分析" class="headerlink" title="Xception图像分析"></a><font size="5" color="red">Xception图像分析</font></h1><p><img src="/images/Feature_extraction/Xception_A.png" alt="Xception"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_Relu, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.block = keras.Sequential()</span><br><span class="line">        if name.find('separable') == -1:</span><br><span class="line">            self.block.add(keras.layers.Conv2D(filters, kernel_size, strides, padding=padding))</span><br><span class="line">        else:</span><br><span class="line">            self.block.add(keras.layers.SeparableConv2D(filters, kernel_size, strides, padding=padding))</span><br><span class="line">        self.block.add(keras.layers.BatchNormalization())</span><br><span class="line">        if name.find('relu') != -1:</span><br><span class="line">            self.block.add(keras.layers.ReLU())</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line"></span><br><span class="line">        return self.block(inputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def entry_exit_flow(x, filters, relu, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    if relu:</span><br><span class="line">        x = keras.layers.ReLU(name='{}_relu'.format(name))(x)</span><br><span class="line">    x = compose(Conv_Bn_Relu(filters, (3, 3), (1, 1), 'same', name='{}_separableconv_bn_relu1'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(filters, (3, 3), (1, 1), 'same', name='{}_separableconv_bn2'.format(name)),</span><br><span class="line">                keras.layers.MaxPool2D((3, 3), (2, 2), 'same', name='{}_maxpool'.format(name)))(x)</span><br><span class="line">    shortcut = Conv_Bn_Relu(filters, (1, 1), (2, 2), 'same', name='{}_shortcut_conv_bn'.format(name))(shortcut)</span><br><span class="line">    x = keras.layers.Add(name='{}_add'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def middle_flow(x, filters, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = keras.layers.ReLU(name='{}_relu'.format(name))(x)</span><br><span class="line">    x = compose(Conv_Bn_Relu(filters, (3, 3), (1, 1), 'same', name='{}_separableconv_bn_relu1'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(filters, (3, 3), (1, 1), 'same', name='{}_separableconv_bn_relu2'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(filters, (3, 3), (1, 1), 'same', name='{}_separableconv_bn3'.format(name)),)(x)</span><br><span class="line">    x = keras.layers.Add(name='{}_add'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def xception(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_Relu(32, (3, 3), (2, 2), 'valid', name='conv_bn_relu1'),</span><br><span class="line">                Conv_Bn_Relu(64, (3, 3), (1, 1), 'valid', name='conv_bn_relu2'))(x)</span><br><span class="line"></span><br><span class="line">    entry_filters = [128, 256, 728]</span><br><span class="line">    entry_relu = [False, True, True]</span><br><span class="line">    for i in range(len(entry_filters)):</span><br><span class="line">        x = entry_exit_flow(x, entry_filters[i], entry_relu[i], name='entry_flow{}'.format(i + 1))</span><br><span class="line"></span><br><span class="line">    for i in range(8):</span><br><span class="line">        x = middle_flow(x, 728, name='middle_flow{}'.format(i + 1))</span><br><span class="line"></span><br><span class="line">    x = entry_exit_flow(x, 1024, relu=True, name='exit_flow1')</span><br><span class="line"></span><br><span class="line">    x = compose(Conv_Bn_Relu(1536, (3, 3), (1, 1), 'same', name='separableconv_bn_relu1'),</span><br><span class="line">                Conv_Bn_Relu(2048, (3, 3), (1, 1), 'same', name='separableconv_bn_relu2'),</span><br><span class="line">                keras.layers.GlobalAveragePooling2D(name='global_averagepool'),</span><br><span class="line">                keras.layers.Dense(1000, activation='softmax', name='dense'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='Xception')</span><br><span class="line"></span><br><span class="line">    return model</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/Xception_R.png" alt="Xception"></p><h1 id="Xception小结"><a href="#Xception小结" class="headerlink" title="Xception小结"></a><font size="5" color="red">Xception小结</font></h1><p>  Xception是一种复杂的深度学习网络，从上图可以看出Xception模型的参数量可达23M，因为其优秀的特征提取能力，并且网络结构相比Inception-V3较为简单。因此实际任务经常使用，如<strong>语义分割网络DeepLab-V3+使用的特征提取网络就是Xception</strong>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Xception&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>ResNeXt</title>
    <link href="https://USTCcoder.github.io/2020/03/13/feature_extraction%20ResNeXt/"/>
    <id>https://USTCcoder.github.io/2020/03/13/feature_extraction ResNeXt/</id>
    <published>2020-03-13T07:22:15.000Z</published>
    <updated>2020-05-27T01:06:06.352Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">ResNeXt</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>ResNeXt:</strong>是<strong>2017年发表于CVPR</strong>的一个模型，是<strong>ResNet网络的升级版本</strong>。和Inception-ResNet类似，Inception-ResNet可以认为是Inception模型的基础上吸收ResNet残差思想，而ResNext则可以认为是ResNet模型的基础上吸收Inception分块合并思想。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/ResNeXt.png" alt="ResNeXt"></p><h1 id="ResNeXt特点"><a href="#ResNeXt特点" class="headerlink" title="ResNeXt特点"></a><font size="5" color="red">ResNeXt特点</font></h1><p>  <font size="3"><strong>网络结构和ResNet相同</strong>，根据ResNet50，则可以修改为ResNeXt50，根据ResNet101，则可以修改为ResNeXt101，等等</font><br>  <font size="3"><strong>引入Inception模型分块合并思想</strong>，将ResNet中Conv Block和Identity Block中的<strong>普通卷积</strong>变成<strong>GroupConv分组卷积</strong>。提出了<strong>cardinality(基数)</strong>名词，基数为32，相当于分组卷积的组数为32，最后将32组卷积结果合并</font></p><h1 id="Group-Convolution"><a href="#Group-Convolution" class="headerlink" title="Group Convolution"></a><font size="5" color="red">Group Convolution</font></h1><p><img src="/images/Feature_extraction/ShuffleNet_V2_G.png" alt="ShuffleNet_V2"><br>  <font size="3"><strong>Group Convolution(分组卷积)</strong>：<strong>传统卷积是采用一种卷积全连接的思想</strong>，特征图中的每一个像素点都结合了图像中所有通道的信息。而分组卷积特征图像<strong>每一个像素点只利用到一部分原始图像的通道</strong>。</font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>。如果一个64x64x256的图像，经过5x5的卷积核后变为64x64x256的图像，经过普通卷积的参数量为256x(256x5x5+1)=1638656，而分成32组的分组卷积的参数量为256x(8*5x5+1)=51456，参数量缩小了约32倍，当组数变成通道数时，则类似于Depthwise Convolution深度卷积</font></p><h1 id="ResNeXt50图像分析"><a href="#ResNeXt50图像分析" class="headerlink" title="ResNeXt50图像分析"></a><font size="5" color="red">ResNeXt50图像分析</font></h1><p><img src="/images/Feature_extraction/ResNeXt_A.png" alt="ResNeXt"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class GroupConv(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, g_num, name='groupconv'):</span><br><span class="line">        super(GroupConv, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.g_num = g_num</span><br><span class="line">        self.groupconv = [keras.layers.Conv2D(filters // g_num, kernel_size, strides, padding='same', name='{}{}'.format(name, i + 1)) for i in range(g_num)]</span><br><span class="line">        self.concatenate = keras.layers.Concatenate(name='{}_concatenate'.format(name))</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        x_split = tf.split(inputs, self.g_num, axis=-1)</span><br><span class="line">        x = [self.groupconv[i](x_split[i]) for i in range(self.g_num)]</span><br><span class="line">        output = self.concatenate(x)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_Relu, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        if name.find('group') == -1:</span><br><span class="line">            self.conv = keras.layers.Conv2D(filters, kernel_size, strides, padding)</span><br><span class="line">        else:</span><br><span class="line">            self.conv = GroupConv(filters, kernel_size, strides, 32)</span><br><span class="line">        self.bn = keras.layers.BatchNormalization()</span><br><span class="line">        self.relu = keras.layers.ReLU()</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        conv = self.conv(inputs)</span><br><span class="line">        bn = self.bn(conv)</span><br><span class="line">        output = self.relu(bn)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.conv = keras.layers.Conv2D(filters, kernel_size, strides, padding)</span><br><span class="line">        self.bn = keras.layers.BatchNormalization()</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        conv = self.conv(inputs)</span><br><span class="line">        output = self.bn(conv)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def res_block(x, filters, strides, type, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = compose(Conv_Bn_Relu(filters // 2, (1, 1), (1, 1), padding='same', name='{}{}_conv_bn_relu1'.format(type, name)),</span><br><span class="line">                Conv_Bn_Relu(filters // 2, (3, 3), strides, padding='same', name='{}{}_groupconv_bn_relu2'.format(type, name)),</span><br><span class="line">                Conv_Bn(filters, (1, 1), (1, 1), padding='same', name='{}{}_conv_bn3'.format(type, name)))(x)</span><br><span class="line">    if type == 'conv_block':</span><br><span class="line">        shortcut = keras.layers.Conv2D(filters, (1, 1), strides, name='{}{}_shortcut'.format(type, name))(shortcut)</span><br><span class="line">    x = keras.layers.Add(name='{}{}_add'.format(type, name))([x, shortcut])</span><br><span class="line">    x = keras.layers.ReLU(name='{}{}_relu3'.format(type, name))(x)</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def resnext50(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line">    x = compose(keras.layers.ZeroPadding2D((3, 3), name='zeropadding'),</span><br><span class="line">                keras.layers.Conv2D(64, (7, 7), (2, 2), name='conv1'),</span><br><span class="line">                keras.layers.BatchNormalization(name='bn'),</span><br><span class="line">                keras.layers.ReLU(name='relu'),</span><br><span class="line">                keras.layers.MaxPool2D((3, 3), (2, 2), padding='same', name='maxpool'))(x)</span><br><span class="line">    filters = [256, 512, 1024, 2048]</span><br><span class="line">    strides = [(1, 1), (2, 2), (2, 2), (2, 2)]</span><br><span class="line">    times = [3, 4, 6, 3]</span><br><span class="line">    for i in range(len(times)):</span><br><span class="line">        x = res_block(x, filters[i], strides[i], 'conv_block', i + 1)</span><br><span class="line">        for j in range(times[i] - 1):</span><br><span class="line">            x = res_block(x, filters[i], (1, 1), 'identity_block{}_'.format(i + 1), j + 1)</span><br><span class="line">    x = compose(keras.layers.GlobalAveragePooling2D(name='global_averagepool'),</span><br><span class="line">                keras.layers.Dense(1000, activation='softmax', name='dense'))(x)</span><br><span class="line">    model = keras.Model(input_tensor, x, name='ResNeXt50')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = resnext50(input_shape=(224, 224, 3))</span><br><span class="line">    model.build(input_shape=(None, 224, 224, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/ResNeXt_R.png" alt="ResNeXt"></p><h1 id="ResNeXt小结"><a href="#ResNeXt小结" class="headerlink" title="ResNeXt小结"></a><font size="5" color="red">ResNeXt小结</font></h1><p>  ResNeXt是一种非常有效的特征提取网络，ResNeXt参数量为25M，和相同结构的ResNet几乎相同，但是效果优于ResNet，因此是一种实用的特征提取网络。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;ResNeXt&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>Inception-ResNet-V2</title>
    <link href="https://USTCcoder.github.io/2020/03/12/feature_extraction%20Inception-ResNet_V2/"/>
    <id>https://USTCcoder.github.io/2020/03/12/feature_extraction Inception-ResNet_V2/</id>
    <published>2020-03-12T05:39:25.000Z</published>
    <updated>2020-05-27T04:44:28.707Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Inception-ResNet-V2</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Inception-ResNet-V2:</strong>和Inception-V4，Inception-ResNet-V1于<strong>2017年发表在AAAI</strong>同一篇文章中，三者的网络结构基本相同。结合了<strong>Inception-V3</strong>和<strong>ResNet</strong>的优点而成的深度学习网络。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/Inception-ResNet_V2.png" alt="Inception-ResNet_V2"></p><h1 id="Inception-ResNet-V2特点"><a href="#Inception-ResNet-V2特点" class="headerlink" title="Inception-ResNet-V2特点"></a><font size="5" color="red">Inception-ResNet-V2特点</font></h1><p>  <font size="3"><strong>在Inception-V3的基础上，增加了残差结构</strong></font><br>  <font size="3">对网络的输入<strong>增加了Stem层</strong>，不再是Inception-V3中简单的卷积操作</font></p><h1 id="Spatial-Separable-Convolution"><a href="#Spatial-Separable-Convolution" class="headerlink" title="Spatial Separable Convolution"></a><font size="5" color="red">Spatial Separable Convolution</font></h1><p><img src="/images/deep_learning/spatial.png" alt="spatial"><br>  <font size="3"><strong>Spatial Separable Convolution(空间可分离卷积)</strong>：将3x3的卷积分解为3x1的卷积核1x3的卷积，将7x7的卷积分解为7x1的卷积核1x7的卷积.。</font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>。如果一个64x64x256的特征图，经过7x7的卷积核后变为64x64x256的图像，经过普通卷积的参数量为256x(256x7x7+1)=3211520，而空间可分离卷积参数量为2x256x(256x7x1+1)=918016，参数量缩小了约3.5倍。</font></p><h1 id="Inception-ResNet-V2图像分析"><a href="#Inception-ResNet-V2图像分析" class="headerlink" title="Inception-ResNet-V2图像分析"></a><font size="5" color="red">Inception-ResNet-V2图像分析</font></h1><p><font size="4">Inception-ResNet-V2网络结构较大，建议小伙伴们保存到本地放大观看。</font><br><img src="/images/Feature_extraction/Inception-ResNet_V2_A.png" alt="Inception-ResNet_V2"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_Relu, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.conv = keras.layers.Conv2D(filters, kernel_size, strides, padding)</span><br><span class="line">        self.bn = keras.layers.BatchNormalization()</span><br><span class="line">        self.relu = keras.layers.ReLU()</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        conv = self.conv(inputs)</span><br><span class="line">        bn = self.bn(conv)</span><br><span class="line">        output = self.relu(bn)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def reduction_A(x, name):</span><br><span class="line">    x_1 = compose(keras.layers.MaxPooling2D((3, 3), (2, 2), name='{}_part1_maxpool'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_2 = compose(Conv_Bn_Relu(384, (3, 3), (2, 2), padding='valid', name='{}_part2_conv_bn_relu1'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_3 = compose(Conv_Bn_Relu(256, (1, 1), (1, 1), padding='same', name='{}_part3_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(256, (3, 3), (1, 1), padding='same', name='{}_part3_conv_bn_relu2'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(384, (3, 3), (2, 2), padding='valid', name='{}_part3_conv_bn_relu3'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x_1, x_2, x_3])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def reduction_B(x, name):</span><br><span class="line">    x_1 = compose(keras.layers.MaxPooling2D((3, 3), (2, 2), name='{}_part1_maxpool'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_2 = compose(Conv_Bn_Relu(256, (1, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(384, (3, 3), (2, 2), padding='valid', name='{}_part2_conv_bn_relu2'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_3 = compose(Conv_Bn_Relu(256, (1, 1), (1, 1), padding='same', name='{}_part3_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(288, (3, 3), (2, 2), padding='valid', name='{}_part3_conv_bn_relu2'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_4 = compose(Conv_Bn_Relu(256, (1, 1), (1, 1), padding='same', name='{}_part4_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(288, (3, 3), (1, 1), padding='same', name='{}_part4_conv_bn_relu2'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(320, (3, 3), (2, 2), padding='valid', name='{}_part4_conv_bn_relu3'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x_1, x_2, x_3, x_4])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def stem(x, name):</span><br><span class="line">    x = compose(Conv_Bn_Relu(32, (3, 3), (2, 2), padding='valid', name='{}_conv_bn_relu1'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(32, (3, 3), (1, 1), padding='valid', name='{}_conv_bn_relu2'.format(name)),</span><br><span class="line">                Conv_Bn_Relu(64, (3, 3), (1, 1), padding='same', name='{}_conv_bn_relu3'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_1 = keras.layers.MaxPool2D((3, 3), (2, 2), name='{}_4_part1_maxpool'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    x_2 = Conv_Bn_Relu(96, (3, 3), (2, 2), padding='valid', name='{}_4_part2_conv_bn_relu1'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate1'.format(name))([x_1, x_2])</span><br><span class="line"></span><br><span class="line">    x_1 = compose(Conv_Bn_Relu(64, (1, 1), (1, 1), padding='same', name='{}_5_part1_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(96, (3, 3), (1, 1), padding='valid', name='{}_5_part1_conv_bn_relu2'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_2 = compose(Conv_Bn_Relu(64, (1, 1), (1, 1), padding='same', name='{}_5_part2_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(64, (1, 7), (1, 1), padding='same', name='{}_5_part2_conv_bn_relu2'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(64, (7, 1), (1, 1), padding='same', name='{}_5_part2_conv_bn_relu3'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(96, (3, 3), (1, 1), padding='valid', name='{}_5_part2_conv_bn_relu4'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate2'.format(name))([x_1, x_2])</span><br><span class="line"></span><br><span class="line">    x_1 = keras.layers.MaxPool2D((3, 3), (2, 2), name='{}_6_part1_maxpool'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    x_2 = Conv_Bn_Relu(192, (3, 3), (2, 2), padding='valid', name='{}_6_part2_conv_bn_relu1'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate3'.format(name))([x_1, x_2])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def inception_resnet_A(x, name):</span><br><span class="line">    shortcut = x</span><br><span class="line"></span><br><span class="line">    x_1 = compose(Conv_Bn_Relu(32, (1, 1), (1, 1), padding='same', name='{}_part1_conv_bn_relu1'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_2 = compose(Conv_Bn_Relu(32, (1, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(32, (3, 3), (1, 1), padding='same', name='{}_part2_conv_bn_relu2'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_3 = compose(Conv_Bn_Relu(32, (1, 1), (1, 1), padding='same', name='{}_part3_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(48, (3, 3), (1, 1), padding='same', name='{}_part3_conv_bn_relu2'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(64, (3, 3), (1, 1), padding='same', name='{}_part3_conv_bn_relu3'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x_1, x_2, x_3])</span><br><span class="line"></span><br><span class="line">    x = Conv_Bn_Relu(384, (1, 1), (1, 1), padding='same', name='{}_conv_bn_relu1'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Add(name='{}_add'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    x = keras.layers.ReLU(name='{}_relu'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def inception_resnet_B(x, name):</span><br><span class="line">    shortcut = x</span><br><span class="line"></span><br><span class="line">    x_1 = compose(Conv_Bn_Relu(192, (1, 1), (1, 1), padding='same', name='{}_part1_conv_bn_relu1'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_2 = compose(Conv_Bn_Relu(128, (1, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(160, (1, 7), (1, 1), padding='same', name='{}_part2_conv_bn_relu2'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(192, (7, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu3'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x_1, x_2])</span><br><span class="line"></span><br><span class="line">    x = Conv_Bn_Relu(1152, (1, 1), (1, 1), padding='same', name='{}_conv_bn_relu1'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Add(name='{}_add'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    x = keras.layers.ReLU(name='{}_relu'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def inception_resnet_C(x, name):</span><br><span class="line">    shortcut = x</span><br><span class="line"></span><br><span class="line">    x_1 = compose(Conv_Bn_Relu(192, (1, 1), (1, 1), padding='same', name='{}_part1_conv_bn_relu1'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_2 = compose(Conv_Bn_Relu(192, (1, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(224, (1, 7), (1, 1), padding='same', name='{}_part2_conv_bn_relu2'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(256, (7, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu3'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x_1, x_2])</span><br><span class="line"></span><br><span class="line">    x = Conv_Bn_Relu(2144, (1, 1), (1, 1), padding='same', name='{}_conv_bn_relu1'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Add(name='{}_add'.format(name))([x, shortcut])</span><br><span class="line"></span><br><span class="line">    x = keras.layers.ReLU(name='{}_relu'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def inception_resnet_v2(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line">    x = stem(x, 'stem')</span><br><span class="line">    for i in range(10):</span><br><span class="line">        x = inception_resnet_A(x, 'inception_resnet_A_{}'.format(i + 1))</span><br><span class="line">    x = reduction_A(x, 'reduction_A')</span><br><span class="line">    for i in range(20):</span><br><span class="line">        x = inception_resnet_B(x, 'inception_resnet_B_{}'.format(i + 1))</span><br><span class="line">    x = reduction_B(x, 'reduction_B')</span><br><span class="line">    for i in range(10):</span><br><span class="line">        x = inception_resnet_C(x, 'inception_resnet_C_{}'.format(i + 1))</span><br><span class="line">    x = compose(keras.layers.AveragePooling2D((8, 8), (8, 8), name='averagepool'),</span><br><span class="line">                keras.layers.Dropout(0.2, name='dropout'),</span><br><span class="line">                keras.layers.Flatten(name='flatten'),</span><br><span class="line">                keras.layers.Dense(1000, activation='softmax', name='dense'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='Inception_ResNet_V2')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = inception_resnet_v2(input_shape=(299, 299, 3))</span><br><span class="line">    model.build(input_shape=(None, 299, 299, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/Inception-ResNet_V2_R.png" alt="Inception-ResNet_V2"></p><h1 id="Inception-ResNet-V2小结"><a href="#Inception-ResNet-V2小结" class="headerlink" title="Inception-ResNet-V2小结"></a><font size="5" color="red">Inception-ResNet-V2小结</font></h1><p>  Inception-ResNet-V2是一种集<strong>Inception-V3</strong>和<strong>ResNet</strong>所长的深度学习网络，从上图可以看出Inception-ResNet-V2模型的参数量达到60M，但是<strong>由于其网络结构太复杂，比Inception-V3要复杂得多，因此在实际中也较少使用其作为特征提取网络</strong>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Inception-ResNet-V2&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>Inception-V3</title>
    <link href="https://USTCcoder.github.io/2020/03/10/feature_extraction%20Inception_V3/"/>
    <id>https://USTCcoder.github.io/2020/03/10/feature_extraction Inception_V3/</id>
    <published>2020-03-10T04:41:17.000Z</published>
    <updated>2020-05-27T01:21:07.848Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Inception-V3</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Inception-V3:</strong>由<strong>谷歌公司2015年</strong>提出，初始版本是GoogleNet，是<strong>2014年ILSVRC竞赛的第一名</strong>，是一个较为复杂的图像特征提取模型。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/Inception_V3.png" alt="Inception_V3"></p><h1 id="Inception-V3特点"><a href="#Inception-V3特点" class="headerlink" title="Inception-V3特点"></a><font size="5" color="red">Inception-V3特点</font></h1><p>  <font size="3">采用<strong>不同大小的卷积核</strong>，意味着<strong>不同大小的感受野</strong>，得到<strong>不同尺度的特征</strong>，最后将不同尺度的特征进行<strong>拼接融合</strong></font><br>  <font size="3">提出<strong>卷积分解</strong>思想，将<strong>一个5x5的卷积，分解为两个3x3的卷积</strong>，而且将<strong>3x3的卷积分解成一个1x3的卷积和一个3*1的卷积</strong></font></p><h1 id="Spatial-Separable-Convolution"><a href="#Spatial-Separable-Convolution" class="headerlink" title="Spatial Separable Convolution"></a><font size="5" color="red">Spatial Separable Convolution</font></h1><p><img src="/images/deep_learning/spatial.png" alt="spatial"><br>  <font size="3"><strong>Spatial Separable Convolution(空间可分离卷积)</strong>：将3x3的卷积分解为3x1的卷积核1x3的卷积，将7x7的卷积分解为7x1的卷积核1x7的卷积.。</font><br>  <font size="3">主要作用是<strong>大大降低网络的参数量</strong>。如果一个64x64x256的特征图，经过7x7的卷积核后变为64x64x256的图像，经过普通卷积的参数量为256x(256x7x7+1)=3211520，而空间可分离卷积参数量为2x256x(256x7x1+1)=918016，参数量缩小了约3.5倍。</font></p><h1 id="Inception-V3图像分析"><a href="#Inception-V3图像分析" class="headerlink" title="Inception-V3图像分析"></a><font size="5" color="red">Inception-V3图像分析</font></h1><p><font size="4">Inception-V3网络结构较大，建议小伙伴们保存到本地放大观看。</font><br><img src="/images/Feature_extraction/Inception_V3_A.png" alt="Inception_V3"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_Relu, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.conv = keras.layers.Conv2D(filters, kernel_size, strides, padding)</span><br><span class="line">        self.bn = keras.layers.BatchNormalization()</span><br><span class="line">        self.relu = keras.layers.ReLU()</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        conv = self.conv(inputs)</span><br><span class="line">        bn = self.bn(conv)</span><br><span class="line">        output = self.relu(bn)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def reduction_A(x, name):</span><br><span class="line">    x_1 = compose(keras.layers.MaxPooling2D((3, 3), (2, 2), name='{}_part1_maxpool'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_2 = compose(Conv_Bn_Relu(64, (1, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(96, (3, 3), (1, 1), padding='same', name='{}_part2_conv_bn_relu2'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(96, (3, 3), (2, 2), padding='valid', name='{}_part2_conv_bn_relu3'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_3 = compose(Conv_Bn_Relu(384, (3, 3), (2, 2), padding='valid', name='{}_part3_conv_bn_relu1'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x_1, x_2, x_3])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def reduction_B(x, name):</span><br><span class="line">    x_1 = compose(keras.layers.MaxPooling2D((3, 3), (2, 2), name='{}_part1_maxpool'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_2 = compose(Conv_Bn_Relu(192, (1, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(192, (1, 7), (1, 1), padding='same', name='{}_part2_conv_bn_relu2'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(192, (7, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu3'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(192, (3, 3), (2, 2), padding='valid', name='{}_part2_conv_bn_relu4'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_3 = compose(Conv_Bn_Relu(192, (1, 1), (1, 1), padding='same', name='{}_part3_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(320, (3, 3), (2, 2), padding='valid', name='{}_part3_conv_bn_relu2'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x_1, x_2, x_3])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def inception_A(x, filters, name):</span><br><span class="line">    x_1 = compose(keras.layers.AveragePooling2D((3, 3), (1, 1), padding='same', name='{}_averagepool'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(filters, (1, 1), (1, 1), padding='same', name='{}_part1_conv_bn_relu1'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_2 = compose(Conv_Bn_Relu(64, (1, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(96, (3, 3), (1, 1), padding='same', name='{}_part2_conv_bn_relu2'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(96, (3, 3), (1, 1), padding='same', name='{}_part2_conv_bn_relu3'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_3 = compose(Conv_Bn_Relu(48, (1, 1), (1, 1), padding='same', name='{}_part3_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(64, (5, 5), (1, 1), padding='same', name='{}_part3_conv_bn_relu2'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_4 = compose(Conv_Bn_Relu(64, (1, 1), (1, 1), padding='same', name='{}_part4_conv_bn_relu1'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x_1, x_2, x_3, x_4])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def inception_B(x, name):</span><br><span class="line">    x_1 = compose(keras.layers.AveragePooling2D((3, 3), (1, 1), padding='same', name='{}_averagepool'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(192, (1, 1), (1, 1), padding='same', name='{}_part1_conv_bn_relu1'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_2 = compose(Conv_Bn_Relu(128, (1, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(128, (1, 7), (1, 1), padding='same', name='{}_part2_conv_bn_relu2'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(128, (7, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu3'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(128, (1, 7), (1, 1), padding='same', name='{}_part2_conv_bn_relu4'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(192, (7, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu5'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_3 = compose(Conv_Bn_Relu(128, (1, 1), (1, 1), padding='same', name='{}_part3_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(128, (1, 7), (1, 1), padding='same', name='{}_part3_conv_bn_relu2'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(192, (7, 1), (1, 1), padding='same', name='{}_part3_conv_bn_relu3'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_4 = compose(Conv_Bn_Relu(192, (1, 1), (1, 1), padding='same', name='{}_part4_conv_bn_relu1'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x_1, x_2, x_3, x_4])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def inception_C(x, name):</span><br><span class="line">    x_1 = compose(keras.layers.AveragePooling2D((3, 3), (1, 1), padding='same', name='{}_averagepool'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(192, (1, 1), (1, 1), padding='same', name='{}_part1_conv_bn_relu1'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_2 = compose(Conv_Bn_Relu(448, (1, 1), (1, 1), padding='same', name='{}_part2_conv_bn_relu1'.format(name)),</span><br><span class="line">                  Conv_Bn_Relu(384, (3, 3), (1, 1), padding='same', name='{}_part2_conv_bn_relu2'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_2_1 = compose(Conv_Bn_Relu(384, (1, 3), (1, 1), padding='same', name='{}_part2_1_conv_bn_relu1'.format(name)))(x_2)</span><br><span class="line"></span><br><span class="line">    x_2_2 = compose(Conv_Bn_Relu(384, (3, 1), (1, 1), padding='same', name='{}_part2_2_conv_bn_relu1'.format(name)))(x_2)</span><br><span class="line"></span><br><span class="line">    x_2 = keras.layers.Concatenate(name='{}_concatenate_2'.format(name))([x_2_1, x_2_2])</span><br><span class="line"></span><br><span class="line">    x_3 = compose(Conv_Bn_Relu(384, (1, 1), (1, 1), padding='same', name='{}_part3_conv_bn_relu1'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x_3_1 = compose(Conv_Bn_Relu(384, (1, 3), (1, 1), padding='same', name='{}_part3_1_conv_bn_relu1'.format(name)))(x_3)</span><br><span class="line"></span><br><span class="line">    x_3_2 = compose(Conv_Bn_Relu(384, (3, 1), (1, 1), padding='same', name='{}_part3_2_conv_bn_relu1'.format(name)))(x_3)</span><br><span class="line"></span><br><span class="line">    x_3 = keras.layers.Concatenate(name='{}_concatenate_3'.format(name))([x_3_1, x_3_2])</span><br><span class="line"></span><br><span class="line">    x_4 = compose(Conv_Bn_Relu(320, (1, 1), (1, 1), padding='same', name='{}_part4_conv_bn_relu1'.format(name)))(x)</span><br><span class="line"></span><br><span class="line">    x = keras.layers.Concatenate(name='{}_concatenate'.format(name))([x_1, x_2, x_3, x_4])</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def inception_v3(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line">    x = compose(Conv_Bn_Relu(32, (3, 3), (2, 2), padding='valid', name='conv_bn_relu1'),</span><br><span class="line">                Conv_Bn_Relu(32, (3, 3), (1, 1), padding='valid', name='conv_bn_relu2'),</span><br><span class="line">                Conv_Bn_Relu(64, (3, 3), (1, 1), padding='same', name='conv_bn_relu3'),</span><br><span class="line">                keras.layers.MaxPool2D((3, 3), (2, 2), name='maxpool1'),</span><br><span class="line">                Conv_Bn_Relu(80, (1, 1), (1, 1), padding='same', name='conv_bn_relu4'),</span><br><span class="line">                Conv_Bn_Relu(192, (3, 3), (1, 1), padding='valid', name='conv_bn_relu5'),</span><br><span class="line">                keras.layers.MaxPool2D((3, 3), (2, 2), name='maxpool2'))(x)</span><br><span class="line">    inception_A_part1_filters = [32, 64, 64]</span><br><span class="line">    for i in range(3):</span><br><span class="line">        x = inception_A(x, inception_A_part1_filters[i], name='inception_A_{}'.format(i + 1))</span><br><span class="line">    x = reduction_A(x, name='reduction_A')</span><br><span class="line">    for i in range(4):</span><br><span class="line">        x = inception_B(x, name='inception_B_{}'.format(i + 1))</span><br><span class="line">    x = reduction_B(x, name='reduction_B')</span><br><span class="line">    for i in range(2):</span><br><span class="line">        x = inception_C(x, name='inception_C_{}'.format(i + 1))</span><br><span class="line">    x = compose(keras.layers.GlobalAveragePooling2D(name='global_averagepool'),</span><br><span class="line">                keras.layers.Dense(1000, activation='softmax', name='dense'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='Inception-V3')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = inception_v3(input_shape=(299, 299, 3))</span><br><span class="line">    model.build(input_shape=(None, 299, 299, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/Inception_V3_R.png" alt="Inception_V3"></p><h1 id="Inception-V3小结"><a href="#Inception-V3小结" class="headerlink" title="Inception-V3小结"></a><font size="5" color="red">Inception-V3小结</font></h1><p>  Inception-V3是一种复杂的深度学习网络，参数量为22M，<strong>由于其结构过于复杂，很少被其他网络所使用</strong>，但是其<strong>不同感受野和卷积分解的思想给其他网络提供了思路</strong>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Inception-V3&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>ResNet</title>
    <link href="https://USTCcoder.github.io/2020/03/09/feature_extraction%20ResNet/"/>
    <id>https://USTCcoder.github.io/2020/03/09/feature_extraction ResNet/</id>
    <published>2020-03-09T09:22:18.000Z</published>
    <updated>2020-05-27T01:06:45.507Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">ResNet</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>ResNet:</strong>由<strong>华人学者何凯明大神于2015年提出</strong>，其主要体现出了残差相连的优势，故简称ResNet，是<strong>2015年ILSVRC竞赛的第一名</strong>，是一个很好的图像特征提取模型。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/ResNet.png" alt="ResNet"></p><h1 id="ResNet特点"><a href="#ResNet特点" class="headerlink" title="ResNet特点"></a><font size="5" color="red">ResNet特点</font></h1><p>  <font size="3">使用<strong>残差块结构</strong>，使得网络能够更多获取之前的信息，并且使<strong>学习结果对于权重的变化更加敏感</strong></font><br>  <font size="3">使用<strong>瓶颈结构</strong>，先使用1x1的卷积核进行降维，最后再次使用1x1的卷积核升维，可以降低模型的参数量</font><br>  <font size="3"><strong>Conv Block</strong>：作用是<strong>改变图像大小</strong>，输入和输出的尺寸不同，因此<strong>无法直接残差相连</strong>，</font><br>  <font size="3"><strong>Identity Block</strong>：作用是<strong>增加网络深度</strong>，输入和输出的尺寸相同，可以<strong>直接残差相连</strong></font></p><h1 id="不同尺寸ResNet网络结构"><a href="#不同尺寸ResNet网络结构" class="headerlink" title="不同尺寸ResNet网络结构"></a><font size="5" color="red">不同尺寸ResNet网络结构</font></h1><p><img src="/images/Feature_extraction/ResNet_C.png" alt="ResNet"></p><h1 id="ResNet50图像分析"><a href="#ResNet50图像分析" class="headerlink" title="ResNet50图像分析"></a><font size="5" color="red">ResNet50图像分析</font></h1><p><img src="/images/Feature_extraction/ResNet_A.png" alt="ResNet"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn_Relu(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn_Relu, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.conv = keras.layers.Conv2D(filters, kernel_size, strides, padding)</span><br><span class="line">        self.bn = keras.layers.BatchNormalization()</span><br><span class="line">        self.relu = keras.layers.ReLU()</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        conv = self.conv(inputs)</span><br><span class="line">        bn = self.bn(conv)</span><br><span class="line">        output = self.relu(bn)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Conv_Bn(keras.layers.Layer):</span><br><span class="line">    def __init__(self, filters, kernel_size, strides, padding, name):</span><br><span class="line">        super(Conv_Bn, self).__init__()</span><br><span class="line">        self._name = name</span><br><span class="line">        self.conv = keras.layers.Conv2D(filters, kernel_size, strides, padding)</span><br><span class="line">        self.bn = keras.layers.BatchNormalization()</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, **kwargs):</span><br><span class="line">        conv = self.conv(inputs)</span><br><span class="line">        output = self.bn(conv)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def res_block(x, filters, strides, type, name):</span><br><span class="line">    shortcut = x</span><br><span class="line">    x = compose(Conv_Bn_Relu(filters // 4, (1, 1), (1, 1), padding='same', name='{}{}_conv_bn_relu1'.format(type, name)),</span><br><span class="line">                Conv_Bn_Relu(filters // 4, (3, 3), strides, padding='same', name='{}{}_conv_bn_relu2'.format(type, name)),</span><br><span class="line">                Conv_Bn(filters, (1, 1), (1, 1), padding='same', name='{}{}_conv_bn3'.format(type, name)))(x)</span><br><span class="line">    if type == 'conv_block':</span><br><span class="line">        shortcut = keras.layers.Conv2D(filters, (1, 1), strides, name='{}{}_shortcut'.format(type, name))(shortcut)</span><br><span class="line">    x = keras.layers.Add(name='{}{}_add'.format(type, name))([x, shortcut])</span><br><span class="line">    x = keras.layers.ReLU(name='{}{}_relu3'.format(type, name))(x)</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def ResNet50(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line">    x = compose(keras.layers.ZeroPadding2D((3, 3), name='zeropadding'),</span><br><span class="line">                Conv_Bn_Relu(64, (7, 7), (2, 2), padding='valid', name='conv_bn_relu'),</span><br><span class="line">                keras.layers.MaxPool2D((3, 3), (2, 2), padding='same', name='maxpool'))(x)</span><br><span class="line">    filters = [256, 512, 1024, 2048]</span><br><span class="line">    strides = [(1, 1), (2, 2), (2, 2), (2, 2)]</span><br><span class="line">    times = [3, 4, 6, 3]</span><br><span class="line">    for i in range(len(times)):</span><br><span class="line">        x = res_block(x, filters[i], strides[i], 'conv_block', i + 1)</span><br><span class="line">        for j in range(times[i] - 1):</span><br><span class="line">            x = res_block(x, filters[i], (1, 1), 'identity_block{}_'.format(i + 1), j + 1)</span><br><span class="line">    x = compose(keras.layers.GlobalAveragePooling2D(name='global_averagepool'),</span><br><span class="line">                keras.layers.Dense(1000, activation='softmax', name='dense'))(x)</span><br><span class="line">    model = keras.Model(input_tensor, x, name='ResNet50')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line"></span><br><span class="line">    model = resnet50(input_shape=(224, 224, 3))</span><br><span class="line">    model.build(input_shape=(None, 224, 224, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/ResNet_R.png" alt="ResNet"></p><h1 id="ResNet小结"><a href="#ResNet小结" class="headerlink" title="ResNet小结"></a><font size="5" color="red">ResNet小结</font></h1><p>  ResNet是一种非常有效的特征提取网络，由于减少了Dense层的数量，因此参数量<strong>相比于VGG大大减少</strong>，参数量只有25M，因此实际任务经常使用。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;ResNet&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>VGG</title>
    <link href="https://USTCcoder.github.io/2020/03/08/feature_extraction%20VGG/"/>
    <id>https://USTCcoder.github.io/2020/03/08/feature_extraction VGG/</id>
    <published>2020-03-08T07:43:15.000Z</published>
    <updated>2020-05-26T11:48:03.429Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">VGG</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>VGG:</strong>来源于<strong>牛津大学视觉几何组Visual Geometry Group</strong>，故简称VGG，是<strong>2014年ILSVRC竞赛的第二名</strong>，是一个很好的图像特征提取模型。<br><a id="more"></a></p><p><img src="/images/Feature_extraction/VGG.png" alt="VGG"></p><h1 id="VGG特点"><a href="#VGG特点" class="headerlink" title="VGG特点"></a><font size="5" color="red">VGG特点</font></h1><p>  <font size="3">卷积核：VGG全由3x3小卷积核构成，步长为1，填充方式为same</font><br>  <font size="3">池化核：VGG全由2x2池化核构成，步长为2</font><br>  <font size="3">网络层：VGG具有较深的网络层，可以根据需要进行调整</font><br>  <font size="3">参数量：VGG具有较大参数量，主要来源于Flatten层后面的全连接层</font></p><h1 id="不同尺寸VGG网络结构"><a href="#不同尺寸VGG网络结构" class="headerlink" title="不同尺寸VGG网络结构"></a><font size="5" color="red">不同尺寸VGG网络结构</font></h1><p><img src="/images/Feature_extraction/VGG_C.png" alt="VGG"></p><h1 id="VGG16图像分析"><a href="#VGG16图像分析" class="headerlink" title="VGG16图像分析"></a><font size="5" color="red">VGG16图像分析</font></h1><p><img src="/images/Feature_extraction/VGG_A.png" alt="VGG"></p><h1 id="TensorFlow2-0实现"><a href="#TensorFlow2-0实现" class="headerlink" title="TensorFlow2.0实现"></a><font size="4">TensorFlow2.0实现</font></h1><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compose(*funcs):</span><br><span class="line">    if funcs:</span><br><span class="line">        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError('Composition of empty sequence not supported.')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def conv_block(x, filters, times, name):</span><br><span class="line">    for i in range(times):</span><br><span class="line">        x = keras.layers.Conv2D(filters, (3, 3), (1, 1), 'same', activation='relu', name='conv{}_{}'.format(name, i + 1))(x)</span><br><span class="line">    x = keras.layers.MaxPool2D((2, 2), (2, 2), name='maxpool{}'.format(name))(x)</span><br><span class="line"></span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def vgg16(input_shape):</span><br><span class="line">    input_tensor = keras.layers.Input(input_shape, name='input')</span><br><span class="line">    x = input_tensor</span><br><span class="line">    times = [2, 2, 3, 3, 5]</span><br><span class="line">    filters = [64, 128, 256, 512, 512]</span><br><span class="line">    for i in range(len(times)):</span><br><span class="line">        x = conv_block(x, filters[i], times[i], i + 1)</span><br><span class="line">    x = compose(keras.layers.Flatten(name='flatten'),</span><br><span class="line">                keras.layers.Dense(4096, activation='relu', name='dense1'),</span><br><span class="line">                keras.layers.Dense(4096, activation='relu', name='dense2'),</span><br><span class="line">                keras.layers.Dense(1000, activation='softmax', name='dense3'))(x)</span><br><span class="line"></span><br><span class="line">    model = keras.Model(input_tensor, x, name='VGG16')</span><br><span class="line"></span><br><span class="line">    return model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    model = vgg16(input_shape=(224, 224, 3))</span><br><span class="line">    model.build(input_shape=(None, 224, 224, 3))</span><br><span class="line">    model.summary()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/Feature_extraction/VGG_R.png" alt="VGG"></p><h1 id="VGG小结"><a href="#VGG小结" class="headerlink" title="VGG小结"></a><font size="5" color="red">VGG小结</font></h1><p>  VGG是<strong>最简单的一种深度学习网络</strong>，也是一种<strong>非常有效的特征提取模型</strong>。从上图可以看出VGG16模型的参数量达到143M，<strong>因为特征提取时不需要后面的Dense层，可以大大降低网络的规模</strong>。因此实际任务所使用，如<strong>目标检测算法SSD的特征提取网络就是VGG</strong>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;VGG&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征提取网络" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>最长回文字符串(Leetcode 5)</title>
    <link href="https://USTCcoder.github.io/2020/02/13/program%20Leetcode5/"/>
    <id>https://USTCcoder.github.io/2020/02/13/program Leetcode5/</id>
    <published>2020-02-13T00:35:30.000Z</published>
    <updated>2020-09-02T02:17:41.276Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode5.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   最长回文子串问题是一个经典的字符串处理问题，这道题有四种不同的解决思路，当然它们的时间复杂度和空间复杂度也都不相同。</p><a id="more"></a><h1 id="暴力法"><a href="#暴力法" class="headerlink" title="暴力法"></a><font size="5" color="red">暴力法</font></h1><p>暴力法很好理解，遍历所有子串，需要$O(n^2)$的时间复杂度，每个子串比较是否为回文串，需要O(n)的时间复杂度，因此总的时间复杂度为$O(n^3)$，空间复杂度上，如果使用一个临时变量存储子串则需要O(n)，如果直接使用索引则空间复杂度为O(1)，使用临时变量思路更加清晰，在这里我就使用O(n)的空间复杂度。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def longestPalindrome(self, s):</span><br><span class="line">        """</span><br><span class="line">        :s: str</span><br><span class="line">        :rtype: str</span><br><span class="line">        """</span><br><span class="line">        lens = len(s)</span><br><span class="line">        if lens == 0:</span><br><span class="line">            return ''</span><br><span class="line">        for i in range(lens, 0, -1):</span><br><span class="line">            for j in range(lens - i + 1):</span><br><span class="line">                tmp = s[j:j + i]</span><br><span class="line">                for k in range(i // 2):</span><br><span class="line">                    if tmp[k] != tmp[-k - 1]:</span><br><span class="line">                        break</span><br><span class="line">                else:</span><br><span class="line">                    return tmp</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="DP"><a href="#DP" class="headerlink" title="DP"></a><font size="5" color="red">DP</font></h1><p>DP是(Dynamic Programming，动态规划)的简称，动态规划的核心问题是如何建立状态转移方程，而本题有一个天然的状态，即回文状态，如果某个字符串是回文字符串，那么去掉首尾的一个字符，仍然满足回文字符串，因此可以容易的建立状态转移方程，其时间复杂度为$O(n^2)$，空间复杂度也是$O(n^2)$。<br><img src="/images/ALGORITHM/leetcode5_dp.png" alt="dp"><br>有关DP的知识可以参考我的另一篇博客动态规划(Dynamic Programming)。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def longestPalindrome(self, s):</span><br><span class="line">        """</span><br><span class="line">        :s: str</span><br><span class="line">        :rtype: str</span><br><span class="line">        """</span><br><span class="line">        lens = len(s)</span><br><span class="line">        if lens == 0:</span><br><span class="line">            return ''</span><br><span class="line">        result = s[0]</span><br><span class="line">        dp = [[True if col &lt;= row else False for col in range(lens)] for row in range(lens)]</span><br><span class="line">        for length in range(2, lens + 1):</span><br><span class="line">            for j in range(lens - length + 1):</span><br><span class="line">                dp[j][j + length - 1] = dp[j + 1][j + length - 2] and (s[j] == s[j + length - 1])</span><br><span class="line">                if dp[j][j + length - 1] and length &gt; len(result):</span><br><span class="line">                    result = s[j:j + length]</span><br><span class="line">        return result</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="中心扩展算法"><a href="#中心扩展算法" class="headerlink" title="中心扩展算法"></a><font size="5" color="red">中心扩展算法</font></h1><p>中心扩展算法不是一类通用算法，是针对于特定回文字符串的问题的解法，分析如下图，时间复杂度为$O(n^2)$，如果采用一个临时变量存储子串则需要O(n)，如果直接使用索引则空间复杂度为O(1)，使用临时变量思路更加清晰，在这里我就使用O(n)的空间复杂度。<br><img src="/images/ALGORITHM/leetcode5_center.png" alt="center"><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def expandAroundCenter(self, s, left, right):</span><br><span class="line">        while left &gt;= 0 and right &lt; len(s) and s[left] == s[right]:</span><br><span class="line">            left -= 1</span><br><span class="line">            right += 1</span><br><span class="line">        return left + 1, right - 1</span><br><span class="line"></span><br><span class="line">    def longestPalindrome(self, s: str) -&gt; str:</span><br><span class="line">        start, end = 0, 0</span><br><span class="line">        for i in range(len(s)):</span><br><span class="line">            left1, right1 = self.expandAroundCenter(s, i, i)</span><br><span class="line">            left2, right2 = self.expandAroundCenter(s, i, i + 1)</span><br><span class="line">            if right1 - left1 &gt; end - start:</span><br><span class="line">                start, end = left1, right1</span><br><span class="line">            if right2 - left2 &gt; end - start:</span><br><span class="line">                start, end = left2, right2</span><br><span class="line">        return s[start: end + 1]</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="manacher-马拉车-算法"><a href="#manacher-马拉车-算法" class="headerlink" title="manacher(马拉车)算法"></a><font size="5" color="red">manacher(马拉车)算法</font></h1><p>马拉车算法是专门解决回文字符串的问题，其算法的时间复杂度和空间复杂度都可以缩小到O(n)的量级，在长字符串中具有非常显著的优势。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def longestPalindrome(self, s):</span><br><span class="line">        """</span><br><span class="line">        :s: str</span><br><span class="line">        :rtype: str</span><br><span class="line">        """</span><br><span class="line">        new_s = '#'</span><br><span class="line">        for i in s:</span><br><span class="line">            new_s += (i + '#')</span><br><span class="line">        C, p1, R = 0, 0, 0</span><br><span class="line">        len_ns = len(new_s)</span><br><span class="line">        radius = [1] * len_ns</span><br><span class="line">        while R &lt; len_ns - 1:</span><br><span class="line">            p1 += 1</span><br><span class="line">            if p1 &gt; R:</span><br><span class="line">                C = p1</span><br><span class="line">                R += 1</span><br><span class="line">                while 2 * C - (R + 1) &gt;= 0 and R + 1 &lt; len_ns and new_s[R + 1] == new_s[2 * C - (R + 1)]:</span><br><span class="line">                    R += 1</span><br><span class="line">                radius[C] = R - C + 1</span><br><span class="line">            else:</span><br><span class="line">                p2 = 2 * C - p1</span><br><span class="line">                pL = p2 - radius[p2] + 1</span><br><span class="line">                CL = C - radius[C] + 1</span><br><span class="line">                if CL &lt; pL:</span><br><span class="line">                    radius[p1] = radius[p2]</span><br><span class="line">                elif CL &gt; pL:</span><br><span class="line">                    radius[p1] = R - p1 + 1</span><br><span class="line">                else:</span><br><span class="line">                    C = p1</span><br><span class="line">                    while 2 * C - (R + 1) &gt;= 0 and R + 1 &lt; len_ns and new_s[R + 1] == new_s[2 * C - (R + 1)]:</span><br><span class="line">                        R += 1</span><br><span class="line">                    radius[C] = R - C + 1</span><br><span class="line">        longest = max(radius)</span><br><span class="line">        index = radius.index(longest)</span><br><span class="line">        begin_index, end_index = index - longest + 1, index + longest - 1</span><br><span class="line">        return new_s[begin_index:end_index + 1:2] if new_s[begin_index] != '#' else new_s[begin_index + 1:end_index + 1:2]</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  马拉车算法是解决最长回文串的最佳方法，在面试中问到回文串，如果能答出马拉车算法，那是非常给力的，但是动态规划的方法大家也要学会，因为动态规划问题也是面试常考的知识点之一。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 5&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>寻找两个正序数组的中位数(Leetcode 4)</title>
    <link href="https://USTCcoder.github.io/2020/02/11/program%20Leetcode4/"/>
    <id>https://USTCcoder.github.io/2020/02/11/program Leetcode4/</id>
    <published>2020-02-11T02:08:30.000Z</published>
    <updated>2020-09-02T02:17:38.310Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/ALGORITHM/leetcode4.png" alt="1"></p><h1 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a><font size="5" color="red">题目分析</font></h1><p>   寻找两个正序数组的中位数，这道题本身并不难，但是如何使用O(log(m + n))的时间复杂度求解是这道题目的难点。</p><a id="more"></a><h1 id="合并数组"><a href="#合并数组" class="headerlink" title="合并数组"></a><font size="5" color="red">合并数组</font></h1><p>合并有序数组的方法是最容易想到的方法，在归并排序中就用到了这种思想，将两个有序的数组合并为一个，然后直接索引寻找中位数，时间复杂度为$O(m+n)$，空间复杂度为$O(m+n)$。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def findMedianSortedArrays(self, nums1, nums2):</span><br><span class="line">        """</span><br><span class="line">        :nums1: List[int]</span><br><span class="line">        :nums2: List[int]</span><br><span class="line">        :rtype: float</span><br><span class="line">        """</span><br><span class="line">        nums = []</span><br><span class="line">        len1, len2 = len(nums1), len(nums2)</span><br><span class="line">        if len1 == 0:</span><br><span class="line">            nums = nums2[:]</span><br><span class="line">        elif len2 == 0:</span><br><span class="line">            nums = nums1[:]</span><br><span class="line">        else:</span><br><span class="line">            p1 = p2 = 0</span><br><span class="line">            while p1 &lt; len1 and p2 &lt; len2:</span><br><span class="line">                if nums1[p1] &lt; nums2[p2]:</span><br><span class="line">                    nums.append(nums1[p1])</span><br><span class="line">                    p1 += 1</span><br><span class="line">                else:</span><br><span class="line">                    nums.append(nums2[p2])</span><br><span class="line">                    p2 += 1</span><br><span class="line">            nums += nums2[p2:] if p1 == len1 else nums1[p1:]</span><br><span class="line">        return nums[(len1 + len2) // 2] if (len1 + len2) % 2 else (nums[(len1 + len2) // 2 - 1] + nums[(len1 + len2) // 2]) / 2</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="寻找索引"><a href="#寻找索引" class="headerlink" title="寻找索引"></a><font size="5" color="red">寻找索引</font></h1><p>因为不需要其他的数据，因此我们不需要引入一个新数组保存所有数据，只需要记录当前索引即。时间复杂度为$O(m+n)$，空间复杂度为$O(1)$。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def findMedianSortedArrays(self, nums1, nums2):</span><br><span class="line">        """</span><br><span class="line">        :nums1: List[int]</span><br><span class="line">        :nums2: List[int]</span><br><span class="line">        :rtype: float</span><br><span class="line">        """</span><br><span class="line">        p1 = p2 = 0</span><br><span class="line">        len1, len2 = len(nums1), len(nums2)</span><br><span class="line">        prior, current = 0, 0</span><br><span class="line">        for i in range((len1 + len2) // 2 + 1):</span><br><span class="line">            prior = current</span><br><span class="line">            if p1 &lt; len1 and (p2 &gt;= len2 or nums1[p1] &lt; nums2[p2]):</span><br><span class="line">                current = nums1[p1]</span><br><span class="line">                p1 += 1</span><br><span class="line">            else:</span><br><span class="line">                current = nums2[p2]</span><br><span class="line">                p2 += 1</span><br><span class="line">        return current if (len1 + len2) % 2 else (current + prior) / 2</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="最小k个元素"><a href="#最小k个元素" class="headerlink" title="最小k个元素"></a><font size="5" color="red">最小k个元素</font></h1><p>从上面两种算法的时间复杂度可以看出，虽然能够正确的求解出此题，但是达不到题目所要求的时间复杂度，其实通过题目要求的时间复杂度，我们可以推测本题的算法，大概率是使用二分法来进行求解，因为二分法的时间复杂度一般是$O(log(n))$。这种方法参考windliang在Leetcode题解中的思想。时间复杂度为$O(log(m+n))$，空间复杂度为$O(1)$。<br><img src="/images/ALGORITHM/leetcode4_topk.png" alt="topk"><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def findMedianSortedArrays(self, nums1, nums2):</span><br><span class="line">        """</span><br><span class="line">        :nums1: List[int]</span><br><span class="line">        :nums2: List[int]</span><br><span class="line">        :rtype: float</span><br><span class="line">        """</span><br><span class="line">        def getk(a1, begin1, end1, a2, begin2, end2, k):</span><br><span class="line">            len1, len2 = end1 - begin1 + 1, end2 - begin2 + 1</span><br><span class="line">            if len1 &gt; len2:</span><br><span class="line">                return getk(a2, begin2, end2, a1, begin1, end1, k)</span><br><span class="line">            if len1 == 0:</span><br><span class="line">                return a2[begin2 + k - 1]</span><br><span class="line">            if k == 1:</span><br><span class="line">                return min(a1[begin1], a2[begin2])</span><br><span class="line">            index1, index2 = begin1 + min(k // 2, len1) - 1, begin2 + k // 2 - 1</span><br><span class="line">            if a1[index1] &gt; a2[index2]:</span><br><span class="line">                return getk(a1, begin1, end1, a2, index2 + 1, end2, k - k // 2)</span><br><span class="line">            else:</span><br><span class="line">                return getk(a1, index1 + 1, end1, a2, begin2, end2, k - min(k // 2, len1))</span><br><span class="line"></span><br><span class="line">        len1, len2 = len(nums1), len(nums2)</span><br><span class="line">        left, right = (len1 + len2 + 1) // 2, (len1 + len2 + 2) // 2</span><br><span class="line">        return (getk(nums1, 0, len1 - 1, nums2, 0, len2 - 1, left) + getk(nums1, 0, len1 - 1, nums2, 0, len2 - 1, right)) / 2</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="中位数算法"><a href="#中位数算法" class="headerlink" title="中位数算法"></a><font size="5" color="red">中位数算法</font></h1><p>这道题还有更优的算法，利用中位数的性质和二分查找的思想，将两个数组分成左右两个部分，这种方法参考windliang在Leetcode题解中的思想。时间复杂度为$O(log(min(m+n)))$，空间复杂度为$O(1)$。<br><img src="/images/ALGORITHM/leetcode4_search.png" alt="search"><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def findMedianSortedArrays(self, nums1, nums2):</span><br><span class="line">        """</span><br><span class="line">        :nums1: List[int]</span><br><span class="line">        :nums2: List[int]</span><br><span class="line">        :rtype: float</span><br><span class="line">        """</span><br><span class="line">        if len(nums1) &gt; len(nums2):</span><br><span class="line">            nums1, nums2 = nums2, nums1</span><br><span class="line">        len1, len2 = len(nums1), len(nums2)</span><br><span class="line">        min_index, max_index = 0, len1</span><br><span class="line">        while 1:</span><br><span class="line">            i = (min_index + max_index) // 2</span><br><span class="line">            j = (len1 + len2 + 1) // 2 - i</span><br><span class="line">            if i != len1 and nums2[j - 1] &gt; nums1[i]:</span><br><span class="line">                min_index = i + 1</span><br><span class="line">            elif i != 0 and nums1[i - 1] &gt; nums2[j]:</span><br><span class="line">                max_index = i - 1</span><br><span class="line">            else:</span><br><span class="line">                if i == 0:</span><br><span class="line">                    left_max = nums2[j - 1]</span><br><span class="line">                elif j == 0:</span><br><span class="line">                    left_max = nums1[i - 1]</span><br><span class="line">                else:</span><br><span class="line">                    left_max = max(nums1[i - 1], nums2[j - 1])</span><br><span class="line">                if (len1 + len2) % 2 == 1:</span><br><span class="line">                    return left_max</span><br><span class="line">                if i == len1:</span><br><span class="line">                    right_min = nums2[j]</span><br><span class="line">                elif j == len2:</span><br><span class="line">                    right_min = nums1[i]</span><br><span class="line">                else:</span><br><span class="line">                    right_min = min(nums1[i], nums2[j])</span><br><span class="line">                return (left_max + right_min) / 2</span><br></pre></td></tr></tbody></table></figure><p></p><h1 id="刷题总结"><a href="#刷题总结" class="headerlink" title="刷题总结"></a><font size="5" color="red">刷题总结</font></h1><p>  后面跟两种方法，我也是参考别人的题解，不仅思路清晰，而且代码简单，可以说膜拜了。这道题的难点在于二分法的应用，小伙伴们要多刷题，多见一见世面，比较不同算法之间的区别，增强自己的Coding能力。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Leetcode 4&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="刷题记录" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>Garbage Collection(垃圾回收)</title>
    <link href="https://USTCcoder.github.io/2019/11/05/python_garbage%20collection/"/>
    <id>https://USTCcoder.github.io/2019/11/05/python_garbage collection/</id>
    <published>2019-11-05T02:34:27.000Z</published>
    <updated>2020-07-27T06:15:00.229Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/gc.png" alt="gc"></p><h1 id="Garbage-Collection"><a href="#Garbage-Collection" class="headerlink" title="Garbage Collection"></a><font size="5" color="red">Garbage Collection</font></h1><p>  <strong>Garbage Collection(GC，垃圾回收)</strong>：不是Python特有的，而是现在<strong>许多高级语言自带的机制，可以帮助我们更高效的管理我们的内存</strong>，在<strong>C/C++的学习过程中，最重要的两个部分是指针和内存管理，其中内存管理就是指垃圾回收机制，在C/C++中，无法自动帮我们管理内存，如果我们缺少内存则需要申请，但是申请多了会导致内存不足，这时候需要我们手动释放一些内存</strong>。但是<strong>在Python中不需要，系统会自动帮我们释放内存</strong>，这个知识点不需要我们熟练掌握，只需要我们了解即可，运行代码时会自动启用垃圾回收机制。<br><a id="more"></a></p><h1 id="小整数对象池"><a href="#小整数对象池" class="headerlink" title="小整数对象池"></a><font size="5" color="red">小整数对象池</font></h1><p>因为<strong>一些小整数会被我们经常使用，所以在Python中[-5, 256]之间的整数会被固定在某一内存中</strong>，只要是这些整数赋值给某一变量，则那个变量的地址是固定的。<strong>在字符串中也有相似的表现，没有特殊字符的字符串也具有固定的地址</strong>。<br><img src="/images/LANGUAGE/gc1.png" alt="gc1"></p><h1 id="引用计数"><a href="#引用计数" class="headerlink" title="引用计数"></a><font size="5" color="red">引用计数</font></h1><p><strong>Python里每一个东西都是对象，它们的核心就是一个结构体：PyObject，是使用C语言来定义的</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">typedef struct_object {</span><br><span class="line">    int ob_refcnt;</span><br><span class="line">    struct_typeobject *ob_type;</span><br><span class="line">} PyObject;</span><br></pre></td></tr></tbody></table></figure><p></p><p><strong>其中ob_refcnt就是引用计数，当一个对象有新的引用时，这个值会增加，当引用它的对象被删除时，这个值会减少，使用sys.getrefcount()函数可以查看对象的引用计数。当引用计数为0时，该对象的生命就结束了</strong>。</p><p><strong>导致引用计数增加的情况</strong>：</p><ol><li><strong>对象被创建</strong></li><li><strong>对象被引用</strong></li><li><strong>对象作为参数被传入到一个函数中</strong></li><li><strong>对象作为元素，存储在容器中</strong></li></ol><p><strong>导致引用计数减少的情况</strong>：</p><ol><li><strong>对象被销毁</strong></li><li><strong>对象被赋予新的对象</strong></li><li><strong>对象离开作用域</strong></li><li><strong>对象所在容器被销毁</strong></li></ol><p><img src="/images/LANGUAGE/gc2.png" alt="gc2"><br>为什么创建时引用为2呢？因为调用sys.getrefcount(a)时，a作为参数传入到函数中，引用计数又加了1，所以sys,getrefcount()-1才是真正的引用计数次数。</p><p><strong>引用计数的优点</strong>：</p><ol><li><strong>简单</strong></li><li><strong>实时性，当引用计数为0，内存直接释放</strong></li></ol><p><strong>引用计数的缺点</strong>：</p><ol><li><strong>需要一个额外的内存存放引用计数</strong></li><li><strong>存在循环引用的致命缺点</strong></li></ol><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class MyClass:</span><br><span class="line">  pass</span><br><span class="line"></span><br><span class="line">a = MyClass()</span><br><span class="line">b = MyClass()</span><br><span class="line">a.next = b</span><br><span class="line">b.next = a</span><br><span class="line">del a</span><br><span class="line">del b</span><br></pre></td></tr></tbody></table></figure><p>a和b我们都已经不想使用了，因此使用了del，但是<strong>a和b的引用计数都为1，内存仍然没有被释放，这就是循环引用的致命缺点，会导致内存的严重泄漏</strong>。</p><h1 id="隔代回收"><a href="#隔代回收" class="headerlink" title="隔代回收"></a><font size="5" color="red">隔代回收</font></h1><p>在这里不想过多的探讨隔代回收，简单地说<strong>Python中会引入3个链表，所有新创建的对象都会加入到0代链表中，在一定的时间内检查并且扫描所有的循环引用，如果发现了两个对象的循环引用，则将引用计数-1，并且将0代链表中引用计数不为0的对象加入到1代链表中，当0代链表检查一定次数后，检查一次1代链表，重复上述动作，并将1代链表中引用计数不为0的对象加入到2代链表中，当1代链表检查一定次数后，检查一次2代链表</strong>。这就是隔代回收的大致思路。<br><img src="/images/LANGUAGE/gc3.png" alt="gc3"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  关于垃圾回收，不需要小伙伴们过多掌握，<strong>Python中默认开启垃圾回收机制</strong>，小伙伴们只要适当了解，就可以愉快的写代码啦。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Closure &amp; Decorators&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Dynamic &amp; Static(动态图与静态图)</title>
    <link href="https://USTCcoder.github.io/2019/10/28/frame%20dynamic%20vs%20static/"/>
    <id>https://USTCcoder.github.io/2019/10/28/frame dynamic vs static/</id>
    <published>2019-10-28T12:22:40.000Z</published>
    <updated>2020-06-11T02:48:43.131Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Dynamic &amp; Static</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>Dynamic &amp; Static(动态图与静态图):</strong>根据深度学习框架的不同，可以分成静态图框架和动态图框架，其中静态图框架的代表是<strong>TensorFlow1.x，Caffe2</strong>等，而动态图的代表是<strong>TensorFlow2.x，PyTorch</strong>等等。<br><a id="more"></a></p><p><img src="/images/FRAME/frame.png" alt="frame"></p><h1 id="动态图和静态图的区别"><a href="#动态图和静态图的区别" class="headerlink" title="动态图和静态图的区别"></a><font size="5" color="red">动态图和静态图的区别</font></h1><p><strong>静态图：先定义计算图，不断使用，相同类型的数据只定义一次计算图，之后再次运行时不需要重新定义，也不需要重复执行代码，只需要将数据送入数据流即可</strong>。<br><strong>特点：静态图保存了网络结构并且进行图优化，更加高效，但是弊端就是出错时很难进行单步调试，而且代码风格非常繁琐</strong>。</p><p><strong>动态图：每次计算需要重复之前的代码</strong>。<br><strong>特点：更加符合Python的代码风格，便于调试，弊端是效率较低</strong>。</p><h1 id="动态图和静态图的转换"><a href="#动态图和静态图的转换" class="headerlink" title="动态图和静态图的转换"></a><font size="5" color="red">动态图和静态图的转换</font></h1><p><color=red>以TensorFlow2.0为例，因为既可以使用静态图也可以使用动态图，<strong>TensorFlow2.0默认使用动态图机制，如果想使用静态图提高计算效率，可以在函数前加上@tf.function装饰器</strong>。&lt;/font&gt;</color=red></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras as keras</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def preprocess(x, y):</span><br><span class="line">    x = tf.cast(x, dtype=tf.float32) / 255.</span><br><span class="line">    x = tf.reshape(x, (28, 28, 1))</span><br><span class="line">    y = tf.one_hot(y, depth=10)</span><br><span class="line">    y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">    return x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lenet_5():</span><br><span class="line"></span><br><span class="line">    return keras.models.Sequential([keras.layers.Input(shape=(28, 28, 1), name='input'),</span><br><span class="line">                                     keras.layers.Conv2D(6, kernel_size=(5, 5), padding='same', activation='relu', name='conv1'),</span><br><span class="line">                                     keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='maxpool1'),</span><br><span class="line">                                     keras.layers.Conv2D(16, kernel_size=(5, 5), padding='valid', activation='relu', name='conv2'),</span><br><span class="line">                                     keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='maxpool2'),</span><br><span class="line">                                     keras.layers.Flatten(name='flatten'),</span><br><span class="line">                                     keras.layers.Dense(120, activation='relu', name='dense1'),</span><br><span class="line">                                     keras.layers.Dense(84, activation='relu', name='dense2'),</span><br><span class="line">                                     keras.layers.Dense(10, activation='softmax', name='dense3')], name='LeNet-5')</span><br><span class="line"></span><br><span class="line">@tf.function</span><br><span class="line">def dynamic_gradient_descent(train_x, train_y):</span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        y_pred = model(train_x, training=True)</span><br><span class="line">        loss = lossor(train_y, y_pred)</span><br><span class="line">    grads = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(grads, model.trainable_variables))</span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metrics.update_state(train_y, y_pred)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def static_gradient_descent(train_x, train_y):</span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        y_pred = model(train_x, training=True)</span><br><span class="line">        loss = lossor(train_y, y_pred)</span><br><span class="line">    grads = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(grads, model.trainable_variables))</span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metrics.update_state(train_y, y_pred)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    (x, y), (x_test, y_test) = keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">    batch_size = 256</span><br><span class="line">    tf.random.set_seed(22)</span><br><span class="line">    max_epoch = 5</span><br><span class="line"></span><br><span class="line">    db = tf.data.Dataset.from_tensor_slices((x, y))</span><br><span class="line">    db = db.map(preprocess).shuffle(10000).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))</span><br><span class="line">    db_test = db_test.map(preprocess).batch(batch_size)</span><br><span class="line"></span><br><span class="line">    model = lenet_5()</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(1e-3)</span><br><span class="line">    lossor = keras.losses.CategoricalCrossentropy()</span><br><span class="line">    train_loss = keras.metrics.Mean()</span><br><span class="line">    train_metrics = keras.metrics.CategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">    start = time.clock()</span><br><span class="line">    for epoch in range(max_epoch):</span><br><span class="line">        it_train = iter(db)</span><br><span class="line">        it_test = iter(db_test)</span><br><span class="line">        for train_x, train_y in db:</span><br><span class="line">            dynamic_gradient_descent(train_x, train_y)</span><br><span class="line"></span><br><span class="line">        print('Epoch: %d, Train loss: %.6f, Train acc: %.6f' % (epoch, train_loss.result(), train_metrics.result()))</span><br><span class="line"></span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        train_metrics.reset_states()</span><br><span class="line">    print('dynamic graph time is : %.3f' % (time.clock() - start))</span><br><span class="line"></span><br><span class="line">    model = lenet_5()</span><br><span class="line"></span><br><span class="line">    optimizer = keras.optimizers.Adam(1e-3)</span><br><span class="line"></span><br><span class="line">    start = time.clock()</span><br><span class="line">    for epoch in range(max_epoch):</span><br><span class="line">        it_train = iter(db)</span><br><span class="line">        it_test = iter(db_test)</span><br><span class="line">        for train_x, train_y in db:</span><br><span class="line">            static_gradient_descent(train_x, train_y)</span><br><span class="line"></span><br><span class="line">        print('Epoch: %d, Train loss: %.6f, Train acc: %.6f' % (epoch, train_loss.result(), train_metrics.result()))</span><br><span class="line"></span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        train_metrics.reset_states()</span><br><span class="line">    print('static graph time is : %.3f' % (time.clock() - start))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/frame1.png" alt="case1"><br>从上面这个例子可以看出，<strong>使用静态图计算和使用动态图计算达到了相同的效果，但是动态图的计算速度却比静态图慢很多，但是在静态图函数中加入断点无法进入函数，动态图可以加入断点进行调试</strong>，因此需要小伙伴们自行选择。当然也可以<strong>选择fit或者train_on_batch进行训练，它们都是使用静态图进行计算的</strong>。</p><p><color=red>下面我们探究为什么静态图无法加入断点进行调试？&lt;/font&gt;<br></color=red></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def dynamic():</span><br><span class="line">    tf.print('tf.print in dynamic graph')</span><br><span class="line">    print('print in dynamic graph')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@tf.function</span><br><span class="line">def static():</span><br><span class="line">    tf.print('tf.print in static graph')</span><br><span class="line">    print('print in static graph')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for i in range(3):</span><br><span class="line">    static()</span><br><span class="line"></span><br><span class="line">print('*' * 20)</span><br><span class="line"></span><br><span class="line">for i in range(3):</span><br><span class="line">    dynamic()</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/FRAME/frame2.png" alt="case2"><br>从这个例子中可以看出，<strong>在调用@tf.function修饰的函数时分为两步，第一步是建立图的过程，第二步是输入数据的过程</strong>。<strong>第一步建立图的过程是静态执行模式</strong>，执行普通的Python语言，对<strong>其中的数据Tensor数据是没有数值的，类似于TensorFlow1.x中的placeholder，建立图时的计算只是建立运算关系和节点</strong>，因此会输出print in static graph一次，<strong>第二步输入数据时，才会将数据送入各个节点之中进行计算</strong>，在这个例子中没有输入数据，但是tf.print函数也是一个节点，因此在节点计算时会输出三次tf.print中的内容，而print函数不是节点，因此<strong>只会在第一步执行一次，以后无论执行多少次相同运算图的内容，print函数都不会执行，除非重新建立运算图</strong>。而<strong>动态图执行过程中，每一次都要进入dynamic函数中输出tf.print函数和print函数</strong>，因此每一个都会输出三次。</p><p><color=red>我们验证一下上面的思想，如何重新建立运算图，而且是否重新建立运算图会重新执行print语句&lt;/font&gt;<br></color=red></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@tf.function</span><br><span class="line">def static(x):</span><br><span class="line">    tf.print(x)</span><br><span class="line">    print(x)</span><br><span class="line">    tf.print('tf.print in static graph')</span><br><span class="line">    print('print in static graph')</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print('inputs type is tensor—int32'.center(40, '-'))</span><br><span class="line">static(tf.constant(1, dtype=tf.int32))</span><br><span class="line">static(tf.constant(2, dtype=tf.int32))</span><br><span class="line"></span><br><span class="line">print('inputs type is tensor-float32'.center(40, '-'))</span><br><span class="line">static(tf.constant(1.1, dtype=tf.float32))</span><br><span class="line">static(tf.constant(2.2, dtype=tf.float32))</span><br><span class="line"></span><br><span class="line">print('inputs type is tensor—int32'.center(40, '-'))</span><br><span class="line">static(tf.constant(3, dtype=tf.int32))</span><br><span class="line"></span><br><span class="line">print('inputs type is int32'.center(40, '-'))</span><br><span class="line">static(1)</span><br><span class="line"></span><br><span class="line">print('inputs type is int32'.center(40, '-'))</span><br><span class="line">static(2)</span><br><span class="line"></span><br><span class="line">print('inputs type is variable—int32'.center(40, '-'))</span><br><span class="line">static(tf.Variable(1, name='x', dtype=tf.int32))</span><br><span class="line"></span><br><span class="line">print('inputs type is variable—int32'.center(40, '-'))</span><br><span class="line">static(tf.Variable(1, name='x', dtype=tf.int32))</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/FRAME/frame3.png" alt="case3"><br>从这个例子可以明显看出<strong>同种类型和维度的常量输入只会建立一次运算图，并且保存到字典中，如果发现相同类型和维度的输入，则会调用之前建立好的运算图</strong>。第一次传入值为1的张量时，建立了一次运算图，所以会出现print in static graph的输出，而且print这个张量时，不显示数值，只显示s类型，维度和名称，因此第二次传入值为2的张量时，不需要重新建立运算图。但是当第三次传入一个值为1.1的float类型张量时，因为<strong>数据的类型不同，因此会重新建立运算图，当然之前的运算图也会保存下来</strong>，因为再次传入值为3的张量时，也没有重新建立运算图，而是调用了之前保存的运算图。但<strong>值得注意的是，python中的常数和Tensor变量，即使这个变量名称，类型和维度都相同，每次也都需要重新建立运算图，因此传入参数时尽量传入常量Tensor类型</strong>。</p><p><color=red><strong>使用tf.executing_eagerly()可以查看此时是静态执行模式还是动态执行模式，如果是静态执行模式，则正在搭建静态图或者数据在静态图中进行流动</strong>。&lt;/font&gt;<br></color=red></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def dynamic():</span><br><span class="line">    tf.print(tf.executing_eagerly())</span><br><span class="line">    print(tf.executing_eagerly())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@tf.function</span><br><span class="line">def static():</span><br><span class="line">    tf.print(tf.executing_eagerly())</span><br><span class="line">    print(tf.executing_eagerly())</span><br><span class="line"></span><br><span class="line">for i in range(3):</span><br><span class="line">    static()</span><br><span class="line"></span><br><span class="line">print('*' * 20)</span><br><span class="line"></span><br><span class="line">for i in range(3):</span><br><span class="line">    dynamic()</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/FRAME/frame4.png" alt="case4"><br>从这个例子可以看出，执行static函数时，白色的False是在搭建静态图中显示的print函数的内容，因此<strong>搭建静态图时处于静态执行模式中</strong>，红色的三次False是在数据流动中执行tf.print函数中的内容，说明在<strong>数据流的过程中也是处于静态执行模式</strong>。而白色的True和红色的True都表明在<strong>没有@tf.function装饰下，都是处于动态的执行模式</strong>。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  深度学习框架种类繁多，但是都大同小异，本枚菜鸟喜欢使用TensorFlow2版本，因为<strong>可以实现动态图和静态图的自由切换，让使用者学习一种框架就可以了解各种框架的不同，而且有很多的社区和官方文档，对于TensorFlow1.x版本有很多的改进，也是现在最主流的框架之一</strong>，推荐小伙伴们认真学习学习。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Dynamic &amp; Static&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习框架" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Closure &amp; Decorators(闭包和装饰器)</title>
    <link href="https://USTCcoder.github.io/2019/10/28/python_closure%20Decorators/"/>
    <id>https://USTCcoder.github.io/2019/10/28/python_closure Decorators/</id>
    <published>2019-10-28T02:11:05.000Z</published>
    <updated>2020-07-27T06:15:55.412Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/close.png" alt="close"></p><h1 id="Closure-amp-Decorators"><a href="#Closure-amp-Decorators" class="headerlink" title="Closure &amp; Decorators"></a><font size="5" color="red">Closure &amp; Decorators</font></h1><p>  <strong>Closure &amp; Decorators(闭包和装饰器)</strong>：是Python中非常重要的组成部分，其实它们是属于函数的内容，有关函数的内容，可以参考我的另一篇博客Function(函数)，但是装饰器实在是太高级，太重要了，为了让小伙伴们能够轻松的找到，因此我单独写一篇博客谈一谈什么是闭包和装饰器。<br><a id="more"></a></p><h1 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a><font size="5" color="red">闭包</font></h1><p><strong>Closure(闭包)：指在一个函数体的内部定义另一个函数，并且将这个函数的引用返回，使用该函数时的使用方式和普通函数相同，里面定义的函数称之为闭包</strong>。闭包函数<strong>总是可以访问其所在的外部函数中声明的参数和变量</strong>。<br><img src="/images/LANGUAGE/python105.png" alt="closure"></p><p>闭包实现求y=kx+b和函数实现y=kx+b的区别，从下图可以看出，使用闭包时，调用时给k和b赋值，这样以后只需要对x赋值即可，而使用函数，每次调用都需要对k，b，x同时赋值。<br><img src="/images/LANGUAGE/close1.png" alt="closure"></p><h1 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a><font size="5" color="red">装饰器</font></h1><p><strong>Decorators(装饰器)：实质上就是一个闭包，把一个函数作为参数，返回一个替代版本的函数，其本质就是一个返回函数的函数，可以在原函数之前或者之后增加内容。装饰器等价于函数名=闭包函数(函数名)</strong>。<br><img src="/images/LANGUAGE/python106.png" alt="Decorators"></p><p>通用装饰器是一种重要的装饰器，因为<strong>被装饰的函数可能需要输入参数和返回值，而且可能不同的函数需要不同个数不同种类的输入参数和返回值，因此一般情况下我们不在装饰器函数中将其写死，这里直接提供一种通用的装饰器形式</strong>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def func(functionName):</span><br><span class="line">  def func_in(*args, **kwargs):</span><br><span class="line">    ret = functionName(*args, **kwargs)</span><br><span class="line">    return ret</span><br></pre></td></tr></tbody></table></figure><p></p><p><strong>多个装饰器共同装饰某个函数时，按照从下到上的顺序层层装饰，调用时按照从上到下的顺序层层解装饰</strong>。而且我们可以发现，在装饰函数时装饰器函数中的内容已经被执行，即执行到@d2时，d2函数中的print已经被执行，而warpped函数和say函数没有被执行。<br><img src="/images/LANGUAGE/close2.png" alt="Decorators"></p><p><strong>装饰器也可以具有参数，可以在内部根据其参数选择不同的装饰方式</strong>，这要涉及到多重函数闭包。<br><img src="/images/LANGUAGE/close3.png" alt="Decorators"></p><p><strong>类当作装饰器</strong>，需要重写$\underline{}\underline{}init \underline{}\underline{}$方法和$\underline{}\underline{}call\underline{} \underline{}$方法，<strong>@类名时会执行构造函数中的内容，调用函数时会执行call函数中的内容</strong>。<br><img src="/images/LANGUAGE/close4.png" alt="Decorators"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  关于闭包和装饰器，<strong>在某些特殊的场合，或者多人集成开发时会有使用</strong>，有时看别人的代码时会遇到装饰器，为了方便小伙伴们更快速的看懂别人写的代码，所以科普一下装饰器的使用，小伙伴们在自己写代码时可能不需要过多掌握，但是了解之后会使你的Python水平得到提升。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Closure &amp; Decorators&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Iterator &amp; Generator(迭代器和生成器)</title>
    <link href="https://USTCcoder.github.io/2019/10/23/python_Iterator%20Generator/"/>
    <id>https://USTCcoder.github.io/2019/10/23/python_Iterator Generator/</id>
    <published>2019-10-23T14:30:15.000Z</published>
    <updated>2020-07-27T06:14:25.518Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/iterator.png" alt="iterator"></p><h1 id="Iterator-amp-Generator"><a href="#Iterator-amp-Generator" class="headerlink" title="Iterator &amp; Generator"></a><font size="5" color="red">Iterator &amp; Generator</font></h1><p>  <strong>Iterator &amp; Generator(迭代器和生成器)</strong>：是Python中非常重要的组成部分，很多小伙伴搞不清迭代器和生成器到底是什么？如何创建迭代器和生成器？今天给小伙伴们具体讲解一下。<br><a id="more"></a></p><h1 id="Iterator-amp-Iterable"><a href="#Iterator-amp-Iterable" class="headerlink" title="Iterator &amp; Iterable"></a><font size="5" color="red">Iterator &amp; Iterable</font></h1><p>虽然今天是第一次提到Iterable(可迭代对象)和Iterator(迭代器对象)，但是我们一定都使用过可迭代对象。<strong>可迭代对象可以理解为一个实例对象，具有<strong>iter</strong>方法</strong>。而<strong>迭代器对象可以理解为一个实例对象，既有<strong>iter</strong>方法，又有<strong>next</strong>方法</strong>。<strong>一个可迭代对象通过<strong>iter</strong>方法可以转换为一个迭代器对象，而迭代器对象始终是一个可迭代对象，调用<strong>iter</strong>方法等于其自身</strong>。简单地说，可以使用for循环遍历的都是可迭代对象，我们<strong>常见的list，tuple，set，dict，str等容器都是可迭代对象，但是不是迭代器对象。通过<strong>iter</strong>方法可以转换为迭代器对象</strong>。</p><p>通过collection库下面的Iterator类和Iterable可以判定对象是否为迭代器对象还是可迭代对象。<br><img src="/images/LANGUAGE/iterator1.png" alt="iterator"></p><p>根据上面的分析可以知道，我们创建的类只要定义了<strong>next</strong>方法和<strong>iter</strong>方法，就是一个迭代器。<br><img src="/images/LANGUAGE/iterator2.png" alt="iterator"></p><p>我们还需要掌握一个知识点，<strong>迭代器对象是一次性消费对象</strong>，因为它们没有把所有的值存在内存中，而是在运行时产生值，当我们使用迭代器对象时，迭代器对象的内容就会消失。如下例，使用了列表容器产生一个可迭代对象a=[1, 2, 3, 4, 5]，并且使用<strong>iter</strong>方法产生了迭代器对象b，当next(b)时，会输出1，此时1就会从b中消失，同理再次使用next(b)时，2就会从b中消失，当list(b)时，所有数据都会从b中消失，但是a时不会变化的，因此我们需要另一个迭代器对象时，需要使用iter(a)再次产生一个。<br><img src="/images/LANGUAGE/iterator3.png" alt="iterator"></p><h1 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a><font size="5" color="red">生成器</font></h1><p><strong>Generator(生成器)也是一种迭代器，也是一次性消费对象</strong>，在写代码的时候，尤其是深度学习领域，具有大量的数据集输入，<strong>如果将数据全部保存，会受到内存的限制</strong>，而且很多数据本次用不到，这时候生成器就发挥了巨大的作用，每次使用时生成值。<strong>其实现一般有两种方式，一个是生成式，注意生成式不是列表生成式，而是将列表生成式中的中括号改成小括号，一个是使用关键字yield的函数</strong>。<br><img src="/images/LANGUAGE/iterator4.png" alt="generator"></p><p><strong>在调用带有yield的函数生成器时，返回的是一个生成器对象，当函数运行到yield时，将yield的值返回，程序会暂停，并且保存当前函数的所有运行信息，当使用next方法时，程序从上次停留的地方继续执行</strong>。<br><img src="/images/LANGUAGE/iterator5.png" alt="generator"></p><p><strong>生成器函数也可以接收输入值，通过send方法可以向生成器中传值</strong>。注意第一次只能通过send传入None，或者调用next方法，如果使用send不写输入参数则会报错，如果使用send传入某个非None的值也会报错。<br><img src="/images/LANGUAGE/iterator6.png" alt="generator"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  <strong>迭代器和生成器是Python中非常重要的内容，能够在特殊的情况下发挥出巨大的优势</strong>，小伙伴们务必掌握它。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Iterator &amp; Generator&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>二叉树的遍历</title>
    <link href="https://USTCcoder.github.io/2019/09/28/skill%20tree/"/>
    <id>https://USTCcoder.github.io/2019/09/28/skill tree/</id>
    <published>2019-09-28T08:57:36.000Z</published>
    <updated>2020-07-19T11:54:08.827Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">二叉树的遍历</font></strong></center><p></p><h1 id="二叉树介绍"><a href="#二叉树介绍" class="headerlink" title="二叉树介绍"></a><font size="5" color="red">二叉树介绍</font></h1><p>  <strong>二叉树(Binary tree):是一种常见的数据结构，也是学习计算机必须要掌握的一种模型</strong>，与之相关的概念有很多，完全二叉树，二叉搜索树，二叉排序树，红黑树等等，相关的算法也有很多，深度优先搜索，广度优先搜索等等，今天的主要内容不在于此。有个朋友问我，二叉树有3种遍历方式，给出两种如何计算第三种，今天给小伙伴们理一理。<br><a id="more"></a></p><h1 id="先序遍历-preorder-traversal"><a href="#先序遍历-preorder-traversal" class="headerlink" title="先序遍历(preorder traversal)"></a><font size="5" color="red">先序遍历(preorder traversal)</font></h1><p><img src="/images/SKILL/tree_preorder.png" alt="0"><br>  <font size="3">（1）访问根节点</font><br>  <font size="3">（2）遍历左子树(在遍历左子树的时候也按照先序遍历)</font><br>  <font size="3">（3）遍历右子树(在遍历右子树的时候也按照先序遍历)</font></p><h1 id="中序遍历-inorder-traversal"><a href="#中序遍历-inorder-traversal" class="headerlink" title="中序遍历(inorder traversal)"></a><font size="5" color="red">中序遍历(inorder traversal)</font></h1><p><img src="/images/SKILL/tree_inorder.png" alt="0"><br>  <font size="3">（1）遍历左子树(在遍历左子树的时候也按照中序遍历)</font><br>  <font size="3">（2）访问根节点</font><br>  <font size="3">（3）遍历右子树(在遍历右子树的时候也按照中序遍历)</font></p><h1 id="后序遍历-postorder-traversal"><a href="#后序遍历-postorder-traversal" class="headerlink" title="后序遍历(postorder traversal)"></a><font size="5" color="red">后序遍历(postorder traversal)</font></h1><p><img src="/images/SKILL/tree_postorder.png" alt="0"><br>  <font size="3">（1）遍历左子树(在遍历左子树的时候也按照后序遍历)</font><br>  <font size="3">（2）遍历右子树(在遍历右子树的时候也按照后序遍历)</font><br>  <font size="3">（3）访问根节点</font></p><h1 id="给定先序和中序，求解后序遍历"><a href="#给定先序和中序，求解后序遍历" class="headerlink" title="给定先序和中序，求解后序遍历"></a><font size="5" color="red">给定先序和中序，求解后序遍历</font></h1><p>  先序遍历，因为先访问根节点，因此可以得出第一个节点就是根节点。<br>  中序遍历，先访问左子树，然后访问根节点，因此根节点之前的都是根节点的左子树，根节点之后的都是根节点的右子树。<br>  从根节点的左子树中进行迭代，重复上述过程。</p><p>  就以上面这个情况为例，先序遍历的结果是ABHFDECKG，中序遍历的结果是HBDFAEKCG，我们进行分析。</p><ol><li>先序可以知道根节点A，在中序遍历中A之前为HBDF是A的左子树，A之后为EKCG为A的右子树。</li><li>从HBDF中寻找根节点，在先序遍历中，B是HBDF中第一个出现的节点，因此B是根节点，在中序遍历中B之前为H是B的左子树，B之后为DF是B的右子树。</li><li>从DF中寻找根节点，在先序遍历中，F是DF中第一个出现的节点，因此F是根节点，在中序遍历中F之前为D是F的左子树。F没有右子树。此时A的左子树全部遍历完毕。</li><li>从EKCG中寻找根节点，在先序遍历中，E是EKCG中第一个出现的节点，因此E是根节点，在中序遍历中E之前没有节点，E没有左子树，E之后为KCG是E的右子树。</li><li>从KCG中寻找根节点，在先序遍历中，C是KCG中第一个出现的节点，因此C是根节点，在中序遍历中C之前为K是左子树，C之后为G是右子树。此时遍历结束。</li><li>通过之前的遍历，可以重建出这个树的全貌，因此再通过后序遍历读出节点顺序即可。结果为HDFBKGCEA，小伙伴们可以进行尝试能否推导出来。</li></ol><h1 id="给定中序和后序，求解先序遍历"><a href="#给定中序和后序，求解先序遍历" class="headerlink" title="给定中序和后序，求解先序遍历"></a><font size="5" color="red">给定中序和后序，求解先序遍历</font></h1><p>  后序遍历，因为最后访问根节点，因此可以得出最后一个节点就是根节点。<br>  中序遍历，先访问左子树，然后访问根节点，因此根节点之前的都是根节点的左子树，根节点之后的都是根节点的右子树。<br>  从根节点的左子树中进行迭代，重复上述过程。</p><p>  就以上面这个情况为例，中序遍历的结果是HBDFAEKCG，后序遍历的结果是HDFBKGCEA，我们进行分析。</p><ol><li>后序可以知道根节点A，在中序遍历中A之前为HBDF是A的左子树，A之后为EKCG为A的右子树。</li><li>从HBDF中寻找根节点，在后序遍历中，B是HBDF中最后一个出现的节点，因此B是根节点，在中序遍历中B之前为H是B的左子树，B之后为DF是B的右子树。</li><li>从DF中寻找根节点，在后序遍历中，F是DF中最后一个出现的节点，因此F是根节点，在中序遍历中F之前为D是F的左子树。F没有右子树。此时A的左子树全部遍历完毕。</li><li>从EKCG中寻找根节点，在后序遍历中，E是EKCG中第一个出现的节点，因此E是根节点，在中序遍历中E之前没有节点，E没有左子树，E之后为KCG是E的右子树。</li><li>从KCG中寻找根节点，在后序遍历中，C是KCG中第一个出现的节点，因此C是根节点，在中序遍历中C之前为K是左子树，C之后为G是右子树。此时遍历结束。</li><li>通过之前的遍历，可以重建出这个树的全貌，因此再通过先序遍历读出节点顺序即可。结果为ABHFDECKG，小伙伴们也可以进行尝试能否推导出来。</li></ol><h1 id="给定先序和后序，求解中序遍历"><a href="#给定先序和后序，求解中序遍历" class="headerlink" title="给定先序和后序，求解中序遍历"></a><font size="5" color="red">给定先序和后序，求解中序遍历</font></h1><p>这个是无法求解的，因为已知先序和后序，出现了过多的信息冗余，导致有效信息不足，以上题来说，先序的第一个节点一定是A，那么后序的最后一个节点也一定是A，这就是无效信息，下图展示了一个极端的例子。<br><img src="/images/SKILL/tree_postorder.png" alt="0"><br>在这个图中，小伙伴们可以写一下它们的先序遍历和后序遍历，就会发现它们的先序遍历和后序遍历都是一样的，因为它们只有左子树或者只有右子树，可以将左子树和右子树看成一个整体，因此两个遍历的结果是相同的，所以不能够通过先序和后序得出树得全貌。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font size="5" color="red">小结</font></h1><p>  树的遍历是经典的笔试考题，掌握小伙伴的基本功力，因此小伙伴们一定要学习如何通过两种遍历得到另一种遍历的过程。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;二叉树的遍历&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="Computer Science" scheme="https://USTCcoder.github.io/categories/Computer-Science/"/>
    
      <category term="二叉树的遍历" scheme="https://USTCcoder.github.io/categories/Computer-Science/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux(操作系统)</title>
    <link href="https://USTCcoder.github.io/2019/09/23/skill%20Linux/"/>
    <id>https://USTCcoder.github.io/2019/09/23/skill Linux/</id>
    <published>2019-09-23T00:10:40.000Z</published>
    <updated>2020-05-20T06:57:13.795Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Linux</font></strong></center><p></p><h1 id="Linux介绍"><a href="#Linux介绍" class="headerlink" title="Linux介绍"></a><font size="5" color="red">Linux介绍</font></h1><p>  <strong>Linux:</strong>在1991年10月被<strong>Linus Torvalds(林纳斯·托瓦兹)</strong>创建，并于<strong>1994年发布了Linux-v1.0</strong>。是一套<strong>免费使用</strong>和<strong>自由传播</strong>的<strong>类Unix</strong>操作系统，是一个基于POSIX和Unix的多用户、多任务、支持多线程和多CPU的操作系统。<br><a id="more"></a></p><p><img src="/images/SKILL/linux1.jpg" alt="1"></p><h1 id="Linux特点"><a href="#Linux特点" class="headerlink" title="Linux特点"></a><font size="5" color="red">Linux特点</font></h1><p>  <font size="3">Linux是一款完全免费的操作系统，用户可以通过各种渠道下载使用。</font><br>  <font size="3">Linux是一款完全开源的操作系统，用户可以任意修改其源代码。</font><br>  <font size="3">Linux支持多用户，多任务的使用方式，对不同的用户有着不同的权利。</font><br>  <font size="3">Linux具有良好的界面，同时具有字符界面和图形界面。</font><br>  <font size="3">Linux支持多平台，可以运行在多种硬件平台上。</font><br>  <font size="3">Linux内核高效稳定，可使用户方便地建立防火墙，服务器。</font></p><h1 id="Linux注意事项"><a href="#Linux注意事项" class="headerlink" title="Linux注意事项"></a><font size="5" color="red">Linux注意事项</font></h1><p>  <font size="3">Linux严格区分大小写，大小写不同，命令不同。</font><br>  <font size="3">Linux中所有内容以文件形式保存，包括硬件。</font><br>  <font size="3">Linux和Windows不同，不靠扩展名来区分文件类型。</font><br>  <font size="3">Linux所有的设备都需要先挂载之后才能使用。</font><br>  <font size="3">Windows下的程序不能直接在Linux中安装运行。</font></p><h1 id="Linux常用命令"><a href="#Linux常用命令" class="headerlink" title="Linux常用命令"></a><font size="5" color="red">Linux常用命令</font></h1><p>  <font size="3">此命令都是运行在Ubuntu系统中，不同的Linux系列，如Debian系列和Redhat命令大同小异。</font></p><h2 id="显示目录文件"><a href="#显示目录文件" class="headerlink" title="显示目录文件"></a><font size="4">显示目录文件</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>ls -a</td><td>显示所有的文件，包括隐含文件(以.开头)</td></tr><tr><td>ls -l</td><td>显示文件的详细信息，不包括隐含文件</td></tr><tr><td>ls -h</td><td>以人性化形式显示文件，不包括隐含文件</td></tr><tr><td>ls -i</td><td>查看文件的i结点，不包括隐含文件</td></tr></tbody></table></div><p><img src="/images/SKILL/linux1.png" alt="1"></p><p>  <font size="4" color="red">文件信息字段</font><br>  <font size="3">第一项第一个字母：文件的类型，-代表二进制文件，d代表目录，l代表软连接文件</font><br>  <font size="3">第一项第二个字母到第四个字母：文件所有者可对文件的操作</font><br>  <font size="3">第一项第五个字母到第七个字母：组内成员可对文件的操作</font><br>  <font size="3">第一项第八个字母到第十个字母：其他用户可对文件的操作</font><br>  <font size="3">三个字母为一组，从左到右分别是读(r)，写(w)，执行(x)</font><br>  <font size="3">第二项：文件硬链接数</font><br>  <font size="3">第三项：文件的所有者</font><br>  <font size="3">第四项：文件的所属组</font><br>  <font size="3">第五项：文件所占用的字节空间</font><br>  <font size="3">第六项：文件最近的访问时间</font><br>  <font size="3">第七项：文件名</font></p><h2 id="目录处理命令"><a href="#目录处理命令" class="headerlink" title="目录处理命令"></a><font size="4">目录处理命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>mkdir -p dicname</td><td>递归创建名为dicname的文件夹</td></tr><tr><td>mkdir dicname</td><td>创建名为dicname的文件夹</td></tr><tr><td>cd dicname</td><td>切换目录</td></tr><tr><td>cd ..</td><td>返回上一级目录</td></tr><tr><td>pwd</td><td>显示当前目录</td></tr><tr><td>rmdir dicname</td><td>删除空目录</td></tr><tr><td>cp old new</td><td>复制文件</td></tr><tr><td>cp -r old new</td><td>复制目录</td></tr><tr><td>cp -p old new</td><td>保留属性复制</td></tr><tr><td>mv old new</td><td>剪切文件</td></tr><tr><td>rm filename</td><td>删除文件</td></tr><tr><td>rm -r dicname</td><td>删除目录</td></tr><tr><td>rm -f</td><td>强制执行</td></tr></tbody></table></div><p><img src="/images/SKILL/linux2.png" alt="2"></p><h2 id="文件处理命令"><a href="#文件处理命令" class="headerlink" title="文件处理命令"></a><font size="4">文件处理命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>touch filename</td><td>创建名为filename的空文件</td></tr><tr><td>cat filename</td><td>显示filename文件内容</td></tr><tr><td>cat -n filename</td><td>显示filename文件内容，并显示行号</td></tr><tr><td>tac</td><td>反向显示文件内容</td></tr><tr><td>less</td><td>分页显示文件内容(pageup和pagedown)</td></tr><tr><td>head -n m</td><td>显示文件前m行</td></tr><tr><td>tail -n m</td><td>显示文件后m行</td></tr></tbody></table></div><p><img src="/images/SKILL/linux4.png" alt="4"></p><h2 id="链接命令"><a href="#链接命令" class="headerlink" title="链接命令"></a><font size="4">链接命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>ln filename hardlink</td><td>生成链接到filename的硬链接</td></tr><tr><td>ln -s filename softlink</td><td>生成链接到filename的软链接</td></tr></tbody></table></div><p>  <font size="4" color="red">硬链接</font><br>  <font size="3">硬链接除了文件名所在地，其余信息都相同。</font><br>  <font size="3">硬链接后，文件信息中的链接数会加1。</font><br>  <font size="3">硬链接文件的i节点和源文件相同。</font><br>  <font size="3">硬链接不能跨分区。</font><br>  <font size="3">硬链接不能针对目录。</font><br>  <font size="3">源文件丢失，仍然可以访问硬链接。</font></p><p>  <font size="4" color="red">软链接</font><br>  <font size="3">软链接类似于Windows中的快捷方式，有箭头指向。</font><br>  <font size="3">软链接后，文件信息中的链接数不变。</font><br>  <font size="3">软链接文件的i节点和源文件不同。</font><br>  <font size="3">软链接可以跨分区。</font><br>  <font size="3">软链接可以针对目录。</font><br>  <font size="3">源文件丢失，不能访问软链接。</font></p><p><img src="/images/SKILL/linux3.png" alt="3"></p><h2 id="权限管理命令"><a href="#权限管理命令" class="headerlink" title="权限管理命令"></a><font size="4">权限管理命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>chmod op filename</td><td>修改filename文件的权限</td></tr><tr><td>chmod -R op dicname</td><td>修改dicname目录下所有文件的权限</td></tr><tr><td>chown own filename_or_dicname</td><td>修改文件或目录的所有者</td></tr><tr><td>chgrp grp filename_or_dicname</td><td>修改文件或目录的所属组</td></tr><tr><td>umask</td><td>查看当前用户新建文件的缺省权限</td></tr></tbody></table></div><p>  <font size="4" color="red">文件权限</font><br>  <font size="3">r：读权限，可以查看文件内容，如cat, tac, less, head, tail等</font><br>  <font size="3">w：写权限，可以修改文件内容，如vi, vim等</font><br>  <font size="3">x：执行权限，可以执行文件，如script, command等</font></p><p>  <font size="4" color="red">目录权限</font><br>  <font size="3">r：读权限，可以列出目录内容，如ls等</font><br>  <font size="3">w：写权限，可以在目录中创建删除文件，如touch, mkdir, rmdir, rm等</font><br>  <font size="3">x：执行权限，可以进入目录，如cd等</font></p><p>  <font size="4" color="red">修改权限两种操作</font><br>  <font size="3">u/g/o/a +/-/= r/w/x对所有者/所属组/其他用户/所有人 加/减/赋值 读/写/执行权限</font><br>  <font size="3">如 chmod g+w, o-r filename 对filename的所属组添加写权限，对其他用户删除读权限</font></p><p>  <font size="3">直接利用数字进行赋值权限</font><br>  <font size="3">如 chmod 644 filename 对filename的所有者添加读写权限，对所属组和其他用户添加读权限</font><br><img src="/images/SKILL/linux6.png" alt="6"></p><h2 id="文件搜索命令"><a href="#文件搜索命令" class="headerlink" title="文件搜索命令"></a><font size="4">文件搜索命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>find range condition</td><td>查找range范围内符合条件的文件或目录</td></tr><tr><td>locate filename</td><td>从文件资料库中查找文件</td></tr><tr><td>which command</td><td>搜索命令所在目录及别名信息</td></tr><tr><td>whereis command</td><td>搜索命令所在目录及帮助文档路径</td></tr><tr><td>grep string file</td><td>在文件中搜索字符串匹配的行并输出</td></tr><tr><td>grep -i string file</td><td>不区分字符串大小写搜索匹配的行并输出</td></tr><tr><td>grep -v string file</td><td>在文件中排除与字符串匹配的行并输出</td></tr><tr><td>grep -n string file</td><td>在文件中搜索字符串匹配的行并输出，并显示行数</td></tr></tbody></table></div><p>  <font size="4" color="red">find搜索匹配条件</font><br>  <font size="3">*：匹配字符多个</font><br>  <font size="3">如*init指以init结尾的文件名，*init*指包含init的文件名</font><br>  <font size="3">?：匹配单个字符</font><br>  <font size="3">如???init指以init结尾长度为7的文件名</font><br><img src="/images/SKILL/linux5.png" alt="5"></p><h2 id="帮助命令"><a href="#帮助命令" class="headerlink" title="帮助命令"></a><font size="4">帮助命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>man command</td><td>获得命令的帮助信息</td></tr><tr><td>whatis command</td><td>查看命令的功能</td></tr><tr><td>apropos filename</td><td>查看配置文件的简要信息</td></tr><tr><td>command —help</td><td>查看命令的常见选项</td></tr><tr><td>help command</td><td>查看shell内置命令的帮助信息</td></tr></tbody></table></div><p><img src="/images/SKILL/linux7.png" alt="7"></p><h2 id="网络命令"><a href="#网络命令" class="headerlink" title="网络命令"></a><font size="4">网络命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>write username</td><td>给用户发信息</td></tr><tr><td>wall</td><td>发广播信息</td></tr><tr><td>ping ip</td><td>对ip地址发数据包检测网络连通性，一直在发送</td></tr><tr><td>ping -c n ip</td><td>对ip地址发数据包检测网络连通性，发送n次</td></tr><tr><td>ifconfig net_card ip</td><td>配置网卡的ip地址</td></tr><tr><td>mail username</td><td>查看发送的电子邮件</td></tr><tr><td>last</td><td>列出登录系统的用户信息</td></tr><tr><td>lastlog</td><td>查看所有用户最后一次登录的时间</td></tr><tr><td>lastlog -u userid</td><td>查看用户id为userid(不是用户名)的用户最后一次登录时间</td></tr><tr><td>traceroute url</td><td>查看数据包到主机间的路径</td></tr><tr><td>netstat -t</td><td>显示网络中的TCP协议</td></tr><tr><td>netstat -u</td><td>显示网络中的UDP协议</td></tr><tr><td>netstat -l</td><td>监听网络服务</td></tr><tr><td>netstat -r</td><td>查看路由表</td></tr><tr><td>netstat -n</td><td>显示ip地址和端口号</td></tr></tbody></table></div><p><img src="/images/SKILL/linux8.png" alt="8"></p><h2 id="关机重启命令"><a href="#关机重启命令" class="headerlink" title="关机重启命令"></a><font size="4">关机重启命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>shutdown -c</td><td>取消上一个关机命令</td></tr><tr><td>shutdown -h +n</td><td>n分钟后关机</td></tr><tr><td>shutdown -h time</td><td>到达时间time关机</td></tr><tr><td>shutdown -h now</td><td>现在关机</td></tr><tr><td>shutdown -r +n</td><td>n分钟后重启</td></tr><tr><td>shutdown -r time</td><td>到达时间time重启</td></tr><tr><td>shutdown -r now</td><td>现在重启</td></tr></tbody></table></div><h2 id="修改系统默认运行级别"><a href="#修改系统默认运行级别" class="headerlink" title="修改系统默认运行级别"></a><font size="4">修改系统默认运行级别</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>vim /etc/inittab</td><td>修改系统默认运行级别</td></tr><tr><td>runlevel</td><td>查看系统运行级别</td></tr></tbody></table></div><p>  <font size="4" color="red">系统运行级别</font><br>  <font size="3">0：关机</font><br>  <font size="3">1：单用户模式</font><br>  <font size="3">2：不完全多用户模式，不含 NFS服务</font><br>  <font size="3">3：完全多用户模式务</font><br>  <font size="3">4：未分配</font><br>  <font size="3">5：图形界面</font><br>  <font size="3">6：重启</font></p><h2 id="用户配置文件"><a href="#用户配置文件" class="headerlink" title="用户配置文件"></a><font size="4">用户配置文件</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>less /etc/passwd</td><td>查看用户信息文件</td></tr><tr><td>less /etc/shadow</td><td>查看用户影子文件</td></tr><tr><td>less /etc/group</td><td>查看组信息文件</td></tr><tr><td>less /etc/gshadow</td><td>查看组影子文件</td></tr></tbody></table></div><p>  <font size="4" color="red">用户信息文件字段</font><br>  <font size="3">第一个字段：用户名称</font><br>  <font size="3">第二个字段：密码标志</font><br>  <font size="3">第三个字段：用户ID(UID)</font><br>  <font size="3">第四个字段：用户初始组ID(GID)</font><br>  <font size="3">第五个字段：用户说明</font><br>  <font size="3">第六个字段：家用户</font><br>  <font size="3">第七个字段：登录后的shell</font></p><p>  <font size="4" color="red">用户影子文件字段</font><br>  <font size="3">第一个字段：用户名称</font><br>  <font size="3">第二个字段：加密密码</font><br>  <font size="3">第三个字段：密码最后一次修改时间</font><br>  <font size="3">第四个字段：两次密码修改间隔</font><br>  <font size="3">第五个字段：密码有效期</font><br>  <font size="3">第六个字段：密码到期前的警告天数</font><br>  <font size="3">第七个字段：密码过期后的宽限天数</font><br>  <font size="3">第八个字段：账号失效时间</font><br>  <font size="3">第九个字段：保留字段</font></p><p>  <font size="4" color="red">组信息文件字段</font><br>  <font size="3">第一个字段：组名称</font><br>  <font size="3">第二个字段：组密码标志</font><br>  <font size="3">第三个字段：组ID(GID)</font><br>  <font size="3">第四个字段：组中附加用户</font></p><p>  <font size="4" color="red">组影子文件字段</font><br>  <font size="3">第一个字段：组名称</font><br>  <font size="3">第二个字段：组密码</font><br>  <font size="3">第三个字段：组管理员用户名</font><br>  <font size="3">第四个字段：组中附加用户</font></p><h2 id="用户管理命令"><a href="#用户管理命令" class="headerlink" title="用户管理命令"></a><font size="4">用户管理命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>useradd username</td><td>添加新用户</td></tr><tr><td>useradd -u username</td><td>添加新用户，手动指定用户id</td></tr><tr><td>useradd -d username</td><td>添加新用户，手动指定家目录</td></tr><tr><td>useradd -c username</td><td>添加新用户，手动指定用户说明</td></tr><tr><td>useradd -G username</td><td>添加新用户，手动指定附加组</td></tr><tr><td>useradd -g username</td><td>添加新用户，手动指定初始组</td></tr><tr><td>useradd -s username</td><td>添加新用户，手动指定登录shell</td></tr><tr><td>passwd username</td><td>设置用户密码</td></tr><tr><td>passwd -s username</td><td>查询用户密码状态</td></tr><tr><td>passwd -l username</td><td>暂时锁定用户</td></tr><tr><td>passwd -u username</td><td>解锁用户</td></tr><tr><td>userdel username</td><td>删除用户</td></tr><tr><td>change -l username</td><td>列出用户的详细密码状态</td></tr><tr><td>change -d username</td><td>修改密码最后一次更改日期</td></tr><tr><td>change -m username</td><td>修改两次密码修改间隔</td></tr><tr><td>change -M username</td><td>修改密码有效期</td></tr><tr><td>change -W username</td><td>修改密码过期前警告天数</td></tr><tr><td>change -I username</td><td>修改密码过期后宽限天数</td></tr><tr><td>change -E username</td><td>修改账号失效时间</td></tr></tbody></table></div><h2 id="用户组管理命令"><a href="#用户组管理命令" class="headerlink" title="用户组管理命令"></a><font size="4">用户组管理命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>groupadd groupname</td><td>添加新用户组</td></tr><tr><td>groupadd -g groupname</td><td>添加新用户，手动指定组id</td></tr><tr><td>groupmod -g groupname</td><td>修改组id</td></tr><tr><td>groupmod -n groupname</td><td>修改组名</td></tr><tr><td>groupdel groupname</td><td>删除用户组</td></tr><tr><td>gpasswd -a groupname</td><td>将用户加入用户组</td></tr><tr><td>gpasswd -a groupname</td><td>将用户从用户组删除</td></tr></tbody></table></div><h2 id="文件系统管理命令"><a href="#文件系统管理命令" class="headerlink" title="文件系统管理命令"></a><font size="4">文件系统管理命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>df -a dirname</td><td>显示文件的系统信息</td></tr><tr><td>df -h dirname</td><td>以人性化方式显示文件的系统信息</td></tr><tr><td>df -a dirname</td><td>显示文件的系统类型</td></tr><tr><td>du -a dicname</td><td>显示每个子文件的磁盘占用量</td></tr><tr><td>du -h dicname</td><td>以人性化方式显示每个子文件的磁盘占用量</td></tr><tr><td>du -s dicname</td><td>统计总占用量</td></tr><tr><td>fdisk -l</td><td>查看u盘的设备文件名</td></tr><tr><td>mount -t vfat device dicname</td><td>将u盘文件名device挂载到dicname文件名中</td></tr><tr><td>umount device</td><td>卸载挂载device</td></tr></tbody></table></div><h2 id="终端常用命令"><a href="#终端常用命令" class="headerlink" title="终端常用命令"></a><font size="4">终端常用命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>Ctrl + Alt +t</td><td>打开终端</td></tr><tr><td>Ctrl + d</td><td>退出终端</td></tr><tr><td>Ctrl + c</td><td>终止正在运行的程序</td></tr><tr><td>Ctrl + l</td><td>清屏</td></tr><tr><td>Ctrl + s</td><td>锁住终端</td></tr><tr><td>Ctrl + q</td><td>解锁终端</td></tr><tr><td>Ctrl + r</td><td>查找历史命令</td></tr><tr><td>^up</td><td>查看上一个历史记录</td></tr><tr><td>^down</td><td>查看下一个历史记录</td></tr><tr><td>su</td><td>切换到root用户</td></tr><tr><td>su username</td><td>切换到username用户</td></tr><tr><td>sudo command</td><td>以root权限执行命令</td></tr><tr><td>TAB</td><td>命令与文件补全</td></tr><tr><td>alias</td><td>查看命令的别名</td></tr><tr><td>alias newname command</td><td>给原命令起一个别名</td></tr></tbody></table></div><h2 id="重定向命令"><a href="#重定向命令" class="headerlink" title="重定向命令"></a><font size="4">重定向命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>command &gt; filename</td><td>将正确命令的结果覆盖到filename文件中</td></tr><tr><td>command &gt;&gt; filename</td><td>将正确命令的结果追加到filename文件中</td></tr><tr><td>command 2&gt; filename</td><td>将错误命令的结果覆盖到filename文件中</td></tr><tr><td>command 2&gt;&gt; filename</td><td>将错误命令的结果追加到filename文件中</td></tr><tr><td>command &amp;&gt; filename</td><td>将命令的结果覆盖到filename文件中(无论正确与否)</td></tr><tr><td>command &amp;&gt;&gt; filename</td><td>将命令的结果追加到filename文件中(无论正确与否)</td></tr><tr><td>command &gt;&gt; filename1 &amp;&gt;&gt; filename2</td><td>将正确的命令结果追加到filename1，错误命令的结果追加到到filename2</td></tr></tbody></table></div><h2 id="多命令"><a href="#多命令" class="headerlink" title="多命令"></a><font size="4">多命令</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>command1 ; command2 ; …</td><td>多个命令顺序执行，命令1出错命令2也会执行</td></tr><tr><td>command1 &amp;&amp; command2 &amp;&amp; …</td><td>逻辑与，命令1正确执行，命令2才会执行</td></tr><tr><td>command1 ll command2 ll …</td><td>逻辑或，命令1不正确执行，命令2才会执行</td></tr><tr><td>command 2&gt;&gt; filename</td><td>将错误命令的结果追加到filename文件中</td></tr><tr><td>command &amp;&gt; filename</td><td>将命令的结果覆盖到filename文件中(无论正确与否)</td></tr><tr><td>command &amp;&gt;&gt; filename</td><td>将命令的结果追加到filename文件中(无论正确与否)</td></tr><tr><td>command &gt;&gt; filename1 &amp;&gt;&gt; filename2</td><td>将正确的命令结果追加到filename1，错误命令的结果追加到到filename2</td></tr><tr><td>command1 l command2</td><td>管道符，命令1的正确输出作为命令2的操作对象</td></tr></tbody></table></div><p><img src="/images/SKILL/linux9.png" alt="9"></p><h2 id="特殊符号"><a href="#特殊符号" class="headerlink" title="特殊符号"></a><font size="4">特殊符号</font></h2><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>?</td><td>匹配一个任意字符</td></tr><tr><td>*</td><td>匹配任意多个字符</td></tr><tr><td>[]</td><td>匹配[]中任意一个字符</td></tr><tr><td>[-]</td><td>匹配[]中任意一个字符，-代表范围，如0-9</td></tr><tr><td>[^]</td><td>匹配不在[]中的任意一个字符</td></tr><tr><td>‘’</td><td>单引号将其作为一个整体，其中的所有特殊符号都是普通符号</td></tr><tr><td>“”</td><td>双引号中$调用变量的值，`引用命令，\转义字符</td></tr><tr><td>``</td><td>反引号中的内容为系统命令</td></tr><tr><td>$</td><td>美元符号调用变量的值</td></tr><tr><td>\</td><td>反斜杠指转义符</td></tr><tr><td>#</td><td>注释符</td></tr></tbody></table></div><h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a><font size="4">正则表达式</font></h2><p>  <font size="3">在Linux中，正则表达式是用来在文件中匹配符合条件的字符串，正则是包含匹配，grep，awk，sed等命令支持正则表达式。</font><br>  <font size="3">在Linux中，通配符是用来匹配符合条件的文件名，通配符是完全匹配，ls，find，cp等命令不支持正则表达式，只能使用通配符进行匹配。</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>*</td><td>前一个字符匹配0次或任意多次</td></tr><tr><td>.</td><td>匹配除换行符外任意一个字符</td></tr><tr><td>^</td><td>匹配行首</td></tr><tr><td>$</td><td>匹配行尾</td></tr><tr><td>[]</td><td>匹配括号中指定的任意一个字符</td></tr><tr><td>[^]</td><td>匹配除了括号中指定的任意一个字符</td></tr><tr><td>\</td><td>转义字符</td></tr><tr><td>{n}</td><td>表示前面的字符出现n次</td></tr><tr><td>{n,}</td><td>表示前面的字符出现不小于n次</td></tr><tr><td>{n,m}</td><td>表示前面的字符至少出现n次，至多m次</td></tr></tbody></table></div><h2 id="字符处理命令"><a href="#字符处理命令" class="headerlink" title="字符处理命令"></a><font size="4">字符处理命令</font></h2><p>|  符号   | 说明  |<br>|  cut -f n filename  |  提取文件第n列  |<br>|  cut -d interval -f n filename   |  设定分隔符提取文件第n列  |<br>|  printf “type_and_format” content  |  格式化输出  |<br>|  awk “condition1 {action1} condition2 {action2} …” filename  |  对filename中符合condition的进行action操作  |<br>|  awk “BEGIN{action} condition1 {action1} condition2 {action2} …” filename  |  在对filename操作之前进行action操作  |<br>|  awk “BEGIN{FS=’c’} condition1 {action1} condition2 {action2} …” filename  |  以c为分隔符进行操作，默认分隔符为制表符  |<br>|  awk “END{action} condition1 {action1} condition2 {action2} …” filename  |  在对filename操作之后进行action操作  |<br>|  sed -n  |  把只经过sed处理的输出，原文件不变化  |<br>|  sed -e  |  允许对输入数据应用多条sed命令  |<br>|  sed -i  |  直接对文件进行修改，不显示输出  |<br>|  sed a  |  在当前行后添加一行或多行  |<br>|  sed c  |  对当前行替换  |<br>|  sed i  |  对当前行插入  |<br>|  sed d  |  删除指定的行  |<br>|  sed p |  打印指定的行  |<br>|  sed s  |  字符替换，格式为”行范围/s/old_str/new_str/g”  |<br>|  sort  |  以数值型排序  |<br>|  sort -f  |  忽略大小写排序  |<br>|  sort -n |  以数值型排序  |<br>|  sort -r  |  反向排序  |<br>|  sort -t |  指定分隔符，默认为制表符  |<br>|  sort -k n[,m]  |  按照指定的字段，从n开始到m结束  |<br>|  wc  |  输入字符串，Ctrl+d结束，统计行数，单词数和字符数  |<br>|  wc filename  |  统计filename文件中的行数，单词数和字符数  |<br>|  wc -c  |  输入字符串，Ctrl+d结束，统计字符数  |<br>|  wc -w  |  输入字符串，Ctrl+d结束，统计单词数  |<br>|  wc -l  |  输入字符串，Ctrl+d结束，统计行数  |<br><img src="/images/SKILL/linux11.png" alt="11"></p><h2 id="系统管理"><a href="#系统管理" class="headerlink" title="系统管理"></a><font size="4">系统管理</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>ps aux</td><td>查看系统中的所有进程</td></tr><tr><td>top</td><td>查看系统的健康状态，默认每3秒更新一次</td></tr><tr><td>top -d</td><td>设置更新的秒数</td></tr><tr><td>top P</td><td>以CPU使用率排序</td></tr><tr><td>top M</td><td>以内存使用率排序</td></tr><tr><td>top N</td><td>以进程号(PID)排序</td></tr><tr><td>q</td><td>退出top</td></tr><tr><td>pstree</td><td>查看进程树</td></tr><tr><td>pstree -p</td><td>查看进程树，并显示进程号</td></tr><tr><td>pstree -u</td><td>查看进程树，并显示所属用户</td></tr><tr><td>kill -l</td><td>查看可用的进程信号</td></tr><tr><td>kill PID</td><td>正常关闭进程号为PID的进程</td></tr><tr><td>kill -n PID</td><td>对进程号为PID的进程进行n操作，n可以通过kill -l查询</td></tr><tr><td>killall p_name</td><td>杀死进程名为p_name的进程</td></tr><tr><td>killall p_name</td><td>杀死进程名为p_name的进程</td></tr><tr><td>killall p_name</td><td>杀死进程名为p_name的进程</td></tr><tr><td>command &amp;</td><td>将命令放在后台执行</td></tr><tr><td>command + Ctrl + z</td><td>将命令放在后台，且不执行</td></tr><tr><td>jobs</td><td>显示后台的进程</td></tr><tr><td>jobs -l</td><td>显示后台的进程，并显示PID</td></tr><tr><td>fg</td><td>恢复第一个后台暂停的进程到前台执行</td></tr><tr><td>fg %work_id</td><td>恢复工作号为work_id后台暂停的进程到前台执行，注意工作号和PID不同</td></tr><tr><td>bg</td><td>恢复第一个后台暂停的进程到后台执行</td></tr><tr><td>bg %work_id</td><td>恢复工作号为work_id后台暂停的进程到后台执行，注意工作号和PID不同</td></tr><tr><td>vmstat flu_time flu_num</td><td>查看系统资源，间隔为flu_time，次数为flu_num</td></tr><tr><td>free -b</td><td>以字节为单位查看内存使用状态</td></tr><tr><td>free -k</td><td>以KB为单位查看内存使用状态，默认以KB为单位</td></tr><tr><td>free -m</td><td>以MB为单位查看内存使用状态</td></tr><tr><td>free -g</td><td>以GB为单位查看内存使用状态</td></tr><tr><td>free -b</td><td>以字节为单位查看内存使用状态</td></tr><tr><td>cat /proc/cpuinfo</td><td>查看CPU的详细信息</td></tr><tr><td>uptime</td><td>显示系统的启动时间和平均负载</td></tr><tr><td>uptime -a</td><td>查看系统所有相关信息</td></tr><tr><td>uptime -r</td><td>查看内核版本</td></tr><tr><td>uptime -s</td><td>查看内核名称</td></tr><tr><td>file /bin/ls</td><td>查看操作系统的位数</td></tr><tr><td>lsb_release -a</td><td>查询Linux的发行版本</td></tr><tr><td>lsof -c string</td><td>列出以字符串开头的进程打开的文件</td></tr><tr><td>lsof -u username</td><td>列出某个用户的进程打开的文件</td></tr><tr><td>lsof -p pid</td><td>列出某个PID进程打开的文件</td></tr></tbody></table></div><p><img src="/images/SKILL/linux18.png" alt="18"></p><h2 id="定时任务"><a href="#定时任务" class="headerlink" title="定时任务"></a><font size="4">定时任务</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>crontab -e</td><td>编辑定时任务</td></tr><tr><td>crontab -l</td><td>查询定时任务</td></tr><tr><td>crontab -r</td><td>删除所有定时任务</td></tr></tbody></table></div><p>  <font size="4" color="red">编辑定时任务格式</font><br>  <font size="3">* * * * * 执行的任务</font><br>  <font size="3">第一个*：一小时当中的第几分钟(0-59)</font><br>  <font size="3">第二个*：一天当中的第几小时(0-23)</font><br>  <font size="3">第三个*：一个月当中的第几天(1-31)</font><br>  <font size="3">第四个*：一年当中的第几月(1-12)</font><br>  <font size="3">第五个*：一周当中的星期几(0-7)</font></p><p>  <font size="4" color="red">编辑定时任务特殊符号</font><br>  <font size="3">*：代表任何时间</font><br>  <font size="3">,：代表不连续时间</font><br>  <font size="3">-：代表连续的时间范围</font><br>  <font size="3">*/n：代表每隔多久执行一次</font></p><p>  <font size="3">编辑定时任务例子</font><br>  <font size="3">45 22 * * * command：每天的22:45执行命令</font><br>  <font size="3">30 12 * * 1 command：每周一的12:30执行命令</font><br>  <font size="3">0 8,12,16 * * * command：每天的8:00，12:00和16:00都执行命令</font><br>  <font size="3">0 5 * * 1-6 command：每周一的5:00执行命令</font><br>  <font size="3">0/10 4 * * * command：每天的凌晨4点，每隔10分钟执行一次</font><br>  <font size="3">0 0 1,15 * 1 command：每周一的0:00和每个月的1号和15号都会执行命令，尽量不要这样写</font><br><img src="/images/SKILL/linux19.png" alt="19"></p><h2 id="备份与恢复"><a href="#备份与恢复" class="headerlink" title="备份与恢复"></a><font size="4">备份与恢复</font></h2><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>dump -level</td><td>设定level备份级别，0为完全备份，1-9为增量备份，对分区可以增量备份，对于文件或目录只能完全备份</td></tr><tr><td>dump -f b_file o_file</td><td>将o_file备份到b_file中</td></tr><tr><td>dump -u</td><td>备份成功后，把备份时间记录在/etc/dumpdates文件</td></tr><tr><td>dump -v</td><td>显示备份过程中更多的输出信息</td></tr><tr><td>dump -j</td><td>调用bzlib库压缩备份文件，将备份文件压缩为.bz2格式</td></tr><tr><td>dump -W</td><td>显示允许被dump的分区的备份等级及备份时间</td></tr><tr><td>restore -C</td><td>比较备份数据和实际数据的变换</td></tr><tr><td>restore -i</td><td>进入交互模式，手工选择需要恢复的文件</td></tr><tr><td>restore -t</td><td>查看模式，用于查看备份文件中拥有哪些数据</td></tr><tr><td>restore -r</td><td>还原模式，用于数据还原</td></tr><tr><td>restore -f</td><td>指定备份文件的文件名</td></tr></tbody></table></div><h2 id="Shell基础"><a href="#Shell基础" class="headerlink" title="Shell基础"></a><font size="4">Shell基础</font></h2><p>  <font size="3">Shell是一个命令行解释器，为用户提供一个向Linux内核发送请求以便运行程序的系统级程序，用户可以用Shell启动，挂起，停止甚至编写程序</font></p><p>  <font size="3">Shell是一个功能强大的解释性脚本语言，易编写，易调试，灵活性强，且在Shell中可以直接调用Linux系统命令。</font></p><p>  <font size="3">Bash是Linux的基本Shell，执行时使用/文件名或者bash 文件名</font></p><h3 id="定义变量"><a href="#定义变量" class="headerlink" title="定义变量"></a><font size="3">定义变量</font></h3><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>var_name=’xxx’</td><td>定义变量，等于号前后不要加空格</td></tr><tr><td>$var_name</td><td>调用变量</td></tr><tr><td>unset var_name</td><td>删除变量</td></tr><tr><td>set</td><td>查看所有变量</td></tr><tr><td>echo</td><td>输出</td></tr><tr><td>read -t n var_name</td><td>给var_name读入一个值，等待n秒</td></tr><tr><td>read -p str var_name</td><td>显示str字符串，再给var_name读入一个值</td></tr></tbody></table></div><h3 id="位置参数变量"><a href="#位置参数变量" class="headerlink" title="位置参数变量"></a><font size="3">位置参数变量</font></h3><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>$n</td><td>0代表命令本身，1-9代表第一到第九个参数，10以上要用{}括起来</td></tr><tr><td>$*</td><td>所有参数，看作一个整体</td></tr><tr><td>$@</td><td>所有参数，每个参数分开对待</td></tr><tr><td>$#</td><td>所有参数的个数</td></tr></tbody></table></div><h3 id="预定义变量"><a href="#预定义变量" class="headerlink" title="预定义变量"></a><font size="3">预定义变量</font></h3><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>$?</td><td>判断最后一次执行命令的返回状态，为0则正确，否则错误</td></tr><tr><td>$$</td><td>当前进程的进程号(PID)</td></tr><tr><td>$!</td><td>后台运行的最后一个进程的进程号</td></tr></tbody></table></div><h3 id="声明变量类型"><a href="#声明变量类型" class="headerlink" title="声明变量类型"></a><font size="3">声明变量类型</font></h3><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>declare - var_name</td><td>给变量设定类型属性</td></tr><tr><td>declare + var_name</td><td>取消变量的类型属性</td></tr><tr><td>declare -i var_name</td><td>将变量声明为整型</td></tr><tr><td>declare -x var_name</td><td>将变量声明为环境变量</td></tr><tr><td>declare -p var_name</td><td>显示变量的类型</td></tr></tbody></table></div><p>  <font size="3">Linux中的变量都默认为字符串型，数值运算时要采用var_name=$((运算式))的格式。</font><br><img src="/images/SKILL/linux10.png" alt="10"></p><h3 id="条件判断"><a href="#条件判断" class="headerlink" title="条件判断"></a><font size="3">条件判断</font></h3><div class="table-container"><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>[ -d filename ]</td><td>注意中括号前后都有空格，判断文件是否存在，并且是否为目录文件，是目录文件为真</td></tr><tr><td>[ -e filename ]</td><td>判断文件是否存在，存在为真</td></tr><tr><td>[ -f filename ]</td><td>判断文件是否存在，并且是否为普通文件，是普通文件为真</td></tr><tr><td>[ -r filename ]</td><td>判断文件是否存在，并且是否具有读权限(无论是拥有者还是所属组还是其他用户，只要有就为真)</td></tr><tr><td>[ -w filename ]</td><td>判断文件是否存在，并且是否具有写权限</td></tr><tr><td>[ -x filename ]</td><td>判断文件是否存在，并且是否具有执行权限</td></tr><tr><td>[ filename1 -nt filename2 ]</td><td>判断文件1是否比文件2新(修改时间)</td></tr><tr><td>[ filename1 -ot filename2 ]</td><td>判断文件1是否比文件2旧</td></tr><tr><td>[ filename1 -ef filename2 ]</td><td>判断文件1和文件2的i节点是否一致，可以判断硬链接</td></tr><tr><td>[ int1 -eq int2 ]</td><td>判断整数1是否等于整数2</td></tr><tr><td>[ int1 -ne int2 ]</td><td>判断整数1是否不等于整数2</td></tr><tr><td>[ int1 -gt int2 ]</td><td>判断整数1是否大于整数2</td></tr><tr><td>[ int1 -lt int2 ]</td><td>判断整数1是否小于整数2</td></tr><tr><td>[ int1 -ge int2 ]</td><td>判断整数1是否大于等于整数2</td></tr><tr><td>[ int1 -le int2 ]</td><td>判断整数1是否小于等于整数2</td></tr><tr><td>[ -z str ]</td><td>判断字符串是否为空</td></tr><tr><td>[ -n str ]</td><td>判断字符串是否非空</td></tr><tr><td>[ str1==str2 ]</td><td>判断字符串1是否等于字符串2</td></tr><tr><td>[ str1!=str2 ]</td><td>判断字符串1是否不等于字符串2</td></tr><tr><td>[ judge1 -a judge2 ]</td><td>逻辑与，判断1和判断2都为真则为真</td></tr><tr><td>[ judge1 -o judge2 ]</td><td>逻辑或，判断1和判断2都为假则为假</td></tr><tr><td>[ !judge ]</td><td>逻辑非，判断取反</td></tr></tbody></table></div><p><img src="/images/SKILL/linux12.png" alt="12"></p><h3 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a><font size="3">流程控制</font></h3><p>  <font size="3">Linux中Shell脚本的第一句为#!bin/bash，说明下面的内容都是Shell脚本。</font></p><p>  <font size="3">执行脚本时，先将文件的权限变为可执行文件，然后./filename或者直接使用bash filename</font></p><h3 id="if条件流程控制"><a href="#if条件流程控制" class="headerlink" title="if条件流程控制"></a><font size="3">if条件流程控制</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 单分支if条件语句格式</span><br><span class="line"># if [ condition ]</span><br><span class="line">#     then</span><br><span class="line">#         程序</span><br><span class="line"># fi</span><br><span class="line"># 如果condition成立则执行程序，否则不执行</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 双分支if条件语句格式</span><br><span class="line"># if [ condition ]</span><br><span class="line">#     then</span><br><span class="line">#         程序1</span><br><span class="line">#     else</span><br><span class="line">#         程序2</span><br><span class="line"># fi</span><br><span class="line"># 如果condition成立则执行程序1，否则执行程序2</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/linux13.png" alt="13"></p><h3 id="case条件流程控制"><a href="#case条件流程控制" class="headerlink" title="case条件流程控制"></a><font size="3">case条件流程控制</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 多分支case条件语句</span><br><span class="line"># case $变量名 in</span><br><span class="line">#     "value1")</span><br><span class="line">#         程序1</span><br><span class="line">#         ;;</span><br><span class="line">#     "value2")</span><br><span class="line">#         程序2</span><br><span class="line">#         ;;</span><br><span class="line">#     *)</span><br><span class="line">#         程序n</span><br><span class="line">#         ;;</span><br><span class="line"># esac</span><br><span class="line"># 如果变量的值为value1执行程序1，变量值为value2执行程序2，...，如果都不等于则执行程序n</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/linux15.png" alt="15"></p><h3 id="for循环流程控制"><a href="#for循环流程控制" class="headerlink" title="for循环流程控制"></a><font size="3">for循环流程控制</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># for 变量 in value1 value2 ...</span><br><span class="line">#     do</span><br><span class="line">#         程序</span><br><span class="line">#     done</span><br><span class="line"># 和Python语言的for循环类似，将value1，value2，...，依次带入程序，不同value之间用空格分开</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># for ((初始值;循环控制条件;变量变换))</span><br><span class="line">#     do</span><br><span class="line">#         程序</span><br><span class="line">#     done</span><br><span class="line"># 和C语言的for循环类似，将变量带入程序，每次循环结束时更改变量的值，并且判断循环条件是否满足</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/linux14.png" alt="14"></p><h3 id="while循环流程控制"><a href="#while循环流程控制" class="headerlink" title="while循环流程控制"></a><font size="3">while循环流程控制</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># while [ condition ]</span><br><span class="line">#     do</span><br><span class="line">#         程序</span><br><span class="line">#     done</span><br><span class="line"># 如果满足条件则进入循环，每次循环结束要重新判断是否满足条件</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/linux16.png" alt="16"></p><h3 id="until循环流程控制"><a href="#until循环流程控制" class="headerlink" title="until循环流程控制"></a><font size="3">until循环流程控制</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># until [ condition ]</span><br><span class="line">#     do</span><br><span class="line">#         程序</span><br><span class="line">#     done</span><br><span class="line"># 如果不满足条件则进入循环，每次循环结束要重新判断是否满足条件</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/linux17.png" alt="17"></p><h1 id="Linux小结"><a href="#Linux小结" class="headerlink" title="Linux小结"></a><font size="5" color="red">Linux小结</font></h1><p>  因为Linux的安全性和开源特性，使得Linux受到广泛的青睐，尤其是在公司层面。在个人机上，多数人为了使用方便，采用Windows系统。但是为了公司服务器运维稳定，可靠，方便，Linux是使用最广泛的操作系统。因此学会Linux是程序员的必修课之一。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Linux&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="Computer Science" scheme="https://USTCcoder.github.io/categories/Computer-Science/"/>
    
      <category term="Linux" scheme="https://USTCcoder.github.io/categories/Computer-Science/Linux/"/>
    
    
  </entry>
  
  <entry>
    <title>Vim(文本编辑器)</title>
    <link href="https://USTCcoder.github.io/2019/09/22/skill%20Vim/"/>
    <id>https://USTCcoder.github.io/2019/09/22/skill Vim/</id>
    <published>2019-09-22T04:28:20.000Z</published>
    <updated>2020-05-20T06:57:40.096Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Vim</font></strong></center><p></p><h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a><font size="5" color="red">原理介绍</font></h1><p>  <strong>Vim</strong>是一个类似于Vi的著名的功能强大、高度可定制的<strong>文本编辑器</strong>。代码补全、编译及错误跳转等方便编程的<strong>功能特别丰富</strong>，在程序员中被广泛使用，是<strong>类Unix</strong>系统用户最喜欢的文本编辑器。<br><a id="more"></a></p><h1 id="Vim特点"><a href="#Vim特点" class="headerlink" title="Vim特点"></a><font size="5" color="red">Vim特点</font></h1><p>  <font size="3">解放程序员的双手，这一点非常重要，使得程序员在编辑文档时可以不再依赖于鼠标</font><br>  <font size="3">Vim具有高效率的移动，在行内，文件内可以随意的移动，可以节约大量时间。</font><br>  <font size="3">Vim具有高效率的输入，可以有很多的插入方式，复制，粘贴，剪切都非常方便。</font></p><h1 id="Vim关系图"><a href="#Vim关系图" class="headerlink" title="Vim关系图"></a><font size="5" color="red">Vim关系图</font></h1><p><img src="/images/SKILL/Vim2.png" alt="2"></p><h1 id="Vim应用"><a href="#Vim应用" class="headerlink" title="Vim应用"></a><font size="5" color="red">Vim应用</font></h1><p><img src="/images/SKILL/Vim1.png" alt="1"></p><h2 id="进入Vim"><a href="#进入Vim" class="headerlink" title="进入Vim"></a><font size="4">进入Vim</font></h2><p>  <font size="3">在终端中输入vim filename 进入Vim，如果存在该文件则编辑该文件，如果不存在该文件则创建一个新文件并编辑该文件</font></p><h2 id="插入"><a href="#插入" class="headerlink" title="插入"></a><font size="4">插入</font></h2><p>  <font size="3">一般是由命令模式进入插入模式</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>a</td><td>在光标所在字符后插入</td></tr><tr><td>A</td><td>在光标所在行尾插入</td></tr><tr><td>i</td><td>在光标所在字符前插入</td></tr><tr><td>I</td><td>在光标所在行首插入</td></tr><tr><td>o</td><td>在光标下插入新行</td></tr><tr><td>O</td><td>在光标上插入新行</td></tr></tbody></table></div><h2 id="光标移动"><a href="#光标移动" class="headerlink" title="光标移动"></a><font size="4">光标移动</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>k</td><td>光标向上移动一行</td></tr><tr><td>nk</td><td>光标向上移动n行</td></tr><tr><td>j</td><td>光标向下移动一行</td></tr><tr><td>nj</td><td>光标向下移动n行</td></tr><tr><td>h</td><td>光标向左移动一行</td></tr><tr><td>nh</td><td>光标向左移动n行</td></tr><tr><td>l</td><td>光标向右移动一行</td></tr><tr><td>nl</td><td>光标向右移动n行</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>b</td><td>光标移动到前一个单词的首字母上</td></tr><tr><td>nb</td><td>光标移动到前n个单词的首字母上</td></tr><tr><td>w</td><td>光标移动到后一个单词的首字母上</td></tr><tr><td>nw</td><td>光标移动到后n个单词的首字母上</td></tr><tr><td>ge</td><td>光标移动到前一个单词的尾字母上</td></tr><tr><td>nge</td><td>光标移动到前n个单词的尾字母上</td></tr><tr><td>e</td><td>光标移动到后一个单词的尾字母上</td></tr><tr><td>ne</td><td>光标移动到后n个单词的尾字母上</td></tr><tr><td>n<space></space></td><td>光标移动到后n个字符上</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>gg</td><td>光标移动到第一行首</td></tr><tr><td>G</td><td>光标移动到最后一行首</td></tr><tr><td>ngg或nG或:n</td><td>光标移动到第n行首</td></tr><tr><td>-</td><td>光标移动到上一行行首</td></tr><tr><td>n-</td><td>光标移动到上n行行首</td></tr><tr><td>+</td><td>光标移动到下一行行首</td></tr><tr><td>n+</td><td>光标移动到下n行行首</td></tr><tr><td>0</td><td>光标移动到该行行首</td></tr><tr><td>$</td><td>光标移动到该行行尾</td></tr><tr><td>n$</td><td>光标移动到下n行行尾</td></tr><tr><td>^</td><td>光标移动到该行首字母</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>Home</td><td>光标移动到该行行首(可以在插入模式下使用)</td></tr><tr><td>End</td><td>光标移动到该行行尾(可以在插入模式下使用)</td></tr></tbody></table></div><h2 id="删除-剪切-字符"><a href="#删除-剪切-字符" class="headerlink" title="删除(剪切)字符"></a><font size="4">删除(剪切)字符</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>x</td><td>删除光标所在字符</td></tr><tr><td>nx</td><td>删除光标处及后n-1个字符</td></tr><tr><td>X</td><td>删除光标前一个字符</td></tr><tr><td>nX</td><td>删除光标前n个字符</td></tr><tr><td>dd</td><td>删除光标所在行</td></tr><tr><td>ndd</td><td>删除光标所在行及后n-1行</td></tr><tr><td>dw</td><td>删除光标所在处的单词</td></tr><tr><td>dnw</td><td>删除光标所在处及后n-1个单词</td></tr><tr><td>dG</td><td>删除光标所在行到文件末尾的所有行</td></tr><tr><td>dgg</td><td>删除光标所在行到文件开始的所有行</td></tr><tr><td>d/word</td><td>删除从光标所在处到单词word的所有文本</td></tr><tr><td>D</td><td>删除光标所在处到该行尾的所有内容</td></tr><tr><td>:nd</td><td>删除第n行</td></tr><tr><td>:n1,n2d</td><td>删除n1行到n2行</td></tr></tbody></table></div><h2 id="复制"><a href="#复制" class="headerlink" title="复制"></a><font size="4">复制</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>yy</td><td>复制光标所在行</td></tr><tr><td>nyy</td><td>复制光标所在行及后n-1行</td></tr><tr><td>yw</td><td>复制光标所在处的单词</td></tr><tr><td>ynw</td><td>复制光标所在处及后n-1个单词</td></tr><tr><td>yG</td><td>复制光标所在行到文件末尾的所有行</td></tr><tr><td>ygg</td><td>复制光标所在行到文件开始的所有行</td></tr><tr><td>y/word</td><td>复制从光标所在处到单词word的所有文本</td></tr><tr><td>Y</td><td>复制光标所在处到该行尾的所有内容</td></tr><tr><td>:ny</td><td>复制第n行</td></tr><tr><td>:n1,n2y</td><td>复制n1行到n2行</td></tr></tbody></table></div><h2 id="粘贴"><a href="#粘贴" class="headerlink" title="粘贴"></a><font size="4">粘贴</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>p</td><td>粘贴在当前光标下</td></tr><tr><td>P</td><td>粘贴在当前光标上</td></tr></tbody></table></div><h2 id="可视"><a href="#可视" class="headerlink" title="可视"></a><font size="4">可视</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>v</td><td>进入可视(选择)模式</td></tr><tr><td>d</td><td>删除光标内容</td></tr><tr><td>y</td><td>复制光标内容</td></tr></tbody></table></div><h2 id="可视-1"><a href="#可视-1" class="headerlink" title="可视"></a><font size="4">可视</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>v</td><td>进入可视(选择)模式</td></tr><tr><td>y</td><td>复制光标内容</td></tr><tr><td>d</td><td>剪切光标内容</td></tr></tbody></table></div><h2 id="撤回和恢复"><a href="#撤回和恢复" class="headerlink" title="撤回和恢复"></a><font size="4">撤回和恢复</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>u</td><td>复原前一次操作</td></tr><tr><td>Ctrl + r</td><td>重做上个动作</td></tr></tbody></table></div><h2 id="翻页操作"><a href="#翻页操作" class="headerlink" title="翻页操作"></a><font size="4">翻页操作</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>Ctrl + f</td><td>屏幕向下移动一页</td></tr><tr><td>Ctrl + b</td><td>屏幕向上移动一页</td></tr><tr><td>Ctrl + d</td><td>屏幕向下移动半页</td></tr><tr><td>Ctrl + u</td><td>屏幕向下移动半页</td></tr></tbody></table></div><h2 id="设置行号"><a href="#设置行号" class="headerlink" title="设置行号"></a><font size="4">设置行号</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>:set nu</td><td>显示所有的行号</td></tr><tr><td>:set nonu</td><td>取消显示行号</td></tr></tbody></table></div><h2 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a><font size="4">搜索</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>/word</td><td>向光标之下寻找/word的字符串</td></tr><tr><td>?word</td><td>向光标之上寻找/word的字符串</td></tr><tr><td>n</td><td>继续向下查找下一个匹配的字符串</td></tr><tr><td>N</td><td>继续向上查找下一个匹配的字符串</td></tr><tr><td>:noh</td><td>取消高亮显示</td></tr><tr><td>:set ic</td><td>查找忽略大小写</td></tr><tr><td>:set noic</td><td>查找不忽略大小写</td></tr></tbody></table></div><h2 id="替换"><a href="#替换" class="headerlink" title="替换"></a><font size="4">替换</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>r</td><td>取代光标处的字符串</td></tr><tr><td>R</td><td>从光标处开始替换字符，Esc结束替换</td></tr><tr><td>:n1,n2s/word1/word2/g</td><td>在n1到n2行之间将word1换成word2</td></tr><tr><td>:%s/word1/word2/g</td><td>将文档中所有的word1换成word2</td></tr><tr><td>:%s/word1/word2/g</td><td>将文档中所有的word1换成word2，取代前询问</td></tr></tbody></table></div><h2 id="替换-1"><a href="#替换-1" class="headerlink" title="替换"></a><font size="4">替换</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>r</td><td>取代光标处的字符串</td></tr><tr><td>R</td><td>从光标处开始替换字符，Esc结束替换</td></tr><tr><td>:n1,n2s/word1/word2/g</td><td>在n1到n2行之间将word1换成word2</td></tr><tr><td>:%s/word1/word2/g</td><td>将文档中所有的word1换成word2</td></tr><tr><td>:%s/word1/word2/g</td><td>将文档中所有的word1换成word2，取代前询问</td></tr><tr><td>:ab word1 word2</td><td>文档中以后出现的word1字符自动转换为word2</td></tr></tbody></table></div><h2 id="定义快捷键"><a href="#定义快捷键" class="headerlink" title="定义快捷键"></a><font size="4">定义快捷键</font></h2><p>  <font size="3">在命令模式下</font><br>  <font size="3">:map ctrl+v+字符 命令 可以用来定义快捷键</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>:map ctrl+v+p I#<esc></esc></td><td>按ctrl+v+p则会在行首加#注释</td></tr><tr><td>:map ctrl+v+H iHello World !<esc></esc></td><td>按ctrl+v+H会插入Hello World !</td></tr></tbody></table></div><h2 id="连续行注释"><a href="#连续行注释" class="headerlink" title="连续行注释"></a><font size="4">连续行注释</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>:n1,n2s/^/xxx/g</td><td>在n1到n2行的行首加xxx</td></tr><tr><td>:n1,n2s/^xxx//g</td><td>将n1到n2行行首的xxx删去</td></tr><tr><td>:n1,n2s/^/\/\//g</td><td>在n1到n2行的行首加//，\/代表/</td></tr></tbody></table></div><h2 id="保存和退出"><a href="#保存和退出" class="headerlink" title="保存和退出"></a><font size="4">保存和退出</font></h2><p>  <font size="3">在命令模式下</font></p><div class="table-container"><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>:w</td><td>保存修改</td></tr><tr><td>:w filename</td><td>另存为filename文件</td></tr><tr><td>:Wq或:wq!或ZZ</td><td>保存修改并退出</td></tr><tr><td>:q!</td><td>不保存修改并退出</td></tr></tbody></table></div><h1 id="Vim小结"><a href="#Vim小结" class="headerlink" title="Vim小结"></a><font size="5" color="red">Vim小结</font></h1><p>  Vim作为一款古老的文本编辑器，但是它具有许多有用的功能，使其可以与现代文本编辑器竞争，也是作为Linux系统自带的文本编辑器Vi的升级版，可以完全解放双手，对于程序员来说，Vim是必不可少的技能之一。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Vim&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="Computer Science" scheme="https://USTCcoder.github.io/categories/Computer-Science/"/>
    
      <category term="Vim" scheme="https://USTCcoder.github.io/categories/Computer-Science/Vim/"/>
    
    
  </entry>
  
  <entry>
    <title>Markdown</title>
    <link href="https://USTCcoder.github.io/2019/09/21/skill%20Markdown/"/>
    <id>https://USTCcoder.github.io/2019/09/21/skill Markdown/</id>
    <published>2019-09-21T06:58:40.000Z</published>
    <updated>2020-05-20T06:57:28.317Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Markdown</font></strong></center><p></p><h1 id="Markdown介绍"><a href="#Markdown介绍" class="headerlink" title="Markdown介绍"></a><font size="5" color="red">Markdown介绍</font></h1><p>  <strong>Markdown</strong>:是一种可以使用<strong>普通文本编辑器</strong>编写的<strong>标记语言</strong>，通过简单的<strong>标记语法</strong>，它可以使普通文本内容<strong>具有一定的格式</strong>。<br><a id="more"></a></p><h1 id="Markdown语法"><a href="#Markdown语法" class="headerlink" title="Markdown语法"></a><font size="5" color="red">Markdown语法</font></h1><h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a><font size="4">标题</font></h2><p>  <font size="3">用#表示标题，几级标题对应几个#，最多6个，注意#和标题直接有空格</font><br>  <font size="3"># 一级标题</font><br>  <font size="3">## 二级标题</font><br>  <font size="3">### 三级标题</font><br>  <font size="3">#### 四级标题</font><br>  <font size="3">##### 五级标题</font><br>  <font size="3">###### 六级标题</font><br><img src="/images/SKILL/markdown1.png" alt="1"></p><h2 id="字体"><a href="#字体" class="headerlink" title="字体"></a><font size="4">字体</font></h2><div class="table-container"><table><thead><tr><th>符号</th><th>效果</th></tr></thead><tbody><tr><td>*斜体文本*</td><td><em>斜体文本</em></td></tr><tr><td>_斜体文本_</td><td><em>斜体文本</em></td></tr><tr><td>**粗体文本**</td><td><strong>粗体文本</strong></td></tr><tr><td>__粗体文本__</td><td><strong>粗体文本</strong></td></tr><tr><td>***粗斜体文本***</td><td><strong><em>粗斜体文本</em></strong></td></tr><tr><td>___粗斜体文本___</td><td><strong><em>粗斜体文本</em></strong></td></tr></tbody></table></div><h2 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a><font size="4">分割线</font></h2><p>  <font size="3">***</font></p><hr><p>  <font size="3">星号中间可以有空格也可以没有空格</font></p><p>  <font size="3">- - -</font></p><hr><p>  <font size="3">减号中间必须有空格</font></p><h2 id="删除线"><a href="#删除线" class="headerlink" title="删除线"></a><font size="4">删除线</font></h2><p>  <font size="3">&lt;u&gt;这是一条删除线&lt;\u&gt;</font><br>  <u>这是一条删除线</u></p><h2 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a><font size="4">脚注</font></h2><p>  <font size="3">用中括号创建[脚注]。</font><br>  <font size="3">[脚注]: 脚注中的内容</font></p><p>  用中括号创建<a href="脚注中的内容">脚注</a>。</p><h2 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a><font size="4">无序列表</font></h2><p>  <font size="3">无序列表使用*或+或-作为列表标记</font><br>  <font size="3">符号和内容之间要用空格分开</font></p><p>  - 第一项<br>  * 第二项<br>  + 第三项 </p><ul><li>第一项</li></ul><ul><li>第二项</li></ul><ul><li>第三项 </li></ul><h2 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a><font size="4">有序列表</font></h2><p>  <font size="3">无序列表使用数字加上.作为列表标记</font><br>  <font size="3">符号和内容之间要用空格分开</font></p><ol><li>第一项</li><li>第二项</li><li>第三项 </li></ol><h2 id="列表嵌套"><a href="#列表嵌套" class="headerlink" title="列表嵌套"></a><font size="4">列表嵌套</font></h2><p>  <font size="3">列表嵌套只要在子列表中添加四个空格即为下一层列表</font></p><p>  1. 第一层：<br>      - 第一层的第一个内容<br>      - 第一层的第二个内容<br>  2. 第二层：<br>      - 第二层的第一个内容<br>      - 第二层的第二个内容</p><ol><li>第一层：<ul><li>第一层的第一个内容</li><li>第一层的第二个内容</li></ul></li><li>第二层：<ul><li>第二层的第一个内容</li><li>第二层的第二个内容</li></ul></li></ol><h2 id="区块"><a href="#区块" class="headerlink" title="区块"></a><font size="4">区块</font></h2><p>  <font size="3">区块使用大于号&gt;作为标记</font><br>  <font size="3">符号和内容之间要用空格分开</font></p><p>  &gt; 这是一个区块<br>  &gt; &gt; 这是一个子区块</p><blockquote><p>这是一个区块</p><blockquote><p>这是一个子区块</p></blockquote></blockquote><h2 id="代码引用"><a href="#代码引用" class="headerlink" title="代码引用"></a><font size="4">代码引用</font></h2><p>  <font size="3">代码引用使用反引号`作为标记</font><br>  <font size="3">还可以使用反引号```大段代码```</font></p><p>  `print(‘Hello Markdown’)`<br>  <code>print('Hello Markdown')</code></p><p>  ```<br>  def my_print():<br>      print(‘Hello’)<br>      print(‘Markdown’)<br>  ```<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def my_print():</span><br><span class="line">    print('Hello')</span><br><span class="line">    print('Markdown')</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a><font size="4">链接</font></h2><p>  <font size="3">引用链接的格式为：[链接名称](链接地址)</font></p><p>  这是一个搜索引擎[百度](www.baidu.com)</p><p>  这是一个搜索引擎<a href="www.baidu.com">百度</a></p><h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a><font size="4">图片</font></h2><p>  <font size="3">放置图片的格式为：![图片文字](图片地址)</font></p><p>  ![本地图片](/images/SKILL/markdown1.jpg)<br><img src="/images/SKILL/markdown.jpg" alt="本地图片"></p><h2 id="表格"><a href="#表格" class="headerlink" title="表格"></a><font size="4">表格</font></h2><p>  <font size="3">表格的格式为：</font><br>  <font size="3">| 表头1 | 表头2 |</font><br>  <font size="3">| --- | --- |</font><br>  <font size="3">| 单元格1 | 单元格2 |</font><br>  <font size="3">| 单元格3 | 单元格4 |</font></p><div class="table-container"><table><thead><tr><th>表头1</th><th>表头2</th></tr></thead><tbody><tr><td>单元格1</td><td>单元格2</td></tr><tr><td>单元格3</td><td>单元格4</td></tr></tbody></table></div><h2 id="HTML元素"><a href="#HTML元素" class="headerlink" title="HTML元素"></a><font size="4">HTML元素</font></h2><p>  <font size="3">Markdown支持很多HTML元素，不逐一介绍，感兴趣可以查阅HTML元素</font></p><p>  5&lt;sup&gt;2&lt;/sup&gt; + x&lt;sub&gt;n&lt;/sub&gt;</p><p>  5<sup>2</sup> + x<sub>n</sub></p><h2 id="公式"><a href="#公式" class="headerlink" title="公式"></a><font size="4">公式</font></h2><p>  <font size="3">Markdown使用TeX或LaTeX格式的数学公式来实现，会根据需要加载 Mathjax 对数学公式进行渲染。</font></p><p>  <font size="3">Markdown在公式两端加上$输入文中公式</font></p><p>  平方和公式：$(a + b)^2 = a^2 + 2ab + b^2$</p><p>  <font size="3">Markdown在公式两端加上$$另起一行输入公式</font></p><p>  平方差公式：</p><script type="math/tex; mode=display">(a - b)^2 = a^2 - 2ab + b^2</script><h2 id="转义字符"><a href="#转义字符" class="headerlink" title="转义字符"></a><font size="4">转义字符</font></h2><p>  <font size="3">绝大多数字符都可以用\转义，但是下表字符要用指定的编号转义</font></p><div class="table-container"><table><thead><tr><th>名称</th><th>符号</th><th>效果</th></tr></thead><tbody><tr><td>空格</td><td>&amp;nbsp;</td><td>&nbsp;</td></tr><tr><td>小于号</td><td>&amp;lt;</td><td>&lt;</td></tr><tr><td>大于号</td><td>&amp;gt;</td><td>&gt;</td></tr><tr><td>与符号</td><td>&amp;amp;</td><td>&amp;</td></tr><tr><td>单引号</td><td>&amp;apos;</td><td>'</td></tr><tr><td>双引号</td><td>&amp;quot;</td><td>"</td></tr></tbody></table></div><h1 id="Markdown小结"><a href="#Markdown小结" class="headerlink" title="Markdown小结"></a><font size="5" color="red">Markdown小结</font></h1><p>  Markdown的语法简洁明了，学习容易，而且功能强大，因此很多人用它写博客，我的所有博客都是采用Markdown来写的，也是为了查询的方便，故写下了这篇文字。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Markdown&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="Computer Science" scheme="https://USTCcoder.github.io/categories/Computer-Science/"/>
    
      <category term="Markdown" scheme="https://USTCcoder.github.io/categories/Computer-Science/Markdown/"/>
    
    
  </entry>
  
  <entry>
    <title>LaTeX</title>
    <link href="https://USTCcoder.github.io/2019/09/20/skill%20Latex/"/>
    <id>https://USTCcoder.github.io/2019/09/20/skill Latex/</id>
    <published>2019-09-20T04:19:40.000Z</published>
    <updated>2020-05-24T01:44:29.352Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">LaTeX</font></strong></center><p></p><h1 id="LaTeX介绍"><a href="#LaTeX介绍" class="headerlink" title="LaTeX介绍"></a><font size="5" color="red">LaTeX介绍</font></h1><p>  <strong>LaTeX</strong>:是一种基于<strong>TEX的排版系统</strong>，由美国计算机学家莱斯利·兰伯特(Leslie Lamport)在20世纪80年代初期开发，利用这种格式，对于<strong>生成复杂表格和数学公式</strong>，表现得尤为突出。<br><a id="more"></a></p><h1 id="LaTeX数学公式"><a href="#LaTeX数学公式" class="headerlink" title="LaTeX数学公式"></a><font size="5" color="red">LaTeX数学公式</font></h1><h2 id="格式说明"><a href="#格式说明" class="headerlink" title="格式说明"></a><font size="4">格式说明</font></h2><p><img src="/images/SKILL/latex1.png" alt="LaTeX"></p><h2 id="常见希腊字母"><a href="#常见希腊字母" class="headerlink" title="常见希腊字母"></a><font size="4">常见希腊字母</font></h2><p><img src="/images/SKILL/latex2.png" alt="LaTeX"></p><h2 id="顶部符号"><a href="#顶部符号" class="headerlink" title="顶部符号"></a><font size="4">顶部符号</font></h2><p><img src="/images/SKILL/latex3.png" alt="LaTeX"></p><h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a><font size="4">常用函数</font></h2><p><img src="/images/SKILL/latex4.png" alt="LaTeX"></p><h2 id="算术运算"><a href="#算术运算" class="headerlink" title="算术运算"></a><font size="4">算术运算</font></h2><p><img src="/images/SKILL/latex5.png" alt="LaTeX"></p><h2 id="微分运算"><a href="#微分运算" class="headerlink" title="微分运算"></a><font size="4">微分运算</font></h2><p><img src="/images/SKILL/latex6.png" alt="LaTeX"></p><h2 id="关系运算"><a href="#关系运算" class="headerlink" title="关系运算"></a><font size="4">关系运算</font></h2><p><img src="/images/SKILL/latex7.png" alt="LaTeX"></p><h2 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a><font size="4">逻辑运算</font></h2><p><img src="/images/SKILL/latex8.png" alt="LaTeX"></p><h2 id="集合运算"><a href="#集合运算" class="headerlink" title="集合运算"></a><font size="4">集合运算</font></h2><p><img src="/images/SKILL/latex9.png" alt="LaTeX"></p><h2 id="特殊符号"><a href="#特殊符号" class="headerlink" title="特殊符号"></a><font size="4">特殊符号</font></h2><p><img src="/images/SKILL/latex10.png" alt="LaTeX"></p><h2 id="空格"><a href="#空格" class="headerlink" title="空格"></a><font size="4">空格</font></h2><p><img src="/images/SKILL/latex11.png" alt="LaTeX"></p><h2 id="字体颜色"><a href="#字体颜色" class="headerlink" title="字体颜色"></a><font size="4">字体颜色</font></h2><p><img src="/images/SKILL/latex12.png" alt="LaTeX"></p><h2 id="常用数学符号"><a href="#常用数学符号" class="headerlink" title="常用数学符号"></a><font size="4">常用数学符号</font></h2><p><img src="/images/SKILL/latex13.png" alt="LaTeX"></p><p><img src="/images/SKILL/latex14.png" alt="LaTeX"></p><h2 id="多行符号"><a href="#多行符号" class="headerlink" title="多行符号"></a><font size="4">多行符号</font></h2><p><img src="/images/SKILL/latex15.png" alt="LaTeX"></p><h1 id="LaTeX小结"><a href="#LaTeX小结" class="headerlink" title="LaTeX小结"></a><font size="5" color="red">LaTeX小结</font></h1><p>  LaTeX作为当下流行的排版系统，许多文档都是以LaTeX排版的，对于公式较多的学术文档，LaTeX是必不可少的。此网页上的所有公式都是使用LaTeX编辑的，也是为了查询的方便，故写下了这篇文字。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;LaTeX&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="Computer Science" scheme="https://USTCcoder.github.io/categories/Computer-Science/"/>
    
      <category term="LaTeX" scheme="https://USTCcoder.github.io/categories/Computer-Science/LaTeX/"/>
    
    
  </entry>
  
  <entry>
    <title>Object-Oriented(面向对象)</title>
    <link href="https://USTCcoder.github.io/2019/09/18/python_class/"/>
    <id>https://USTCcoder.github.io/2019/09/18/python_class/</id>
    <published>2019-09-18T08:14:15.000Z</published>
    <updated>2020-07-27T06:16:00.031Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python14.jpg" alt="14"></p><h1 id="Object-Oriented介绍"><a href="#Object-Oriented介绍" class="headerlink" title="Object-Oriented介绍"></a><font size="5" color="red">Object-Oriented介绍</font></h1><p>  面向对象是一种非常重要的编程思想，把数据和操作放在一起，作为一个整体，称为对象。Python的面向对象和C++类似，也具有面向对象的三大特点，封装，继承和多态。<br><a id="more"></a></p><h1 id="面向过程与面向对象"><a href="#面向过程与面向对象" class="headerlink" title="面向过程与面向对象"></a><font size="5" color="red">面向过程与面向对象</font></h1><h2 id="面向过程的编程思想"><a href="#面向过程的编程思想" class="headerlink" title="面向过程的编程思想"></a><font size="4">面向过程的编程思想</font></h2><p>  <font size="3">自上而下顺序执行，逐步求精。</font><br>  <font size="3">程序结构按照功能分为若干模块，各部分相对独立。</font><br>  <font size="3">每一模块内部均是由顺序，选择，循环三种基本结构。</font><br>  <font size="3">程序流程在写程序时就已经确定。</font></p><h2 id="面向对象的编程思想"><a href="#面向对象的编程思想" class="headerlink" title="面向对象的编程思想"></a><font size="4">面向对象的编程思想</font></h2><p>  <font size="3">把数据和操作放在一起，作为一个整体，称为对象。</font><br>  <font size="3">对同类对象抽象出其共性，形成类。</font><br>  <font size="3">类通过一个外部接口与外界操作。</font><br>  <font size="3">程序流程在用户使用时决定。</font></p><h1 id="Python面向对象应用"><a href="#Python面向对象应用" class="headerlink" title="Python面向对象应用"></a><font size="5" color="red">Python面向对象应用</font></h1><h2 id="Python创建类"><a href="#Python创建类" class="headerlink" title="Python创建类"></a><font size="4">Python创建类</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 类就是一种物体的总称，具有属性(成员变量)和行为(成员方法)两个特征。其本身并不占用内存空间，其实例化的对象占内存空间。</span><br><span class="line"># class 类名(父类): </span><br><span class="line">#     属性</span><br><span class="line">#     行为</span><br><span class="line"></span><br><span class="line"># 类名一般首字母大写，object是所有类的父类，一般没有合适的父类就写object。</span><br><span class="line">class Person(object):</span><br><span class="line"># 定义属性</span><br><span class="line">    name = ''</span><br><span class="line">    age = 0</span><br><span class="line">    height = 0</span><br><span class="line">    weight = 0</span><br><span class="line"></span><br><span class="line"># 定义行为，注意成员方法的参数一般以self当作第一个参数(可以为其他的单词，但是几乎都使用self)，其中self就代表类的实例，哪个对象调用方法哪个对象就是self，类似于C/C++中的this指针。</span><br><span class="line">    def eat(self, food):</span><br><span class="line">        print('eat' + food)</span><br><span class="line"></span><br><span class="line">    def sleep(self):</span><br><span class="line">        print('I need sleep')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python88.png" alt="88"></p><h2 id="Python实例化对象"><a href="#Python实例化对象" class="headerlink" title="Python实例化对象"></a><font size="4">Python实例化对象</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class Person(object):</span><br><span class="line">    name = ''</span><br><span class="line">    age = 0</span><br><span class="line">    height = 0</span><br><span class="line">    weight = 0</span><br><span class="line"></span><br><span class="line">    def eat(self, food):</span><br><span class="line">        print('eat' + food)</span><br><span class="line"></span><br><span class="line">    def sleep(self):</span><br><span class="line">        print('I need sleep')</span><br><span class="line"></span><br><span class="line"># 类似于女娲造人一样，有了人型模具，女娲可以按照模具创建人类对象</span><br><span class="line"># 对象名 = 类名(参数列表) 用类名实例化对象，注意如果没有参数，也不能省略括号。</span><br><span class="line">per = Person()</span><br><span class="line">print(per)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python89.png" alt="89"></p><h2 id="Python访问对象属性"><a href="#Python访问对象属性" class="headerlink" title="Python访问对象属性"></a><font size="4">Python访问对象属性</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class Person(object):</span><br><span class="line">    name = ''</span><br><span class="line">    age = 0</span><br><span class="line">    height = 0</span><br><span class="line">    weight = 0</span><br><span class="line"></span><br><span class="line">    def eat(self, food):</span><br><span class="line">        print('eat ' + food)</span><br><span class="line"></span><br><span class="line">    def sleep(self):</span><br><span class="line">        print('I need sleep')</span><br><span class="line"></span><br><span class="line"># 对象名.属性名 访问该对象的某一个属性，也可以通过这种方法对该对象的某一属性赋值</span><br><span class="line">per = Person()</span><br><span class="line">print(per.name + '的年龄为：' + str(per.age))</span><br><span class="line">per.name, per.age = '张三', 18</span><br><span class="line">print(per.name + '的年龄为：' + str(per.age))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python90.png" alt="90"></p><h2 id="Python访问对象方法"><a href="#Python访问对象方法" class="headerlink" title="Python访问对象方法"></a><font size="4">Python访问对象方法</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class Person(object):</span><br><span class="line">    name = ''</span><br><span class="line">    age = 0</span><br><span class="line">    height = 0</span><br><span class="line">    weight = 0</span><br><span class="line"></span><br><span class="line">    def eat(self, food):</span><br><span class="line">        print('eat ' + food)</span><br><span class="line"></span><br><span class="line">    def sleep(self):</span><br><span class="line">        print('I need sleep')</span><br><span class="line"></span><br><span class="line"># 对象名.方法名(参数列表) 访问该对象的某一个方法，注意self参数不需要传值</span><br><span class="line">per = Person()</span><br><span class="line">per.eat('apple')</span><br><span class="line">per.sleep()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python91.png" alt="91"></p><h2 id="Python类的构造函数"><a href="#Python类的构造函数" class="headerlink" title="Python类的构造函数"></a><font size="4">Python类的构造函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># def __init__(self, 参数列表): 构造函数在使用类创建对象的时候自动调用，如果不显示的写出构造函数，默认自动添加一个空的构造函数</span><br><span class="line">class Person(object):</span><br><span class="line">    def __init__(self, name, age, height, weight):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.height = height</span><br><span class="line">        self.weight = weight</span><br><span class="line"></span><br><span class="line">    def eat(self, food):</span><br><span class="line">        print('eat ' + food)</span><br><span class="line"></span><br><span class="line">    def sleep(self):</span><br><span class="line">        print('I need sleep')</span><br><span class="line"></span><br><span class="line"># 类名实例化对象时给对象的属性赋值</span><br><span class="line">per = Person('李四', 20, 180, 140)</span><br><span class="line">print(per.name + '的年龄为：' + str(per.age))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python92.png" alt="92"></p><h2 id="Python类的析构函数"><a href="#Python类的析构函数" class="headerlink" title="Python类的析构函数"></a><font size="4">Python类的析构函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># def __del__(self): 析构函数在释放对象的时候自动调用，如果不显示的写出析构函数，默认自动添加一个空的析构函数</span><br><span class="line">class Person(object):</span><br><span class="line">    def __del__(self):</span><br><span class="line">        print('Destroy the object')</span><br><span class="line"></span><br><span class="line">    def eat(self, food):</span><br><span class="line">        print('eat ' + food)</span><br><span class="line"></span><br><span class="line">    def sleep(self):</span><br><span class="line">        print('I need sleep')</span><br><span class="line"></span><br><span class="line"># 类名实例化对象时给对象的属性赋值</span><br><span class="line">per = Person()</span><br><span class="line">del per</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python93.png" alt="93"></p><h2 id="Python中类属性和对象属性"><a href="#Python中类属性和对象属性" class="headerlink" title="Python中类属性和对象属性"></a><font size="4">Python中类属性和对象属性</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class Person(object):</span><br><span class="line">    talent = None</span><br><span class="line"></span><br><span class="line">    def __init__(self, name, age):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line"></span><br><span class="line"># 类属性是写在类内部的属性，而对象属性是用self定义或者在外部动态定义的。</span><br><span class="line"># 对象属性的优先级高于类属性，如果一个对象没有对象属性，则使用其类属性，如果添加了对象属性，则使用其对象属性。</span><br><span class="line"># 在此代码中talent属于类属性，可以通过类名.属性调用，name和age属于对象属性，通过对象名.属性调用。</span><br><span class="line"># 注意：尽量不要将对象属性与类属性同名，因为对象属性会屏蔽类属性，当删除对象属性后，又能使用类属性了。</span><br><span class="line">per = Person('张三', 18)</span><br><span class="line">print(Person.talent)</span><br><span class="line">print(per.talent)</span><br><span class="line">per.talent = 'Python'</span><br><span class="line">print(Person.talent)</span><br><span class="line">print(per.talent)</span><br><span class="line"></span><br><span class="line"># 类方法和对象方法类似，类方法是在类内部的方法，而对象方式是在外部动态定义的方法，一般不常用，作为了解即可。</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python100.png" alt="100"></p><h2 id="Python类打印函数"><a href="#Python类打印函数" class="headerlink" title="Python类打印函数"></a><font size="4">Python类打印函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># def __str__(self): 在调用print打印对象时自动调用，是一个给用户使用的描述对象的方法，如果不显示的写出，默认返回类的名称和所处的内存地址</span><br><span class="line">class Person(object):</span><br><span class="line">    def __init__(self, name, age, height, weight):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.height = height</span><br><span class="line">        self.weight = weight</span><br><span class="line"> </span><br><span class="line">    def __str__(self):</span><br><span class="line">        return "姓名：%s  年龄：%d，身高：%.1f，体重：%.1f" %(self.name, self.age, self.height, self.weight)</span><br><span class="line"></span><br><span class="line">per = Person('王五', 21, 178, 160)</span><br><span class="line">print(per)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python94.png" alt="94"></p><h2 id="Python动态语言"><a href="#Python动态语言" class="headerlink" title="Python动态语言"></a><font size="4">Python动态语言</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class Person(object):</span><br><span class="line">    def __init__(self, name, age, height, weight):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.height = height</span><br><span class="line">        self.weight = weight</span><br><span class="line"></span><br><span class="line">per = Person('赵六', 22, 186, 170)</span><br><span class="line">print(per.name + '的存款为：' + str(per.money))</span><br><span class="line"></span><br><span class="line"># per本来没有money属性，可以动态添加money属性</span><br><span class="line">per.money = 10000</span><br><span class="line">print(per.name + '的存款为：' + str(per.money))</span><br><span class="line"></span><br><span class="line"># 还可以动态添加方法，需要从types中导入Method类</span><br><span class="line">from types import MethodType </span><br><span class="line"></span><br><span class="line">def say(self):</span><br><span class="line">    print('my name is ' + self.name)</span><br><span class="line"></span><br><span class="line">per.say()</span><br><span class="line"></span><br><span class="line"># 对象名.方法名 = MethodType(添加的函数名, 对象名)</span><br><span class="line">per.say = MethodType(say, per)</span><br><span class="line">per.say()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python95.png" alt="95"></p><h2 id="Python中动态添加限制"><a href="#Python中动态添加限制" class="headerlink" title="Python中动态添加限制"></a><font size="4">Python中动态添加限制</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from types import MethodType </span><br><span class="line"></span><br><span class="line"># 通过定义__slots__ = (成员属性或者成员方法名) 使对象中的成员必须存在于元组之中，可以限制对象随意动态添加成员</span><br><span class="line">class Person(object):</span><br><span class="line">    __slots__ = ('name', 'age', 'money')</span><br><span class="line"></span><br><span class="line">    def __init__(self, name, age):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line"></span><br><span class="line">per = Person('赵六', 22)</span><br><span class="line">per.money = 10000</span><br><span class="line">print(per.name + '的存款为：' + str(per.money))</span><br><span class="line">per.weight = 160</span><br><span class="line"></span><br><span class="line">def say(self):</span><br><span class="line">    print('my name is ' + self.name)</span><br><span class="line"></span><br><span class="line">per.say = MethodType(say, per)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python101.png" alt="101"></p><h2 id="Python类中的共有和私有"><a href="#Python类中的共有和私有" class="headerlink" title="Python类中的共有和私有"></a><font size="4">Python类中的共有和私有</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># __属性名/函数名 如果想让内部成员不被外部直接访问，在成员属性或方法前加__(两个下划线)，但是在内部是可以使用的</span><br><span class="line"># 如果想修改其值只能通过自定义一个函数，实现对某些成员变量进行修改操作，这样也达到了一种保护作用</span><br><span class="line">class Person(object):</span><br><span class="line">    def __init__(self, name, age, height, weight):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.height = height</span><br><span class="line">        self.__weight = weight</span><br><span class="line"></span><br><span class="line">    def set_weight(self, weight):</span><br><span class="line">        self.__weight = weight</span><br><span class="line"></span><br><span class="line">    def enquire(self):</span><br><span class="line">        print('体重为：' + str(self.__weight))</span><br><span class="line"></span><br><span class="line">per = Person('王五', 21, 178, 160)</span><br><span class="line">per.enquire()</span><br><span class="line">print(per.__weight)</span><br><span class="line">per.set_weight(150)</span><br><span class="line">per.enquire()</span><br><span class="line"></span><br><span class="line"># 原因是Python解释器将__成员变成了_(一个下划线)类名__(两个下划线)成员名(即在此将__weight改成了_Person__weight)，因此不是绝对私有的</span><br><span class="line">print(per._Person__weight)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python96.png" alt="96"></p><h2 id="Python类中的-property装饰器"><a href="#Python类中的-property装饰器" class="headerlink" title="Python类中的@property装饰器"></a><font size="4">Python类中的@property装饰器</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># 对私有成员想要便捷的访问和修改可以使用@property装饰器，相当于调用了get和set方法，可以让受限制的成员也能够直接使用.语法</span><br><span class="line"># @property </span><br><span class="line"># def 变量名(self):</span><br><span class="line">#     return self.__变量名</span><br><span class="line"># @变量名.setter</span><br><span class="line"># def 变量名(self, 变量名):</span><br><span class="line">#    xxx</span><br><span class="line"># 以上变量名都是未加下划线的变量名，@property下面的内容相当于get方法，@变量名.setter下面的内容相当于set方法。</span><br><span class="line"></span><br><span class="line">class Person(object):</span><br><span class="line">    def __init__(self, name, age, height, weight):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.height = height</span><br><span class="line">        self.__weight = weight</span><br><span class="line"></span><br><span class="line">    def set_weight(self, weight):</span><br><span class="line">        self.__weight = weight</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def weight(self):</span><br><span class="line">        return self.__weight</span><br><span class="line">    @weight.setter</span><br><span class="line">    def weight(self, weight):</span><br><span class="line">        self.__weight = weight</span><br><span class="line"></span><br><span class="line">per = Person('王五', 21, 178, 160)</span><br><span class="line"></span><br><span class="line"># 调用时也是直接调用对象名.原名即可，不需要加双下划线__</span><br><span class="line">print(per.weight)</span><br><span class="line">per.weight = 150</span><br><span class="line">print(per.weight)</span><br><span class="line"></span><br><span class="line"># 原因是Python解释器将__成员变成了_(一个下划线)类名__(两个下划线)成员名(即在此将__weight改成了_Person__weight)，因此不是绝对私有的</span><br><span class="line">print(per._Person__weight)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python96.png" alt="96"></p><h2 id="Python类的单继承"><a href="#Python类的单继承" class="headerlink" title="Python类的单继承"></a><font size="4">Python类的单继承</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">class Person(object):</span><br><span class="line">    def __init__(self, name, age, height, weight):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.height = height</span><br><span class="line">        self.weight = weight</span><br><span class="line"></span><br><span class="line">    def eat(self, food):</span><br><span class="line">        print('eat ' + food)</span><br><span class="line"></span><br><span class="line">    def sleep(self):</span><br><span class="line">        print('I need sleep')</span><br><span class="line"></span><br><span class="line"># 对于世间万物，大多数都有这一般和特殊的关系，如人和程序员之间的关系，程序员继承了人的所有特点，但是又有一些特殊的特点。将人这个类别称为子类(基类)，将程序员这个类别称为父类(超类)。</span><br><span class="line"># 所有的类都是继承于object类，继承可以大大简化代码，提高代码的健壮性和安全性。</span><br><span class="line"># class 类名(父类): 子类继承父类，需要在类名后面的括号中写入父类名，继承时调用父类的__init__只需要写super(子类名, self).__init__(参数列表)即可，参数列表中不需要写self</span><br><span class="line"># 注意父类的私有成员子类可以继承过来，但是无法直接使用，只能通过父类的自定义函数访问。</span><br><span class="line"># 子类特有的成员和之前定义普通类时相同</span><br><span class="line">class Programmer(Person):</span><br><span class="line">    def __init__(self, name, age, height, weight, language):</span><br><span class="line">        super(Programmer, self).__init__(name, age, height, weight)</span><br><span class="line">        self.language = language</span><br><span class="line"></span><br><span class="line">pro = Programmer('钱七', 23, 181, 135, 'Python')</span><br><span class="line">pro.eat('watermelon')</span><br><span class="line">print(pro.language)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python97.png" alt="97"></p><h2 id="Python类的多继承"><a href="#Python类的多继承" class="headerlink" title="Python类的多继承"></a><font size="4">Python类的多继承</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">class Father(object):</span><br><span class="line">    def __init__(self, name, age, talent):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.talent = talent</span><br><span class="line"></span><br><span class="line">    def play(self):</span><br><span class="line">        print('play computer games')</span><br><span class="line"></span><br><span class="line">    def sleep(self):</span><br><span class="line">        print('Father need sleep')</span><br><span class="line"></span><br><span class="line">class Mother(object):</span><br><span class="line">    def __init__(self, name, age, beauty):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.beauty = beauty</span><br><span class="line"></span><br><span class="line">    def shopping(self):</span><br><span class="line">        print('go shopping')</span><br><span class="line"></span><br><span class="line">    def sleep(self):</span><br><span class="line">        print('Mother need sleep')</span><br><span class="line"></span><br><span class="line"># 除了单继承外也有多继承的情况，比如遗传就是一种典型的多继承，孩子要继承父亲和母亲的特点</span><br><span class="line"># class 类名(父类): 子类继承父类，如果需要多继承，则在类名后面的括号中写入多个父类名</span><br><span class="line"># 继承时构造函数中调用父类的__init__，只需要写父类名.__init__(self, 参数列表)即可使用，注意要写self</span><br><span class="line"># 注意父类中方法名相同，默认调用的是在括号中排在前面的父类中的方法</span><br><span class="line">class Child(Father, Mother):</span><br><span class="line">    def __init__(self, name, age, talent, beauty):</span><br><span class="line">        Father.__init__(self, name, age, talent)</span><br><span class="line">        Mother.__init__(self, name, age, beauty)</span><br><span class="line"></span><br><span class="line">chi = Child('辛巴', 22, 'Python', 99)</span><br><span class="line">print(chi.name + '的年龄为：' + str(chi.age))</span><br><span class="line">print('我的才能是：' + chi.talent)</span><br><span class="line">print('我的颜值是：' + str(chi.beauty))</span><br><span class="line">chi.play()</span><br><span class="line">chi.shopping()</span><br><span class="line">chi.sleep()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python98.png" alt="98"></p><h2 id="Python类的多态"><a href="#Python类的多态" class="headerlink" title="Python类的多态"></a><font size="4">Python类的多态</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">class Animal(object):</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    def eat(self, food):</span><br><span class="line">        print(self.name + ' eat ' + food)</span><br><span class="line"></span><br><span class="line">class Cat(Animal):</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        super(Cat, self).__init__(name)</span><br><span class="line"></span><br><span class="line">class Mouse(Animal):</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        super(Mouse, self).__init__(name)</span><br><span class="line"></span><br><span class="line">class Person(object):</span><br><span class="line">    def feed(self, animal, food):</span><br><span class="line">        animal.eat(food)</span><br><span class="line"></span><br><span class="line"># 对于继承自同一类的多个类具有多态的性质，即子类有多种表现形态</span><br><span class="line"># 此代码中动物类就是父类，猫类和老鼠类都继承自动物类，所以猫类和鼠类都有父类的方法</span><br><span class="line"># 因此传入不同的子类对象具有不同的表现形态</span><br><span class="line"></span><br><span class="line">per = Person()</span><br><span class="line">tom = Cat('Tom')</span><br><span class="line">jerry = Mouse('Jerry')</span><br><span class="line">per.feed(tom, 'fish')</span><br><span class="line">per.feed(jerry, 'rice')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python99.png" alt="99"></p><h2 id="Python运算符重载"><a href="#Python运算符重载" class="headerlink" title="Python运算符重载"></a><font size="4">Python运算符重载</font></h2><p><img src="/images/LANGUAGE/python109.png" alt="109"><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 为了使运算方便，有时需要进行运算符重载，典型的就是字符串的相加，1 + 2 = 3，但是字符串相加就是字符串的连接。</span><br><span class="line"># 说明对字符串的加法进行了重新定义，使其可以完成相加操作。同理，对自己写的类也可以进行运算符重载，使两个类可以做运算。</span><br><span class="line"># 要重载什么操作就查操作对应的特殊函数，然后将其重载即可。</span><br><span class="line"></span><br><span class="line">class Person(object):</span><br><span class="line">    def __init__(self, money):</span><br><span class="line">        self.money = money</span><br><span class="line"></span><br><span class="line">    def __add__(self, other):</span><br><span class="line">        return Person(self.money + other.money)</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return 'money：' + str(self.money)</span><br><span class="line"></span><br><span class="line"># 此代码重载了加法运算符和打印操作</span><br><span class="line">per1 = Person(100)</span><br><span class="line">print(per1)</span><br><span class="line">per2 = Person(200)</span><br><span class="line">print(per2)</span><br><span class="line">per3 = per1 + per2</span><br><span class="line">print(per3)</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/LANGUAGE/python103.png" alt="103"></p><h1 id="Object-Oriented小结"><a href="#Object-Oriented小结" class="headerlink" title="Object-Oriented小结"></a><font size="5" color="red">Object-Oriented小结</font></h1><p>  Object-Oriented面向对象是计算机语言中一种重要的思想，它的出现将程序员从一个执行者变成了一个管理者，程序员在使用时只需要知道能做什么，而不需要知道具体如何实现，而且每次创建对象时只需要一行代码，调用某一函数时也不需要关心内部的结构，大大减少了阅读代码的时间和内存的占用情况。因此面向过程是程序员的必经之路，需要熟练的掌握。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Object-Oriented(面向对象)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Import(导入模块)</title>
    <link href="https://USTCcoder.github.io/2019/09/17/python_import/"/>
    <id>https://USTCcoder.github.io/2019/09/17/python_import/</id>
    <published>2019-09-17T08:28:45.000Z</published>
    <updated>2020-07-27T06:14:50.461Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python13.jpg" alt="13"></p><h1 id="Import介绍"><a href="#Import介绍" class="headerlink" title="Import介绍"></a><font size="5" color="red">Import介绍</font></h1><p>  Python中使用Import导入模块，类似于C/C++中的include，但是使用起来更加灵活和方便，可以导入整个模块或者导入模块的某一部分。<br><a id="more"></a></p><h1 id="Import应用"><a href="#Import应用" class="headerlink" title="Import应用"></a><font size="5" color="red">Import应用</font></h1><h2 id="导入路径"><a href="#导入路径" class="headerlink" title="导入路径"></a><font size="4">导入路径</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line"># sys.path 查询导包时的路径顺序，当有相同名字的包出现时，则按照顺序查询是否在此路径，路径中如果含有两个文件夹即两个包，都含有相同名字的.py文件，当导入该.py文件时，需要写清楚包名.文件名</span><br><span class="line">print(sys.path)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python78.png" alt="78"></p><h2 id="安装第三方模块"><a href="#安装第三方模块" class="headerlink" title="安装第三方模块"></a><font size="4">安装第三方模块</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># 在虚拟环境中输入pip install xxx  安装xxx模块</span><br></pre></td></tr></tbody></table></figure><h2 id="import语句"><a href="#import语句" class="headerlink" title="import语句"></a><font size="4">import语句</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># import module1, module2, ... 一次性引入多个模块，使用模块时格式为：模块名.函数名/变量名</span><br><span class="line">import sys, time</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">end = time.clock()</span><br><span class="line">res = end - start</span><br><span class="line"></span><br><span class="line"># import module as module_name 一个模块名如果较长，书写不方便，可以将其改名为module_name</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([1, 2, 3])</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python79.png" alt="79"></p><h2 id="from-…-import语句"><a href="#from-…-import语句" class="headerlink" title="from … import语句"></a><font size="4">from … import语句</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># from module import name1, name2, ... 从module模块中引入一个指定的部分到当前命名空间，使用时不需要加模块名</span><br><span class="line">from numpy import array, arange</span><br><span class="line"></span><br><span class="line">a = array([1, 2, 3])</span><br><span class="line">b = arange(5) </span><br><span class="line"></span><br><span class="line"># from module import name as new_name 一个函数名或变量名如果较长，书写不方便，可以将其改名为new_name</span><br><span class="line">from numpy import linspace as lsp</span><br><span class="line"></span><br><span class="line">c = lsp(0, 10, 6)</span><br><span class="line"></span><br><span class="line"># from module import * 把module模块中所有的内容全部导入当前模块</span><br><span class="line">from numpy import *</span><br><span class="line"></span><br><span class="line">d = zeros((3, 3))</span><br><span class="line"></span><br><span class="line"># from .... import语句存在着危险性，如果下面定义了同名的函数，则会覆盖引入的函数</span><br><span class="line">from numpy import *</span><br><span class="line"></span><br><span class="line">def zeros(* par):</span><br><span class="line">    return 0</span><br><span class="line"></span><br><span class="line">e = zeros((3, 3))</span><br><span class="line"></span><br><span class="line"># 如果有一个.py文件需要被导入，也需要单独的运行此文件。但是如果直接导入该文件，会自动执行该文件，如果希望运行时不执行该文件需要在.py文件中写入</span><br><span class="line"># if __name__ == "__main__": 然后主程序程序写在下面，这样单独运行该文件时会执行主程序，被导入时不会执行主程序。</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python80.png" alt="80"></p><h1 id="Import小结"><a href="#Import小结" class="headerlink" title="Import小结"></a><font size="5" color="red">Import小结</font></h1><p>  Python之所以被称为胶水语言，主要归功于Import的强大功能。随着Python的火热，各个领域都为Python提供功能强大的接口，如计算机视觉领域有opencv库，机器学习领域有sklearn库，深度学习领域有TensorFlow, Torch库，数据分析领域有Numpy, Matplotlib库等等，这为Python的使用者提供非常大的便捷。而且在大型的工程应用中，往往需要写很多的子文件，也需要Import的帮助，因此要熟练掌握这些导入模块的应用。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Import(导入模块)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Function(函数)</title>
    <link href="https://USTCcoder.github.io/2019/09/16/python_function/"/>
    <id>https://USTCcoder.github.io/2019/09/16/python_function/</id>
    <published>2019-09-16T08:14:15.000Z</published>
    <updated>2020-07-27T06:15:21.963Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python10.jpg" alt="10"></p><h1 id="Function介绍"><a href="#Function介绍" class="headerlink" title="Function介绍"></a><font size="5" color="red">Function介绍</font></h1><p>  Python中的Function一种重要的调用方式，和C/C++类似，通过参数的传递和数据的返回完成所预期的目的。<br><a id="more"></a></p><h1 id="Python内建Function"><a href="#Python内建Function" class="headerlink" title="Python内建Function"></a><font size="5" color="red">Python内建Function</font></h1><h2 id="Python进制转换函数"><a href="#Python进制转换函数" class="headerlink" title="Python进制转换函数"></a><font size="4">Python进制转换函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = 100</span><br><span class="line"></span><br><span class="line"># bin(x) 对x用二进制字符串表示</span><br><span class="line">bin_a = bin(a)</span><br><span class="line"></span><br><span class="line"># oct(x) 对x用八进制字符串表示</span><br><span class="line">oct_a = oct(a)</span><br><span class="line"></span><br><span class="line"># hex(x) 对x用十六进制字符串表示</span><br><span class="line">hex_a = hex(a)</span><br><span class="line"></span><br><span class="line"># int(x, mode) 2 &lt;= mode &lt;= 36，将x转换为mode进制</span><br><span class="line">int(bin_a, 2)</span><br><span class="line">int(oct_a, 8)</span><br><span class="line">int(hex_a, 16)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python112.png" alt="112"></p><h2 id="Python求和函数，最大值最小值函数"><a href="#Python求和函数，最大值最小值函数" class="headerlink" title="Python求和函数，最大值最小值函数"></a><font size="4">Python求和函数，最大值最小值函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># sum(iterable) 对于列表，元组， 集合来说sum是指所有元素之和，前提是元素可以求和，对于字典来说sum是指关键字Key求和</span><br><span class="line">sum({1, 2, 3, 4})</span><br><span class="line">sum({1:11, 2:22, 3:33, 4:44})</span><br><span class="line"></span><br><span class="line"># max(iterable) 用法同sum，求元素的最大值</span><br><span class="line">max({1, 2, 3, 4})</span><br><span class="line">max({1:11, 2:22, 3:33, 4:44})</span><br><span class="line"></span><br><span class="line"># min(iterable) 用法同sum，求元素的最小值</span><br><span class="line">min({1, 2, 3, 4})</span><br><span class="line">min({1:11, 2:22, 3:33, 4:44})</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python60.png" alt="60"></p><h2 id="Python长度函数"><a href="#Python长度函数" class="headerlink" title="Python长度函数"></a><font size="4">Python长度函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># len(iterable) 求可迭代对象的长度，即其中包含的元素个数</span><br><span class="line">len([1, 2, 3, 4])</span><br><span class="line">len((1, 2, (3, [4, 5]), 6))</span><br><span class="line">len({1:11, 2:22, 3:33, 4:44})</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python61.png" alt="61"></p><h2 id="Python排序函数"><a href="#Python排序函数" class="headerlink" title="Python排序函数"></a><font size="4">Python排序函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = [1,5,3,2,4]</span><br><span class="line">b = {1:11, 5:55, 3:33, 2:22, 4:44}</span><br><span class="line">c = [(1, 5), (4, 8), (3, 7), (2, 9)]</span><br><span class="line"></span><br><span class="line"># sorted(iterable, key, reverse=False) 对可迭代对象按照key进行排序，如果对字典进行排序则对其关键字进行排序，reverse为True指从大到小排序，默认为从小到大排序</span><br><span class="line">sorted(a)</span><br><span class="line">sorted(b)</span><br><span class="line">sorted(c)</span><br><span class="line">sorted(c, key=lambda x:(x[1], x[0]))</span><br><span class="line">sorted(c, key=lambda x:(x[1], x[0]), reverse=True)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python62.png" alt="62"></p><h2 id="Python翻转函数"><a href="#Python翻转函数" class="headerlink" title="Python翻转函数"></a><font size="4">Python翻转函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = [1,5,3,2,4]</span><br><span class="line">b = [(1, 5), (4, 8), (3, 7), (2, 9)]</span><br><span class="line"></span><br><span class="line"># reversed(iterable) 返回翻转后的迭代器对象，可以用list，tuple等进行转换，字典无法进行翻转操作</span><br><span class="line">reversed(a)</span><br><span class="line">list(reversed(a))</span><br><span class="line">tuple(reversed(b))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python63.png" alt="63"></p><h2 id="Python枚举函数"><a href="#Python枚举函数" class="headerlink" title="Python枚举函数"></a><font size="4">Python枚举函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = [1,5,3,2,4]</span><br><span class="line">b = {1:11, 5:55, 3:33, 2:22, 4:44}</span><br><span class="line">c = [(1, 5), (4, 8), (3, 7), (2, 9)]</span><br><span class="line"></span><br><span class="line"># enumerate(iterable) 返回从0开始枚举的迭代器对象，可以用list，tuple等进行转换，对字典枚举则对其关键字进行枚举</span><br><span class="line">reversed(a)</span><br><span class="line">enumerate(a)</span><br><span class="line">list(enumerate(a))</span><br><span class="line">list(enumerate(b))</span><br><span class="line">tuple(enumerate(c))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python64.png" alt="64"></p><h2 id="Python打包函数"><a href="#Python打包函数" class="headerlink" title="Python打包函数"></a><font size="4">Python打包函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = [1,5,3,2,4]</span><br><span class="line">b = {1:11, 5:55, 3:33, 2:22, 4:44}</span><br><span class="line">c = [(1, 5), (4, 8), (3, 7), (2, 9)]</span><br><span class="line"></span><br><span class="line"># zip(iterable1, iterable2) 将两个迭代器对象打包，合并成一个迭代器对象，打包元素按照元素数量少的进行打包，对字典打包则对其关键字进行打包</span><br><span class="line">zip(a, b)</span><br><span class="line">list(zip(a, b))</span><br><span class="line">tuple(zip(a, c))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python65.png" alt="65"></p><h2 id="Python删除函数"><a href="#Python删除函数" class="headerlink" title="Python删除函数"></a><font size="4">Python删除函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = True</span><br><span class="line">b = [1,5,3,2,4]</span><br><span class="line"></span><br><span class="line"># del obj 将obj删除，不存在该对象</span><br><span class="line">del a</span><br><span class="line">del b</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python66.png" alt="66"></p><h2 id="Python中input函数"><a href="#Python中input函数" class="headerlink" title="Python中input函数"></a><font size="4">Python中input函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># input() 从标准输入中读取一行文本，返回该内容的字符串</span><br><span class="line">str = input('请输入:')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python87.png" alt="87"></p><h2 id="Python中print函数"><a href="#Python中print函数" class="headerlink" title="Python中print函数"></a><font size="4">Python中print函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># print(obj1, obj2, ..., end='\n') 将obj1, obj2, ...按顺序输出，以空格分开，end是输出后的结尾字符，默认为换行符</span><br><span class="line">print([1, 2, 3], (4, 5, 6))</span><br><span class="line">print('hello world' + '\n' + 'hello python ', end='end')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python84.png" alt="84"></p><h2 id="Python推导式"><a href="#Python推导式" class="headerlink" title="Python推导式"></a><font size="4">Python推导式</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = [1, 2, 3, 4, 5]</span><br><span class="line"></span><br><span class="line"># [x for x in iterable] 从iterable中逐一获取元素，并生成列表</span><br><span class="line">[2 ** x for x in a]</span><br><span class="line">[[i + j * 3 for i in range(3)] for j in range(3)]</span><br><span class="line"></span><br><span class="line"># {x for x in iterable} 从iterable中逐一获取元素，并生成集合</span><br><span class="line">{2 ** x for x in a}</span><br><span class="line"></span><br><span class="line"># {x: y for x in iterable} 从iterable中逐一获取元素，并生成字典</span><br><span class="line">{x: 2 ** x for x in a}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python67.png" alt="67"></p><h2 id="Python中lambda表达式"><a href="#Python中lambda表达式" class="headerlink" title="Python中lambda表达式"></a><font size="4">Python中lambda表达式</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># lambda arg1, arg2: function 一行表达式简单实现函数，参数为arg1, arg2, ...，函数体为function</span><br><span class="line">f = lambda x, y: x * y</span><br><span class="line">f(3, 4)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python68.png" alt="68"></p><h2 id="Python中filter函数"><a href="#Python中filter函数" class="headerlink" title="Python中filter函数"></a><font size="4">Python中filter函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># filter(function or None, iterable) 若第一个参数为None则返回iterable中为真的元素，并生成迭代器对象，若第一个参数为function，则将iterable中的每个元素带入函数，将为真的元素生成迭代器对象</span><br><span class="line">filter(None, [x % 3 for x in range(10)])</span><br><span class="line">list(filter(None, [x % 3 for x in range(10)]))</span><br><span class="line"></span><br><span class="line">filter(lambda x: x % 3, range(10))</span><br><span class="line">list(filter(lambda x: x % 3, range(10)))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python69.png" alt="69"></p><h2 id="Python中map函数"><a href="#Python中map函数" class="headerlink" title="Python中map函数"></a><font size="4">Python中map函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = '12345'</span><br><span class="line">f = lambda x: 2 ** x</span><br><span class="line"></span><br><span class="line"># map(function, iterable) 将iterable中的元素带入函数，返回函数生成的迭代器对象</span><br><span class="line">map(int, a)</span><br><span class="line">list(map(int, a))</span><br><span class="line"></span><br><span class="line">map(f, range(5))</span><br><span class="line">tuple(map(f, range(5)))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python70.png" alt="70"></p><h1 id="Python自定义Function"><a href="#Python自定义Function" class="headerlink" title="Python自定义Function"></a><font size="5" color="red">Python自定义Function</font></h1><h2 id="Python中def定义函数"><a href="#Python中def定义函数" class="headerlink" title="Python中def定义函数"></a><font size="4">Python中def定义函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 定义名为function_name的函数，形式参数为arg1, arg2, ...，函数体为function，返回值为value(可以无返回值)，与C/C++不同，可以有多个返回值</span><br><span class="line"># def function_name(arg1, arg2, ...): </span><br><span class="line">#     function</span><br><span class="line">#     return value</span><br><span class="line"></span><br><span class="line">def my_pow(a, b):</span><br><span class="line">    return a ** b, b ** a</span><br><span class="line"></span><br><span class="line"># function_name(x1, x2, ...) 调用名为function_name的函数，实际参数为x1, x2, ...，调用时将实参本身传递到形参，可以按顺序传入参数或者手动指定参数</span><br><span class="line">c, d = my_pow(3, 4)</span><br><span class="line">e, f = my_pow(b=4, a=3)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python71.png" alt="71"></p><h2 id="Python函数的默认参数"><a href="#Python函数的默认参数" class="headerlink" title="Python函数的默认参数"></a><font size="4">Python函数的默认参数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># def function_name(arg1, arg2=x) 设置函数的默认参数为arg2，其值为x，调用时如果没有赋值则赋值为x，注意默认参数只能放在非默认参数之后</span><br><span class="line">def my_pow(a, b=2):</span><br><span class="line">    return b ** a</span><br><span class="line"></span><br><span class="line">a = my_pow(5)</span><br><span class="line">b = my_pow(3, 3)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python81.png" alt="81"></p><h2 id="Python函数的收集参数"><a href="#Python函数的收集参数" class="headerlink" title="Python函数的收集参数"></a><font size="4">Python函数的收集参数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># def function_name(*args) 将所有的参数都打包起来，调用时传入多少个都可以，都生成一个元组，如果没有传入参数则是一个空元组。</span><br><span class="line"></span><br><span class="line">def my_func(*par):</span><br><span class="line">    print(par)</span><br><span class="line">    print('参数的个数为:', len(par))</span><br><span class="line"></span><br><span class="line">my_func(1, 2, [3, 4], '56', {7, 8, 9})</span><br><span class="line"></span><br><span class="line"># def function_name(**kwargs) 将所有的参数都打包起来，调用时传入多少个都可以，都生成一个字典，传入参数时必须使用键值对的形式，如果没有传入参数则是一个空字典。</span><br><span class="line"></span><br><span class="line">def my_func(**par):</span><br><span class="line">    print(par)</span><br><span class="line">    print('参数的个数为:', len(par))</span><br><span class="line"></span><br><span class="line">my_func(a=1, b=2, c=[1, 2])</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python82.png" alt="82"></p><h2 id="Python星号的用法"><a href="#Python星号的用法" class="headerlink" title="Python星号的用法"></a><font size="4">Python星号的用法</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 用作运算符，一个星号代表乘法，两个星号代表乘方</span><br><span class="line">a, b = 2, 3</span><br><span class="line">a * b</span><br><span class="line">a ** b</span><br><span class="line"></span><br><span class="line"># 定义函数时表示收集，*args表示将位置参数都装入元组args中，**kwargs表示将位置参数都装入字典kwargs中</span><br><span class="line">def test(*args, **kwargs):</span><br><span class="line">  print(args[0])</span><br><span class="line">  print(kwargs['a'])</span><br><span class="line"></span><br><span class="line">test(1, 2, 3, a='aa', b='bb')</span><br><span class="line"></span><br><span class="line"># 调用函数时表示分散，*L表示将可迭代对象中的每个元素作为位置参数传入，**D表示将字典的键值对作为位置参数传入</span><br><span class="line">def test(a, b, c):</span><br><span class="line">  print(a)</span><br><span class="line"></span><br><span class="line">L = [1, 2]</span><br><span class="line">D = {'c': 3}</span><br><span class="line">test(*L, **D)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python110.png" alt="110"></p><h2 id="Python函数的全局变量"><a href="#Python函数的全局变量" class="headerlink" title="Python函数的全局变量"></a><font size="4">Python函数的全局变量</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 在函数中使用和外部名称相同的变量时，会建立一个同名的局部变量，即使修改了其值，也只是修改了局部变量额值，函数调用完毕后，外部的变量仍然没有被修改</span><br><span class="line">a = 10</span><br><span class="line"></span><br><span class="line">def my_func():</span><br><span class="line">    a = 5</span><br><span class="line">    print(a)</span><br><span class="line"></span><br><span class="line">my_func()</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"># 如果想在函数中修改外部变量的值，则需要将其声明为全局变量，和C/C++不同，C/C++是在定义时将变量声明为全局，而Python中是在外部先定义，然后在函数内部使用关键字global将其声明为全局</span><br><span class="line">a = 10</span><br><span class="line"></span><br><span class="line">def my_func():</span><br><span class="line">    global a</span><br><span class="line">    a = 5</span><br><span class="line">    print(a)</span><br><span class="line"></span><br><span class="line">my_func()</span><br><span class="line">print(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python83.png" alt="83"></p><h2 id="Python函数的非局部变量"><a href="#Python函数的非局部变量" class="headerlink" title="Python函数的非局部变量"></a><font size="4">Python函数的非局部变量</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># nonlocal关键字修饰变量后标识变量是上一层函数中的局部变量，对变量进行修改就是修改上一层函数中的局部变量，如果上一层函数中不存在同名的局部变量，则会报错，经常用于闭包和装饰器中，有关闭包和装饰器的相关知识可以参考我的另一篇博客Closure &amp; Decorators(闭包和装饰器)。</span><br><span class="line">def decorators(func):</span><br><span class="line">  a = 1</span><br><span class="line">  print("正在装饰")</span><br><span class="line">  print("外层局部变量a的值为: ", a)</span><br><span class="line">  def wrapper():</span><br><span class="line">    func()</span><br><span class="line">    nonlocal a</span><br><span class="line">    a += 1</span><br><span class="line">    print("内层局部变量a的值为: ", a)</span><br><span class="line">  return wrapper</span><br><span class="line"></span><br><span class="line">@decorators</span><br><span class="line">def say():</span><br><span class="line">  print("hello world!")</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python111.png" alt="111"></p><h2 id="Python内嵌函数"><a href="#Python内嵌函数" class="headerlink" title="Python内嵌函数"></a><font size="4">Python内嵌函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 在函数内部可以定义另一个函数，这样外部函数无法调用该函数，仅仅能在定义该函数的函数中使用，这样可以使主程序更加清晰。内嵌函数的使用频率较低，了解即可。</span><br><span class="line">def test1():</span><br><span class="line">    print('This is test1')</span><br><span class="line">    def test2():</span><br><span class="line">        print('This is test2')</span><br><span class="line">    test2()</span><br><span class="line"></span><br><span class="line">test1()</span><br><span class="line">test2()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python104.png" alt="104"></p><h2 id="Python偏函数"><a href="#Python偏函数" class="headerlink" title="Python偏函数"></a><font size="4">Python偏函数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 偏函数是指对函数参数中默认值的控制</span><br><span class="line"># functools模块中有一个partial方法，可以完成对默认值的控制</span><br><span class="line"># new_func = functools.partial(old_func, para=x) 将原函数old_func中的para参数固定为x</span><br><span class="line">import functools</span><br><span class="line"></span><br><span class="line">a = int('1010', base=2)</span><br><span class="line"></span><br><span class="line">int2 = functools.partial(int, base=2)</span><br><span class="line">b = int2('1010')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python107.png" alt="107"></p><h1 id="Function小结"><a href="#Function小结" class="headerlink" title="Function小结"></a><font size="5" color="red">Function小结</font></h1><p>  Function函数是计算机语言中一种重要的调用方式，无论是在何种语言中，函数的使用都是至关重要的，有了函数可以使代码更加整洁和清晰，模块与模块之间达到高内聚低耦合，大大提高代码的可读性。可以节省大因此使用的频率非常高，所以要灵活掌握Function的应用。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Function(函数)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Special Structure(特殊结构)</title>
    <link href="https://USTCcoder.github.io/2019/09/15/python_special/"/>
    <id>https://USTCcoder.github.io/2019/09/15/python_special/</id>
    <published>2019-09-15T11:14:27.000Z</published>
    <updated>2020-07-27T06:13:52.065Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python11.jpg" alt="11"></p><h1 id="Special-Structure介绍"><a href="#Special-Structure介绍" class="headerlink" title="Special Structure介绍"></a><font size="5" color="red">Special Structure介绍</font></h1><p>  Python中的特殊结构指除了条件，循环之外的结构，如异常处理结构，else语句，with语句以及assert语句。<br><a id="more"></a></p><h1 id="Special-Structure分类"><a href="#Special-Structure分类" class="headerlink" title="Special Structure分类"></a><font size="5" color="red">Special Structure分类</font></h1><h2 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a><font size="4">异常处理</font></h2><script type="math/tex; mode=display">\begin{array}{|c|c|} 名称 & 描述 \\  AssertionError & 断言失败 \\ AttributeError & 访问位置的对象属性 \\ IndexError & 索引错误 \\ KeyError & 关键字错误 \\ NameError & 访问不存在的变量 \\ OSError & 操作系统异常 \\ OverflowError & 数值运算超过限制 \\ SyntaxError & 语法错误 \\ TypeError & 不同类型的无效操作 \\ ZeroDivisionError & 除数为0 \\ \end{array}</script><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># try: 语句1 except: 语句2 (finally: 语句3) 执行语句1如果发生异常则直接跳到except中执行语句2，如果没有异常则不执行语句2，最终都要执行finally中的语句3，finally可省略</span><br><span class="line"># try:</span><br><span class="line">#     语句1(可能出错的步骤)</span><br><span class="line"># except ErrorName(默认为所有异常都抛弃):</span><br><span class="line">#     语句2(出现错误的处理过程)</span><br><span class="line"># finally:</span><br><span class="line">#     语句3(最终都要执行的代码)</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    print('除以0之前')</span><br><span class="line">    a = 5 / 0</span><br><span class="line">    print('除以0之后')</span><br><span class="line">except ZeroDivisionError:</span><br><span class="line">    a = 5</span><br><span class="line">    print('except')</span><br><span class="line">finally:</span><br><span class="line">    print('finally')</span><br><span class="line">print(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python72.png" alt="72"></p><h2 id="else语句"><a href="#else语句" class="headerlink" title="else语句"></a><font size="4">else语句</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"># else语句有4种组合方式，可以和if组合成if condition: 语句1 else: 语句2，可以和while组合成while condition: 循环语句 else 语句，可以和for组合成for target in iterable: 循环语句 else: 语句，可以和try组合成try: 语句1 except: 语句2 else: 语句3</span><br><span class="line"></span><br><span class="line"># if condition: 语句1 else: 语句2，如果condition正确则执行语句1，否则执行语句2</span><br><span class="line"># if condition:</span><br><span class="line">#     语句1</span><br><span class="line"># else:</span><br><span class="line">#     语句2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># while condition: 循环语句 else 语句，如果循环执行完毕则执行else中的语句，如果中间跳出循环则不执行else中的语句</span><br><span class="line"># while condition:</span><br><span class="line">#     循环语句</span><br><span class="line"># else:</span><br><span class="line">#     语句</span><br><span class="line">a = 3</span><br><span class="line">while a &gt; 0:</span><br><span class="line">    a -= 1</span><br><span class="line">else:</span><br><span class="line">    print('else')</span><br><span class="line"></span><br><span class="line"># for target in iterable: 循环语句 else: 语句，如果循环执行完毕则执行else中的语句，如果中间跳出循环则不执行else中的语句</span><br><span class="line"># for target in iterable: </span><br><span class="line">#     循环语句</span><br><span class="line"># else:</span><br><span class="line">#     语句</span><br><span class="line">a = 3</span><br><span class="line">for i in range(3):</span><br><span class="line">    a -= 1</span><br><span class="line">else:</span><br><span class="line">    print('else')</span><br><span class="line"></span><br><span class="line"># try: 语句1 except: 语句2 else: 语句3，如果发生异常则不执行else后面的语句3，没有异常则执行else后面的语句3</span><br><span class="line"># try:</span><br><span class="line">#     语句1</span><br><span class="line"># except:</span><br><span class="line">#     语句2</span><br><span class="line"># else: </span><br><span class="line">#     语句3</span><br><span class="line">try:</span><br><span class="line">    a = 4 / 0</span><br><span class="line">except:</span><br><span class="line">    a = 4 / 1</span><br><span class="line">else:</span><br><span class="line">    print('else')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python73.png" alt="73"></p><h2 id="with语句"><a href="#with语句" class="headerlink" title="with语句"></a><font size="4">with语句</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># with 语句1: 语句2 用于文件操作或者进程线程互斥时，语句2执行完之后会自动释放语句1所产生的资源，不需要再手动完成后续处理</span><br><span class="line"># with 语句1:</span><br><span class="line">#     语句2</span><br><span class="line">with open('dm01.txt') as f:</span><br><span class="line">    print(f.read())</span><br><span class="line">print(f.closed)</span><br><span class="line">``` </span><br><span class="line">![74](/images/LANGUAGE/python74.png)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## &lt;font size=4&gt;assert语句&lt;/font&gt;</span><br></pre></td></tr></tbody></table></figure><h1 id="assert-condition-当condition为假，程序自动崩溃，并且抛出AssertError异常，condition为真，不做任何操作"><a href="#assert-condition-当condition为假，程序自动崩溃，并且抛出AssertError异常，condition为真，不做任何操作" class="headerlink" title="assert condition 当condition为假，程序自动崩溃，并且抛出AssertError异常，condition为真，不做任何操作"></a>assert condition 当condition为假，程序自动崩溃，并且抛出AssertError异常，condition为真，不做任何操作</h1><h1 id="assert-condition"><a href="#assert-condition" class="headerlink" title="assert condition"></a>assert condition</h1><p>assert 3 &lt; 0<br>print(f.closed)<br>```<br><img src="/images/LANGUAGE/python75.png" alt="75"></p><h1 id="Special-Structure小结"><a href="#Special-Structure小结" class="headerlink" title="Special Structure小结"></a><font size="5" color="red">Special Structure小结</font></h1><p>  Special Structure特殊结构使用频率相对较低，但是在维护代码稳定性方面有着重要的应用，当出现异常时不会终止程序，而是具有友善的提示界面，适合于企业级应用开发，因此要熟练掌握这些特殊结构的应用。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Special Structure(特殊结构)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>File(文件)</title>
    <link href="https://USTCcoder.github.io/2019/09/15/python_file/"/>
    <id>https://USTCcoder.github.io/2019/09/15/python_file/</id>
    <published>2019-09-15T01:36:45.000Z</published>
    <updated>2020-07-27T06:15:44.419Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python12.jpg" alt="12"></p><h1 id="File介绍"><a href="#File介绍" class="headerlink" title="File介绍"></a><font size="5" color="red">File介绍</font></h1><p>  学习一门语言，最终目的是解决实际的生活问题，尤其是Python语言，出色的应用在人工智能领域。但是面对大数据的浪潮，数据的读取是一个关键的问题，因此文件操作是我们必须要掌握的内容。<br><a id="more"></a></p><h1 id="File应用"><a href="#File应用" class="headerlink" title="File应用"></a><font size="5" color="red">File应用</font></h1><h2 id="File打开"><a href="#File打开" class="headerlink" title="File打开"></a><font size="4">File打开</font></h2><script type="math/tex; mode=display">\begin{array}{|c|c|} 模式 & 描述 \\ x & 写模式，新建一个文件，如果该文件已存在则会报错。 \\ b & 二进制模式。 \\ + & 打开一个文件进行更新(可读可写)。 \\ r & 以只读方式打开文件。文件的指针将会放在文件的开头。 \\ w & 以写入方式打开文件。如果该文件已存在则从开头开始编辑。否则创建新文件。 \\ a & 以追加方式打开文件，如果该文件不存在则创建新文件写入。 \\ \end{array}</script><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None) 最重要的</span><br><span class="line">f = open('dm01.txt', mode='a+') 以读写的方式打开文件dm01.txt，用于在文件后追加内容</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python76.png" alt="76"></p><h2 id="File操作"><a href="#File操作" class="headerlink" title="File操作"></a><font size="4">File操作</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># file.flush() 刷新内部缓存，将缓存区的数据写入文件</span><br><span class="line"></span><br><span class="line"># file.read(size) 从文件读取size大小的字节，默认读取所有字节 </span><br><span class="line"></span><br><span class="line"># file.readline() 读取整行，包括换行符</span><br><span class="line"></span><br><span class="line"># file.readlines() 读取所有行，并以列表形式返回</span><br><span class="line"></span><br><span class="line"># file.seek(n) 设置文件指针当前位置指向n</span><br><span class="line"></span><br><span class="line"># file.tell() 返回文件指针当前位置 </span><br><span class="line"></span><br><span class="line"># file.write() 将字符串写入文件，返回写入的字符长度</span><br><span class="line"></span><br><span class="line"># file.writelines() 向文件写入一个序列的字符串列表，如果需要换行则加入每行的换行符</span><br><span class="line"></span><br><span class="line"># flie.close() 关闭文件，关闭后无法进行读写操作</span><br><span class="line">f = open('dm01.txt', mode='r+', encoding='utf-8')</span><br><span class="line">f</span><br><span class="line">f.tell()</span><br><span class="line">f.seek(0)</span><br><span class="line">f.readlines()</span><br><span class="line">f.writelines(['\nfile append1\nfile append2'])</span><br><span class="line">f.seek(0)</span><br><span class="line">f.readlines()</span><br><span class="line">f.close()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python77.png" alt="77"></p><h1 id="File小结"><a href="#File小结" class="headerlink" title="File小结"></a><font size="5" color="red">File小结</font></h1><p>  对文件的操作经常使用于文本的批量化修改，大数据的读取或者网页爬虫的应用。这些都属于较高级别的应用领域，因此初学者很少使用到File文件操作，但是为了以后应用的方便，学好文件操作是必不可少的！</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;File(文件)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Set(集合)</title>
    <link href="https://USTCcoder.github.io/2019/09/14/python_set/"/>
    <id>https://USTCcoder.github.io/2019/09/14/python_set/</id>
    <published>2019-09-14T11:36:45.000Z</published>
    <updated>2020-07-27T06:14:14.136Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python9.jpg" alt="9"></p><h1 id="Set介绍"><a href="#Set介绍" class="headerlink" title="Set介绍"></a><font size="5" color="red">Set介绍</font></h1><p>  Python中的Set是集合的概念，其和列表，元组最大的区别是集合中不存在相同的元素。<br><a id="more"></a></p><h1 id="Set操作"><a href="#Set操作" class="headerlink" title="Set操作"></a><font size="5" color="red">Set操作</font></h1><h2 id="Python创建集合"><a href="#Python创建集合" class="headerlink" title="Python创建集合"></a><font size="4">Python创建集合</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># set(iterable) 将可迭代对象转换为set类型</span><br><span class="line">a = set(range(5))</span><br><span class="line"></span><br><span class="line"># {a, b, c, ...} 创建元素为a, b, c, ...的集合，注意空集合不能写{}，要用set()定义，{}默认为字典类型</span><br><span class="line">b = {1, 2, 3, 4, 5}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python56.png" alt="56"></p><h2 id="Python集合运算"><a href="#Python集合运算" class="headerlink" title="Python集合运算"></a><font size="4">Python集合运算</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = {1, 2, 3, 4, 5}</span><br><span class="line">b = {3, 4, 5, 6, 7}</span><br><span class="line"></span><br><span class="line"># |(求并集)，&amp;(求交集)，-(求差集)，^(求对称差集)，==(比较是否相等)，!=(比较是否不等)，&lt;(比较是否为真子集)，&lt;=(比较是否为子集)，&gt;(比较是否为真超集)，&gt;=(比较是否为超集)</span><br><span class="line">a | b</span><br><span class="line">a &amp; b</span><br><span class="line">a - b</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python57.png" alt="57"></p><h2 id="Python向集合中增加，删除，修改元素"><a href="#Python向集合中增加，删除，修改元素" class="headerlink" title="Python向集合中增加，删除，修改元素"></a><font size="4">Python向集合中增加，删除，修改元素</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a = {1, 2, 3, 4, 5}</span><br><span class="line"></span><br><span class="line"># obj.add(ele) 向集合中添加元素，不能添加集合，列表，字典类型，可以添加数字，字符串，元组类型，如果已经存在于集合中，则不做任何操作</span><br><span class="line">a.add(6)</span><br><span class="line">a.add(3)</span><br><span class="line"></span><br><span class="line"># obj.remove(ele) 从集合中删除元素ele，如果没有该元素会报错</span><br><span class="line">a.remove(1)</span><br><span class="line"></span><br><span class="line"># obj.discard(ele) 从集合中删除元素ele，如果没有该元素不会报错</span><br><span class="line">a.discard(2)</span><br><span class="line"></span><br><span class="line"># obj.pop() 从集合中删除第一个元素并返回该元素</span><br><span class="line">a.pop()</span><br><span class="line"></span><br><span class="line"># obj.clear() 删除所有元素</span><br><span class="line">a.clear()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python59.png" alt="59"></p><h1 id="Set小结"><a href="#Set小结" class="headerlink" title="Set小结"></a><font size="5" color="red">Set小结</font></h1><p>  Set集合是使用频率相对较低，但是如果使用则会大大提高效率，在统计类别时，很多物品属于同一类，此时不需要关心该物品，建立集合时，可以节省大量的内存空间和用户查询时间，因此在某些特定情况下能发挥独特的优势，所以也要熟练掌握Set的应用。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Set(集合)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Dict(字典)</title>
    <link href="https://USTCcoder.github.io/2019/09/14/python_dict/"/>
    <id>https://USTCcoder.github.io/2019/09/14/python_dict/</id>
    <published>2019-09-14T08:14:15.000Z</published>
    <updated>2020-07-27T06:15:50.261Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python8.jpg" alt="8"></p><h1 id="Dict介绍"><a href="#Dict介绍" class="headerlink" title="Dict介绍"></a><font size="5" color="red">Dict介绍</font></h1><p>  Python中的Dict是一种哈希表的数据结构，是由键值对构成的，一个Key对应一个Value，方便数据的查找。<br><a id="more"></a></p><h1 id="Dict操作"><a href="#Dict操作" class="headerlink" title="Dict操作"></a><font size="5" color="red">Dict操作</font></h1><h2 id="Python创建字典"><a href="#Python创建字典" class="headerlink" title="Python创建字典"></a><font size="4">Python创建字典</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># dict.fromkeys(key, value) 生成以key为关键字，value为值的字典，初始时所有的key对应的值都相同，默认为None</span><br><span class="line">a = dict.fromkeys([1, 2, 3])</span><br><span class="line"></span><br><span class="line"># {a:aa, b:bb, ...}创建Key为a, b, ...，Value为aa, bb, ...的字典</span><br><span class="line">b = {1:None, 2:None, 3:None}</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python50.png" alt="50"></p><h2 id="Python索引字典元素"><a href="#Python索引字典元素" class="headerlink" title="Python索引字典元素"></a><font size="4">Python索引字典元素</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = {1:'one', 2:2, 3:True}</span><br><span class="line"></span><br><span class="line"># 和列表，元组，字符串不同，字典的索引是无序的，只能根据Key来索引</span><br><span class="line">a[1]</span><br><span class="line"></span><br><span class="line"># obj.get(key, value) 若key在关键字中，则返回其值，否则返回value，默认为None</span><br><span class="line">a.get(1)</span><br><span class="line"></span><br><span class="line"># obj.setdefault(key, value) 若key在关键字中，则返回其值，否则给字典添加(key, value)value默认为None</span><br><span class="line">a.setdefault(4, 'four')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python51.png" alt="51"></p><h2 id="Python获得所有的键，值信息"><a href="#Python获得所有的键，值信息" class="headerlink" title="Python获得所有的键，值信息"></a><font size="4">Python获得所有的键，值信息</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = {1:'one', 2:2, 3:True}</span><br><span class="line"></span><br><span class="line"># obj.keys() 获取字典的所有键</span><br><span class="line">a.keys()</span><br><span class="line"></span><br><span class="line"># obj.values() 获取字典的所有值</span><br><span class="line">a.values()</span><br><span class="line"></span><br><span class="line"># obj.items() 获取字典的所有键值对</span><br><span class="line">a.items()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python52.png" alt="52"></p><h2 id="Python增加，删除，修改字典元素"><a href="#Python增加，删除，修改字典元素" class="headerlink" title="Python增加，删除，修改字典元素"></a><font size="4">Python增加，删除，修改字典元素</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a = {1:'one', 2:2, 3:True}</span><br><span class="line"></span><br><span class="line"># obj[key] = value 将字典的key对应的值改为value，如果不存在则增加一个键值对(key, value)</span><br><span class="line">a[3] = False</span><br><span class="line">a[4] = 'FOUR'</span><br><span class="line"></span><br><span class="line"># del obj[key] 将字典中的key对应的键值对删除</span><br><span class="line">del a[4]</span><br><span class="line"></span><br><span class="line"># obj.pop(key) 弹出关键字位key的元素，并返回其Value</span><br><span class="line">a.pop(1)</span><br><span class="line"></span><br><span class="line"># obj.popitem() 弹出最后一个元素，并返回其键值对</span><br><span class="line">a.popitem()</span><br><span class="line"></span><br><span class="line"># obj.clear() 删除所有元素</span><br><span class="line">a.clear()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python53.png" alt="53"></p><h2 id="Python字典合并"><a href="#Python字典合并" class="headerlink" title="Python字典合并"></a><font size="4">Python字典合并</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = {1:'one', 2:2, 3:True}</span><br><span class="line">b = {1:'ONE', 4:4, 5:False}</span><br><span class="line"></span><br><span class="line"># obj.update(obj1) 给字典obj添加另一个字典obj1，两个字典取并集，如果具有相同的Key则值为obj1中key对应的值</span><br><span class="line">a.update(b)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python54.png" alt="54"></p><h2 id="Python判断关键字是否在字典中"><a href="#Python判断关键字是否在字典中" class="headerlink" title="Python判断关键字是否在字典中"></a><font size="4">Python判断关键字是否在字典中</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = {1:'one', 2:2, 3:True}</span><br><span class="line"></span><br><span class="line"># key in obj，判断key是否在obj的关键字中，key not in obj，判断key是否不在obj的关键字中</span><br><span class="line">b = 2 in a</span><br><span class="line">c = 4 not in a</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python55.png" alt="55"></p><h1 id="Dict小结"><a href="#Dict小结" class="headerlink" title="Dict小结"></a><font size="5" color="red">Dict小结</font></h1><p>  Dict字典是Python中一种重要的数据结构，在大数据的存储或者统计各分数段人数时，有时为了便于查询，需要建立字典，索引时只需要关键字Key，可以节省大量的内存空间和用户查询时间，因此使用的频率非常高，所以要灵活掌握Dict的应用。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Dict(字典)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Str(字符串)</title>
    <link href="https://USTCcoder.github.io/2019/09/13/python_str/"/>
    <id>https://USTCcoder.github.io/2019/09/13/python_str/</id>
    <published>2019-09-13T02:44:24.000Z</published>
    <updated>2020-07-27T06:14:05.723Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python7.jpg" alt="7"></p><h1 id="Str介绍"><a href="#Str介绍" class="headerlink" title="Str介绍"></a><font size="5" color="red">Str介绍</font></h1><p>  Python中的Str是可迭代对象，类似于C/C++中的字符串，但是更加灵活，具有很多内置的API。<br><a id="more"></a></p><h1 id="Str操作"><a href="#Str操作" class="headerlink" title="Str操作"></a><font size="5" color="red">Str操作</font></h1><h2 id="Python创建字符串"><a href="#Python创建字符串" class="headerlink" title="Python创建字符串"></a><font size="4">Python创建字符串</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># str(obj) 将obj转换为字符串，一般用来将数字转换为字符串</span><br><span class="line">a = str(123)</span><br><span class="line"></span><br><span class="line"># 'abc...'或者"abc..." 创建值为abc...的字符串，如果字符串本身具有单引号则创建时要用双引号，如果字符串本身具有双引号，则创建时要用单引号</span><br><span class="line">b = 'Hello Python'</span><br><span class="line">c = "Hello World"</span><br><span class="line">d = 'I love "Python"'</span><br><span class="line">e = "I love 'coding'"</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python30.png" alt="30"></p><h2 id="Python索引字符串元素"><a href="#Python索引字符串元素" class="headerlink" title="Python索引字符串元素"></a><font size="4">Python索引字符串元素</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 和C/C++相同，通过中括号[]索引字符串元素，可以通过:运算符获取连续的索引，负数索引为从后向前索引，-1代表最后一个元素，-2代表倒数第二个元素</span><br><span class="line">a = 'Hello Python'</span><br><span class="line">b = a[:2]</span><br><span class="line">c = a[4]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python31.png" alt="31"></p><h2 id="Python向字符串中增加，删除，修改元素"><a href="#Python向字符串中增加，删除，修改元素" class="headerlink" title="Python向字符串中增加，删除，修改元素"></a><font size="4">Python向字符串中增加，删除，修改元素</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Python中字符串没有append，pop等API，如果想修改元素必须采用算术运算修改元素</span><br><span class="line">a = 'Hello pythan'</span><br><span class="line">a = a[:-2] + 'o' + a[-1]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python32.png" alt="32"></p><h2 id="Python字符串大小写转换"><a href="#Python字符串大小写转换" class="headerlink" title="Python字符串大小写转换"></a><font size="4">Python字符串大小写转换</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">a = 'heLLo pythOn'</span><br><span class="line"></span><br><span class="line"># obj.capitalize() 第一个字符改为大写，其他字符改为小写并返回</span><br><span class="line">b = a.capitalize()</span><br><span class="line"></span><br><span class="line"># obj.title() 将每个单词的第一个字符改为大写，其他字符改为小写并返回</span><br><span class="line">c = a.title()</span><br><span class="line"></span><br><span class="line"># obj.lower() 将大写转换为小写</span><br><span class="line">d =a.lower()</span><br><span class="line"></span><br><span class="line"># obj.upper() 将小写转换为大写</span><br><span class="line">e = a.upper()</span><br><span class="line"></span><br><span class="line"># obj.swapcase() 将大小写字符翻转，大写变小写，小写变大写</span><br><span class="line">f = a.swapcase()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python33.png" alt="33"></p><h2 id="Python字符串大小比较"><a href="#Python字符串大小比较" class="headerlink" title="Python字符串大小比较"></a><font size="4">Python字符串大小比较</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># obj1 op obj2 将两个字符串进行大小比较，从第一个元素开始比较，如果相同继续比较</span><br><span class="line">a = 'hello world'</span><br><span class="line">b = 'hello python'</span><br><span class="line">a &gt; b</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python34.png" alt="34"></p><h2 id="Python字符串乘法"><a href="#Python字符串乘法" class="headerlink" title="Python字符串乘法"></a><font size="4">Python字符串乘法</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># obj * n，n为正整数，将obj复制n次</span><br><span class="line">a = 'hello world '</span><br><span class="line">b = a * 3</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python35.png" alt="35"></p><h2 id="Python判断元素是否在字符串中"><a href="#Python判断元素是否在字符串中" class="headerlink" title="Python判断元素是否在字符串中"></a><font size="4">Python判断元素是否在字符串中</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># data in obj，判断data是否在obj中，data not in obj，判断data是否不在obj中</span><br><span class="line">a = 'hello world'</span><br><span class="line">b = 'he' in a</span><br><span class="line">c = 'she' not in a</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python36.png" alt="36"></p><h2 id="Python求某个元素出现的次数"><a href="#Python求某个元素出现的次数" class="headerlink" title="Python求某个元素出现的次数"></a><font size="4">Python求某个元素出现的次数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># obj.count(data) 求data在字符串中出现的次数</span><br><span class="line">a = 'hello world'</span><br><span class="line">a.count('l')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python37.png" alt="37"></p><h2 id="Python求某个元素的索引"><a href="#Python求某个元素的索引" class="headerlink" title="Python求某个元素的索引"></a><font size="4">Python求某个元素的索引</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># obj.index(data, begin, end) 从begin到end-1中索引第一次出现data的位置，默认从第一个元素到最后一个元素</span><br><span class="line">a = 'hello world'</span><br><span class="line">a.index('l')</span><br><span class="line">a.index('l', 4)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python38.png" alt="38"></p><h2 id="Python字符串与列表或元组的转换"><a href="#Python字符串与列表或元组的转换" class="headerlink" title="Python字符串与列表或元组的转换"></a><font size="4">Python字符串与列表或元组的转换</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = 'string'</span><br><span class="line">b = ['L', 'i', 's', 't']</span><br><span class="line">c = ('T', 'u', 'p', 'l', 'e')</span><br><span class="line"></span><br><span class="line"># list(obj)或者tuple(obj) 字符串转换成列表或者元组，转换后列表或者元组的每一个元素为一个字符</span><br><span class="line">d = list(a)</span><br><span class="line">e = tuple(a)</span><br><span class="line"></span><br><span class="line"># ''.join(obj) 将obj转换为字符串</span><br><span class="line">f = ''.join(b)</span><br><span class="line">g = ''.join(c)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python39.png" alt="39"></p><h2 id="Python将字符串翻转"><a href="#Python将字符串翻转" class="headerlink" title="Python将字符串翻转"></a><font size="4">Python将字符串翻转</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = 'hello python'</span><br><span class="line"></span><br><span class="line"># 通过索引翻转[::-1]</span><br><span class="line">a[::-1]</span><br><span class="line"></span><br><span class="line"># Python不允许字符串进行翻转，但是可以借助列表进行翻转，然后再转换为字符串即可</span><br><span class="line">a = list(a)</span><br><span class="line">a.reverse()</span><br><span class="line">a = ''.join(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python40.png" alt="40"></p><h2 id="Python将字符串排序"><a href="#Python将字符串排序" class="headerlink" title="Python将字符串排序"></a><font size="4">Python将字符串排序</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = 'abcabbc'</span><br><span class="line"></span><br><span class="line"># Python不允许字符串进行排序，但是可以借助列表进行排序，然后再转换为字符串即可</span><br><span class="line">a = list(a)</span><br><span class="line">a.sort()</span><br><span class="line">a = ''.join(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python41.png" alt="41"></p><h2 id="Python判断字符串类型"><a href="#Python判断字符串类型" class="headerlink" title="Python判断字符串类型"></a><font size="4">Python判断字符串类型</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a = '1a2b3c'</span><br><span class="line">b = 'abcdef'</span><br><span class="line">c = '123456'</span><br><span class="line"></span><br><span class="line"># obj.isalnum() 判断字符串是否全是字母或数字</span><br><span class="line">a.isalnum()</span><br><span class="line"></span><br><span class="line"># obj.isalpha() 判断字符串是否全是字符</span><br><span class="line">b.isalpha()</span><br><span class="line"></span><br><span class="line"># obj.isdigit() 判断字符串是否全是数字</span><br><span class="line">c.isdigit()</span><br><span class="line"></span><br><span class="line"># obj.islower() 判断字符串是否全是小写字母</span><br><span class="line">b.islower()</span><br><span class="line"></span><br><span class="line"># obj.isupper() 判断字符串是否全是大写字母</span><br><span class="line">b.isupper()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python42.png" alt="42"></p><h2 id="Python字符串居中"><a href="#Python字符串居中" class="headerlink" title="Python字符串居中"></a><font size="4">Python字符串居中</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = '----------'</span><br><span class="line"></span><br><span class="line"># obj.center(width, fillchar) 字符串居中，总长度为width，两边填充的字符为fillchar，默认为空格</span><br><span class="line">a.center(20)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python43.png" alt="43"></p><h2 id="Python查找子串出现的次数"><a href="#Python查找子串出现的次数" class="headerlink" title="Python查找子串出现的次数"></a><font size="4">Python查找子串出现的次数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = 'abcabbc'</span><br><span class="line"></span><br><span class="line"># obj.count(str, begin, end) 查找子串str从begin到end-1出现的次数，默认从第一个到最后一个</span><br><span class="line">a.count('ab')</span><br><span class="line">a.count('ab', 1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python44.png" alt="44"></p><h2 id="Python查询字符串开头或者结尾是否为某一子串"><a href="#Python查询字符串开头或者结尾是否为某一子串" class="headerlink" title="Python查询字符串开头或者结尾是否为某一子串"></a><font size="4">Python查询字符串开头或者结尾是否为某一子串</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = 'abcabbc'</span><br><span class="line"></span><br><span class="line"># obj.startswith(str, begin, end) 判断obj从begin到end-1是否以str开头</span><br><span class="line">a.startswith('abc')</span><br><span class="line"></span><br><span class="line"># obj.endswith(str, begin, end) 判断obj从begin到end-1是否以str结尾</span><br><span class="line">a.endswith('bc')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python45.png" alt="45"></p><h2 id="Python求字符串中子串的索引"><a href="#Python求字符串中子串的索引" class="headerlink" title="Python求字符串中子串的索引"></a><font size="4">Python求字符串中子串的索引</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = 'abcabbc'</span><br><span class="line"></span><br><span class="line"># obj.find(str, begin, end) 从左边查找子串str从begin到end-1出现的索引，默认从第一个到最后一个</span><br><span class="line">a.find('abb')</span><br><span class="line">a.find('abbds')</span><br><span class="line"></span><br><span class="line"># obj.rfind(str, begin, end) 从右边查找子串str从begin到end-1出现的索引，默认从第一个到最后一个</span><br><span class="line">a.rfind('abb')</span><br><span class="line">a.rfind('abbds')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python46.png" alt="46"></p><h2 id="Python将字符串左边或右边的字符删去"><a href="#Python将字符串左边或右边的字符删去" class="headerlink" title="Python将字符串左边或右边的字符删去"></a><font size="4">Python将字符串左边或右边的字符删去</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = 'abcabba'</span><br><span class="line"></span><br><span class="line"># obj.lstrip(str) 将左边的str字符删去，如果str为字符串则代表字符串中的所有字符都删去</span><br><span class="line">a.lstrip('a')</span><br><span class="line"></span><br><span class="line"># obj.rstrip(str) 将右边的str字符删去，如果str为字符串则代表字符串中的所有字符都删去</span><br><span class="line">a.rstrip('a')</span><br><span class="line"></span><br><span class="line"># obj.strip(str) 将左边和右边的str字符删去，如果str为字符串则代表字符串中的所有字符都删去</span><br><span class="line">a.strip('a')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python47.png" alt="47"></p><h2 id="Python替换字符串"><a href="#Python替换字符串" class="headerlink" title="Python替换字符串"></a><font size="4">Python替换字符串</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = 'abcabbc'</span><br><span class="line"></span><br><span class="line"># obj.replace(old, new) 将old子串用new替代</span><br><span class="line">a.replace('abb', 'cdd')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python48.png" alt="48"></p><h2 id="Python拆分字符串"><a href="#Python拆分字符串" class="headerlink" title="Python拆分字符串"></a><font size="4">Python拆分字符串</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = 'abcabbc'</span><br><span class="line"></span><br><span class="line"># obj.partition(str) 将字符串拆分为3部分，str之前的部分，str，str之后的部分</span><br><span class="line"></span><br><span class="line"># obj.split(str) 将obj按照str拆分，默认为空格拆分</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python49.png" alt="49"></p><h2 id="Python字符串格式化"><a href="#Python字符串格式化" class="headerlink" title="Python字符串格式化"></a><font size="4">Python字符串格式化</font></h2><script type="math/tex; mode=display">\begin{array}{|c|c|} 格式 & 描述 \\  \%c & 以ASCII码格式化字符 \\ \%s & 格式化字符串 \\ \%d & 格式化整数 \\ \%m.nf & 格式化浮点数，m指总长度，n指小数点后面的精度，不够在左侧补空格 \\ \%-m.nf & 格式化浮点数，m指总长度，n指小数点后面的精度，不够在右侧补空格 \\ \end{array}</script><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># '{}...{}...'.format('xxx', 'yyy', ...) 将xxx，yyy填入字符串的花括号中</span><br><span class="line">s1 = '{}的网址是"{}"'.format('阿里巴巴', 'www.alibabagroup.com')</span><br><span class="line">s2 = '{0}的网址是"{1}"'.format('腾讯', 'www.tencent.com')</span><br><span class="line">s3 = '{name}的网址是"{site}"'.format(name='百度', site='www.baidu.com')</span><br><span class="line"></span><br><span class="line"># '{0:x}...{1:y}...'.format('xxx', 'yyy', ...) 将xxx，yyy填入字符串的花括号中，x和y指输入的格式，可以达到美化效果</span><br><span class="line">s4 = '{name:10s}==&gt;{id:10d}'.format(name='张三', id=1)</span><br><span class="line"></span><br><span class="line"># '格式1, 格式2, 格式3' %(数据1, 数据2, 数据3) 将数据1以格式1的方式，数据2以格式2的方式，数据3以格式3的方式放入字符串中，注意格式与数据直接没有逗号连接</span><br><span class="line">s5 = '%s：%d/%d/%d' %('今天的日期为', 2019, 9, 18)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python86.png" alt="86"></p><h1 id="Str小结"><a href="#Str小结" class="headerlink" title="Str小结"></a><font size="5" color="red">Str小结</font></h1><p>  Str字符串是Python中一种常见的结构，在实际的应用中，经常有许多数据无法用数字表示，如姓名，地址等信息，因此使用的频率也是非常高的，所以要灵活掌握Str的应用。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Str(字符串)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Tuple(元组)</title>
    <link href="https://USTCcoder.github.io/2019/09/12/python_tuple/"/>
    <id>https://USTCcoder.github.io/2019/09/12/python_tuple/</id>
    <published>2019-09-12T01:46:15.000Z</published>
    <updated>2020-07-27T06:13:59.706Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python6.jpg" alt="5"></p><h1 id="Tuple介绍"><a href="#Tuple介绍" class="headerlink" title="Tuple介绍"></a><font size="5" color="red">Tuple介绍</font></h1><p>  Python中的Tuple类似于一种带上枷锁的列表，功能和List类似，但是不能够修改其中的元素和顺序。<br><a id="more"></a></p><h1 id="Tuple操作"><a href="#Tuple操作" class="headerlink" title="Tuple操作"></a><font size="5" color="red">Tuple操作</font></h1><h2 id="Python创建元组"><a href="#Python创建元组" class="headerlink" title="Python创建元组"></a><font size="4">Python创建元组</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># tuple(iterable) 将可迭代对象转换为tuple类型</span><br><span class="line">a = tuple(range(5))</span><br><span class="line"></span><br><span class="line"># (a, b, c, ...) 或者 a, b, c, ...创建元素为a, b, c, ...的元组，创建单元素元组时，需要加逗号,</span><br><span class="line">b = (1, 3.14, 'hello world', True, [1, 2, 3])</span><br><span class="line">c = 1, 2, 3</span><br><span class="line">d = (2,)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python20.png" alt="20"></p><h2 id="Python索引元组元素"><a href="#Python索引元组元素" class="headerlink" title="Python索引元组元素"></a><font size="4">Python索引元组元素</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 通过中括号[]索引元组元素，和List相同</span><br><span class="line">a = (1, 3.14, 'hello world', True, (1, 2, 3)) </span><br><span class="line">b = a[1:3]</span><br><span class="line">c = a[4]</span><br><span class="line">d = a[4][1]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python21.png" alt="21"></p><h2 id="Python向元组中增加，删除，修改元素"><a href="#Python向元组中增加，删除，修改元素" class="headerlink" title="Python向元组中增加，删除，修改元素"></a><font size="4">Python向元组中增加，删除，修改元素</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = (1, 3, 6, 7, 9)</span><br><span class="line"></span><br><span class="line"># Python中不允许在原来的元组上修改任何元素，List中的append，pop，都无法使用，如果想修改元素必须采用算术运算修改元素</span><br><span class="line">b = a[:2] + (5,) + a[3:]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python22.png" alt="22"></p><h2 id="Python元组大小比较"><a href="#Python元组大小比较" class="headerlink" title="Python元组大小比较"></a><font size="4">Python元组大小比较</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># obj1 op obj2 将两个元组进行大小比较，从第一个元素开始比较，如果相同继续比较</span><br><span class="line">a = (1, 3.14, 'hello world', True, [1, 2, 3])</span><br><span class="line">b = (1, 2.71, 'hello world', True, [1, 2, 3]) </span><br><span class="line">a &gt; b</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python23.png" alt="23"></p><h2 id="Python元组乘法"><a href="#Python元组乘法" class="headerlink" title="Python元组乘法"></a><font size="4">Python元组乘法</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># obj * n，n为正整数，将obj复制n次</span><br><span class="line">a = (1, 3.14, True, [1, 2, 3])</span><br><span class="line">b = a * 3</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python24.png" alt="24"></p><h2 id="Python判断元素是否在元组中"><a href="#Python判断元素是否在元组中" class="headerlink" title="Python判断元素是否在元组中"></a><font size="4">Python判断元素是否在元组中</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># data in obj，判断data是否在obj中，data not in obj，判断data是否不在obj中</span><br><span class="line">a = (1, 3.14, True, [1, 2, 3])</span><br><span class="line">b = 3.14 in a</span><br><span class="line">c = True not in a</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python25.png" alt="25"></p><h2 id="Python求某个元素出现的次数"><a href="#Python求某个元素出现的次数" class="headerlink" title="Python求某个元素出现的次数"></a><font size="4">Python求某个元素出现的次数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># obj.count(data) 求data在元组中出现的次数</span><br><span class="line">a = (1, 3, 1, 2, 5)</span><br><span class="line">a.count(1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python26.png" alt="26"></p><h2 id="Python求某个元素的索引"><a href="#Python求某个元素的索引" class="headerlink" title="Python求某个元素的索引"></a><font size="4">Python求某个元素的索引</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># obj.index(data, begin, end) 从begin到end-1中索引第一次出现data的位置，默认从第一个元素到最后一个元素</span><br><span class="line">a = (1, 3, 1, 2, 5)</span><br><span class="line">a.index(1)</span><br><span class="line">a.index(1, 1, 3)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python27.png" alt="27"></p><h2 id="Python将元组翻转"><a href="#Python将元组翻转" class="headerlink" title="Python将元组翻转"></a><font size="4">Python将元组翻转</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = (1, 3, 1, 2, 5)</span><br><span class="line"></span><br><span class="line"># 通过索引翻转[::-1]</span><br><span class="line">a[::-1]</span><br><span class="line"></span><br><span class="line"># Python不允许元组进行翻转，但是元组和列表都是可迭代对象，可以互相转换，于是可以先转换为列表进行翻转，然后再转换为元组即可</span><br><span class="line">a = list(a)</span><br><span class="line">a.reverse()</span><br><span class="line">a = tuple(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python28.png" alt="28"></p><h2 id="Python将列表排序"><a href="#Python将列表排序" class="headerlink" title="Python将列表排序"></a><font size="4">Python将列表排序</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Python不允许元组进行排序，但是元组和列表都是可迭代对象，可以互相转换，于是可以先转换为列表进行排序，然后再转换为元组即可</span><br><span class="line">a = (1, 3, 1, 2, 5)</span><br><span class="line">a = list(a)</span><br><span class="line">a.sort()</span><br><span class="line">a = tuple(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python29.png" alt="29"></p><h1 id="Tuple小结"><a href="#Tuple小结" class="headerlink" title="Tuple小结"></a><font size="5" color="red">Tuple小结</font></h1><p>  Tuple元组是Python中一种常见的结构，由于元组的元素操作存在限制，因此可以用来存储固定不变的数据，防止出现误操作使数据修改，如存放个人出生年月，身份证号等信息，因此使用的频率也是非常高的，所以要灵活掌握Tuple的应用。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Tuple(元组)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>List(列表)</title>
    <link href="https://USTCcoder.github.io/2019/09/11/python_list/"/>
    <id>https://USTCcoder.github.io/2019/09/11/python_list/</id>
    <published>2019-09-11T07:39:15.000Z</published>
    <updated>2020-07-27T06:14:18.663Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python5.jpg" alt="5"></p><h1 id="List介绍"><a href="#List介绍" class="headerlink" title="List介绍"></a><font size="5" color="red">List介绍</font></h1><p>  Python中的List是可迭代对象，类似于C/C++中的数组，但是比数组更加灵活，可以动态的随意增加和删除元素，还可以存储不同的数据类型。<br><a id="more"></a></p><h1 id="List操作"><a href="#List操作" class="headerlink" title="List操作"></a><font size="5" color="red">List操作</font></h1><h2 id="Python创建列表"><a href="#Python创建列表" class="headerlink" title="Python创建列表"></a><font size="4">Python创建列表</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># list(iterable) 将可迭代对象转换为list类型</span><br><span class="line">a = list(range(5))</span><br><span class="line"></span><br><span class="line"># [a, b, c, ...] 创建元素为a, b, c, ...的列表</span><br><span class="line">b = [1, 3.14, 'hello world', True, [1, 2, 3]]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python9.png" alt="9"></p><h2 id="Python索引列表元素"><a href="#Python索引列表元素" class="headerlink" title="Python索引列表元素"></a><font size="4">Python索引列表元素</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 和C/C++相同，通过中括号[]索引列表元素，可以通过:运算符获取连续的索引，负数索引为从后向前索引，-1代表最后一个元素，-2代表倒数第二个元素</span><br><span class="line">a = [1, 3.14, 'hello world', True, [1, 2, 3]] </span><br><span class="line">b = a[1:3]</span><br><span class="line">c = a[4]</span><br><span class="line">d = a[4][1]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python10.png" alt="10"></p><h2 id="Python向列表中增加元素"><a href="#Python向列表中增加元素" class="headerlink" title="Python向列表中增加元素"></a><font size="4">Python向列表中增加元素</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># obj.append(obj1) 在obj末尾追加obj1，可以追加一个数据，也可以追加一个列表</span><br><span class="line">a = [1, 3.14, 'hello world', True, [1, 2, 3]] </span><br><span class="line">a.append(-1)</span><br><span class="line">a.append([1, 2, 3])</span><br><span class="line"></span><br><span class="line"># obj1 + obj2 将两个列表相加，obj2会追加在obj1的后面</span><br><span class="line">b = a + [[1, 2, 3]] # 得到的值等价于a.append([1, 2, 3])，但是a.append()只能在a后面追加，如果想赋值给b而不想改变a就要通过加法运算</span><br><span class="line">c = a + [1] # 得到的值等价于a.append(1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python11.png" alt="11"></p><h2 id="Python从列表中删除元素"><a href="#Python从列表中删除元素" class="headerlink" title="Python从列表中删除元素"></a><font size="4">Python从列表中删除元素</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = [1, 3.14, 'hello world', True, [1, 2, 3]] </span><br><span class="line"></span><br><span class="line"># obj.remove(data) 从obj中删除首次出现值为data的元素</span><br><span class="line">a.remove(3.14)</span><br><span class="line"></span><br><span class="line"># del obj 删除obj数据，是用来删除不用的变量，也可以删除列表中的某个元素</span><br><span class="line">del a[1]</span><br><span class="line"></span><br><span class="line"># obj.pop(n) 弹出第n个元素并返回该元素</span><br><span class="line">b = a.pop(-1)</span><br><span class="line"></span><br><span class="line"># obj.clear() 删除所有元素</span><br><span class="line">a.clear()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python12.png" alt="12"></p><h2 id="Python列表大小比较"><a href="#Python列表大小比较" class="headerlink" title="Python列表大小比较"></a><font size="4">Python列表大小比较</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># obj1 op obj2 将两个列表进行大小比较，从第一个元素开始比较，如果相同继续比较</span><br><span class="line">a = [1, 3.14, 'hello world', True, [1, 2, 3]] </span><br><span class="line">b = [1, 2.71, 'hello world', True, [1, 2, 3]] </span><br><span class="line">a &gt; b</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python13.png" alt="13"></p><h2 id="Python列表乘法"><a href="#Python列表乘法" class="headerlink" title="Python列表乘法"></a><font size="4">Python列表乘法</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># obj * n，n为正整数，将obj复制n次</span><br><span class="line">a = [1, 3.14, True, [1, 2, 3]] </span><br><span class="line">b = a * 3</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python14.png" alt="14"></p><h2 id="Python判断元素是否在列表中"><a href="#Python判断元素是否在列表中" class="headerlink" title="Python判断元素是否在列表中"></a><font size="4">Python判断元素是否在列表中</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># data in obj，判断data是否在obj中，data not in obj，判断data是否不在obj中</span><br><span class="line">a = [1, 3.14, True, [1, 2, 3]]</span><br><span class="line">b = 3.14 in a</span><br><span class="line">c = True not in a</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python15.png" alt="15"></p><h2 id="Python求某个元素出现的次数"><a href="#Python求某个元素出现的次数" class="headerlink" title="Python求某个元素出现的次数"></a><font size="4">Python求某个元素出现的次数</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># obj.count(data) 求data在列表中出现的次数</span><br><span class="line">a = [1, 3, 1, 2, 5]</span><br><span class="line">a.count(1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python16.png" alt="16"></p><h2 id="Python求某个元素的索引"><a href="#Python求某个元素的索引" class="headerlink" title="Python求某个元素的索引"></a><font size="4">Python求某个元素的索引</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># obj.index(data, begin, end) 从begin到end-1中索引第一次出现data的位置，默认从第一个元素到最后一个元素</span><br><span class="line">a = [1, 3, 1, 2, 5]</span><br><span class="line">a.index(1)</span><br><span class="line">a.index(1, 1, 3)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python17.png" alt="17"></p><h2 id="Python将列表翻转"><a href="#Python将列表翻转" class="headerlink" title="Python将列表翻转"></a><font size="4">Python将列表翻转</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = [1, 3, 1, 2, 5]</span><br><span class="line"></span><br><span class="line"># 通过索引翻转[::-1]</span><br><span class="line">a[::-1]</span><br><span class="line"></span><br><span class="line"># obj.reverse() 将列表反转</span><br><span class="line">a.reverse()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python18.png" alt="18"></p><h2 id="Python将列表排序"><a href="#Python将列表排序" class="headerlink" title="Python将列表排序"></a><font size="4">Python将列表排序</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># obj.sort(key, reverse=False) 将obj按照关键字key进行排序，reverse=False默认为从小到大排序，reverse=True为从大到小排序</span><br><span class="line">a = [1, 3, 1, 2, 5]</span><br><span class="line">a.sort()</span><br><span class="line">a.sort(reverse=True)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python19.png" alt="19"></p><h1 id="List小结"><a href="#List小结" class="headerlink" title="List小结"></a><font size="5" color="red">List小结</font></h1><p>  List列表是Python中最灵活的一种结构，没有任何的限制，可以代替栈和队列的各种操作，因此使用的频率也是非常高的，所以要灵活掌握List的应用。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;List(列表)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Python基础</title>
    <link href="https://USTCcoder.github.io/2019/09/10/python_foundation/"/>
    <id>https://USTCcoder.github.io/2019/09/10/python_foundation/</id>
    <published>2019-09-10T10:39:15.000Z</published>
    <updated>2020-07-27T06:15:35.238Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python.jpg" alt="2"></p><h1 id="Python介绍"><a href="#Python介绍" class="headerlink" title="Python介绍"></a><font size="5" color="red">Python介绍</font></h1><p>  在前面已经介绍了Python的由来，这里主要介绍Python的基础知识，包括Python运算和Python结构。<br><a id="more"></a></p><h1 id="Python运算"><a href="#Python运算" class="headerlink" title="Python运算"></a><font size="5" color="red">Python运算</font></h1><p><img src="/images/LANGUAGE/python3.jpg" alt="3"></p><h2 id="Python创建变量"><a href="#Python创建变量" class="headerlink" title="Python创建变量"></a><font size="4">Python创建变量</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Python中的常用数据类型可以直接赋值即定义，和C，Java等语言不同，且可以同时赋值多个变量</span><br><span class="line">a = 10</span><br><span class="line">b = 3.14</span><br><span class="line">c, d = 'Hello Python', True</span><br><span class="line"></span><br><span class="line"># 可以用type(obj) 查看创建的变量类型，isinstance(obj, type)查看变量obj和type是否为相同类型</span><br><span class="line">type(a)</span><br><span class="line">type(b)</span><br><span class="line">isinstance(c, str)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python1.png" alt="1"></p><h2 id="Python算术运算"><a href="#Python算术运算" class="headerlink" title="Python算术运算"></a><font size="4">Python算术运算</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># +(加)，-(减)，*(乘)，/(除)，//(地板除)，**(乘方)，%(求余)，整数除法/可以得到小数，和C，Java等语言不同</span><br><span class="line">a, b = 9, 4</span><br><span class="line">a / b</span><br><span class="line">a // b</span><br><span class="line">a ** b</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python2.png" alt="2"></p><h2 id="Python关系运算"><a href="#Python关系运算" class="headerlink" title="Python关系运算"></a><font size="4">Python关系运算</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># &gt;(大于)，&lt;(小于)，&gt;=(大于等于)，&lt;=(小于等于)，==(等于)，!=(不等于)，和C，Java等语言不同，字符串也可以直接比较大小，字符串之间根据ASCII码值越大则字符串越大，先比较第一个，如果相同继续向下比较</span><br><span class="line">a, b = 3.14, 1.414</span><br><span class="line">c, d = 'Hello', 'Python'</span><br><span class="line">a &gt; b</span><br><span class="line">c &gt; d</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python108.png" alt="108"></p><h2 id="Python逻辑运算"><a href="#Python逻辑运算" class="headerlink" title="Python逻辑运算"></a><font size="4">Python逻辑运算</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># and(与)，or(或)，not(非)，和C，Java等语言不同，用&amp;&amp;，||，!表示与或非</span><br><span class="line">a, b = True, False</span><br><span class="line">a and b</span><br><span class="line">a or b</span><br><span class="line">not a</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python3.png" alt="3"></p><h2 id="Python条件表达式"><a href="#Python条件表达式" class="headerlink" title="Python条件表达式"></a><font size="4">Python条件表达式</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># x if condition else y 判断condition，为真则x，为假则y，和C，Java等语言不同(b?x:y)</span><br><span class="line">a, b, c, d = 1, 2, 3, 4</span><br><span class="line">x = a if c &gt; d else b</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python4.png" alt="4"></p><h1 id="Python结构"><a href="#Python结构" class="headerlink" title="Python结构"></a><font size="5" color="red">Python结构</font></h1><p><img src="/images/LANGUAGE/python4.jpg" alt="4"></p><h2 id="Python条件结构"><a href="#Python条件结构" class="headerlink" title="Python条件结构"></a><font size="4">Python条件结构</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Python条件结构if condition1: 语句1 elif condition2: 语句2 else 语句3 先判断if后面的condition1是否成立，如果成立则执行语句1并结束判断，否则判断elif后面的condition2是否成立，如果成立则执行语句2并结束判断，否则执行语句3，和C，Java等语言不同(if ... else if ... else ...)</span><br><span class="line"># if condition1:</span><br><span class="line">#     语句1</span><br><span class="line"># elif condition2:</span><br><span class="line">#     语句2</span><br><span class="line"># else:</span><br><span class="line">#     语句3</span><br><span class="line">if 3 &gt; 5:</span><br><span class="line">    a = 1</span><br><span class="line">elif 3 &lt; 5:</span><br><span class="line">    a = 2</span><br><span class="line">else:</span><br><span class="line">    a = 3</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python5.png" alt="5"></p><h2 id="Python循环结构-while"><a href="#Python循环结构-while" class="headerlink" title="Python循环结构(while)"></a><font size="4">Python循环结构(while)</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># while condition: 循环语句 while循环，先判断condition是否满足，如果满足则进入循环执行循环语句，否则循环结束，和C，Java等语言不同(还有do ... while)</span><br><span class="line"># while condition:</span><br><span class="line">#     循环语句</span><br><span class="line">res, i = 0, 1</span><br><span class="line">while i &lt; 10:</span><br><span class="line">    res, i = res + i, i + 1</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python6.png" alt="6"></p><h2 id="Python循环结构-for"><a href="#Python循环结构-for" class="headerlink" title="Python循环结构(for)"></a><font size="4">Python循环结构(for)</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># range(start=0, end, step=1) 从start开始到end-1，start默认为0，step默认为1，返回步长为step的所有数字组成的迭代器</span><br><span class="line"></span><br><span class="line"># for target in iterable: 循环语句 for循环，target从可迭代对象iterable中逐次取出然后执行循环语句，和C，Java等语言不同(for(初始条件;终止条件;自变量变化操作))</span><br><span class="line"># for target in iterable:</span><br><span class="line">#     循环语句</span><br><span class="line">res = 0</span><br><span class="line">for i in range(1, 10):</span><br><span class="line">    res += i</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python7.png" alt="7"></p><h2 id="break，continue语句"><a href="#break，continue语句" class="headerlink" title="break，continue语句"></a><font size="4">break，continue语句</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 对于循环结构来说，break和continue语句非常重要，用法和C，Java等语言相同</span><br><span class="line"># break：终止并跳出循环</span><br><span class="line"># continue：终止本次循环，进入下一次循环</span><br><span class="line"></span><br><span class="line">res_break, res_continue = 0, 0</span><br><span class="line"></span><br><span class="line"># 1 + 2 + 3 + 4 = 10</span><br><span class="line">for i in range(1, 10):</span><br><span class="line">    if i == 5:</span><br><span class="line">        break </span><br><span class="line">    res_break += i</span><br><span class="line"></span><br><span class="line"># 1 + 2 + 3 + 4 + 6 + 7 + 8 + 9 = 40</span><br><span class="line">for i in range(1, 10):</span><br><span class="line">    if i == 5:</span><br><span class="line">        continue</span><br><span class="line">    res_continue += i</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LANGUAGE/python8.png" alt="8"></p><h1 id="Python小结"><a href="#Python小结" class="headerlink" title="Python小结"></a><font size="5" color="red">Python小结</font></h1><p>  基础部分每种语言都大同小异，因为基础部分是所有语言的基础，学习每一种语言都离不开运算操作和算法结构，虽然难度较小，但是非常重要，无论以后从事什么样的研究，基础能力都是必不可少的，因此需要熟练掌握。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Python基础&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Git(分布式版本控制系统)</title>
    <link href="https://USTCcoder.github.io/2019/09/10/skill%20Git/"/>
    <id>https://USTCcoder.github.io/2019/09/10/skill Git/</id>
    <published>2019-09-10T09:10:40.000Z</published>
    <updated>2020-05-20T06:56:38.830Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Git</font></strong></center><p></p><h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a><font size="5" color="red">原理介绍</font></h1><p>  <strong>Git:是一个开源的分布式版本控制系统</strong>，可以有效、高速地处理<strong>从很小到非常大的项目</strong>版本管理。特别适合于<strong>企业级项目</strong>的使用，因此也成为了<strong>程序员必备技能</strong>之一。Git中的大多数语句是Linux语句，因为最初它就是由Linux之父Linus Torvalds帮助管理 Linux 内核开发而开发的。<br><a id="more"></a></p><h1 id="Git特点"><a href="#Git特点" class="headerlink" title="Git特点"></a><font size="5" color="red">Git特点</font></h1><p>  <font size="3">直接记录快照，而非差异比较，速度非常快。</font><br>  <font size="3">几乎所有操作都是本地执行，方便管理。</font><br>  <font size="3">时刻保存数据完整性，保证信息的不丢失。</font><br>  <font size="3">适合于分布式开发，服务器压力小。</font><br>  <font size="3">开发者之间可以容易的解决冲突。</font></p><h1 id="Git关系图"><a href="#Git关系图" class="headerlink" title="Git关系图"></a><font size="5" color="red">Git关系图</font></h1><p>  <font size="3">Git的逻辑较为复杂，一定要理解逻辑图，并记住状态之间的相互转换。</font><br><img src="/images/SKILL/git.png" alt="0"></p><h1 id="Git应用"><a href="#Git应用" class="headerlink" title="Git应用"></a><font size="5" color="red">Git应用</font></h1><h2 id="创建管理员身份"><a href="#创建管理员身份" class="headerlink" title="创建管理员身份"></a><font size="4">创建管理员身份</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># git config --global user.name XXX 创建管理员用户名XXX</span><br><span class="line"></span><br><span class="line"># git config --global user.email YYY 创建管理员邮箱YYY</span><br></pre></td></tr></tbody></table></figure><h2 id="将文件夹变成管理库文件夹"><a href="#将文件夹变成管理库文件夹" class="headerlink" title="将文件夹变成管理库文件夹"></a><font size="4">将文件夹变成管理库文件夹</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># git init 创建管理库文件夹</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git1.png" alt="1"></p><h2 id="在文件夹中添加文件"><a href="#在文件夹中添加文件" class="headerlink" title="在文件夹中添加文件"></a><font size="4">在文件夹中添加文件</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># touch xxx 添加文件xxx</span><br><span class="line">touch dm01.py</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git2.png" alt="2"></p><h2 id="查看文件的状态"><a href="#查看文件的状态" class="headerlink" title="查看文件的状态"></a><font size="4">查看文件的状态</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># git status 查看文件状态(详细叙述)</span><br><span class="line"></span><br><span class="line"># git status -s 查看文件状态(简写)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git3.png" alt="3"></p><h2 id="将文件加入文件管理库"><a href="#将文件加入文件管理库" class="headerlink" title="将文件加入文件管理库"></a><font size="4">将文件加入文件管理库</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># git add xxx 将xxx文件加入文件管理库</span><br><span class="line">git add dm01.py</span><br><span class="line"></span><br><span class="line"># git add . 将所有文件加入文件管理库</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git4.png" alt="4"></p><h2 id="提交文件"><a href="#提交文件" class="headerlink" title="提交文件"></a><font size="4">提交文件</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># git commit -m xxx yyy 提交yyy文件到xxx版本中，如果只有xxx，则提交当前所有可提交文件至xxx版本</span><br><span class="line">git commit -m dm01.py</span><br><span class="line"></span><br><span class="line"># git commit --amend --no-edit xxx 将xxx文件添加到最新的版本</span><br><span class="line"></span><br><span class="line"># git commit --amend -m xxx 为最后的版本添加xxx说明</span><br><span class="line"></span><br><span class="line"># git commit --amend 编辑文本说明</span><br><span class="line"></span><br><span class="line"># git commit -am xxx 将所有文件先添加为Staged状态，再进行提交，等价于git add . + git commit -m xxx</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git5.png" alt="5"></p><h2 id="编辑文件"><a href="#编辑文件" class="headerlink" title="编辑文件"></a><font size="4">编辑文件</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># vim xxx 用vim编辑xxx文件，文件状态由Unmodified变为Modified，再次提交时需要先add变成Staged状态</span><br><span class="line">vim dm01.py</span><br><span class="line"></span><br><span class="line">print('First addition!')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git6.png" alt="6"></p><h2 id="查看之前已提交的更改"><a href="#查看之前已提交的更改" class="headerlink" title="查看之前已提交的更改"></a><font size="4">查看之前已提交的更改</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># git log 查看文件的具体信息</span><br><span class="line"></span><br><span class="line"># git log --oneline 查看文件的简要信息</span><br><span class="line"></span><br><span class="line"># git log --graph 以图的形式查看文件的具体信息</span><br><span class="line"></span><br><span class="line"># git log --graph --oneline 以图的形式查看文件的简要信息</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git7.png" alt="7"></p><h2 id="查看之前更改的内容"><a href="#查看之前更改的内容" class="headerlink" title="查看之前更改的内容"></a><font size="4">查看之前更改的内容</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># git diff 查看Modified与之前提交的区别</span><br><span class="line"></span><br><span class="line"># git diff --cached 查看Stage与之前提交的区别</span><br><span class="line"></span><br><span class="line"># git diff HEAD 同时有Modified和Stage状态，查看总区别</span><br><span class="line"></span><br><span class="line"># git diff ID1, ID2 查看两个历史快照的区别，ID1为之前的，ID2是后面的，其中输入git log --oneline中显示在最前面的编号即为ID</span><br><span class="line">git diff 84cefee 521a873</span><br><span class="line"></span><br><span class="line"># git diff ID 比较ID与当前目录内容</span><br><span class="line">git diff 84cefee</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git8.png" alt="8"></p><h2 id="将文件从Staged状态返回至Modified状态"><a href="#将文件从Staged状态返回至Modified状态" class="headerlink" title="将文件从Staged状态返回至Modified状态"></a><font size="4">将文件从Staged状态返回至Modified状态</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim dm01.py # 将dm01状态变为红色的M，即为Modified状态</span><br><span class="line"></span><br><span class="line">git add dm01.py #将dm01从红色的M变成绿色的M，即为Staged状态</span><br><span class="line"></span><br><span class="line"># git reset xxx 将xxx从Staged状态返回至Modified状态</span><br><span class="line">git reset dm01.py</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git9.png" alt="9"></p><h2 id="查看每一步的变化"><a href="#查看每一步的变化" class="headerlink" title="查看每一步的变化"></a><font size="4">查看每一步的变化</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># git reflog 查看对文件夹进行的所有操作，并且可以得到ID号和对应的指针</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git10.png" alt="10"></p><h2 id="回到过去"><a href="#回到过去" class="headerlink" title="回到过去"></a><font size="4">回到过去</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># git reset --method HEAD~n 回到前n个版本，method=soft为回滚快照，移动HEAD指向(仓库区域)，method=mixed(默认)为将快照回滚到临时区域，method=hard将快照还原到工作区，可以通过打开文件看到</span><br><span class="line"></span><br><span class="line"># git reset --method ID/point 回到指定的文件号/指针，method同上</span><br><span class="line"></span><br><span class="line"># git checkout ID -- xxx 对xxx文件回到ID状态，内容也会改变</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git11.png" alt="11"></p><h2 id="创建分支"><a href="#创建分支" class="headerlink" title="创建分支"></a><font size="4">创建分支</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># git branch 查看所有分支和当前所处分支</span><br><span class="line"></span><br><span class="line"># git branch xxx 创建xxx分支</span><br><span class="line">git branch br01</span><br><span class="line"></span><br><span class="line"># git checkout -b xxx 创建xxx分支</span><br><span class="line">git checkout -b br02</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git12.png" alt="12"></p><h2 id="切换分支"><a href="#切换分支" class="headerlink" title="切换分支"></a><font size="4">切换分支</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># git checkout xxx 切换到xxx分支</span><br><span class="line">git checkout br01</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git13.png" alt="13"></p><h2 id="删除分支"><a href="#删除分支" class="headerlink" title="删除分支"></a><font size="4">删除分支</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># git branch -d xxx 删除xxx分支，删除时不能位于该分支，该方法不能删除未合并的分支</span><br><span class="line">git branch -d br02</span><br><span class="line"></span><br><span class="line"># git branch -D xxx 删除未合并的xxx分支</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git14.png" alt="14"></p><h2 id="合并分支"><a href="#合并分支" class="headerlink" title="合并分支"></a><font size="4">合并分支</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># git merge xxx 在当前分支下，合并xxx分支</span><br><span class="line">git merge br01</span><br><span class="line"></span><br><span class="line"># 如果在新分支里面修改提交的文件，在原分支也经过了修改提交，则会出现报错，需要手动修改文件内容，然后再添加提交即可</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git15.png" alt="15"></p><h2 id="查看已合并的分支和未合并的分支"><a href="#查看已合并的分支和未合并的分支" class="headerlink" title="查看已合并的分支和未合并的分支"></a><font size="4">查看已合并的分支和未合并的分支</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># git branch --merged 查看已合并的分支</span><br><span class="line"></span><br><span class="line">#git branch --no-merged 查看未合并的分支</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git16.png" alt="16"></p><h2 id="换基操作"><a href="#换基操作" class="headerlink" title="换基操作"></a><font size="4">换基操作</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># git rebase xxx 当一个分支添加功能时，此时如果主分支发生修改，开发人员想基于修改过的主分支接着添加功能，可以使用换基操作，紧接着xxx之后进行修改</span><br></pre></td></tr></tbody></table></figure><h2 id="暂存区"><a href="#暂存区" class="headerlink" title="暂存区"></a><font size="4">暂存区</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># git stash 将现在的文件放到暂存区中(分支中存在未提交的文件，无法切换分支。此时可以将现在的文件放到暂存区中，然后切换分支)。</span><br><span class="line"></span><br><span class="line"># git stash list 查看暂存区的文件</span><br><span class="line"></span><br><span class="line"># git stash apply stash@{n} 从暂存区中恢复编号为n的文件(默认为最近一个)，暂存区文件不变</span><br><span class="line"></span><br><span class="line"># git stash drop stash@{n} 从暂存区中删除编号为n的文件(默认为最近一个)</span><br><span class="line"></span><br><span class="line"># git stash pop 从暂存区中取出最近的一个文件，暂存区中少一个文件，类似于栈的pop，等价于git stash apply + git stash drop</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git17.png" alt="17"></p><h2 id="链接到GitHub"><a href="#链接到GitHub" class="headerlink" title="链接到GitHub"></a><font size="4">链接到GitHub</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 打开GitHub，并新建一个仓库</span><br><span class="line"></span><br><span class="line"># git remote add origin xxx 链接到远程xxx地址，即为GitHub仓库地址</span><br><span class="line"></span><br><span class="line"># git push -u origin yyy 将yyy分支推上去，一般是master分支</span><br></pre></td></tr></tbody></table></figure><h2 id="Git三个区域"><a href="#Git三个区域" class="headerlink" title="Git三个区域"></a><font size="4">Git三个区域</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 工作区域：存放项目的地方，可以观察到的文件夹即为工作区域，修改的内容可以通过打开文件看到</span><br><span class="line"></span><br><span class="line"># 临时区域：stage区域，用于临时存放改动，即将提交的区域</span><br><span class="line"></span><br><span class="line"># 仓库区域：最终存放的版本数据，HEAD指向最终提交的内容</span><br></pre></td></tr></tbody></table></figure><h2 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a><font size="4">删除文件</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># git rm 删除工作区域和临时区域中的文件，但是仓库区域依然可以看到</span><br><span class="line"></span><br><span class="line"># git rm -f 强行删除工作区域和临时区域中的文件</span><br><span class="line">git rm -f dm01.py</span><br><span class="line"></span><br><span class="line"># git rm --cached 删除临时区的文件</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git18.png" alt="18"></p><h2 id="修改文件名"><a href="#修改文件名" class="headerlink" title="修改文件名"></a><font size="4">修改文件名</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># git mv old_name new_name 将原文件名old_name换为new_name</span><br><span class="line">git mv dm02.py dm03.py</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/SKILL/git19.png" alt="19"></p><h1 id="Git小结"><a href="#Git小结" class="headerlink" title="Git小结"></a><font size="5" color="red">Git小结</font></h1><p>  Git作为当下最红的分布式版本控制系统，使得程序员之间的协作更为方便，因此各大公司都使用Git作为一项必备技能。因为Git的命令大多为Linux语言，所以习惯于Windows平台的用户需要多多练习。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Git&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="Computer Science" scheme="https://USTCcoder.github.io/categories/Computer-Science/"/>
    
      <category term="Git" scheme="https://USTCcoder.github.io/categories/Computer-Science/Git/"/>
    
    
  </entry>
  
  <entry>
    <title>Python介绍</title>
    <link href="https://USTCcoder.github.io/2019/09/10/python_introduction/"/>
    <id>https://USTCcoder.github.io/2019/09/10/python_introduction/</id>
    <published>2019-09-10T00:17:15.000Z</published>
    <updated>2020-07-27T06:14:31.665Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LANGUAGE/python.png" alt="0"></p><h1 id="python由来"><a href="#python由来" class="headerlink" title="python由来"></a><font size="5" color="red">python由来</font></h1><p>  Python的创始人为荷兰人吉多·范罗苏姆(Guido van Rossum)。1989年圣诞节期间，在阿姆斯特丹，Guido为了打发圣诞节的无趣，决心开发一个新的脚本解释程序。选中Python作为该编程语言的名字，是取自英国20世纪70年代首播的电视喜剧(Monty Python’s Flying Circus)<br><a id="more"></a></p><h1 id="语言的比较"><a href="#语言的比较" class="headerlink" title="语言的比较"></a><font size="5" color="red">语言的比较</font></h1><p>  <font size="3">将其他语言翻译成机器语言的工具称为编译器，编译的方式有两种，一种是编译，一种是解释</font><br>  <font size="3">编译型语言：C/C++，Pascal等语言都属于编译型语言，先由编译器生成可执行文件，运行时不需要重新编译，直接使用编译的结果即可，因此程序执行效率高，跨平台能力差。</font><br>  <font size="3">解释型语言：Java，Python等语言都属于解释型语言，运行时由解释器逐行解释每一句源代码，每次运行都需要解释一次，因此程序执行效率低，跨平台能力强。</font><br><img src="/images/LANGUAGE/python1.jpg" alt="1"></p><h1 id="Python与C-C-的具体区别"><a href="#Python与C-C-的具体区别" class="headerlink" title="Python与C/C++的具体区别"></a><font size="5" color="red">Python与C/C++的具体区别</font></h1><p>  <font size="3">（1）代码格式，Python中的代码块以缩进标志，具有相同缩进的处于同一代码块，而C/C++以花括号对{}标志。</font><br>  <font size="3">（2）注释形式，Python中的注释以#开始或者以’’’xxx’’’完成大段注释，而C/C++以双斜杠//标志或者/<em>xxx</em>/完成大段注释。</font><br>  <font size="3">（3）定义变量，Python可以直接赋值a=5，而C/C++必须写int a=5。</font><br>  <font size="3">（4）赋值操作，Python可以同时赋值多个a, b=5, 6，而C/C++只能单独赋值。</font><br>  <font size="3">（5）除法操作，Python中对两个整数进行除法时，结果可以为小数，3 / 5 =0.6，而C/C++结果为0。</font><br>  <font size="3">（6）乘方操作，Python中**表示乘方，而C/C++没有乘方操作。</font><br>  <font size="3">（7）逻辑操作，Python中and，or，not表示与或非，而C/C++用&amp;&amp;，||，!表示。</font><br>  <font size="3">（8）条件表达式，Python中x if condition else y 判断condition为条件表达式，而C/C++用condition?x:y表示。</font><br>  <font size="3">（9）条件语句，Python中if … elif… else …，而C/C++用if … else if … else …。</font><br>  <font size="3">（10）自加自减，Python中没有i++或者++i，而C/C++有。</font><br>  <font size="3">（11）do … while语句，Python中没有do … while语句，而C/C++有。</font><br>  <font size="3">（12）大数字运算，Python中支持大整数的运算，而C/C++有数据类型的限制，一旦超过范围会出现问题。</font><br>  <font size="3">（13）：运算，Python中支持冒号运算获取连续索引，而C/C++冒号和问号一起作为三目运算符。</font><br>  <font size="3">（14）索引操作，Python中支持负数索引，-1代表最好一个元素，而C/C++不支持。</font><br>  <font size="3">（15）内置数据结构，Python中具有很多非常好用的内置数据结构，如列表，元组，字典等，而C/C++没有。</font><br>  <font size="3">（16）内置函数，Python中具有很多非常好用的内置函数，如len(), sorted()等，而C/C++没有。</font><br>  <font size="3">（17）内置语法结构，Python中具有很多非常好用的内置语法结构，如with语法，lambda表达式等，而C/C++没有。</font><br>  <font size="3">（18）函数返回值，Python中函数返回值可以同时返回多个值，而C/C++只能返回一个。</font><br>  <font size="3">（19）函数定义，Python中用def加函数名定义，且输入参数没有类型名，而C/C++必须先写返回值的类型名加函数名，且输入参数也必须有类型名。</font><br>  <font size="3">（20）宏定义，Python中没有宏定义，而C/C++可以使用define。</font><br>  <font size="3">（21）指针，Python中没有指针的概念，而C/C++指针是最重要也是最复杂的内容。</font><br>  <font size="3">（22）else，Python中else可以和if, while, for, try结合，而C/C++else一般和if结合在一起。</font><br>  <font size="3">（23）pass，Python中pass代表后面没有内容，也可以用三个小数点(…)表示，而C/C++没有pass，后面没有代码代表没有内容。</font><br>  <font size="3">（24）导入模块，Python中使用import导入模块，而C/C++使用include导入模块。</font><br>  <font size="3">（25）全局变量，Python中使用global在使用处声明，而且在外部也要先定义该变量，而C/C++用extern或者在函数外部直接定义。</font><br>  <font size="3">（26）对象指针，Python中类对象的指针为函数列表中的第一个参数，一般为self，而C/C++用this指针表示。</font><br>  <font size="3">（27）私有变量，Python中在属性前加(__)两个下划线，而C/C++用private定义。</font><br>  <font size="3">（28）动态语言，Python中可以动态给一个对象添加属性或方法，而C/C++必须提前定义，无法动态添加。</font><br>  <font size="3">（29）垃圾回收，Python中利用引用计数为主，隔代回收为辅对垃圾进行回收，而C/C++必须手动回收，也是最重要的环节，保证程序稳定性。</font><br>  <font size="3">（30）for循环，Python中for i in range(a)，并且在for中修改a的值，for循环仍然执行a次，这可能产生问题。</font></p><h1 id="Python特点"><a href="#Python特点" class="headerlink" title="Python特点"></a><font size="5" color="red">Python特点</font></h1><p>  <font size="3">简单性：Python语法非常简单，相比于大学通用课程的C省去了最晦涩的指针。</font><br>  <font size="3">简洁性：Python代码量约为Java的五分之一，故有”人生苦短，我用Python”。</font><br>  <font size="3">标准库：Python拥有强大的标准库，能解决大多数使用者的需求。</font><br>  <font size="3">社区强：Python社区非常强大，能够提供大量的第三方模块。</font><br>  <font size="3">可移植：Python由于其开源本质，已经被移植在很多平台。</font></p><h1 id="Python小结"><a href="#Python小结" class="headerlink" title="Python小结"></a><font size="5" color="red">Python小结</font></h1><p>  Python被称为胶水语言，主要归功于python库的强大，使得在机器学习，数据挖掘等方向火热的今天受到了广泛的关注，在各个软件排行榜中，Python都以最快的速度上升，由于其简单易学，许多高校也将传统的入门语言从C转向了Python，因此在AI盛行的时代，拥抱Python吧！</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Python介绍&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="语言学习" scheme="https://USTCcoder.github.io/categories/python/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>PyTorch</title>
    <link href="https://USTCcoder.github.io/2019/09/08/frame%20pytorch/"/>
    <id>https://USTCcoder.github.io/2019/09/08/frame pytorch/</id>
    <published>2019-09-08T05:13:15.000Z</published>
    <updated>2020-06-09T12:07:02.532Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">PyTorch</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>PyTorch:</strong>是Facebook公司于2017年1月发布的<strong>神经网络，深度学习开发平台</strong>。但是PyTorch的历史可以追溯到2002年，当时Torch使用了一种小众语言Lua作为借口，使用人数较少，在2017年推出了Python接口的Torch，故称为PyTorch，现在也称为了当下最流行的深度学习框架之一。<br><a id="more"></a></p><p><img src="/images/LIBRARY/pytorch.jpg" alt="pytorch"></p><h1 id="PyTorch特点"><a href="#PyTorch特点" class="headerlink" title="PyTorch特点"></a><font size="5" color="red">PyTorch特点</font></h1><p>  <font size="3">PyTorch具有高度的简洁性：和TensorFlow1.x版本有较大差距，便于用户使用和理解。</font><br>  <font size="3">PyTorch具有较快的速度：PyTorch的速度表现胜过TensorFlow和Keras等框架。</font><br>  <font size="3">PyTorch使用方便：PyTorch写代码非常的优雅，所思即所写，不用考虑太多关于框架本身的束缚。</font><br>  <font size="3">PyTorch具有活跃的社区，目前由作者亲自维护，供广大用户的学习和交流。</font><br>  <font size="3">PyTorch具有功能强大的可视化组建Visdom，可以在训练时监控训练过程。</font></p><h1 id="PyTorch应用"><a href="#PyTorch应用" class="headerlink" title="PyTorch应用"></a><font size="5" color="red">PyTorch应用</font></h1><h2 id="PyTorch创建tensor"><a href="#PyTorch创建tensor" class="headerlink" title="PyTorch创建tensor"></a><font size="4">PyTorch创建tensor</font></h2><h3 id="tensor，arange方法"><a href="#tensor，arange方法" class="headerlink" title="tensor，arange方法"></a><font size="3">tensor，arange方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># torch.tensor(data, dtype) 将data转换为tensor，类型为dtype</span><br><span class="line">a = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"># torch.arange(start, end, steps, dtype) 产生连续的tensor，从start开始到end结束，步长为step</span><br><span class="line">b = torch.arange(1, 10, 2)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch1.png" alt="1"></p><h3 id="zeros，zeros-like，ones，ones-like，eye方法"><a href="#zeros，zeros-like，ones，ones-like，eye方法" class="headerlink" title="zeros，zeros_like，ones，ones_like，eye方法"></a><font size="3">zeros，zeros_like，ones，ones_like，eye方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># torch.zeros(size, dtype=None) 生成形状为shape值全为0的tensor</span><br><span class="line">a = torch.zeros((2,3))</span><br><span class="line"></span><br><span class="line"># torch.ones(size, dtype=None) 生成形状为shape值全为1的tensor</span><br><span class="line">b = torch.ones((2,3))</span><br><span class="line"></span><br><span class="line"># torch.zeros_like(obj, dtype) 生成形状与array相同，值全为0的tensor</span><br><span class="line">c = torch.zeros_like(b)</span><br><span class="line"></span><br><span class="line"># torch.ones_like(obj, dtype) 生成形状与array相同，值全为1的tensor</span><br><span class="line">d = torch.ones_like(a)</span><br><span class="line"></span><br><span class="line"># torch.eye(n, m) 生成n行m列的单位矩阵，m默认等于n</span><br><span class="line">e = torch.eye(3, 4)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch2.png" alt="2"></p><h3 id="rand，randn，randperm，randint方法"><a href="#rand，randn，randperm，randint方法" class="headerlink" title="rand，randn，randperm，randint方法"></a><font size="3">rand，randn，randperm，randint方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># torch.rand(size) 生成形状为size的(0-1)均匀分布随机数</span><br><span class="line">a = torch.rand((2,3))</span><br><span class="line"></span><br><span class="line"># torch.randn(size) 生成形状为size的标准高斯分布随机数</span><br><span class="line">b = torch.randn((2,3))</span><br><span class="line"></span><br><span class="line"># torch.randperm(n) 生成0到n-1整数的随机排列</span><br><span class="line">c = torch.randperm(10)</span><br><span class="line"></span><br><span class="line"># torch.randint(low, high, size) 生成形状为size，最小值为low，最大值为high-1的随机整数</span><br><span class="line">d = torch.randint(1, 10, (3, 3))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch3.png" alt="3"></p><h3 id="linspace，logspace方法"><a href="#linspace，logspace方法" class="headerlink" title="linspace，logspace方法"></a><font size="3">linspace，logspace方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># torch.linspace(start, end, steps=100, dtype=None) 将start到stop等分成steps个点(默认为100个点)，包括end点</span><br><span class="line">a = torch.linspace(10, 20, 6)</span><br><span class="line"></span><br><span class="line"># torch.logspace(start, stop, steps=100, base=10.0, dtype=None) 将start到stop等分成steps个点(默认为100个点)，每一个点i的值为base的i次幂</span><br><span class="line">b = torch.logspace(1, 2, 10)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch4.png" alt="4"></p><h3 id="is-tensor，numel，from-numpy，numpy方法"><a href="#is-tensor，numel，from-numpy，numpy方法" class="headerlink" title="is_tensor，numel，from_numpy，numpy方法"></a><font size="3">is_tensor，numel，from_numpy，numpy方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = torch.arange(10, 20, 2)</span><br><span class="line">b = np.arange(10, 20, 2)</span><br><span class="line"></span><br><span class="line"># torch.is_tensor(obj) 判断obj是否为tensor</span><br><span class="line">torch.is_tensor(a)</span><br><span class="line"></span><br><span class="line"># torch.numel(obj) 计算obj中的元素个数</span><br><span class="line">torch.numel(a)</span><br><span class="line"></span><br><span class="line"># torch.from_numpy(ndarray) 将ndarray数组类型转换为tensor</span><br><span class="line">c = torch.from_numpy(b)</span><br><span class="line"></span><br><span class="line"># obj.numpy() 返回obj的ndarray数组类型</span><br><span class="line">d = c.numpy()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch5.png" alt="5"></p><h3 id="shape，dtype方法"><a href="#shape，dtype方法" class="headerlink" title="shape，dtype方法"></a><font size="3">shape，dtype方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.randint(1, 9, (3, 3))</span><br><span class="line"></span><br><span class="line"># obj.shape 查看obj的形状</span><br><span class="line">a.shape</span><br><span class="line"></span><br><span class="line"># obj.dtype 查看obj的元素类型</span><br><span class="line">a.dtype</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch8.png" alt="8"></p><h2 id="PyTorch切片与索引"><a href="#PyTorch切片与索引" class="headerlink" title="PyTorch切片与索引"></a><font size="4">PyTorch切片与索引</font></h2><h3 id="索引"><a href="#索引" class="headerlink" title="[]索引"></a><font size="3">[]索引</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.arange(27).reshape((3, 3, 3))</span><br><span class="line"></span><br><span class="line"># obj[index0][index1]...等价于obj[index0, index1, ...] 索引</span><br><span class="line">a[1][1][1]</span><br><span class="line">a[1, 1, 1]</span><br><span class="line"></span><br><span class="line"># obj[start, end, step] 切片索引</span><br><span class="line">a[0:2, 0:2, 0:2]</span><br><span class="line"></span><br><span class="line"># obj[...] ...可以代替连续的:</span><br><span class="line">a[..., 0]</span><br><span class="line">a[0, ...]</span><br><span class="line">a[0, ..., 0]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch6.png" alt="6"></p><h3 id="gather方法"><a href="#gather方法" class="headerlink" title="gather方法"></a><font size="3">gather方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.arange(9).reshape((3, 3))</span><br><span class="line"></span><br><span class="line"># torch.gather(tensor, dim, index) 按照dim和index对tensor进行索引</span><br><span class="line">b = torch.gather(a, dim=0, index=torch.tensor([[0, 1, 2], [1, 2, 0], [2, 0, 1]])) 按照第一个维度行开始索引，[0, 1, 2]代表第一行的三个元素来自于a中的第一行，第二行，第三行，列按顺序第一列，第二列，第三列，即第一行的元素为a[0][0]，a[1][1]，a[2][2]</span><br><span class="line">c = torch.gather(a, dim=1, index=torch.tensor([[0, 1, 2], [1, 2, 0], [2, 0, 1]])) 按照第二个维度列开始索引，[0, 1, 2]代表第一行的三个元素来自于a中的第一列，第二列，第三列，都属于第一行，即第一行的元素为a[0][0]，a[0][1]，a[0][2]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch2.png" alt="2"></p><h2 id="PyTorch维度变换"><a href="#PyTorch维度变换" class="headerlink" title="PyTorch维度变换"></a><font size="4">PyTorch维度变换</font></h2><h3 id="reshape，squeeze，unsqueeze，transpose方法"><a href="#reshape，squeeze，unsqueeze，transpose方法" class="headerlink" title="reshape，squeeze，unsqueeze，transpose方法"></a><font size="3">reshape，squeeze，unsqueeze，transpose方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.arange(12)</span><br><span class="line"></span><br><span class="line"># obj.reshape(shape) 将obj的形状变为shape</span><br><span class="line">b = a.reshape((3, 4))</span><br><span class="line"></span><br><span class="line"># obj.unsqueeze(dim) 在dim上插入一个大小为1的轴</span><br><span class="line">c = b.unsqueeze(1)</span><br><span class="line"></span><br><span class="line"># obj.squeeze(dim) 将大小为1的轴dim删去，默认为所有大小为1的轴，如果大小不为1则不删去</span><br><span class="line">d = c.squeeze()</span><br><span class="line"></span><br><span class="line"># obj.transpose(dim0, dim1) 将obj的轴0和轴1调换，obj.t()将二维obj转置</span><br><span class="line">e = b.transpose(0, 1)</span><br><span class="line">f = b.t()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch9.png" alt="9"></p><h3 id="torch-broadcast-tensors方法"><a href="#torch-broadcast-tensors方法" class="headerlink" title="torch.broadcast_tensors方法"></a><font size="3">torch.broadcast_tensors方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.arange(3)</span><br><span class="line">b = torch.ones((3, 3, 3))</span><br><span class="line"></span><br><span class="line"># torch.broadcast_tensors(a, b) 将a广播为b的大小，返回a广播后的tensor和b</span><br><span class="line">c, d = torch.broadcast_tensors(a, b)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch10.png" alt="10"></p><h2 id="PyTorch合并与分割"><a href="#PyTorch合并与分割" class="headerlink" title="PyTorch合并与分割"></a><font size="4">PyTorch合并与分割</font></h2><h3 id="cat，stack，chunk，split方法"><a href="#cat，stack，chunk，split方法" class="headerlink" title="cat，stack，chunk，split方法"></a><font size="3">cat，stack，chunk，split方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.arange(6).reshape((2, 3))</span><br><span class="line">b = torch.arange(7,13).reshape((2, 3))</span><br><span class="line"></span><br><span class="line"># torch.cat(tensors, dim) 将多个tensors按照dim进行合并</span><br><span class="line">c = torch.cat([a, b], dim=1)</span><br><span class="line"></span><br><span class="line"># tf.stack(tensors, dim) 增加一个新维度，并合并到该维度</span><br><span class="line">d = torch.stack([a, b], dim=0)</span><br><span class="line"></span><br><span class="line"># torch.chunk(tensor, chunks, dim) 对tensor按照dim轴进行拆分成chunks份</span><br><span class="line">e, f = torch.chunk(d, 2, dim=0)</span><br><span class="line"></span><br><span class="line"># torch.split(tensor, split_size_or_sections, dim) 对tensor按照dim轴进行拆分，如果希望均匀拆分则num_or_size_splits为常数，代表每部分的个数(不同于TensorFlow，代表分成多少个部分)，否则输入一个列表，代表每一部分的数量</span><br><span class="line">h = torch.split(d, 1, dim=2)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch11.png" alt="11"></p><h2 id="PyTorch数据统计"><a href="#PyTorch数据统计" class="headerlink" title="PyTorch数据统计"></a><font size="4">PyTorch数据统计</font></h2><h3 id="max，min方法"><a href="#max，min方法" class="headerlink" title="max，min方法"></a><font size="3">max，min方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.randperm(9).reshape((3, 3))</span><br><span class="line"></span><br><span class="line"># torch.max(obj, dim) 求在指定轴dim的最大值及其索引，默认为求全局最大值</span><br><span class="line">b = torch.max(a, dim=0)</span><br><span class="line"></span><br><span class="line"># torch.min(obj, dim) 求在指定轴dim的最小值及其索引，默认为求全局最小值</span><br><span class="line">c = torch.min(a, dim=0)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch12.png" alt="12"></p><h3 id="dist，mean，median，mode方法"><a href="#dist，mean，median，mode方法" class="headerlink" title="dist，mean，median，mode方法"></a><font size="3">dist，mean，median，mode方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.randint(3, 7, (3, 3), dtype=torch.float32).reshape((3, 3))</span><br><span class="line">b = torch.randint(3, 7, (3, 3), dtype=torch.float32).reshape((3, 3))</span><br><span class="line"></span><br><span class="line"># torch.dist(tensor1, tensor2, p) 计算tensor1-tensor2的p范数，要先转换成float32格式进行计算</span><br><span class="line">c = torch.dist(a, b, 2)</span><br><span class="line"></span><br><span class="line"># torch.mean(tensor, dim) 计算tensor的平均值</span><br><span class="line">d = torch.mean(a, dim=0)</span><br><span class="line"></span><br><span class="line"># torch.median(tensor, dim) 计算tensor的中位数及其索引</span><br><span class="line">e = torch.median(a, dim=0)</span><br><span class="line"></span><br><span class="line"># torch.mode(tensor, dim) 计算tensor的众数及其索引</span><br><span class="line">f = torch.mode(a, dim=0)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch13.png" alt="13"></p><h3 id="sort，topk方法"><a href="#sort，topk方法" class="headerlink" title="sort，topk方法"></a><font size="3">sort，topk方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.randperm(9).reshape((3, 3))</span><br><span class="line"></span><br><span class="line"># torch.sort(tensor, dim=None, descending=False) 将tensor按照dim维度进行排序(默认为最后一个维度)，descending=False默认递增排序，descending=True为递减排序</span><br><span class="line">b = torch.sort(a)</span><br><span class="line"></span><br><span class="line"># torch.topk(tensor, k, dim=None, largest=True, sorted=True) 求tensor最大或最小的k个值，dim默认为最后一个维度，largest=True表示最大k个值，largest=False表示最小k个值，sorted=True表示默认排序，sorted=False表示不排序</span><br><span class="line">c = torch.topk(a, 2)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch14.png" alt="14"></p><h2 id="PyTorch张量限幅"><a href="#PyTorch张量限幅" class="headerlink" title="PyTorch张量限幅"></a><font size="4">PyTorch张量限幅</font></h2><h3 id="clamp方法"><a href="#clamp方法" class="headerlink" title="clamp方法"></a><font size="3">clamp方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.randperm(9, dtype=torch.float32).reshape((3, 3))</span><br><span class="line"></span><br><span class="line"># torch.clamp(tensor, min, max) 将tensor中，小于min的值赋值为min，大于max的值赋值为max</span><br><span class="line">b = torch.clamp(a, 3, 6)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch16.png" alt="16"></p><h2 id="PyTorch数学运算"><a href="#PyTorch数学运算" class="headerlink" title="PyTorch数学运算"></a><font size="4">PyTorch数学运算</font></h2><h3 id="常规运算方法"><a href="#常规运算方法" class="headerlink" title="常规运算方法"></a><font size="3">常规运算方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.randint(-4, 5, (3, 3), dtype=torch.float32)</span><br><span class="line">b = torch.randperm(9, dtype=torch.float32).reshape((3, 3))</span><br><span class="line"></span><br><span class="line"># tensor1 op tensor2 将tensor1与tensor2进行常规的数学运算，如果维度大小不同，则进行广播，如果不能广播则报错</span><br><span class="line">a + b</span><br><span class="line">a * b</span><br><span class="line"></span><br><span class="line"># torch.abs(tensor) 求tensor的绝对值</span><br><span class="line">torch.abs(a)</span><br><span class="line"></span><br><span class="line"># torch.sqrt(tensor) 求tensor的平方根</span><br><span class="line">torch.sqrt(b)</span><br><span class="line"></span><br><span class="line"># torch.sin(tensor) 求tensor的正弦值</span><br><span class="line">torch.sin(a)</span><br><span class="line"></span><br><span class="line"># torch.exp(tensor) 求e的tensor次幂</span><br><span class="line">torch.exp(a)</span><br><span class="line"></span><br><span class="line"># torch.log(tensor) 求tensor以自然对数为底的值</span><br><span class="line">torch.log(b)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch15.png" alt="15"></p><h3 id="ceil，floor，round，frac，trunc方法"><a href="#ceil，floor，round，frac，trunc方法" class="headerlink" title="ceil，floor，round，frac，trunc方法"></a><font size="3">ceil，floor，round，frac，trunc方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.rand((3, 3)) * 10</span><br><span class="line"></span><br><span class="line"># torch.ceil(tensor) 将tensor的小数部分上取整</span><br><span class="line">b = torch.ceil(a)</span><br><span class="line"></span><br><span class="line"># torch.floor(tensor) 将tensor的小数部分下取整</span><br><span class="line">c = torch.floor(a)</span><br><span class="line"></span><br><span class="line"># torch.round(tensor) 将tensor的小数部分四舍五入</span><br><span class="line">d = torch.round(a)</span><br><span class="line"></span><br><span class="line"># torch.frac(tensor) 保留tensor的小数部分</span><br><span class="line">e = torch.frac(a)</span><br><span class="line"></span><br><span class="line"># torch.trunc(tensor) 保留tensor的整数部分</span><br><span class="line">f = torch.trunc(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch17.png" alt="17"></p><h3 id="sign，sigmoid，kthvalue方法"><a href="#sign，sigmoid，kthvalue方法" class="headerlink" title="sign，sigmoid，kthvalue方法"></a><font size="3">sign，sigmoid，kthvalue方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.randn((3, 3))</span><br><span class="line"></span><br><span class="line"># torch.sign(tensor) 求tensor作用在符号函数上的值</span><br><span class="line">b = torch.sign(a)</span><br><span class="line"></span><br><span class="line"># torch.sigmoid(tensor) 求tensor作用在sigmoid函数上的值</span><br><span class="line">c = torch.sigmoid(a)</span><br><span class="line"></span><br><span class="line"># torch.kthvalue(tensor, k, dim=None) 求tensor第k小的值，dim默认为最后一个维度</span><br><span class="line">d = torch.kthvalue(a, 2)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch18.png" alt="18"></p><h3 id="eq，equal方法"><a href="#eq，equal方法" class="headerlink" title="eq，equal方法"></a><font size="3">eq，equal方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.randint(1, 5, (3, 3))</span><br><span class="line">b = torch.randint(1, 5, (3, 3))</span><br><span class="line"></span><br><span class="line"># torch.eq(tensor1, tensor2) 等价于tensor1 == tensor2，比较tensor1和tensor2是否相等，在如果相等则对应位置为True，否则为False</span><br><span class="line">torch.eq(a, b)</span><br><span class="line"></span><br><span class="line"># torch.equal(tensor1, tensor2) 比较tensor1和tensor2是否相等，如果相等则返回True，否则为False</span><br><span class="line">torch.equal(a, b)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch19.png" alt="19"></p><h2 id="PyTorch线性代数"><a href="#PyTorch线性代数" class="headerlink" title="PyTorch线性代数"></a><font size="4">PyTorch线性代数</font></h2><h3 id="diag，trace，tril，triu方法"><a href="#diag，trace，tril，triu方法" class="headerlink" title="diag，trace，tril，triu方法"></a><font size="3">diag，trace，tril，triu方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.randperm(9).reshape((3, 3))</span><br><span class="line"></span><br><span class="line"># torch.diag(tensor, diagonal=0) 如果tensor为一维张量，返回二维张量，如果tensor为二维张量，返回对角线张量，diagonal代表对角线的偏移量</span><br><span class="line">b = torch.diag(a)</span><br><span class="line"></span><br><span class="line"># torch.trace(tensor) 求tensor的迹</span><br><span class="line">c = torch.trace(a)</span><br><span class="line"></span><br><span class="line"># torch.tril(tensor, k=0) 返回tensor的下三角张量，偏移为k</span><br><span class="line">d = torch.tril(a)</span><br><span class="line"></span><br><span class="line"># torch.triu(tensor, k=0) 返回tensor的上三角张量，偏移为k</span><br><span class="line">e = torch.triu(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch20.png" alt="20"></p><h3 id="inverse，eig，svd方法"><a href="#inverse，eig，svd方法" class="headerlink" title="inverse，eig，svd方法"></a><font size="3">inverse，eig，svd方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.randperm(9, dtype=torch.float32).reshape((3, 3))</span><br><span class="line"></span><br><span class="line"># torch.inverse(tensor) 求tensor的逆矩阵</span><br><span class="line">b = torch.inverse(a)</span><br><span class="line"></span><br><span class="line"># torch.eig(tensor, eigenvectors=False) 求tensor的特征值，eigenvectors=False默认不计算特征向量，eigenvectors=True计算特征向量</span><br><span class="line">c = torch.eig(a, True)</span><br><span class="line"></span><br><span class="line"># torch.svd(tensor) 求tensor的奇异值分解</span><br><span class="line">U, S, V = torch.svd(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch21.png" alt="21"></p><h3 id="dot，mm，mv方法"><a href="#dot，mm，mv方法" class="headerlink" title="dot，mm，mv方法"></a><font size="3">dot，mm，mv方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">a = torch.randperm(9).reshape((3, 3))</span><br><span class="line">b = torch.tensor([1, 2, 3])</span><br><span class="line"></span><br><span class="line"># torch.dot(tensor1, tensor2) 两个一维向量点乘</span><br><span class="line">c = torch.dot(b, b)</span><br><span class="line"></span><br><span class="line"># torch.mm(tensor1, tensor2) 矩阵乘法</span><br><span class="line">d = torch.mm(a, a)</span><br><span class="line"></span><br><span class="line"># torch.mv(tensor1, tensor2) 矩阵tensor1乘向量tensor2</span><br><span class="line">e = torch.mv(a, b)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pytorch22.png" alt="22"></p><h2 id="PyTorch深度学习"><a href="#PyTorch深度学习" class="headerlink" title="PyTorch深度学习"></a><font size="4">PyTorch深度学习</font></h2><h3 id="functional-函数-模块"><a href="#functional-函数-模块" class="headerlink" title="functional(函数)模块"></a><font size="3">functional(函数)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from torch.nn import functional as f</span><br><span class="line"></span><br><span class="line"># f.relu(input) ReLu激活函数</span><br><span class="line"></span><br><span class="line"># f.sigmoid(input) Sigmoid激活函数</span><br><span class="line"></span><br><span class="line"># f.tanh(input) tanh激活函数</span><br><span class="line"></span><br><span class="line"># f.softmax(input) softmax层</span><br><span class="line"></span><br><span class="line"># f.mse_loss(input, target) 计算input和target的均方差</span><br><span class="line"></span><br><span class="line"># f.binary_cross_entropy(input, target) 计算input和target二分类交叉熵</span><br><span class="line"></span><br><span class="line"># f.cross_entropy(input, target)计算input和target的交叉熵，里面内置了softmax层，因此不需要先经过softmax</span><br></pre></td></tr></tbody></table></figure><h3 id="autograd-自动求导-模块"><a href="#autograd-自动求导-模块" class="headerlink" title="autograd(自动求导)模块"></a><font size="3">autograd(自动求导)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># torch.tensor(data ,requires_grad=True) 创建时使其具有可导属性</span><br><span class="line"></span><br><span class="line"># obj.requires_grad_() 使已经创建的obj具有可导属性</span><br><span class="line"></span><br><span class="line"># torch.autograd.grad(outputs, inputs) outputs对inputs进行自动求导，前提是保证inputs具有可导属性</span><br><span class="line">torch.autograd.grad(res, w)</span><br><span class="line"></span><br><span class="line"># obj.backward() 对obj进行从后向前自动求导，然后调用变量的grad成员变量即可得到其导数</span><br><span class="line">res.backward()</span><br><span class="line">w.grad</span><br></pre></td></tr></tbody></table></figure><h3 id="optim-优化器-模块"><a href="#optim-优化器-模块" class="headerlink" title="optim(优化器)模块"></a><font size="3">optim(优化器)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># torch.optim.SGD(lr=0.01, momentum=0.0, dampending=0, weight_decay=0.0, nesterov=False) 随机梯度下降优化器，学习率lr默认为0.01，动量momentum默认为0，动量抑制因子为0，学习率衰减decay默认为0，默认不使用nesterov动量</span><br><span class="line"></span><br><span class="line"># torch.optim.RMSprop(lr=0.001, alpha=0.9, eps=None, weight_decay=0.0, momentum=0.0) RMSProp优化器，学习率lr默认为0.001，参数alpha默认为0.99，模糊因子epsilon默认为None，学习率衰减weight_decay默认为0，动量momentum默认为0</span><br><span class="line"></span><br><span class="line"># torch.optim.Adam(lr=0.001, betas=(0.9, 0.999), eps=None, weight_decay=0.0) Adam优化器，学习率lr默认为0.001，参数beta_1默认为0.9, 参数beta_2默认为0.999，模糊因子epsilon默认为None，学习率衰减weight_decay默认为0</span><br></pre></td></tr></tbody></table></figure><h3 id="CPU与GPU模块"><a href="#CPU与GPU模块" class="headerlink" title="CPU与GPU模块"></a><font size="3">CPU与GPU模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># torch.device(device) 返回设备对象</span><br><span class="line">device = torch.device('cuda:0')</span><br><span class="line"></span><br><span class="line"># obj.to(device) 返回一个device设备上的对象</span><br><span class="line">a = a.to(device)</span><br><span class="line"></span><br><span class="line"># obj.cpu() 返回一个CPU对象</span><br><span class="line"></span><br><span class="line"># obj.cuda() 返回一个GPU对象</span><br></pre></td></tr></tbody></table></figure><h3 id="datasets-数据集-模块"><a href="#datasets-数据集-模块" class="headerlink" title="datasets(数据集)模块"></a><font size="3">datasets(数据集)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.datasets as dsets</span><br><span class="line">import torchvision</span><br><span class="line"></span><br><span class="line"># dsets.XXX(root, train=True, transform, download=False) 从root为打开本地目录下载XXX数据集，train=True默认为训练集，train=False为测试集，download=False默认为不从网上下载，download=True为从网站上下载，数据预处理的部分为transform</span><br><span class="line">train = dsets.MNIST(root='mnist', train=True, transform=torchvision.transforms.ToTensor(), download=False)</span><br></pre></td></tr></tbody></table></figure><h3 id="data-数据-模块"><a href="#data-数据-模块" class="headerlink" title="data(数据)模块"></a><font size="3">data(数据)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import torch.utils.data as Data</span><br><span class="line"></span><br><span class="line"># Data.DataLoader(datasets, batch_size, shuffle=False) 读取已经加载的数据集datasets，并且分成batch，每个batch的大小为batch_size，shuffle=False默认不打乱顺序，shuffle=True为打乱batch的顺序</span><br><span class="line"></span><br><span class="line"># Data.random.split(data, [train_size, test_size]) 将data分成训练集和测试集，每部分大小为[train_size, test_size]</span><br></pre></td></tr></tbody></table></figure><h3 id="nn-神经网络-模块"><a href="#nn-神经网络-模块" class="headerlink" title="nn(神经网络)模块"></a><font size="3">nn(神经网络)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line"># nn.Sequential() 时序容器，可以传入多个网络，和TensorFlow不同，只能一个一个传入，并用逗号分隔</span><br><span class="line"></span><br><span class="line"># nn.Linear(in_features, out_features) 创建线性层，输入维度为in_features，输出维度为out_features</span><br><span class="line"></span><br><span class="line"># nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0) 创建一个卷积层，输入信号通道为in_channels，输出信号通道为out_channels，卷积核大小为kernel_size，核移动的步长为stride，padding为补0的层数</span><br><span class="line"></span><br><span class="line"># nn.MaxPool2d(kernel_size, stride=None, padding=0) 创建一个最大值池化层(平均值池化为AvgPool2d)，核大小为kernel_size，核移动的步长默认为核的大小，padding为补0的层数</span><br><span class="line"></span><br><span class="line"># nn.BatchNorm1d(num_features, eps=1e-5, momentum=0.1, affine=True) 创建标准化层(二维为BatchNorm2d)，num_features为要训练的数据量，</span><br><span class="line"></span><br><span class="line"># nn.RNN(input_size, hidden_size, num_layers, nonlinearity='tanh') 创建RNN层，输入特征数量input_size，隐层结点数hidden_size，RNN层数num_layers，非线性激活函数nonlinearity默认为tanh </span><br><span class="line"></span><br><span class="line"># nn.LSTM(input_size, hidden_size, num_layers, nonlinearity='tanh') 创建长短期记忆网络层，输入特征数量input_size，隐层结点数hidden_size，RNN层数num_layers，非线性激活函数nonlinearity默认为tanh </span><br><span class="line"></span><br><span class="line"># nn.Dropout(p=0.5) 创建dropout层(二维为Dropout2d)，随机丢弃结点的概率为p，默认为0.5</span><br><span class="line"></span><br><span class="line"># nn.Module 自定义网络层的基类</span><br><span class="line">class MyLayer(nn.Module):</span><br><span class="line">    def __init__(self, input_dim, output_dim):</span><br><span class="line">        super(MyLayer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.kernel = nn.Parameter(torch.randn(input_dim, output_dim))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(output_dim))</span><br><span class="line"></span><br><span class="line">    def forward(self, inputs):</span><br><span class="line">        out = inputs @ self.kernel + self.bias</span><br><span class="line">        return out</span><br></pre></td></tr></tbody></table></figure><h3 id="transforms-数据变换-模块"><a href="#transforms-数据变换-模块" class="headerlink" title="transforms(数据变换)模块"></a><font size="3">transforms(数据变换)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.transforms as transforms</span><br><span class="line"></span><br><span class="line"># transforms.Normalize(mean, std) 标准化操作(类方法)</span><br><span class="line"></span><br><span class="line"># transforms.RandomRotation(n) 将图片旋转-n到n度</span><br><span class="line"></span><br><span class="line"># transforms.ToTensor() 将numpy图片转换为tensor</span><br><span class="line"></span><br><span class="line"># transforms.ToPILImage() 将tensor转换为numpy图片形式</span><br></pre></td></tr></tbody></table></figure><h3 id="models-模型-模块"><a href="#models-模型-模块" class="headerlink" title="models(模型)模块"></a><font size="3">models(模型)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.models as models</span><br><span class="line"></span><br><span class="line"># models.resnet18(pretrained=False) 获得resnet18网络结构，默认是没有经过训练的，pretrained=True是获得经过训练的参数，便于在少样本时进行迁移学习</span><br><span class="line"></span><br><span class="line"># models.AlexNet(pretrained=False) 获得AlexNet网络结构，默认是没有经过训练的，pretrained=True是获得经过训练的参数，便于在少样本时进行迁移学习</span><br><span class="line"></span><br><span class="line"># models.VGG16(pretrained=False) 获得VGG16网络结构，默认是没有经过训练的，pretrained=True是获得经过训练的参数，便于在少样本时进行迁移学习</span><br></pre></td></tr></tbody></table></figure><h3 id="save-保持-模块"><a href="#save-保持-模块" class="headerlink" title="save(保持)模块"></a><font size="3">save(保持)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># torch.save(obj, filename) 将obj保存在以.pkl结尾的filename文件中</span><br><span class="line"></span><br><span class="line"># torch.load(filename) 读取filename文件中的数据信息</span><br></pre></td></tr></tbody></table></figure><h1 id="PyTorch小结"><a href="#PyTorch小结" class="headerlink" title="PyTorch小结"></a><font size="5" color="red">PyTorch小结</font></h1><p>  由于PyTorch的简洁性和优雅性，使得PyTorch对于入门学习的人来说非常的友好，现在PyTorch也是最热门的深度学习框架之一，具有较大的潜力。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;PyTorch&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习框架" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>TensorFlow</title>
    <link href="https://USTCcoder.github.io/2019/09/06/frame%20TensorFlow/"/>
    <id>https://USTCcoder.github.io/2019/09/06/frame TensorFlow/</id>
    <published>2019-09-06T04:16:40.000Z</published>
    <updated>2020-06-09T12:06:36.212Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">TensorFlow</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  <strong>TensorFlow:</strong>是谷歌公司于2015年11月9日推出的一个划时代的<strong>神经网络，深度学习开发平台</strong>。TensorFlow是一个庞大的系统，结构复杂，功能强大，利用<strong>数据流图(Data Flow Graphs)</strong>进行数值计算的开源软件库，数据流图中的<strong>结点(Node)</strong>代表数学运算操作，<strong>边(Edge)</strong>代表节点之间流通的数据，即张量(Tensor)。<br><a id="more"></a></p><p><img src="/images/FRAME/tensorflow.jpg" alt="tensorflow"></p><h1 id="TensorFlow特点"><a href="#TensorFlow特点" class="headerlink" title="TensorFlow特点"></a><font size="5" color="red">TensorFlow特点</font></h1><p>  <font size="3">TensorFlow具有高度的灵活性：只要能够将计算表示为一个数据流，就可以使用TensorFlow进行运算。</font><br>  <font size="3">TensorFlow具有强的可移植性：TensorFlow支持CPU和GPU运算，并且可以运行在个人电脑，服务器，移动设备等。</font><br>  <font size="3">TensorFlow运算简单：内部实现了自动求导方式，像搭积木一样，只要建好运算图，不需要关心求导的复杂程度。</font><br>  <font size="3">TensorFlow具有功能强大的可视化组建TensorBoard，可以在训练时监控训练过程。</font></p><h1 id="TensorFlow应用"><a href="#TensorFlow应用" class="headerlink" title="TensorFlow应用"></a><font size="5" color="red">TensorFlow应用</font></h1><h2 id="TensorFlow创建tensor"><a href="#TensorFlow创建tensor" class="headerlink" title="TensorFlow创建tensor"></a><font size="4">TensorFlow创建tensor</font></h2><h3 id="constant方法"><a href="#constant方法" class="headerlink" title="constant方法"></a><font size="3">constant方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># tf.constant(value, dtype=None, shape=None) 创建一个形状为shape，类型为dtype，值为value的张量</span><br><span class="line">a = tf.constant([[1, 2, 3], [4, 5, 6]])</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow1.png" alt="1"></p><h3 id="CPU，GPU方法"><a href="#CPU，GPU方法" class="headerlink" title="CPU，GPU方法"></a><font size="3">CPU，GPU方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[1, 2, 3], [4, 5, 6]])</span><br><span class="line"></span><br><span class="line"># obj.device() 查看tensor的环境是CPU还是GPU</span><br><span class="line">a.device</span><br><span class="line"></span><br><span class="line"># obj.gpu() 返回一个新tensor位于GPU </span><br><span class="line">b = a.gpu()</span><br><span class="line"></span><br><span class="line"># obj.cpu() 返回一个新tensor位于CPU</span><br><span class="line">c = b.cpu()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow2.png" alt="3"></p><h3 id="numpy，shape，ndim，dtype方法"><a href="#numpy，shape，ndim，dtype方法" class="headerlink" title="numpy，shape，ndim，dtype方法"></a><font size="3">numpy，shape，ndim，dtype方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[1, 2, 3], [4, 5, 6]])</span><br><span class="line"></span><br><span class="line"># obj.numpy() 返回一个新的ndarray，即将tensor变成numpy数组</span><br><span class="line">b = a.numpy</span><br><span class="line"></span><br><span class="line"># obj.shape 查看tensor的维度信息</span><br><span class="line">a.shape</span><br><span class="line"></span><br><span class="line"># obj.ndim 查看tensor的维度数</span><br><span class="line">a.ndim</span><br><span class="line"></span><br><span class="line"># obj.dtype 查看tensor的类型</span><br><span class="line">a.dtype</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow3.png" alt="3"></p><h3 id="convert-to-tensor，cast方法"><a href="#convert-to-tensor，cast方法" class="headerlink" title="convert_to_tensor，cast方法"></a><font size="3">convert_to_tensor，cast方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line"></span><br><span class="line"># tf.convert_to_tensor(value, dtype=None) 将value转换为tensor形式，常用来将numpy格式转换为tensor格式</span><br><span class="line">b = tf.convert_to_tensor(a)</span><br><span class="line"></span><br><span class="line"># tf.cast(value, dtype) 返回一个值为value，类型为dtype的tensor，常用来修改tensor的类型</span><br><span class="line">c = tf.cast(b, dtype=tf.float32)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow4.png" alt="4"></p><h3 id="zeros，ones，fill，random方法"><a href="#zeros，ones，fill，random方法" class="headerlink" title="zeros，ones，fill，random方法"></a><font size="3">zeros，ones，fill，random方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># tf.zeros(shape) 产生形状为shape的值全为0的tensor</span><br><span class="line">a = tf.zeros((3, 3))</span><br><span class="line"></span><br><span class="line"># tf.ones(shape) 产生形状为shape的值全为1的tensor</span><br><span class="line">b = tf.ones((3, 3))</span><br><span class="line"></span><br><span class="line"># tf.fill(shape, value) 产生形状为shape的值全为value的tensor</span><br><span class="line">c = tf.fill((3, 3), -1) </span><br><span class="line"></span><br><span class="line"># tf.random.normal(shape, mean=0, stddev=1) 产生形状为shape的高斯分布，均值默认为0，标准差默认为1</span><br><span class="line">d = tf.random.normal((3, 3)) </span><br><span class="line"></span><br><span class="line"># tf.random.uniform(shape, mean=0, stddev=1) 产生形状为shape的均匀分布，默认为(0-1)均匀分布</span><br><span class="line">e = tf.random.uniform((3, 3)) </span><br><span class="line"></span><br><span class="line"># tf.random.shuffle(value, seed) 将value按照种子seed打散</span><br><span class="line">f = tf.random.shuffle(d)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow5.png" alt="5"></p><h3 id="zeros-like，ones-like，one-hot，range方法"><a href="#zeros-like，ones-like，one-hot，range方法" class="headerlink" title="zeros_like，ones_like，one_hot，range方法"></a><font size="3">zeros_like，ones_like，one_hot，range方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.random.normal((3, 3))</span><br><span class="line"></span><br><span class="line"># tf.zeros_like(input, dtype) 产生一个大小和input相同值为全0的tensor</span><br><span class="line">b = tf.zeros_like(a)</span><br><span class="line"></span><br><span class="line"># tf.ones_like(input, dtype) 产生一个大小和input相同值为全1的tensor</span><br><span class="line">c = tf.ones_like(a)</span><br><span class="line"></span><br><span class="line"># tf.range(start, end, interval, dtype) 产生从start到end，步长为interval的连续张量，用法同numpy中的arange</span><br><span class="line">d = tf.range(10)</span><br><span class="line"></span><br><span class="line"># tf.one_hot(indices, depth) 将indices转换为独热编码，深度为depth</span><br><span class="line">e = tf.one_hot(d, depth=10)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow6.png" alt="6"></p><h2 id="TensorFlow切片与索引"><a href="#TensorFlow切片与索引" class="headerlink" title="TensorFlow切片与索引"></a><font size="4">TensorFlow切片与索引</font></h2><h3 id="索引"><a href="#索引" class="headerlink" title="[]索引"></a><font size="3">[]索引</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(27).reshape((3, 3, 3))</span><br><span class="line">b = tf.convert_to_tensor(a)</span><br><span class="line"></span><br><span class="line"># obj[index0][index1]...等价于obj[index0, index1, ...] 索引</span><br><span class="line">b[1][1][1]</span><br><span class="line">b[1, 1, 1]</span><br><span class="line"></span><br><span class="line"># obj[start, end, step] 切片索引</span><br><span class="line">b[0:2, 0:2, 0:2]</span><br><span class="line"></span><br><span class="line"># obj[...] ...可以代替连续的:</span><br><span class="line">b[..., 0]</span><br><span class="line">b[0, ...]</span><br><span class="line">b[0, ..., 0]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow7.png" alt="7"></p><h3 id="gather，gather-nd，boolean-mask方法"><a href="#gather，gather-nd，boolean-mask方法" class="headerlink" title="gather，gather_nd，boolean_mask方法"></a><font size="3">gather，gather_nd，boolean_mask方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(27).reshape((3, 3, 3))</span><br><span class="line">b = tf.convert_to_tensor(a)</span><br><span class="line"></span><br><span class="line"># tf.gather(obj, axis, indices) 在维度axis上索引indices，并且连接成一个新的tensor</span><br><span class="line">c = tf.gather(b, axis=0, indices=[1, 0]) # 在第一个维度上选择前两个，并且将顺序调换</span><br><span class="line">d = tf.gather(b, axis=1, indices=[1, 0]) # 在第二个维度上选择前两个，并且将顺序调换</span><br><span class="line"></span><br><span class="line"># tf.gather_nd(obj, indices) 在多维度上索引indices，并且连接成一个新的tensor</span><br><span class="line">e = tf.gather_nd(b, indices=[[0, 0], [1, 1]]) # 索引第1个维度的第一个和第2个维度的第二个数据</span><br><span class="line">f = tf.gather_nd(b, indices=[[0, 0, 0], [0, 0, 1], [0, 0, 2]]) # 索引第1个维度的第1行的第1列，第1个维度的第1行的第2列，第1个维度的第1行的第3列</span><br><span class="line"></span><br><span class="line"># tf.boolean_mask(obj, mask, axis) 在axis轴上掩模索引</span><br><span class="line">g = tf.boolean_mask(b, [[True,True,False],[True,False,False],[False,False,False]], axis=0) # 在第一维度上选择前两行，在第二维度上选择第一行，一共三行</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow8.png" alt="8"></p><h2 id="TensorFlow维度变化"><a href="#TensorFlow维度变化" class="headerlink" title="TensorFlow维度变化"></a><font size="4">TensorFlow维度变化</font></h2><h3 id="reshape，transpose，expand-dims，squeeze方法"><a href="#reshape，transpose，expand-dims，squeeze方法" class="headerlink" title="reshape，transpose，expand_dims，squeeze方法"></a><font size="3">reshape，transpose，expand_dims，squeeze方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(4).reshape((2, 1, 2))</span><br><span class="line">b = tf.constant(a)</span><br><span class="line"></span><br><span class="line"># tf.reshape(tensor, shape) 返回形状为shape的tensor</span><br><span class="line">c = tf.reshape(a, (2, 2))</span><br><span class="line"></span><br><span class="line"># tf.transpose(tensor, perm=None) 将轴进行调换，默认翻转轴</span><br><span class="line">d = tf.transpose(b, (1, 2, 0))</span><br><span class="line"></span><br><span class="line"># tf.expand_dims(tensor, axis) 在axis上增加一个维度</span><br><span class="line">e = tf.expand_dims(b, axis=3)</span><br><span class="line"></span><br><span class="line"># tf.squeeze(tensor, axis=None) 将axis上为1的维度删去，默认删除所有为1的维度</span><br><span class="line">f = tf.squeeze(e)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow9.png" alt="9"></p><h3 id="broadcast-to，tile方法"><a href="#broadcast-to，tile方法" class="headerlink" title="broadcast_to，tile方法"></a><font size="3">broadcast_to，tile方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.range(3)</span><br><span class="line">b = tf.reshape(a, (1, 3))</span><br><span class="line"></span><br><span class="line"># tf.broadcast_to(input, shape) 返回一个广播后的tensor</span><br><span class="line">c = tf.broadcast_to(a, (3, 3))</span><br><span class="line"></span><br><span class="line"># tf.tile(input, multiples) 返回一个经过复制的tensor</span><br><span class="line">d = tf.tile(b, [2, 2])</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow10.png" alt="10"></p><h2 id="TensorFlow合并与分割"><a href="#TensorFlow合并与分割" class="headerlink" title="TensorFlow合并与分割"></a><font size="4">TensorFlow合并与分割</font></h2><h3 id="concat，stack，unstack，split方法"><a href="#concat，stack，unstack，split方法" class="headerlink" title="concat，stack，unstack，split方法"></a><font size="3">concat，stack，unstack，split方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(6).reshape((2, 3))</span><br><span class="line">b = np.arange(3, 9).reshape((2, 3))</span><br><span class="line">c = tf.constant(a)</span><br><span class="line">d = tf.constant(b)</span><br><span class="line"></span><br><span class="line"># tf.concat(values, axis) 将多个values按照axis轴进行合并</span><br><span class="line">e = tf.concat([c, d], axis=1)</span><br><span class="line"></span><br><span class="line"># tf.stack(values, axis) 增加一个新维度，并合并到该维度</span><br><span class="line">f = tf.stack([c, d], axis=1)</span><br><span class="line"></span><br><span class="line"># tf.unstack(value, axis) 对value按照axis轴进行拆分</span><br><span class="line">g = tf.unstack(f, axis=1)</span><br><span class="line"></span><br><span class="line"># tf.split(value, num_or_size_splits, axis) 对value按照axis轴进行拆分，如果希望均匀拆分则num_or_size_splits为常数，否则输入一个列表，代表每一部分的数量</span><br><span class="line">h = tf.split(e, 3, axis=1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow11.png" alt="11"></p><h2 id="tensorflow数据统计"><a href="#tensorflow数据统计" class="headerlink" title="tensorflow数据统计"></a><font size="4">tensorflow数据统计</font></h2><h3 id="reduce-min，reduce-max，reduce-mean方法"><a href="#reduce-min，reduce-max，reduce-mean方法" class="headerlink" title="reduce_min，reduce_max，reduce_mean方法"></a><font size="3">reduce_min，reduce_max，reduce_mean方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.reshape(tf.random.shuffle(tf.range(1, 13)), (3, 4))</span><br><span class="line"></span><br><span class="line"># tf.reduce_min(input_tensor, axis=None) 求input_tensor在axis上的最小值，默认为全局最小值</span><br><span class="line">b = tf.reduce_min(a)</span><br><span class="line"></span><br><span class="line"># tf.reduce_max(input_tensor, axis=None) 求input_tensor在axis上的最大值，默认为全局最大值</span><br><span class="line">c = tf.reduce_max(a, axis=0)</span><br><span class="line"></span><br><span class="line"># tf.reduce_mean(input_tensor, axis=None) 求input_tensor在axis上的平均值，默认为全局平均值</span><br><span class="line">d = tf.reduce_mean(a, axis=1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow12.png" alt="12"></p><h3 id="argmax，argmin方法"><a href="#argmax，argmin方法" class="headerlink" title="argmax，argmin方法"></a><font size="3">argmax，argmin方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.reshape(tf.random.shuffle(tf.range(1, 13)), (3, 4))</span><br><span class="line"></span><br><span class="line"># tf.argmax(input, axis=0) 求input在axis轴的最大值索引，默认在第一个轴</span><br><span class="line">b = tf.argmax(a, axis=0)</span><br><span class="line"></span><br><span class="line"># tf.argmin(input, axis=0) 求input在axis轴的最小值索引，默认在第一个轴</span><br><span class="line">c = tf.argmin(a, axis=1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow13.png" alt="13"></p><h3 id="equal，unique方法"><a href="#equal，unique方法" class="headerlink" title="equal，unique方法"></a><font size="3">equal，unique方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[1, 1], [0, 0]])</span><br><span class="line">b = tf.constant([[1, 0], [1, 0]])</span><br><span class="line">c = tf.constant([1,2,3,2,1])</span><br><span class="line"></span><br><span class="line"># tf.equal(x, y) 比较x和y是否相等，在如果相等则对应位置为True，否则为False</span><br><span class="line">d = tf.equal(a, b)</span><br><span class="line"></span><br><span class="line"># tf.unique(x) 找出x中有多少不同的元素，并返回其索引</span><br><span class="line">e = tf.unique(c)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow14.png" alt="14"></p><h3 id="norm，top-k方法"><a href="#norm，top-k方法" class="headerlink" title="norm，top_k方法"></a><font size="3">norm，top_k方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.reshape(tf.random.shuffle(tf.range(1, 13, dtype=tf.float32)), (3, 4))</span><br><span class="line"></span><br><span class="line"># tf.norm(tensor, str, axis=-1) 求tensor的p范数，默认为全局p范数</span><br><span class="line">b = tf.norm(a, 2, axis=0)</span><br><span class="line"></span><br><span class="line"># tf.math.top_k(input, k, bool=True) 在最后一个维度求input中前k个最大值或最小值及其索引，bool=True为最大值，bool=False为最小值。</span><br><span class="line">c = tf.math.top_k(a, 2, True)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow15.png" alt="15"></p><h2 id="tensorflow排序"><a href="#tensorflow排序" class="headerlink" title="tensorflow排序"></a><font size="4">tensorflow排序</font></h2><h3 id="sort，argsort方法"><a href="#sort，argsort方法" class="headerlink" title="sort，argsort方法"></a><font size="3">sort，argsort方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.reshape(tf.random.shuffle(tf.range(1, 13)), (3, 4))</span><br><span class="line"></span><br><span class="line"># tf.sort(values, direction='ASCENDING', axis=-1) 按axis轴对values进行排序，返回排序后的结果，direction='ASCENDING'代表递增排序，'DESCENDING'代表递减排序</span><br><span class="line">b = tf.sort(a, axis=0)</span><br><span class="line"></span><br><span class="line"># tf.argsort(values, direction='ASCENDING', axis=-1) 按axis轴对values进行排序，返回排序后的索引，direction='ASCENDING'代表递增排序，'DESCENDING'代表递减排序</span><br><span class="line">c = tf.argsort(a, axis=1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow16.png" alt="16"></p><h2 id="tensorflow张量限幅"><a href="#tensorflow张量限幅" class="headerlink" title="tensorflow张量限幅"></a><font size="4">tensorflow张量限幅</font></h2><h3 id="maximun，minimum方法"><a href="#maximun，minimum方法" class="headerlink" title="maximun，minimum方法"></a><font size="3">maximun，minimum方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.reshape(tf.random.shuffle(tf.range(1, 13)), (3, 4))</span><br><span class="line">b = tf.reshape(tf.random.shuffle(tf.range(1, 13)), (3, 4))</span><br><span class="line"></span><br><span class="line"># tf.maximun(x, y) 取x，y中的大数并返回</span><br><span class="line">c = tf.maximum(a, b)</span><br><span class="line"></span><br><span class="line"># tf.minimun(x, y) 取x，y中的小数并返回</span><br><span class="line">d = tf.minimum(a, b)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow17.png" alt="17"></p><h3 id="clip-by-value方法"><a href="#clip-by-value方法" class="headerlink" title="clip_by_value方法"></a><font size="3">clip_by_value方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.reshape(tf.random.shuffle(tf.range(1, 13)), (3, 4))</span><br><span class="line"></span><br><span class="line"># tf.clip_by_value(t, clip_value_min, clip_value_max) 将t中小于clip_value_min的值赋值为clip_value_min，大于clip_value_max的值赋值为clip_value_max</span><br><span class="line">b = tf.clip_by_value(a, 3, 7)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow18.png" alt="18"></p><h2 id="tensorflow数学运算"><a href="#tensorflow数学运算" class="headerlink" title="tensorflow数学运算"></a><font size="4">tensorflow数学运算</font></h2><h3 id="常规运算方法"><a href="#常规运算方法" class="headerlink" title="常规运算方法"></a><font size="3">常规运算方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.reshape(tf.random.shuffle(tf.range(1, 5, dtype=tf.float32)), (2, 2))</span><br><span class="line">b = tf.reshape(tf.random.shuffle(tf.range(1, 5, dtype=tf.float32)), (2, 2))</span><br><span class="line"></span><br><span class="line"># tensor1 op tensor2 将tensor1与tensor2进行常规的数学运算，如果维度大小不同，则进行广播，如果不能广播则报错</span><br><span class="line">c = a + b</span><br><span class="line">d = a ** b</span><br><span class="line"></span><br><span class="line"># tf.sqrt(x) 将x进行开方运算</span><br><span class="line">e = tf.sqrt(a)</span><br><span class="line"></span><br><span class="line"># tf.exp(x) 求e的x次幂</span><br><span class="line">f = tf.exp(a)</span><br><span class="line"></span><br><span class="line"># tf.math.log(x) 求x以e为底的对数</span><br><span class="line">g = tf.math.log(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow19.png" alt="19"></p><h3 id="matmul方法"><a href="#matmul方法" class="headerlink" title="matmul方法"></a><font size="3">matmul方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.reshape(tf.random.shuffle(tf.range(1, 5, dtype=tf.float32)), (2, 2))</span><br><span class="line">b = tf.reshape(tf.random.shuffle(tf.range(1, 5, dtype=tf.float32)), (2, 2))</span><br><span class="line"></span><br><span class="line"># tf.matmul(a, b) a和b的矩阵乘法，等价于a @ b</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line">d = a @ b</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow20.png" alt="20"></p><h3 id="where方法"><a href="#where方法" class="headerlink" title="where方法"></a><font size="3">where方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[False, True], [True, False]])</span><br><span class="line">b = tf.constant([[1, 2], [3, 4]])</span><br><span class="line">c = tf.constant([[-1, -2], [-3, -4]])</span><br><span class="line"></span><br><span class="line"># tf.where(obj) 返回obj中True的位置的索引</span><br><span class="line">d = tf.where(a)</span><br><span class="line"></span><br><span class="line"># tf.where(tensor1, tensor2, tensor3) 如果tensor1对应位置为True则从tensor2中取得相应元素，否则从tensor3中取得相应元素</span><br><span class="line">e = tf.where(a, b, c)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow21.png" alt="21"></p><h2 id="tensorflow深度学习"><a href="#tensorflow深度学习" class="headerlink" title="tensorflow深度学习"></a><font size="4">tensorflow深度学习</font></h2><h3 id="datasets-数据集-模块"><a href="#datasets-数据集-模块" class="headerlink" title="datasets(数据集)模块"></a><font size="3">datasets(数据集)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.keras import datasets</span><br><span class="line"></span><br><span class="line"># datasets.XXX.load_data() # 下载XXX数据集，常用的有mnist，cifar10，cifar100等等，此时数据为numpy格式，并不是tensor</span><br><span class="line">(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data() # 下载mnist手写数字数据集</span><br><span class="line"></span><br><span class="line">x_train.shape</span><br><span class="line">y_train.shape</span><br><span class="line"></span><br><span class="line">x_test.shape</span><br><span class="line">y_test.shape</span><br><span class="line"></span><br><span class="line">y_train[:6]</span><br><span class="line">y_train_onehot = tf.one_hot(y_train[:6], depth=10)</span><br><span class="line">y_train_onehot</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow22.png" alt="22"></p><h3 id="data-数据操作-模块"><a href="#data-数据操作-模块" class="headerlink" title="data(数据操作)模块"></a><font size="3">data(数据操作)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># tf.data.Dataset.from_tensor_slices(tensor) 将tensor数据转换成一个可迭代的对象</span><br><span class="line">db = tf.data.Dataset.from_tensor_slices(x_train)</span><br><span class="line"></span><br><span class="line"># next(iter(db)) 返回下一个迭代器的内容</span><br><span class="line"></span><br><span class="line"># db.shuffle(n) 将0-n的迭代器内容打散，使其训练更加合理</span><br><span class="line"></span><br><span class="line"># db.map(function) 将迭代器中的数据全部经过function处理，常用于图片的预处理等功能</span><br><span class="line"></span><br><span class="line">#db.batch(n) 将迭代器中的内容分成很多个batch，每一个batch中有n个数据</span><br><span class="line"></span><br><span class="line"># db.repeat(n) 将db对象迭代n次，默认为无限迭代</span><br></pre></td></tr></tbody></table></figure><h3 id="nn-神经网络-模块"><a href="#nn-神经网络-模块" class="headerlink" title="nn(神经网络)模块"></a><font size="3">nn(神经网络)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># tf.nn.relu(features) ReLu激活函数</span><br><span class="line"></span><br><span class="line"># tf.nn.sigmoid(features) Sigmoid激活函数</span><br><span class="line"></span><br><span class="line"># tf.nn.tanh(features) tanh激活函数</span><br><span class="line"></span><br><span class="line"># tf.nn.softmax(logits) softmax层</span><br></pre></td></tr></tbody></table></figure><h3 id="optimizers-优化器-模块"><a href="#optimizers-优化器-模块" class="headerlink" title="optimizers(优化器)模块"></a><font size="3">optimizers(优化器)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.keras import optimizers</span><br><span class="line"></span><br><span class="line"># optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False) 随机梯度下降优化器，学习率lr默认为0.01，动量momentum默认为0，学习率衰减decay默认为0，默认不使用nesterov动量</span><br><span class="line"></span><br><span class="line"># optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0) RMSprop优化器，学习率lr默认为0.001，参数rho默认为0.9，模糊因子epsilon默认为None，学习率衰减decay默认为0.0</span><br><span class="line"></span><br><span class="line"># optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0) Adam优化器，学习率lr默认为0.001，参数beta_1默认为0.9, 参数beta_2默认为0.999，模糊因子epsilon默认为None，学习率衰减decay默认为0</span><br></pre></td></tr></tbody></table></figure><h3 id="layers-网络层-模块"><a href="#layers-网络层-模块" class="headerlink" title="layers(网络层)模块"></a><font size="3">layers(网络层)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.keras import layers</span><br><span class="line"></span><br><span class="line"># layers.Dense(out_dim) 建立一个全连接层，结点数为out_dim个</span><br><span class="line"></span><br><span class="line"># layers.Layer 自定义网络层的基类</span><br><span class="line">class MyDense(layers.Layer):</span><br><span class="line">    def __init__(self, input_dim, output_dim):</span><br><span class="line">        super(MyDense, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.kernel = self.add_variable('w', [input_dim, output_dim])</span><br><span class="line">        self.bias = self.add_variable('b', [output_dim])</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, training=None):</span><br><span class="line">        out = inputs @ self.kernel + self.bias</span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line"># layers.Dropout(rate) 建立一个Dropout层，失活的比率为rate，保持连接的比率为1-rate，和PyTorch相同</span><br><span class="line"></span><br><span class="line"># layers.Flatten() 建立一个Flatten层，将数据展平成一维</span><br><span class="line"></span><br><span class="line"># layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid') 建立一个卷积层，卷积核的个数为filters，核的大小为kernel_size，strides为模板移动的步长，padding为是否在周围补0，valid为不补0，same为补0保证大小不变</span><br><span class="line"></span><br><span class="line"># layers.MaxPool2D(pool_size=(2, 2), strides=None, padding='valid') 建立一个最大值池化层(平均值池化为AveragePooling2D)，池化层模板为pool_size，strides为模板移动的步长，padding为是否在周围补0，valid为不补0，same为补0保证大小不变</span><br><span class="line"></span><br><span class="line"># layers.SimpleRNNCell(units, activation='tanh', dropout=0.0, return_sequences=False, unroll=False) 建立一个循环神经网络层，单元数为units，dropout丢弃百分百为0.0，return_sequences=False返回序列最后一个输出，return_sequences=True返回全部序列，unroll=True，则网络将展开，否则将使用符号循环。</span><br><span class="line"></span><br><span class="line"># layers.LSTM(units, activation='tanh', dropout=0.0, return_sequences=False, unroll=False) 建立一个长短期记忆网络，单元数为units，dropout丢弃百分百为0.0，return_sequences=False返回序列最后一个输出，return_sequences=True返回全部序列，unroll=True，则网络将展开，否则将使用符号循环。</span><br></pre></td></tr></tbody></table></figure><h3 id="losses-误差计算-模块"><a href="#losses-误差计算-模块" class="headerlink" title="losses(误差计算)模块"></a><font size="3">losses(误差计算)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># tf.losses.MSE(y, logits) 计算真实值y与预测值logits的均方差</span><br><span class="line"></span><br><span class="line"># tf.losses.categorical_crossentropy(y, logits) 计算真实值y与预测值logits的交叉熵</span><br><span class="line"></span><br><span class="line"># tf.losses.binary_crossentropy(label, prob) 根据标签和概率计算二分类问题的交叉熵</span><br></pre></td></tr></tbody></table></figure><h3 id="Gradient-梯度下降-模块"><a href="#Gradient-梯度下降-模块" class="headerlink" title="Gradient(梯度下降)模块"></a><font size="3">Gradient(梯度下降)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># tf.Variable(tensor) 将tensor赋有可求导属性</span><br><span class="line"></span><br><span class="line"># 将前向运算过程放在tf.GradientTape()中，即可实现自动求导，前提是变量具有可导属性</span><br><span class="line">with tf.GradientTape() as tape：</span><br><span class="line">    XXX</span><br></pre></td></tr></tbody></table></figure><h3 id="TensorBoard模块"><a href="#TensorBoard模块" class="headerlink" title="TensorBoard模块"></a><font size="3">TensorBoard模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 首先要安装TensorBoard，pip install tensorboard</span><br><span class="line"># 新建一个文件夹，命名为logs，在该文件路径下并输入tensorboard --logdir logs  会显示TensorBoard 1.14.0 at http://DESKTOP-1NSILG1:6006/ (Press CTRL+C to quit)</span><br><span class="line"># 建立日志文件tf.summary.create_file_writer(filename)</span><br><span class="line"># 给日志写数据</span><br><span class="line">with summary_writer.as_default():</span><br><span class="line">    tf.summary.scalar(label, data, step) # 给label添加数据data，以step作为x轴</span><br><span class="line">    tf.summary.image(label, img, step) # 给label添加图片img</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow24.png" alt="24"></p><h3 id="visdom模块"><a href="#visdom模块" class="headerlink" title="visdom模块"></a><font size="3">visdom模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 首先要安装visdom，pip install visdom</span><br><span class="line"># 在该文件路径下并输入python -m visdom.server，会出现下面的提示</span><br><span class="line">Checking for scripts.</span><br><span class="line">It's Alive!</span><br><span class="line">INFO:root:Application Started</span><br><span class="line">You can navigate to http://localhost:8097</span><br><span class="line"># 在python文件中写入</span><br><span class="line">from visdom import Visdom</span><br><span class="line"></span><br><span class="line">vis = Visdom()</span><br><span class="line"></span><br><span class="line"># vis.line(Y, X, win, updata) 在win窗口下创建横坐标为X，纵坐标为Y的折线，updata为折线的更新方式</span><br><span class="line"></span><br><span class="line"># vis.images(tensor, win) 在win窗口下显示图片tensor</span><br><span class="line"></span><br><span class="line"># vis.text(text, win) 在win窗口下显示text文本文字</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow24.png" alt="24"></p><h3 id="metrics-衡量指标-模块"><a href="#metrics-衡量指标-模块" class="headerlink" title="metrics(衡量指标)模块"></a><font size="3">metrics(衡量指标)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.keras import metrics</span><br><span class="line"></span><br><span class="line"># metrics.Accuracy() 返回准确度衡量指标对象</span><br><span class="line">acc_metrics = metrics.Accuracy()</span><br><span class="line"></span><br><span class="line"># metrics.Mean() 返回平均值衡量指标对象</span><br><span class="line">mean_metrics = metrics.Mean()</span><br><span class="line"></span><br><span class="line"># obj.updata_state() 向metrics对象中添加数据</span><br><span class="line">acc_metrics.updata_state(loss)</span><br><span class="line">mean_metrics.updata_state(y, pred)</span><br><span class="line"></span><br><span class="line"># obj.result() 将metrics中的数据取出</span><br><span class="line">acc_metrics.result()</span><br><span class="line">mean_metrics.result()</span><br><span class="line"></span><br><span class="line"># obj.reset_states() 清除metrics中的数据</span><br><span class="line">acc_metrics.reset_states()</span><br><span class="line">mean_metrics.reset_states()</span><br></pre></td></tr></tbody></table></figure><h3 id="Model-模型-模块"><a href="#Model-模型-模块" class="headerlink" title="Model(模型)模块"></a><font size="3">Model(模型)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.keras import Sequential, layers</span><br><span class="line">from tensorflow import keras</span><br><span class="line"></span><br><span class="line"># model = Sequential([layer1, layer2, ...]) 创建一个网络结构，第一层为layer1，第二层为layer2，……</span><br><span class="line">model = Sequential([layers.Dense(512), layers.Dense(128), layers.Dense(10)])</span><br><span class="line"></span><br><span class="line"># keras.Model 自定义网络结构的基类</span><br><span class="line">class MyModel(keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.fc1 = MyDense(28 * 28, 256)</span><br><span class="line">        self.fc2 = MyDense(256, 64)</span><br><span class="line">        self.fc3 = MyDense(64, 10)</span><br><span class="line"></span><br><span class="line">    def call(self, inputs, training=None):</span><br><span class="line">        x = self.fc1(inputs)</span><br><span class="line">        x = tf.nn.relu(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = tf.nn.relu(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># model.build(input_shape) 定义网络输入的形状</span><br><span class="line">model.build(input_shape=(None, 784))</span><br><span class="line"></span><br><span class="line"># model.summary() 查看网络结构</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"># model.trainable_variables 查看网络所有可训练参数</span><br><span class="line">model.trainable_variables</span><br><span class="line"></span><br><span class="line"># model.compile(optimizer, loss, metrics) 配置训练模型，设置优化器为optimizer，损失函数为loss，衡量指标为metrics</span><br><span class="line"></span><br><span class="line"># model.fit(db, epoch, validation_data, validation_freq) 训练模型，训练集为db，训练epoch次，validation_freq次对validation_data进行一次测试，防止过拟合</span><br><span class="line"></span><br><span class="line"># model.evaluate(db_test) 对测试集进行测试</span><br><span class="line"></span><br><span class="line"># model.predict(x) 对未知的数据进行预测</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/FRAME/tensorflow23.png" alt="23"></p><h3 id="save-保存-模块"><a href="#save-保存-模块" class="headerlink" title="save(保存)模块"></a><font size="3">save(保存)模块</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># model.save_weights(filename) 将网络的训练参数保存在filename中，仅仅保存权值</span><br><span class="line"># model.load_weights(filename) 读取filename中的训练参数，前提是要创建一个相同结构的网络</span><br><span class="line"></span><br><span class="line"># model.save(filename) 保存网络的所有结构和参数</span><br><span class="line"># tf.keras.models.load_model(filename) 读取模型，不需要创建网络，会生成一个相同的网络</span><br><span class="line"></span><br><span class="line"># tf.saved_model.save(model, filename) 保存网络的所有结构和参数，便于给其他语言提供调用</span><br></pre></td></tr></tbody></table></figure><h1 id="TensorFlow小结"><a href="#TensorFlow小结" class="headerlink" title="TensorFlow小结"></a><font size="5" color="red">TensorFlow小结</font></h1><p>  由于TensorFlow背靠谷歌，具有最全的文档和资源，而且很多模型都有TensorFlow的源码实现，所以拥有较大的用户基数，这样使用户出现问题时能较容易地找到解决方案，这使TensorFlow目前作为最流行的深度学习框架。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;TensorFlow&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习框架" scheme="https://USTCcoder.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>网络流(Network Flows)</title>
    <link href="https://USTCcoder.github.io/2019/09/02/algorithm%20network_flows/"/>
    <id>https://USTCcoder.github.io/2019/09/02/algorithm network_flows/</id>
    <published>2019-09-02T04:55:58.000Z</published>
    <updated>2019-09-06T03:41:17.176Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">网络流</font></strong></center><p></p><h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a><font size="5" color="red">原理介绍</font></h1><p>   <strong>Network Flows:网络流</strong>，是运筹学中的最优化问题，也是图论中的一种理论方法。类比水流的解决问题，与线性规划密切相关，常常用来解决实际的生活问题。<br><a id="more"></a></p><p><img src="/images/ALGORITHM/union1.jpg" alt="1"></p><h1 id="算法基础"><a href="#算法基础" class="headerlink" title="算法基础"></a><font size="5" color="red">算法基础</font></h1><h2 id="图的基本术语"><a href="#图的基本术语" class="headerlink" title="图的基本术语"></a><font size="4">图的基本术语</font></h2><p>  <font size="3">(1)无向图：G中的每条边都是没有方向的，顶点v1和v2之间的边记为(v1,v2)或(v2,v1)。</font><br><img src="/images/ALGORITHM/network1.jpg" alt="2"><br>  <font size="3">(2)有向图：G中的每条边都是有方向的，顶点v1和v2之间的边记为<v1,v2>，不能写成<v2,v1>。</v2,v1></v1,v2></font><br><img src="/images/ALGORITHM/network2.jpg" alt="3"><br>  <font size="3">(3)网：在边上标注距离，时间，花费等等数值，称为边的权值，带有权值的图称为网。</font><br><img src="/images/ALGORITHM/network3.jpg" alt="4"><br>  <font size="3">(4)二分图：如果顶点集V可分割为两个互不相交的子集V1,V2，并且图中的每条边所对应的两个顶点分别属于这两个不同的顶点集，称G为二分图。</font><br><img src="/images/ALGORITHM/network4.jpg" alt="5"></p><h2 id="网络的基本术语"><a href="#网络的基本术语" class="headerlink" title="网络的基本术语"></a><font size="4">网络的基本术语</font></h2><p>  <font size="3">(1)网络：在有向网中，有两个特殊的点，源点s和汇点t，图中各边的方向表示允许的流向，边上的权值表示可允许的最大流量，且两个结点之间最多只有一条边，称这样的图为网络。</font><br><img src="/images/ALGORITHM/network1.png" alt="6"><br>  <font size="3">(2)网络流：网络上的流，即定义在边集E上的非负函数flow称为网络流。</font><br>  <font size="3">(3)可行流：满足容量约束（每个边的实际流量不大于最大容量）和流量守恒（除了源点s和汇点t外，所有内部结点流入量等于流出量）两个性质的网络流称为可行流。</font><br>  <font size="3">(4)最大流：在满足可行流的条件下，在网络中找到一个净输出最大的网络流称为最大流。</font></p><h1 id="经典例题-最大网络流，最短增广路算法"><a href="#经典例题-最大网络流，最短增广路算法" class="headerlink" title="经典例题(最大网络流，最短增广路算法)"></a><font size="5" color="red">经典例题(最大网络流，最短增广路算法)</font></h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  一家公司要把一批货物从工厂运到北京，中间经过若干个城市，已知城市数，连接数和城市之间的最大运输量，求如何运输使运输量最大。<br>  第一行输入结点个数和边数，然后每行输入连通的两个城市以及最大运输量，使用空格分隔。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">6 9 # 结点个数n和边数m</span><br><span class="line">1 2 12 # 说明1号城市和2号城市之间的最大运输量为12</span><br><span class="line">1 3 10</span><br><span class="line">2 4 8</span><br><span class="line">3 2 2</span><br><span class="line">3 5 13</span><br><span class="line">4 3 5</span><br><span class="line">4 6 18</span><br><span class="line">5 4 6</span><br><span class="line">5 6 4</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  如果一条边的容量为n，已经流出了m则该点最多可以正向流出(n-m)或者反向流入m，因此引入一个残余网络，正向代表可增量，即还可以流出的容量，反向代表流量，即已经流出的容量，等于可以流入的流量。<br><img src="/images/ALGORITHM/network2.png" alt="12"><br>  利用深度优先或者广度优先从源点s对该图进行遍历，如果从i点到达j点的值大于0，说明可以流通，则继续。当搜索到t点时，说明该路径是一条可行流，找到该路径上边的最小值，最大流加上该值，然后修改网络，将路径上的边正向减去该值，反向加上该值。重新搜索，直到无法到达t点算法结束。</p><h2 id="python代码实战"><a href="#python代码实战" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">def sap(connect_map, n):</span><br><span class="line">    real_map = [[0 for i in range(n + 1)] for j in range(n + 1)]</span><br><span class="line">    residue_map = [[x for x in row] for row in connect_map]</span><br><span class="line">    max_flow, find_flag = 0, True</span><br><span class="line">    while find_flag:</span><br><span class="line">        queue, find_flag = [[1, [1]]], False</span><br><span class="line">        while queue:</span><br><span class="line">            i, route = queue.pop(0)</span><br><span class="line">            for j in range(1, n + 1):</span><br><span class="line">                if residue_map[i][j] &gt; 0 and j not in route:</span><br><span class="line">                    if j == n:</span><br><span class="line">                        flow, route, queue, find_flag = [], route + [j], [], True</span><br><span class="line">                        for k in range(len(route) - 1):</span><br><span class="line">                            flow.append(residue_map[route[k]][route[k + 1]])</span><br><span class="line">                        min_flow = min(flow)</span><br><span class="line">                        max_flow += min_flow</span><br><span class="line">                        for p in range(len(route) - 1):</span><br><span class="line">                            real_map[route[p]][route[p + 1]] += min_flow</span><br><span class="line">                            residue_map[route[p]][route[p + 1]] -= min_flow</span><br><span class="line">                            residue_map[route[p + 1]][route[p]] += min_flow</span><br><span class="line">                        break</span><br><span class="line">                    else:</span><br><span class="line">                        queue.append([j, route + [j]])</span><br><span class="line">    return real_map, max_flow</span><br><span class="line"></span><br><span class="line">print('请输入结点个数n和边数m:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    n, m = [int(x) for x in line.strip().split()]</span><br><span class="line">    connect_map, label, direction = [[0 for i in range(n + 1)] for j in range(n + 1)], ['v' + str(i) for i in range(1, n + 1)], []</span><br><span class="line">    print('请输入结点个数u,v及边u-v的容量w:')</span><br><span class="line">    for i in range(m):</span><br><span class="line">        u, v, w = [int(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">        connect_map[u][v] = w</span><br><span class="line">        direction.append([u, v])</span><br><span class="line">    real_map, max_flow = sap(connect_map, n)</span><br><span class="line">    for k in direction:</span><br><span class="line">        real_map[k[0]][k[1]] -= real_map[k[1]][k[0]]</span><br><span class="line">        real_map[k[1]][k[0]] = 0</span><br><span class="line">    net_work = pd.DataFrame([[x for x in row[1:]] for row in real_map[1:]], index=label, columns=label)</span><br><span class="line">    print('网络的最大流值为:', max_flow, '\n---------实流网络如下:---------\n', net_work)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果"><a href="#代码运行结果" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/network4.png" alt="14"></p><h1 id="经典例题-最大网络流，重贴标签算法"><a href="#经典例题-最大网络流，重贴标签算法" class="headerlink" title="经典例题(最大网络流，重贴标签算法)"></a><font size="5" color="red">经典例题(最大网络流，重贴标签算法)</font></h1><h2 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  一家公司要把一批货物从工厂运到北京，中间经过若干个城市，已知城市数，连接数和城市之间的最大运输量，求如何运输使运输量最大。<br>  第一行输入结点个数和边数，然后每行输入连通的两个城市以及最大运输量，使用空格分隔。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">6 9 # 结点个数n和边数m</span><br><span class="line">1 2 12 # 说明1号城市和2号城市之间的最大运输量为12</span><br><span class="line">1 3 10</span><br><span class="line">2 4 8</span><br><span class="line">3 2 2</span><br><span class="line">3 5 13</span><br><span class="line">4 3 5</span><br><span class="line">4 6 18</span><br><span class="line">5 4 6</span><br><span class="line">5 6 4</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="算法分析-1"><a href="#算法分析-1" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  在上例算法中，浪费了许多时间，因为要从源点重复进行深度优先遍历或者广度优先遍历，因此有重复的大量计算。<br>  引入混合网络，将残余网络进行优化，同向边为一个元组(cap, flow)记录容量和当前流量，反向边也是一个元组(0, -flow)记录容量个当前流量，可以通过该网络直接看出方向和流量。<br><img src="/images/ALGORITHM/network3.png" alt="13"><br>  (1)从汇点开始，利用广度优先算法对结点添加标签，从0开始，第一次直接访问到的点标记为1，第二次间接访问到的点标记为2，依次贴标签。<br>  (2)如果源点高度大于等于结点数，说明已经找到了最大流，算法结束，否则从源点开始，搜索源点高度-1的点，观察是否可以前进，如果可以，当结点为汇点时，进行增流减流操作(同向边增流，反向边减流)，如果不可以则需要重贴标签。<br>  (3)重贴标签：如果拥有当前结点高度的结点只有一个，则算法结束，否则寻找是否有可行邻接边(容量大于流量)，如果有则令当前结点高度等于邻接点高度的最小值+1，否则令当前结点的高度等于结点数。重新回到(2)</p><h2 id="python代码实战-1"><a href="#python代码实战-1" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">def isap(mix_net, height_table, n):</span><br><span class="line">    max_flow, real_map = 0, [[0 for i in range(n + 1)] for j in range(n + 1)]</span><br><span class="line">    while height_table[1] &lt; n:</span><br><span class="line">        current_node, stack = 1, []</span><br><span class="line">        while current_node != n:</span><br><span class="line">            next_height, flag, index = height_table[current_node] - 1, False, 0</span><br><span class="line">            for i in range(1, n + 1):</span><br><span class="line">                if height_table[i] == next_height and mix_net[current_node][i][0] &gt; mix_net[current_node][i][1]:</span><br><span class="line">                    flag, index = True, i</span><br><span class="line">                    break</span><br><span class="line">            if flag:</span><br><span class="line">                stack.append(current_node)</span><br><span class="line">                current_node = index</span><br><span class="line">                if current_node == n:</span><br><span class="line">                    stack.append(current_node)</span><br><span class="line">                    flow = []</span><br><span class="line">                    for k in range(len(stack) - 1):</span><br><span class="line">                        flow.append(mix_net[stack[k]][stack[k + 1]][0] - mix_net[stack[k]][stack[k + 1]][1])</span><br><span class="line">                    min_flow = min(flow)</span><br><span class="line">                    max_flow += min_flow</span><br><span class="line">                    for p in range(len(stack) - 1):</span><br><span class="line">                        real_map[stack[p]][stack[p + 1]] += min_flow</span><br><span class="line">                        mix_net[stack[p]][stack[p + 1]][1] += min_flow</span><br><span class="line">                        mix_net[stack[p + 1]][stack[p]][1] -= min_flow</span><br><span class="line">            else:</span><br><span class="line">                if height_table.count(height_table[current_node]) == 1:</span><br><span class="line">                    return real_map, max_flow</span><br><span class="line">                min_neibor, no_neibor = n, True</span><br><span class="line">                for i in range(1, n + 1):</span><br><span class="line">                    if mix_net[current_node][i][0] &gt; mix_net[current_node][i][1]:</span><br><span class="line">                        no_neibor, min_neibor = False, min(min_neibor, height_table[i])</span><br><span class="line">                height_table[current_node] = n if no_neibor else min_neibor + 1</span><br><span class="line">                if stack:</span><br><span class="line">                    current_node = stack[-1]</span><br><span class="line">                    stack.pop()</span><br><span class="line">                else:</span><br><span class="line">                    current_node = 1</span><br><span class="line">    return real_map, max_flow</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def init_height(queue):</span><br><span class="line">    global height_table</span><br><span class="line">    while queue:</span><br><span class="line">        j, height = queue.pop(0)</span><br><span class="line">        for i in range(1, n + 1):</span><br><span class="line">            if mix_net[i][j][0] &gt; 0 and height_table[i] == -1:</span><br><span class="line">                height_table[i] = height + 1</span><br><span class="line">                queue.append([i, height + 1])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print('请输入结点个数n和边数m:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    n, m = [int(x) for x in line.strip().split()]</span><br><span class="line">    mix_net, label, height_table, direction = [[[0, 0] for i in range(n + 1)] for j in range(n + 1)], ['v' + str(i) for i in range(1, n + 1)], [-1] * n + [0], []</span><br><span class="line">    print('请输入结点个数u,v及边u-v的容量w:')</span><br><span class="line">    for i in range(m):</span><br><span class="line">        u, v, w = [int(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">        mix_net[u][v], mix_net[v][u] = [w, 0], [0, 0]</span><br><span class="line">        direction.append([u, v])</span><br><span class="line">    init_height([[n, 0]])</span><br><span class="line">    real_map, max_flow = isap(mix_net, height_table, n)</span><br><span class="line">    for k in direction:</span><br><span class="line">        real_map[k[0]][k[1]] -= real_map[k[1]][k[0]]</span><br><span class="line">        real_map[k[1]][k[0]] = 0</span><br><span class="line">    net_work = pd.DataFrame([[x for x in row[1:]] for row in real_map[1:]], index=label, columns=label)</span><br><span class="line">    print('网络的最大流值为:', max_flow, '\n---------实流网络如下:---------\n', net_work)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-1"><a href="#代码运行结果-1" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/network5.png" alt="15"></p><h1 id="经典例题-最小费用最大流"><a href="#经典例题-最小费用最大流" class="headerlink" title="经典例题(最小费用最大流)"></a><font size="5" color="red">经典例题(最小费用最大流)</font></h1><h2 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  一家公司要把一批货物从工厂运到北京，中间经过若干个城市，已知城市数，连接数和城市之间的最大运输量，以及单位货物的运送费用，如何找到一种流量最大费用尽可能小的方法。<br>  第一行输入结点个数和边数，然后每行输入连通的两个城市以及最大运输量，和单位货物运输费用，使用空格分隔。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">6 10 # 结点个数n和边数m</span><br><span class="line">1 2 3 1 # 说明1号城市和2号城市之间的最大运输量为3，单位运输量费用为1</span><br><span class="line">1 3 4 7</span><br><span class="line">2 3 1 1</span><br><span class="line">2 4 6 4</span><br><span class="line">2 5 4 5</span><br><span class="line">3 4 5 3</span><br><span class="line">3 5 3 6</span><br><span class="line">4 6 7 6</span><br><span class="line">5 4 3 3</span><br><span class="line">5 6 3 2</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="算法分析-2"><a href="#算法分析-2" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  (1)最短增广路算法类似，先建立混合网络。<br>  (2)从源点开始搜索，找到一条最短费用路，如果无法搜索到汇点，则算法结束，已经找到最小费用最大流，否则总花费加上该路径边上的最小值乘路径上的所有花费之和作为目前的总花费。<br>  (3)然后更新混合网络，正向增流，反向减流。回到步骤(2)</p><h2 id="python代码实战-2"><a href="#python代码实战-2" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">def spfa(mix_net, n):</span><br><span class="line">    real_map = [[0 for i in range(n + 1)] for j in range(n + 1)]</span><br><span class="line">    max_flow, total_cost, find_flag, min_cost, min_route = 0, 0, True, 65535, [1]</span><br><span class="line">    while min_route:</span><br><span class="line">        queue, find_flag, min_cost, min_route = [[1, [1], 0]], False, 65535, []</span><br><span class="line">        while queue:</span><br><span class="line">            i, route, cost = queue.pop(0)</span><br><span class="line">            if i == n and min_cost &gt; cost:</span><br><span class="line">                min_cost, min_route = cost, route</span><br><span class="line">            for j in range(1, n + 1):</span><br><span class="line">                if mix_net[i][j][0] &gt; mix_net[i][j][1] and j not in route:</span><br><span class="line">                    queue.append([j, route + [j], cost + mix_net[i][j][2]])</span><br><span class="line">        if min_route:</span><br><span class="line">            flow = []</span><br><span class="line">            for k in range(len(min_route) - 1):</span><br><span class="line">                flow.append(mix_net[min_route[k]][min_route[k + 1]][0] - mix_net[min_route[k]][min_route[k + 1]][1])</span><br><span class="line">            min_flow = min(flow)</span><br><span class="line">            max_flow += min_flow</span><br><span class="line">            total_cost += min_cost * min_flow</span><br><span class="line">            for p in range(len(min_route) - 1):</span><br><span class="line">                real_map[min_route[p]][min_route[p + 1]] += min_flow</span><br><span class="line">                mix_net[min_route[p]][min_route[p + 1]][1] += min_flow</span><br><span class="line">                mix_net[min_route[p + 1]][min_route[p]][1] -= min_flow</span><br><span class="line">    return real_map, max_flow, total_cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print('请输入结点个数n和边数m:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    n, m = [int(x) for x in line.strip().split()]</span><br><span class="line">    mix_net, label, direction = [[[0, 0, 0] for i in range(n + 1)] for j in range(n + 1)], ['v' + str(i) for i in range(1, n + 1)], []</span><br><span class="line">    print('请输入结点个数u，v及边u-v的容量w，单位容量费用c:')</span><br><span class="line">    for i in range(m):</span><br><span class="line">        u, v, w, c = [int(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">        mix_net[u][v], mix_net[v][u] = [w, 0, c], [0, 0, -c]</span><br><span class="line">        direction.append([u, v])</span><br><span class="line">    real_map, max_flow, total_cost = spfa(mix_net, n)</span><br><span class="line">    for k in direction:</span><br><span class="line">        real_map[k[0]][k[1]] -= real_map[k[1]][k[0]]</span><br><span class="line">        real_map[k[1]][k[0]] = 0</span><br><span class="line">    net_work = pd.DataFrame([[x for x in row[1:]] for row in real_map[1:]], index=label, columns=label)</span><br><span class="line">    print('网络的最大流值为:', max_flow, '\n网络的最小费用为:', total_cost, '\n---------实流网络如下:---------\n', net_work)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-2"><a href="#代码运行结果-2" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/network6.png" alt="16"></p><h1 id="算法总结"><a href="#算法总结" class="headerlink" title="算法总结"></a><font size="5" color="red">算法总结</font></h1><p>  网络流是一种较为复杂的算法，通常用来解决实际的问题，其模板固定，难点在于如何将问题转化为一个网络流表示的形式，因此需要多加练习，做到熟练掌握。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Network Flows&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="常用算法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>OpenCV</title>
    <link href="https://USTCcoder.github.io/2019/08/20/library%20opencv/"/>
    <id>https://USTCcoder.github.io/2019/08/20/library opencv/</id>
    <published>2019-08-20T09:14:28.000Z</published>
    <updated>2020-07-27T12:01:00.847Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LIBRARY/cv.jpg" alt="0"></p><h1 id="OpenCV介绍"><a href="#OpenCV介绍" class="headerlink" title="OpenCV介绍"></a><font size="5" color="red">OpenCV介绍</font></h1><p>  OpenCV是一个基于BSD许可（开源）发行的跨平台计算机视觉库，可以运行在Linux、Windows、Android和Mac OS操作系统上。它轻量级而且高效，由一系列 C 函数和少量 C++ 类构成，同时提供了Python、Ruby、MATLAB等语言的接口，实现了图像处理和计算机视觉方面的很多通用算法。<br><a id="more"></a></p><h1 id="OpenCV特点"><a href="#OpenCV特点" class="headerlink" title="OpenCV特点"></a><font size="5" color="red">OpenCV特点</font></h1><p>  <font size="3">OpenCV是开源的计算机视觉库，采用C / C++编写，处理速度很快。</font><br>  <font size="3">OpenCV可以提供主流语言的接口，方便开发者调用。</font><br>  <font size="3">OpenCV具有通用的图像/视频载，保存和获取模块，具有底层和高层的应用开发包。</font></p><h1 id="OpenCV应用"><a href="#OpenCV应用" class="headerlink" title="OpenCV应用"></a><font size="5" color="red">OpenCV应用</font></h1><h2 id="OpenCV读取，显示与保存"><a href="#OpenCV读取，显示与保存" class="headerlink" title="OpenCV读取，显示与保存"></a><font size="4">OpenCV读取，显示与保存</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line"></span><br><span class="line"># cv.imread(filename, code) 以code格式读取一张图片，code可以为cv.IMREAD_GRAYSCALE读取一张灰度图像</span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line"></span><br><span class="line"># cv.namedWindow(windowname) 创建一个名为windowname的窗口</span><br><span class="line">cv.namedWindow('lena')</span><br><span class="line"></span><br><span class="line"># cv.imshow(window_name, img) 将img图片显示在窗口处</span><br><span class="line">cv.imshow('lena', img)</span><br><span class="line"></span><br><span class="line"># cv.waitkey(n) 等待用户按键n毫秒，0代表永远等待</span><br><span class="line">cv.waitkey(0)</span><br><span class="line"></span><br><span class="line"># cv.destroyAllWindows() 关闭所有窗口</span><br><span class="line">cv.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"># cv.VideoCapture(n) 获取第n个摄像头，从0开始编号</span><br><span class="line">cv.VideoCapture(0)</span><br><span class="line"></span><br><span class="line"># cv.imwrite(filename, img) 将img图片保存在filename文件中</span><br><span class="line">cv.imwrite('lena1.png', img)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv1.png" alt="1"></p><h2 id="OpenCV图像格式转换"><a href="#OpenCV图像格式转换" class="headerlink" title="OpenCV图像格式转换"></a><font size="4">OpenCV图像格式转换</font></h2><h3 id="cvtColor方法"><a href="#cvtColor方法" class="headerlink" title="cvtColor方法"></a><font size="3">cvtColor方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line"></span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line"></span><br><span class="line"># cv.cvtColor(img, code) 将img转换为code格式，code可以为cv.COLOR_BGR2GRAY将RGB三通道彩色图像转换为灰度图像，cv.COLOR_BGR2HSV将RGB颜色通道转换为HSV颜色通道</span><br><span class="line">img_gray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line">cv.namedWindow('gray')</span><br><span class="line">cv.imshow('gray', img_gray)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv2.png" alt="2"></p><h2 id="OpenCV图像形态学变换"><a href="#OpenCV图像形态学变换" class="headerlink" title="OpenCV图像形态学变换"></a><font size="4">OpenCV图像形态学变换</font></h2><h3 id="morphologyEx方法"><a href="#morphologyEx方法" class="headerlink" title="morphologyEx方法"></a><font size="3">morphologyEx方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">kernel = np.ones((3, 3), np.uint8)</span><br><span class="line">img = cv.imread('lena.jpg', cv2.IMREAD_GRAYSCALE)</span><br><span class="line"></span><br><span class="line"># cv.morphologyEx(img, op, kernel) 将img进行模板为kernel的op操作，其中op可以为cv.MORPH_DILATE膨胀操作，cv.MORPH_ERODE腐蚀操作，cv.MORPH_OPEN开操作，cv.MORPH_CLOSE闭操作，cv.MORPH_GRADIENT梯度操作(膨胀+腐蚀)找出边缘</span><br><span class="line">img_gradient = cv.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)</span><br><span class="line"></span><br><span class="line">cv.namedWindow('gradient')</span><br><span class="line">cv.imshow('gradient', img_gradient)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv3.png" alt="3"></p><h2 id="OpenCV图像形状变换"><a href="#OpenCV图像形状变换" class="headerlink" title="OpenCV图像形状变换"></a><font size="4">OpenCV图像形状变换</font></h2><h3 id="resize方法"><a href="#resize方法" class="headerlink" title="resize方法"></a><font size="3">resize方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line"></span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line">cv.namedWindow('origin')</span><br><span class="line">cv.imshow('origin', img)</span><br><span class="line"></span><br><span class="line"># cv.resize(img, shape) 将img的大小调整为shape</span><br><span class="line">img_resize = cv.resize(img, (500, 500))</span><br><span class="line">cv.namedWindow('resize')</span><br><span class="line">cv.imshow('resize',img_resize)</span><br><span class="line"></span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv7.png" alt="7"></p><h3 id="flip方法"><a href="#flip方法" class="headerlink" title="flip方法"></a><font size="3">flip方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line"></span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line">cv.namedWindow('origin')</span><br><span class="line">cv.imshow('origin', img)</span><br><span class="line"></span><br><span class="line"># cv.flip(img, n) 将img翻转，n&gt;0沿y轴对称翻转，n=0沿x轴对称翻转，n&lt;0沿x轴y轴同时对称翻转</span><br><span class="line">img_flip_x = cv.flip(img, 0)</span><br><span class="line">cv.namedWindow('flip_x')</span><br><span class="line">cv.imshow('flip_x',img_flip_x)</span><br><span class="line"></span><br><span class="line">img_flip_y = cv.flip(img, 1)</span><br><span class="line">cv.namedWindow('flip_y')</span><br><span class="line">cv.imshow('flip_y',img_flip_y)</span><br><span class="line"></span><br><span class="line">img_flip_xy = cv.flip(img, -1)</span><br><span class="line">cv.namedWindow('flip_xy')</span><br><span class="line">cv.imshow('flip_xy',img_flip_xy)</span><br><span class="line"></span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv4.png" alt="4"></p><h3 id="warpAffine方法"><a href="#warpAffine方法" class="headerlink" title="warpAffine方法"></a><font size="3">warpAffine方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv </span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np.random.seed(5)</span><br><span class="line"></span><br><span class="line">m = np.random.rand(2,3)</span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line">cv.namedWindow('origin')</span><br><span class="line">cv.imshow('origin', img)</span><br><span class="line"></span><br><span class="line"># cv.warpAffine(img, m, dsize) 将img进行仿射变换，变换矩阵为m，变换后的大小为dsize</span><br><span class="line">img_affine = cv.warpAffine(img, m, (img.shape[0], img.shape[1]))</span><br><span class="line">cv.namedWindow('affine')</span><br><span class="line">cv.imshow('affine',img_affine)</span><br><span class="line"></span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv5.png" alt="5"></p><h2 id="OpenCV图像操作"><a href="#OpenCV图像操作" class="headerlink" title="OpenCV图像操作"></a><font size="4">OpenCV图像操作</font></h2><h3 id="bitwise方法"><a href="#bitwise方法" class="headerlink" title="bitwise方法"></a><font size="3">bitwise方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np.random.seed(1)</span><br><span class="line"></span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line">img_random = np.random.randint(0, 256, img.shape, dtype=np.uint8)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># cv.bitwise_not(img) 按位取反，等价于~img</span><br><span class="line">img_not = cv.bitwise_not(img)</span><br><span class="line">cv.namedWindow('not')</span><br><span class="line">cv.imshow('not', img_not)</span><br><span class="line"></span><br><span class="line"># cv.bitwise_and(img1, img2) 按位与，等价于img1 &amp; img2</span><br><span class="line">img_and = cv.bitwise_and(img, img_random)</span><br><span class="line">cv.namedWindow('and')</span><br><span class="line">cv.imshow('and', img_and)</span><br><span class="line"></span><br><span class="line"># cv.bitwise_or(img1, img2) 按位或，等价于img1 | img2</span><br><span class="line">img_or = cv.bitwise_or(img, img_random)</span><br><span class="line">cv.namedWindow('or')</span><br><span class="line">cv.imshow('or', img_or)</span><br><span class="line"></span><br><span class="line"># cv.bitwise_xor(img1, img2) 按位异或，等价于img1 ^ img2</span><br><span class="line">img_xor = cv.bitwise_xor(img, img_random)</span><br><span class="line">cv.namedWindow('xor')</span><br><span class="line">cv.imshow('xor', img_xor)</span><br><span class="line"></span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv6.png" alt="6"></p><h3 id="add，subtract，multiply，divide，addweight方法"><a href="#add，subtract，multiply，divide，addweight方法" class="headerlink" title="add，subtract，multiply，divide，addweight方法"></a><font size="3">add，subtract，multiply，divide，addweight方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np.random.seed(1)</span><br><span class="line"></span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line">img_random = np.random.randint(0, 256, img.shape, dtype=np.uint8)</span><br><span class="line"></span><br><span class="line"># cv.add(img1, img2) 将两张图片相加，如果加和大于上限则赋值为上限(200+200=255)，和img1+img2不同，img1+img2如果大于上限则从下限开始计算(200+200=400-256=144)</span><br><span class="line">img_add = cv.add(img, img_random)</span><br><span class="line">cv.namedWindow('add')</span><br><span class="line">cv.imshow('add', img_add)</span><br><span class="line"></span><br><span class="line">img_plus = img + img_random</span><br><span class="line">cv.namedWindow('plus')</span><br><span class="line">cv.imshow('plus', img_plus)</span><br><span class="line"></span><br><span class="line"># cv.subtract(img1, img2) 将两张图片相减，用法同add</span><br><span class="line"></span><br><span class="line"># cv.multiply(img1, img2) 将两张图片相乘，用法同add</span><br><span class="line"></span><br><span class="line"># cv.divide(img1, img2) 将两张图片相除，用法同add</span><br><span class="line"></span><br><span class="line"># cv.addweight(img1, x, img2, y, z) 将两张图片相加，结果为img1 * x + img2 * y + z</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv8.png" alt="8"></p><h3 id="blur，medianBlur，GaussianBlur，filter2D方法"><a href="#blur，medianBlur，GaussianBlur，filter2D方法" class="headerlink" title="blur，medianBlur，GaussianBlur，filter2D方法"></a><font size="3">blur，medianBlur，GaussianBlur，filter2D方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line"></span><br><span class="line">cv.namedWindow('origin')</span><br><span class="line">cv.imshow('origin', img)</span><br><span class="line"></span><br><span class="line"># cv.blur(img, ksize) 将img做均值模糊操作，核大小为ksize</span><br><span class="line">img_blur = cv.blur(img, (5, 5))</span><br><span class="line"></span><br><span class="line">cv.namedWindow('blur')</span><br><span class="line">cv.imshow('blur', img_blur)</span><br><span class="line"></span><br><span class="line"># cv.medianBlur(img, ksize) 将img做中值模糊操作，核大小为ksize</span><br><span class="line">img_median_blur = cv.medianBlur(img, 5)</span><br><span class="line"></span><br><span class="line">cv.namedWindow('median_blur')</span><br><span class="line">cv.imshow('median_blur', img_median_blur)</span><br><span class="line"></span><br><span class="line"># cv.GaussianBlur(img, ksize, sigmaX) 将img做高斯模糊操作，核大小为ksize，σ为sigmaX</span><br><span class="line">img_gauss_blur = cv.GaussianBlur(img, (5, 5), sigmaX=1)</span><br><span class="line"></span><br><span class="line">cv.namedWindow('gauss_blur')</span><br><span class="line">cv.imshow('gauss_blur', img_gauss_blur)</span><br><span class="line"></span><br><span class="line"># cv.filter2D(img, ddepth, kernel) 将img做二位卷积操作，卷积核为kernel，如果要保持大小则ddepth为-1</span><br><span class="line">img_edge = cv.filter2D(img, -1, np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]]))</span><br><span class="line"></span><br><span class="line">cv.namedWindow('edge')</span><br><span class="line">cv.imshow('edge', img_edge)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv9.png" alt="9"></p><h2 id="OpenCV统计"><a href="#OpenCV统计" class="headerlink" title="OpenCV统计"></a><font size="4">OpenCV统计</font></h2><h3 id="getTickCount，getTickFrequency方法"><a href="#getTickCount，getTickFrequency方法" class="headerlink" title="getTickCount，getTickFrequency方法"></a><font size="3">getTickCount，getTickFrequency方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line"></span><br><span class="line"># cv.getTickCount() 计算从开机到当前时间的时钟周期数</span><br><span class="line">t1 = cv.getTickCount()</span><br><span class="line">t2 = cv.getTickCount()</span><br><span class="line"></span><br><span class="line"># cv.getTickFrequency() 获得一秒的时钟周期数，常常用时钟周期数除以该数获得所用时间</span><br><span class="line">f = cv.getTickFrequency()</span><br><span class="line"></span><br><span class="line">print('所用时间为:', (t2-t1) / f)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv10.png" alt="10"></p><h3 id="mean，meanStdDev方法"><a href="#mean，meanStdDev方法" class="headerlink" title="mean，meanStdDev方法"></a><font size="3">mean，meanStdDev方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line"></span><br><span class="line"># cv.mean(img) 计算img每个通道的均值</span><br><span class="line">mean = cv.mean(img)</span><br><span class="line"></span><br><span class="line"># cv.meanStdDev(img) 计算img每个通道的均值和方差</span><br><span class="line">mean1, std1 = cv.meanStdDev(img)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv11.png" alt="11"></p><h3 id="calcHist方法"><a href="#calcHist方法" class="headerlink" title="calcHist方法"></a><font size="3">calcHist方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line">color=['blue','green','red']</span><br><span class="line"></span><br><span class="line"># cv.calcHist(img, channels, mask, histSize, ranges) 绘制img中channels通道的直方图，histSize为直方图大小，ranges为直方图的范围</span><br><span class="line">for i,color_i in enumerate(color):</span><br><span class="line">    hist = cv.calcHist([img],[i],None,[256],[0,255])</span><br><span class="line">    plt.plot(hist,color_i)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv12.png" alt="12"></p><h2 id="OpenCV常用图像操作"><a href="#OpenCV常用图像操作" class="headerlink" title="OpenCV常用图像操作"></a><font size="4">OpenCV常用图像操作</font></h2><h3 id="inRange方法"><a href="#inRange方法" class="headerlink" title="inRange方法"></a><font size="3">inRange方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line">cv.namedWindow('origin')</span><br><span class="line">cv.imshow('origin', img)</span><br><span class="line"></span><br><span class="line">img_hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)</span><br><span class="line"></span><br><span class="line"># cv.inRange(img, lowerb, upperb) 将HSV色彩空间中的颜色过滤，img颜色通道先转换为HSV，然后再进行颜色过滤</span><br><span class="line">res = cv.inRange(img_hsv, np.array([125, 43, 46]), np.array([155, 255, 255]))</span><br><span class="line"></span><br><span class="line">cv.namedWindow('purple')</span><br><span class="line">cv.imshow('purple', res)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><font size="3">HSV空间颜色分布表：</font><script type="math/tex; mode=display">\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|} 颜色 & 黑 & 灰 & 白 & 红 & 橙 & 黄 & 绿 & 青 & 蓝 & 紫 \\\\ \\hline h_{min} & 0 & 0 & 0 & 0 & 11 & 26 & 35 & 78 & 100 & 125\\\\ h_{max} & 180 & 180 & 180 & 10 & 25 & 34 & 77 & 99 & 124 & 155\\\\ s_{min} & 0 & 0 & 0 & 43 & 43 & 43 & 43 & 43 & 43 & 43\\\\ s_{max} & 255 & 43 & 30 & 255 & 255 & 255 & 255 & 255 & 255 & 255\\\\ v_{min} & 0 & 46 & 221 & 46 & 46 & 46 & 46 & 46 & 46 & 46\\\\ v_{max} & 46 & 220 & 255 & 255 & 255 & 255 & 255 & 255 & 255 & 255\\\\ \end{array}</script><p><img src="/images/LIBRARY/cv13.png" alt="13"></p><h3 id="equalizeHist方法"><a href="#equalizeHist方法" class="headerlink" title="equalizeHist方法"></a><font size="4">equalizeHist方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line"></span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line">img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line">cv.namedWindow('origin')</span><br><span class="line">cv.imshow('origin', img)</span><br><span class="line"></span><br><span class="line"># cv.equalizeHist(img) 对img图像进行直方图均衡化</span><br><span class="line">img_equal = cv.equalizeHist(img)</span><br><span class="line"></span><br><span class="line">cv.namedWindow('euqal')</span><br><span class="line">cv.imshow('equal', img_equal)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destoryAllWindows()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv14.png" alt="14"></p><h3 id="compareHist，calcBackProject，matchTemplate方法"><a href="#compareHist，calcBackProject，matchTemplate方法" class="headerlink" title="compareHist，calcBackProject，matchTemplate方法"></a><font size="3">compareHist，calcBackProject，matchTemplate方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># cv.compareHist(hist1, hist2, method) 对两个直方图进行比较，method可以为cv.HISTCMP_BHATTACHARYYA，cv.HISTCMP_CORREL等等</span><br><span class="line"></span><br><span class="line"># cv.calcBackProject(target, channel, hsv_hist, range) 从target中寻找处与模板相似的区域，其中模板为HSV图像的直方图，channel为通道数，range为直方图的长和宽</span><br><span class="line"></span><br><span class="line"># cv.matchTemplate(target, mask, method) 计算图片之间的匹配程度，method可以为cv.TM_SQDIFF_NORMED，cv.TM_CCOERR_NORMED，cv.TM_CCOEFF_NORMED等等</span><br></pre></td></tr></tbody></table></figure><h3 id="line，rectangle，circle方法"><a href="#line，rectangle，circle方法" class="headerlink" title="line，rectangle，circle方法"></a><font size="3">line，rectangle，circle方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># cv.line(img, pt1, pt2, color, thickness) 在img图像上画一条直线，起点坐标为pt1，终点坐标为pt2，颜色为color，线宽为thickness</span><br><span class="line"></span><br><span class="line"># cv.rectangle(img, pt1, pt2, color, thickness) 在img图像上画一个矩形，左上点坐标为pt1，右下点坐标为pt2，颜色为color，线宽为thickness</span><br><span class="line"></span><br><span class="line"># cv.circle(img, center, radius, color, thickness) 在img图像上画一个圆，圆心坐标为center，半径为radius，颜色为color，线宽为thickness</span><br></pre></td></tr></tbody></table></figure><h3 id="threshold方法"><a href="#threshold方法" class="headerlink" title="threshold方法"></a><font size="3">threshold方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line"></span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line">img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)</span><br><span class="line">cv.namedWindow('origin')</span><br><span class="line">cv.imshow('origin', img)</span><br><span class="line"></span><br><span class="line"># cv.threshold(img, thresh, maxval, type) 对灰度图像img进行阈值分割，thresh为指定按照阈值大小分割，最大的灰度值为maxval，分割方法为type，可以为cv.THRESH_OTSU等等，如果指定type则thresh失效</span><br><span class="line">thresh, img_thresh = cv.threshold(img, 0, 255, cv.THRESH_OTSU)</span><br><span class="line"></span><br><span class="line">cv.namedWindow('thresh')</span><br><span class="line">cv.imshow('thresh', img_thresh)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destoryAllWindows()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv15.png" alt="15"></p><h3 id="pyrDown，pyrUp方法"><a href="#pyrDown，pyrUp方法" class="headerlink" title="pyrDown，pyrUp方法"></a><font size="3">pyrDown，pyrUp方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line"></span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line">cv.namedWindow('origin')</span><br><span class="line">cv.imshow('origin', img)</span><br><span class="line"></span><br><span class="line"># cv.pyrDown(img) 对img进行下采样</span><br><span class="line">img_down = cv.pyrDown(img)</span><br><span class="line"></span><br><span class="line">cv.namedWindow('down')</span><br><span class="line">cv.imshow('down', img_down)</span><br><span class="line"></span><br><span class="line"># cv.pyrUp(img) 对img进行上采样</span><br><span class="line">img_up = cv.pyrUp(img)</span><br><span class="line"></span><br><span class="line">cv.namedWindow('up')</span><br><span class="line">cv.imshow('up', img_up)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destoryAllWindows()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv16.png" alt="16"></p><h2 id="OpenCV算子"><a href="#OpenCV算子" class="headerlink" title="OpenCV算子"></a><font size="4">OpenCV算子</font></h2><h3 id="Sobel，Laplacian，Canny方法"><a href="#Sobel，Laplacian，Canny方法" class="headerlink" title="Sobel，Laplacian，Canny方法"></a><font size="3">Sobel，Laplacian，Canny方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import cv2 as cv</span><br><span class="line"></span><br><span class="line">img = cv.imread('lena.jpg')</span><br><span class="line">img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line">cv.namedWindow('origin')</span><br><span class="line">cv.imshow('origin', img)</span><br><span class="line"></span><br><span class="line"># cv.Sobel(img, ddepth, dx, dy) 对img进行Sobel算子滤波，dx=1,dy=0代表水平方向，dx=0,dy=1代表垂直方向</span><br><span class="line">img_sobel = cv.Sobel(img, -1, dx=0, dy=1)</span><br><span class="line"></span><br><span class="line">cv.namedWindow('sobel')</span><br><span class="line">cv.imshow('sobel', img_sobel)</span><br><span class="line"></span><br><span class="line"># cv.Laplacian(img, ddepth) 将img进行Laplacian算子滤波</span><br><span class="line">img_laplacian = cv.Laplacian(img, -1)</span><br><span class="line"></span><br><span class="line">cv.namedWindow('laplacian')</span><br><span class="line">cv.imshow('laplacian', img_laplacian)</span><br><span class="line"></span><br><span class="line"># cv.Canny(img, threshold1, threshold2) 将img进行Canny算子滤波，低阈值为threshold1，高阈值为threshold2</span><br><span class="line">img_canny = cv.Canny(img, 50, 150)</span><br><span class="line"></span><br><span class="line">cv.namedWindow('canny')</span><br><span class="line">cv.imshow('canny', img_canny)</span><br><span class="line">cv.waitKey(0)</span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/cv17.png" alt="17"></p><h2 id="OpenCV霍夫变换"><a href="#OpenCV霍夫变换" class="headerlink" title="OpenCV霍夫变换"></a><font size="4">OpenCV霍夫变换</font></h2><h3 id="HoughLines，HoughCircles方法"><a href="#HoughLines，HoughCircles方法" class="headerlink" title="HoughLines，HoughCircles方法"></a><font size="3">HoughLines，HoughCircles方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cv.HoughLines(img, rho, theta, threshold) 霍夫直线检测，边缘提取后的图像为img，步长为rho，角度步长为theta，阈值为threshold，返回所有直线信息</span><br><span class="line"></span><br><span class="line"># cv.HoughCircles(img, method, dp, minDist, param1, param2, minRadius, maxRadius) 霍夫圆检测，检测方法method，可以为cv.HOUGH_GRADIENT等等，dp为累加器分辨率与图像分辨率的反比，dp越大，累加器数组越小，minDist为检测到原中心的最小距离，如果太靠近则检测不出，param1用于边缘检测的阈值，param2为累加器阈值，越高则越精确，但是圆越少，minRadius为圆的最小半径，maxRadius为圆的最大半径</span><br></pre></td></tr></tbody></table></figure><h2 id="OpenCV轮廓处理"><a href="#OpenCV轮廓处理" class="headerlink" title="OpenCV轮廓处理"></a><font size="4">OpenCV轮廓处理</font></h2><h3 id="findContour，drawContour，contourArea方法"><a href="#findContour，drawContour，contourArea方法" class="headerlink" title="findContour，drawContour，contourArea方法"></a><font size="3">findContour，drawContour，contourArea方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># cv.findContour(img, mode, method) 寻找轮廓，mode表示轮廓的检索模式，可以为cv2.RETR_EXTERNAL，cv2.RETR_TREE等等，method表示轮廓的近似办法，可以为cv2.CHAIN_APPROX_NONE，cv2.CHAIN_APPROX_SIMPLE等等</span><br><span class="line"></span><br><span class="line"># cv.drawContour(img, contours, contourIdx, color, thickness) 根据寻找到的轮廓，画出第contourIdx个轮廓，颜色为color，线宽为thickness</span><br><span class="line"></span><br><span class="line"># cv.contourArea(contour) 计算轮廓的面积</span><br></pre></td></tr></tbody></table></figure><h1 id="OpenCV小结"><a href="#OpenCV小结" class="headerlink" title="OpenCV小结"></a><font size="5" color="red">OpenCV小结</font></h1><p>  通过OpenCV，使用者可以仅需要几行代码，便可以完成一系列图像处理任务，在这里介绍的只是小部分常见的功能，因此在图像处理的研究应用中，OpenCV是必不可少的帮手。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;OpenCV&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="常用库" scheme="https://USTCcoder.github.io/categories/python/%E5%B8%B8%E7%94%A8%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>Scipy</title>
    <link href="https://USTCcoder.github.io/2019/08/16/library%20scipy/"/>
    <id>https://USTCcoder.github.io/2019/08/16/library scipy/</id>
    <published>2019-08-16T09:44:03.000Z</published>
    <updated>2020-07-27T06:13:30.368Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LIBRARY/scipy.jpg" alt="0"></p><h1 id="Scipy介绍"><a href="#Scipy介绍" class="headerlink" title="Scipy介绍"></a><font size="5" color="red">Scipy介绍</font></h1><p>  Scipy是一个用于数学、科学、工程领域的常用软件包，可以处理插值、积分、优化、常微分方程数值解的求解、图像处理、信号处理等问题。它用于有效计算Numpy矩阵，使Numpy和Scipy协同工作，高效解决问题。<br><a id="more"></a></p><h1 id="Scipy特点"><a href="#Scipy特点" class="headerlink" title="Scipy特点"></a><font size="5" color="red">Scipy特点</font></h1><p>  <font size="3">Scipy支持大多数工程数学运算。</font><br>  <font size="3">Scipy每一个子模块都可以完成一类功能。</font><br>  <font size="3">Scipy中的函数类似于MATLAB中的函数，使用方便。</font></p><h1 id="Scipy应用"><a href="#Scipy应用" class="headerlink" title="Scipy应用"></a><font size="5" color="red">Scipy应用</font></h1><h2 id="Scipy常数模块"><a href="#Scipy常数模块" class="headerlink" title="Scipy常数模块"></a><font size="4">Scipy常数模块</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from scipy import constants as C</span><br><span class="line"></span><br><span class="line"># C.c 光速常数</span><br><span class="line">C.c</span><br><span class="line"></span><br><span class="line"># C.h 普朗克常数</span><br><span class="line">C.h</span><br><span class="line"></span><br><span class="line"># C.mile 英里</span><br><span class="line">C.mile</span><br><span class="line"></span><br><span class="line"># C.pi 圆周率π</span><br><span class="line">C.pi</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy1.png" alt="1"></p><h2 id="Scipy特殊函数模块"><a href="#Scipy特殊函数模块" class="headerlink" title="Scipy特殊函数模块"></a><font size="4">Scipy特殊函数模块</font></h2><h3 id="gamma，gammaln方法"><a href="#gamma，gammaln方法" class="headerlink" title="gamma，gammaln方法"></a><font size="3">gamma，gammaln方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from scipy import special as S</span><br><span class="line"></span><br><span class="line"># S.gamma(n) 计算Γ(n)的值</span><br><span class="line">S.gamma(4)</span><br><span class="line"></span><br><span class="line"># S.gammaln(n) 计算ln|Γ(n)|的值，避免Γ(n)过大</span><br><span class="line">S.gammaln(4)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy2.png" alt="2"></p><h3 id="log1p方法"><a href="#log1p方法" class="headerlink" title="log1p方法"></a><font size="3">log1p方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from scipy import special as S</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># S.log1p(n) 计算ln(n+1)的值，使其可以计算很小的数</span><br><span class="line">S.log1p(np.e - 1)</span><br><span class="line">S.log1p(1e-10)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy3.png" alt="3"></p><h2 id="Scipy拟合优化模块"><a href="#Scipy拟合优化模块" class="headerlink" title="Scipy拟合优化模块"></a><font size="4">Scipy拟合优化模块</font></h2><h3 id="fsolve方法"><a href="#fsolve方法" class="headerlink" title="fsolve方法"></a><font size="3">fsolve方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from scipy import optimize as O</span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line">def f(x):</span><br><span class="line">    x0, x1, x2 = x.tolist()</span><br><span class="line">    return [5 * x1 + 3, 4 * x0 ** 2 - 2 * math.sin(x1 * x2), x1 * x2 - 1.5]</span><br><span class="line"></span><br><span class="line"># O.fsolve(f, init) 求非线性方程组的解，f为方程函数，init为初始迭代值</span><br><span class="line">result = O.fsolve(f, [1, 1, 1])</span><br></pre></td></tr></tbody></table></figure><script type="math/tex; mode=display">\left \{\begin{aligned} 5x_1 + 3 & = 0 \\ 4{x_0}^2 -2\sin{x_1 \cdot x_2} & = 0 \\ x_1 \cdot x_2 -1.5 &= 0 \end{aligned} \right.</script><p><img src="/images/LIBRARY/scipy4.png" alt="4"></p><h3 id="leastsq方法"><a href="#leastsq方法" class="headerlink" title="leastsq方法"></a><font size="3">leastsq方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from scipy import optimize as O</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = np.array([8.19,2.72,6.39,8.71,4.7,2.66,3.78])</span><br><span class="line">y = np.array([7.01,2.78,6.47,6.71,4.1,4.23,4.05])</span><br><span class="line"></span><br><span class="line">def residuals(p):</span><br><span class="line">    k, b = p</span><br><span class="line">    return y - (k * x + b)</span><br><span class="line"></span><br><span class="line">r = O.leastsq(residuals, [1,0])</span><br><span class="line"></span><br><span class="line">k, b = r[0]</span><br><span class="line">y_new = x * k + b</span><br><span class="line">print('k=', k, 'b=', b)</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(x, y_new)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy5.png" alt="5"></p><h2 id="Scipy线性代数模块"><a href="#Scipy线性代数模块" class="headerlink" title="Scipy线性代数模块"></a><font size="4">Scipy线性代数模块</font></h2><h3 id="solve方法"><a href="#solve方法" class="headerlink" title="solve方法"></a><font size="3">solve方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from scipy import linalg as L</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([[2, 2, -1], [1, -2, 4], [5, 8, -1]])</span><br><span class="line">b = np.array([[6], [3], [27]])</span><br><span class="line"></span><br><span class="line"># L.solve(A, b) 求线性方程组Ax = b的解</span><br><span class="line">x = L.solve(a, b)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy6.png" alt="6"></p><h3 id="eig，svd方法"><a href="#eig，svd方法" class="headerlink" title="eig，svd方法"></a><font size="3">eig，svd方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from scipy import linalg as L</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([[-2, 1, 1], [0, 2, 0], [-4, 1, 3]])</span><br><span class="line"></span><br><span class="line"># L.eig(array) 求array的特征值和特征向量</span><br><span class="line">m, x = L.eig(a)</span><br><span class="line"></span><br><span class="line"># L.svd(array) 求array的奇异值分解</span><br><span class="line">u, sigma, v = L.svd(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy7.png" alt="7"></p><h2 id="Scipy统计模块"><a href="#Scipy统计模块" class="headerlink" title="Scipy统计模块"></a><font size="4">Scipy统计模块</font></h2><h3 id="norm类，stats，rvs方法"><a href="#norm类，stats，rvs方法" class="headerlink" title="norm类，stats，rvs方法"></a><font size="3">norm类，stats，rvs方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from scipy import stats as ST</span><br><span class="line"></span><br><span class="line"># ST.norm(loc=0, scale=1) 获取偏移为loc(默认为0)，标准差为scale(默认为1)的正态分布(还可以定义其他的分布)</span><br><span class="line">norm_ = ST.norm(loc=1, scale=2)</span><br><span class="line"></span><br><span class="line"># obj.stats() 获取obj分布的期望和方差</span><br><span class="line">mean_norm, var_norm = norm_.stats()</span><br><span class="line"></span><br><span class="line"># obj.rvs(size=shape) 获取大小为shape的obj分布的随机抽样</span><br><span class="line">x = norm_.rvs(size=(100, 100))</span><br><span class="line">mean_x = np.mean(x)</span><br><span class="line">std_x = np.std(x)</span><br><span class="line"></span><br><span class="line"># obj.pdf(x) 获取x处的概率密度函数</span><br><span class="line">pdf_1 = norm_.pdf(1)</span><br><span class="line"></span><br><span class="line"># obj.cdf(x) 获取x处的分布函数</span><br><span class="line">cdf_1 = norm_.cdf(1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy8.png" alt="8"></p><h3 id="rv-discrete类，stats，rvs方法"><a href="#rv-discrete类，stats，rvs方法" class="headerlink" title="rv_discrete类，stats，rvs方法"></a><font size="3">rv_discrete类，stats，rvs方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from scipy import stats as ST</span><br><span class="line"></span><br><span class="line"># ST.rv_discrete(values=(x, p)) 自定义离散概率分布，x为可能的取值，p为对应的概率</span><br><span class="line">discrete_ = ST.rv_discrete(values=([1, 2, 3, 4, 5, 6], [0.75, 0.05, 0.05, 0.05, 0.05, 0.05]))</span><br><span class="line"></span><br><span class="line">discrete_.stats()</span><br><span class="line"></span><br><span class="line">x = discrete_.rvs(size=(100, 100))</span><br><span class="line">mean_x = np.mean(x)</span><br><span class="line">var_x = np.var(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line"># plt.annotate(formulation, xy, xytext, fontsize, arrowprops) 在xy附近添加注解formulation，xytext为注解的位置，fontsize为注解的大小，arrowprops为箭头类型</span><br><span class="line">plt.annotate(r'$x^2-0.5$', (0, -0.5), xytext=(+0.25, 0), arrowprops=dict(arrowstyle='-&gt;', connectionstyle='arc3, rad=0.2'))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy9.png" alt="9"></p><h3 id="binom-pmf方法"><a href="#binom-pmf方法" class="headerlink" title="binom.pmf方法"></a><font size="3">binom.pmf方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from scipy import stats as ST</span><br><span class="line"></span><br><span class="line"># ST.binom.pmf(list, n, p) 进行n次二项分布实验，出现的概率为p，计算出现list中对应值的概率</span><br><span class="line">x = ST.binom.pmf([0,1,2,3,4], 3, 0.8)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy10.png" alt="10"></p><h2 id="Scipy积分模块"><a href="#Scipy积分模块" class="headerlink" title="Scipy积分模块"></a><font size="4">Scipy积分模块</font></h2><h3 id="quad方法"><a href="#quad方法" class="headerlink" title="quad方法"></a><font size="3">quad方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from scipy import integrate as I</span><br><span class="line"></span><br><span class="line">I.quad(f, min_lim, max_lim) 计算函数f的积分(f可以是自定义函数也可以为lambda表达式)，并返回计算产生的误差，积分下限为min_lim，积分上限为max_lim</span><br><span class="line"></span><br><span class="line">pi_half, err = I.quad(lambda x:(1 - x ** 2) ** 0.5, -1, 1)</span><br><span class="line">print('pi:', pi_half * 2, 'err:', err)</span><br></pre></td></tr></tbody></table></figure><script type="math/tex; mode=display">\int_{-1}^{1} \sqrt{1-x^2}\, dx = \frac{\pi}{2}</script><p><img src="/images/LIBRARY/scipy11.png" alt="11"></p><h3 id="dblquad，tplquad方法"><a href="#dblquad，tplquad方法" class="headerlink" title="dblquad，tplquad方法"></a><font size="3">dblquad，tplquad方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from scipy import integrate as I</span><br><span class="line"></span><br><span class="line">f = lambda x, y : (1 - x ** 2 - y ** 2) ** 0.5</span><br><span class="line">f_y = lambda x : (1 - x ** 2) ** 0.5</span><br><span class="line"></span><br><span class="line"># I.dblquad(f, x_min_lim, x_max_lim, y_min_lim, y_max_lim) 计算函数f的二重积分，并返回计算产生的误差，x积分下限为x_min_lim，积分上限为x_max_lim，y积分下限为y_min_lim，积分上限为y_max_lim</span><br><span class="line">res, err = I.dblquad(lambda x, y : (1 - x ** 2 - y ** 2) ** 0.5, -1, 1, lambda x : -1 * f_y(x), f_y)</span><br><span class="line">print('res:', res * 1.5, 'err:', err)</span><br><span class="line"></span><br><span class="line"># I.tplquad(f, x_min_lim, x_max_lim, y_min_lim, y_max_lim, z_min_lim, z_max_lim) 计算函数f的三重积分，并返回计算产生的误差，x积分下限为x_min_lim，积分上限为x_max_lim，y积分下限为y_min_lim，积分上限为y_max_lim，z积分下限为z_min_lim，积分上限为z_max_lim</span><br></pre></td></tr></tbody></table></figure><script type="math/tex; mode=display">\int_{-1}^{1} \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} \sqrt{1-x^2-y^2}\, dydx = \frac{2}{3}\pi</script><p><img src="/images/LIBRARY/scipy12.png" alt="12"></p><h2 id="Scipy插值模块"><a href="#Scipy插值模块" class="headerlink" title="Scipy插值模块"></a><font size="4">Scipy插值模块</font></h2><h3 id="interp1d，interp2d方法"><a href="#interp1d，interp2d方法" class="headerlink" title="interp1d，interp2d方法"></a><font size="3">interp1d，interp2d方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from scipy import interpolate as IP</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = np.linspace(0, 2 * np.pi, 11)</span><br><span class="line">y = np.sin(x)</span><br><span class="line">x_new = np.linspace(0, 2 * np.pi, 51)</span><br><span class="line"></span><br><span class="line"># IP.interp1d(x, y, kind) 对x，y进行一维插值，kind为插值函数类型，可以为'nearest'最近邻插值，'cubic'立方插值等等</span><br><span class="line">f = IP.interp1d(x, y, kind='cubic')</span><br><span class="line">y_new = f(x_new)</span><br><span class="line"></span><br><span class="line">plt.subplot(121)</span><br><span class="line">plt.plot(x, y, label='origin')</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plt.plot(x_new, y_new, label='new')</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"># IP.interp2d(x, y, z, kind) 对x，y，z进行二维插值，kind同interp2d</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy13.png" alt="13"></p><h3 id="UnivariateSpline方法"><a href="#UnivariateSpline方法" class="headerlink" title="UnivariateSpline方法"></a><font size="4">UnivariateSpline方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from scipy import interpolate as IP</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = np.linspace(0, 2 * np.pi, 11)</span><br><span class="line">y = np.sin(x)</span><br><span class="line">x_new = np.linspace(-0.5 * np.pi, 2.5 * np.pi, 51)</span><br><span class="line"></span><br><span class="line"># IP.interp1d(x, y, w, k=3, s=None) 对x，y进行一维插值，w为每个数据的权值，k为插值的阶数(默认为3)，s为曲线的平滑系数(默认为None)，和interp1d不同的是，该方法支持外推操作，即可以插值边缘点之外的部分。</span><br><span class="line">y_new=ip.UnivariateSpline(x, y)(x_new)</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plt.plot(x,y,label='origin')</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plt.plot(x_new,y_new,label='new')</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy14.png" alt="14"></p><h2 id="Scipy信号处理模块"><a href="#Scipy信号处理模块" class="headerlink" title="Scipy信号处理模块"></a><font size="4">Scipy信号处理模块</font></h2><h3 id="medfilt方法"><a href="#medfilt方法" class="headerlink" title="medfilt方法"></a><font size="3">medfilt方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from scipy import signal as SP</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np.random.seed(1)</span><br><span class="line"></span><br><span class="line">x =np.random.randint(0, 9, (5, 5))</span><br><span class="line"></span><br><span class="line"># SP.medfilt(array, kernel_size) 对array进行中值滤波，掩模的大小为kernel_size</span><br><span class="line">y = SP.medfilt(x, 3)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy15.png" alt="15"></p><h3 id="order-filter方法"><a href="#order-filter方法" class="headerlink" title="order_filter方法"></a><font size="3">order_filter方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from scipy import signal as SP</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np.random.seed(1)</span><br><span class="line"></span><br><span class="line">x =np.random.randint(0, 9, (5, 5))</span><br><span class="line"></span><br><span class="line"># SP.order_filter(array, domain, rank) 对array进行模板为domain的排序滤波，rank为第几小的值，rank=0代表最小值滤波，rank=domain.size-1代表最大值滤波</span><br><span class="line">y_min = SP.order_filter(x, np.ones((5, 5)), 0)</span><br><span class="line">y_mid = SP.order_filter(x, np.ones((5, 5)), 12)</span><br><span class="line">y_max = SP.order_filter(x, np.ones((5, 5)), 24)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy16.png" alt="16"></p><h3 id="iirdesign，lfilter方法"><a href="#iirdesign，lfilter方法" class="headerlink" title="iirdesign，lfilter方法"></a><font size="3">iirdesign，lfilter方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">from scipy import signal as SP</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">fs = 100</span><br><span class="line">t = np.arange(0, 2, 1 / fs)</span><br><span class="line">y = np.sin(2 * np.pi * 2 * t) + np.sin(2 * np.pi * 4 * t) + np.sin(2 * np.pi * 6 * t)</span><br><span class="line"></span><br><span class="line"># SP.iirdesign([pass_low, pass_high], [stop_loss, stop_high], gp, gs) 设计IIR滤波器，通带频率为[pass_low × f0, pass_high × f0]，阻带频率为[0, stop_loss × f0] ∪ [stop_high × f0, ∞]，其中f0为采样频率的一半，gp为通带的最大增益衰减，gs为阻带的最小增益衰减，返回值为滤波器分子和分母的系数</span><br><span class="line">b_1, a_1 = SP.iirdesign([0.05, 0.2], [0.01, 0.5], 2, 40)</span><br><span class="line">b_2, a_2 = SP.iirdesign([0.1, 0.2], [0.01, 0.5], 2, 40)</span><br><span class="line"></span><br><span class="line"># SP.lfilter(b, a, x) 计算x经过b，a滤波器的结果</span><br><span class="line">out_1 = SP.lfilter(b_1, a_1, y)</span><br><span class="line">out_2 = SP.lfilter(b_2, a_2, y)</span><br><span class="line"></span><br><span class="line">plt.subplot(321)</span><br><span class="line">plt.plot(t, y, label='origin')</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(322)</span><br><span class="line">plt.plot(np.abs(np.fft.fft(y)), label='origin fft')</span><br><span class="line">plt.xlim((0, fs / 2))</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(323)</span><br><span class="line">plt.plot(t, out_1, label='out_1')</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(324)</span><br><span class="line">plt.plot(np.abs(np.fft.fft(out_1)), label='out_1 fft')</span><br><span class="line">plt.xlim((0, fs / 2))</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(325)</span><br><span class="line">plt.plot(t, out_2, label='out_2')</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(326)</span><br><span class="line">plt.plot(np.abs(np.fft.fft(out_2)), label='out_2 fft')</span><br><span class="line">plt.xlim((0, fs / 2))</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/scipy17.png" alt="17"></p><h2 id="Scipy图像处理模块"><a href="#Scipy图像处理模块" class="headerlink" title="Scipy图像处理模块"></a><font size="4">Scipy图像处理模块</font></h2><p>  <font size="4">Scipy.ndimage 是一个处理多维图像的函数库，其中又包括以下模块。</font><br>  <font size="3">filters 图像滤波器函数库</font><br>  <font size="3">fourier 傅里叶变换函数库</font><br>  <font size="3">interpolation 图像变换函数库</font><br>  <font size="3">morphology 形态学操作函数库</font></p><p>  <font size="4">图像处理有许多更强大的库，如opencv，scikit-image库，在此不做过多介绍，可以参考opencv。</font></p><h1 id="Scipy小结"><a href="#Scipy小结" class="headerlink" title="Scipy小结"></a><font size="5" color="red">Scipy小结</font></h1><p>  通过Scipy，使用者可以仅需要几行代码，便可以完成一系列工程应用。在数据分析，实际项目中，常常需要对数据进行插值、拟合、优化，需要借助Scipy科学计算库的帮助，因此Scipy是工程研究中必不可少的帮手。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Scipy&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="常用库" scheme="https://USTCcoder.github.io/categories/python/%E5%B8%B8%E7%94%A8%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>Matplotlib</title>
    <link href="https://USTCcoder.github.io/2019/08/15/library%20matplotlib/"/>
    <id>https://USTCcoder.github.io/2019/08/15/library matplotlib/</id>
    <published>2019-08-15T13:59:32.000Z</published>
    <updated>2020-07-27T06:13:08.321Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LIBRARY/matplotlib.jpg" alt="0"></p><h1 id="Matplotlib介绍"><a href="#Matplotlib介绍" class="headerlink" title="Matplotlib介绍"></a><font size="5" color="red">Matplotlib介绍</font></h1><p>  Matplotlib是python绘图领域使用最广泛的库。它能让使用者很轻松地将数据图形化，并且提供多样化的输出格式。<br><a id="more"></a></p><h1 id="Matplotlib特点"><a href="#Matplotlib特点" class="headerlink" title="Matplotlib特点"></a><font size="5" color="red">Matplotlib特点</font></h1><p>  <font size="3">Matplotlib支持LaTeX 的公式插入。</font><br>  <font size="3">Matplotlib支持交互式和非交互式绘图。</font><br>  <font size="3">Matplotlib可将图像保存成PNG等多种形式。</font><br>  <font size="3">Matplotlib支持曲线(折线)图、条形图、柱状图、饼图。</font><br>  <font size="3">Matplotlib类似于MATLAB的绘图函数，上手较为简单。</font></p><h1 id="Matplotlib应用"><a href="#Matplotlib应用" class="headerlink" title="Matplotlib应用"></a><font size="5" color="red">Matplotlib应用</font></h1><h2 id="Matplotlib绘制二维直线"><a href="#Matplotlib绘制二维直线" class="headerlink" title="Matplotlib绘制二维直线"></a><font size="4">Matplotlib绘制二维直线</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-1, 1, 21)</span><br><span class="line">y = np.power(x, 2) - 0.5</span><br><span class="line"></span><br><span class="line"># plt.plot(x, y, color, linewidth, linestyle, label, alpha, marker=None) 画出x-y二维图形，color指颜色，linewidth指线宽，linestyle指线的形式，label指图形的标签，alpha指透明度，marker指该点的形状</span><br><span class="line">plt.plot(x, y, color='red', linewidth=0.5, alpha=0.8)</span><br><span class="line"></span><br><span class="line"># plt.show() 显示所画的图形</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib1.png" alt="1"></p><h2 id="matplotlib修饰图形"><a href="#matplotlib修饰图形" class="headerlink" title="matplotlib修饰图形"></a><font size="4">matplotlib修饰图形</font></h2><h3 id="style-use方法"><a href="#style-use方法" class="headerlink" title="style.use方法"></a><font size="3">style.use方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># plt.style.use(option) 设置窗口风格，option可选'dark_background'，'bmh'，'grayscale'，'ggplot'，'fivethirtyeight'</span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line"></span><br><span class="line">x = np.linspace(-1, 1, 21)</span><br><span class="line">y = np.power(x, 2) - 0.5</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib24.png" alt="24"></p><h3 id="grid方法"><a href="#grid方法" class="headerlink" title="grid方法"></a><font size="3">grid方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-1, 1, 21)</span><br><span class="line">y = np.power(x, 2) - 0.5</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line"># plt.grid(b=True, color, linewidth, linestyle) 设置坐标系网格，b=True表示显示网格(默认为True)，False表示关闭网格，color指网格的颜色，linewidth指网格的宽度，linestyle指网格的类型</span><br><span class="line">plt.grid()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib20.png" alt="20"></p><h3 id="grid方法-1"><a href="#grid方法-1" class="headerlink" title="grid方法"></a><font size="3">grid方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-1, 1, 21)</span><br><span class="line">y = np.power(x, 2) - 0.5</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line"># plt.grid(b=True, color, linewidth, linestyle) 设置坐标系网格，b=True表示显示网格(默认为True)，False表示关闭网格，color指网格的颜色，linewidth指网格的宽度，linestyle指网格的类型</span><br><span class="line">plt.grid()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib20.png" alt="20"></p><h3 id="axis方法"><a href="#axis方法" class="headerlink" title="axis方法"></a><font size="3">axis方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-3, 3, 31)</span><br><span class="line">y = x ** 2</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line"># plt.axis(b) 设置坐标系网格，b='on'表示显示坐标轴(默认为'on')，'off'表示关闭坐标轴，'equal'指坐标轴比例相同，'auto'自动调整坐标轴比例</span><br><span class="line">plt.axis('equal')</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib21.png" alt="21"></p><h3 id="subplot方法"><a href="#subplot方法" class="headerlink" title="subplot方法"></a><font size="3">subplot方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-3, 3, 31)</span><br><span class="line">y_1 = x</span><br><span class="line">y_2 = x ** 2</span><br><span class="line">y_3 = np.exp(x)</span><br><span class="line">y_4 = np.log(x)</span><br><span class="line"></span><br><span class="line"># plt.subplot(r, c, n) 将一个窗口分成r行c列，当前子窗口处于第n个</span><br><span class="line">plt.subplot(2,2,1)</span><br><span class="line">plt.plot(x, y_1, label='$x$')</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(2,2,2)</span><br><span class="line">plt.plot(x, y_2, label='$x^2$')</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(2,2,3)</span><br><span class="line">plt.plot(x, y_3, label='$e^x$')</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(2,2,4)</span><br><span class="line">plt.plot(x, y_4, label='$log(x)$')</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib22.png" alt="22"></p><h3 id="subplot2grid方法"><a href="#subplot2grid方法" class="headerlink" title="subplot2grid方法"></a><font size="3">subplot2grid方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-3, 3, 31)</span><br><span class="line">y_1 = x</span><br><span class="line">y_2 = x ** 2</span><br><span class="line">y_3 = np.exp(x)</span><br><span class="line">y_4 = np.log(x)</span><br><span class="line"></span><br><span class="line"># plt.subplot2grid((r, c), (begin, end), colspan=1, rowspan=1) 将一个窗口分成r行c列，当前窗口的位置在(begin, end)，横跨m个单位(默认为1个单位)，纵跨n个单位(默认为1个单位)</span><br><span class="line">plt.subplot2grid((3, 2),(0, 0), colspan=2, rowspan=1)</span><br><span class="line">plt.plot(x, y_1, label='$x$')</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot2grid((3, 2), (1, 0))</span><br><span class="line">plt.plot(x, y_2, label='$x^2$')</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot2grid((3, 2), (1, 1))</span><br><span class="line">plt.plot(x, y_3, label='$e^x$')</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot2grid((3, 2), (2, 0), colspan=2, rowspan=1)</span><br><span class="line">plt.plot(x, y_4, label='$log(x)$')</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib23.png" alt="23"></p><h3 id="xlim，ylim方法"><a href="#xlim，ylim方法" class="headerlink" title="xlim，ylim方法"></a><font size="3">xlim，ylim方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-1, 1, 21)</span><br><span class="line">y = np.power(x, 2) - 0.5</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line"># plt.xlim((x_min, x_max))，plt.ylim((y_min, y_max)) 限制x，y坐标轴的范围</span><br><span class="line">plt.xlim((-1.5, 1.5))</span><br><span class="line">plt.ylim((-1, 1))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib3.png" alt="3"></p><h3 id="xlabel，ylabel方法"><a href="#xlabel，ylabel方法" class="headerlink" title="xlabel，ylabel方法"></a><font size="3">xlabel，ylabel方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-1, 1, 21)</span><br><span class="line">y = np.power(x, 2) - 0.5</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line"># plt.xlabel(x_describe)，plt.ylabel(y_describe) 设置x，y的坐标轴标签</span><br><span class="line">plt.xlabel('x')</span><br><span class="line">plt.ylabel('x^2-0.5')</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib4.png" alt="4"></p><h3 id="xticks，yticks方法"><a href="#xticks，yticks方法" class="headerlink" title="xticks，yticks方法"></a><font size="3">xticks，yticks方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np.random.seed(2)</span><br><span class="line"></span><br><span class="line">x = np.arange(1,13)</span><br><span class="line">y = np.random.randint(50, 100, 12)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line"># plt.xticks(list_x)，plt.yticks(list_y) 设置x，y的角标，支持latex格式。</span><br><span class="line">plt.xticks(x)</span><br><span class="line">plt.yticks([60, 80, 90, 100], [r'$bad$', r'$good$', r'$nice$', r'$excellent$'])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib5.png" alt="5"></p><h3 id="gca方法"><a href="#gca方法" class="headerlink" title="gca方法"></a><font size="3">gca方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-1, 1, 21)</span><br><span class="line">y = np.power(x, 2) - 0.5</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line"># plt.gca 获取坐标脊</span><br><span class="line">ax = plt.gca()</span><br><span class="line"></span><br><span class="line"># 将上方和右侧的坐标轴删除</span><br><span class="line">ax.spines['right'].set_color('none')</span><br><span class="line">ax.spines['top'].set_color('none')</span><br><span class="line"></span><br><span class="line"># 设置x，y轴为下方和左侧的脊</span><br><span class="line">ax.xaxis.set_ticks_position('bottom')</span><br><span class="line">ax.yaxis.set_ticks_position('left')</span><br><span class="line"></span><br><span class="line"># 设置坐标轴原点为(-0.1, -0.2)</span><br><span class="line">ax.spines['bottom'].set_position(('data', -0.2))</span><br><span class="line">ax.spines['left'].set_position(('data', -0.1))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib6.png" alt="6"></p><h3 id="legend方法"><a href="#legend方法" class="headerlink" title="legend方法"></a><font size="3">legend方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-1, 1, 21)</span><br><span class="line">y = np.power(x, 2) + 1</span><br><span class="line">z = np.exp(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.plot(x, z)</span><br><span class="line"></span><br><span class="line"># plt.legend(handles, labels, loc='best') 设置图例，可以在handles和labels中写入多个，handle省略则按照图形的产生顺序设置图例，loc='best'指默认放在空白最好的位置</span><br><span class="line">plt.legend(labels=[r'$x^2+1$', r'$e^x$'])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib7.png" alt="7"></p><h3 id="annotate方法"><a href="#annotate方法" class="headerlink" title="annotate方法"></a><font size="3">annotate方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-1, 1, 21)</span><br><span class="line">y = np.power(x, 2) - 0.5</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line"># plt.annotate(formulation, xy, xytext, fontsize, arrowprops) 在xy附近添加注解formulation，xytext为注解的位置，fontsize为注解的大小，arrowprops为箭头类型</span><br><span class="line">plt.annotate(r'$x^2-0.5$', (0, -0.5), xytext=(+0.25, 0), arrowprops=dict(arrowstyle='-&gt;', connectionstyle='arc3, rad=0.2'))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib8.png" alt="8"></p><h3 id="text方法"><a href="#text方法" class="headerlink" title="text方法"></a><font size="3">text方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-1, 1, 21)</span><br><span class="line">y = np.power(x, 2) - 0.5</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line"></span><br><span class="line"># plt.text(x, y, text, fontdict) 在(x，y)处添加文本文字text，文字的大小颜色在fontdict定义</span><br><span class="line">plt.text(0, -0.2, r'$x^2-0.5$', fontdict={'size':20, 'color':'red'})</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib9.png" alt="9"></p><h2 id="matplotlib绘制散点图"><a href="#matplotlib绘制散点图" class="headerlink" title="matplotlib绘制散点图"></a><font size="4">matplotlib绘制散点图</font></h2><h3 id="scatter方法"><a href="#scatter方法" class="headerlink" title="scatter方法"></a><font size="3">scatter方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(-1, 1, 21)</span><br><span class="line">y = np.power(x, 2) - 0.5</span><br><span class="line"></span><br><span class="line"># plt.scatter(x, y, color, s, label) 绘制x-y散点图，颜色为color，大小为s，标签为label</span><br><span class="line">plt.scatter(x, y, color='blue', s=3.0)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib10.png" alt="10"></p><h2 id="matplotlib绘制条形图"><a href="#matplotlib绘制条形图" class="headerlink" title="matplotlib绘制条形图"></a><font size="4">matplotlib绘制条形图</font></h2><h3 id="bar方法"><a href="#bar方法" class="headerlink" title="bar方法"></a><font size="3">bar方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np.random.seed(1)</span><br><span class="line"></span><br><span class="line">x = np.arange(10)</span><br><span class="line">y = np.random.rand(10)</span><br><span class="line"></span><br><span class="line"># plt.bar(x, y, facecolor, edgecolor) 绘制x-y条形图，facecolor为内部颜色，edgecolor为边缘颜色</span><br><span class="line">plt.bar(x, y)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib11.png" alt="11"></p><h2 id="matplotlib绘制等高线图"><a href="#matplotlib绘制等高线图" class="headerlink" title="matplotlib绘制等高线图"></a><font size="4">matplotlib绘制等高线图</font></h2><h3 id="contourf，contour，clabel方法"><a href="#contourf，contour，clabel方法" class="headerlink" title="contourf，contour，clabel方法"></a><font size="3">contourf，contour，clabel方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x=np.linspace(-1,1,100)</span><br><span class="line">y=np.linspace(-1,1,100)</span><br><span class="line">x, y=np.meshgrid(x, y)</span><br><span class="line">z = np.power(x, 2) + np.power(y, 2)</span><br><span class="line"></span><br><span class="line"># plt.contourf(x, y, z, n, alpha, cmap) 绘制(x, y, z)二维等高线图，n指等高线的条数，alpha为透明度，cmap=plt.hot()绘制热图，cmap=plt.cool()绘制冷图</span><br><span class="line">plt.contourf(x, y, z, 8, alpha=0.5, cmap=plt.hot())</span><br><span class="line"></span><br><span class="line"># plt.contour(x, y, z, n, colors) 绘制(x, y, z)二维等高线</span><br><span class="line">contour=plt.contour(x, y, z, 8, colors='black')</span><br><span class="line"></span><br><span class="line"># plt.clabel(contour, inline=True, fontsize) #在等高线contour中添加数字</span><br><span class="line">plt.clabel(contour)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib12.png" alt="12"></p><h2 id="matplotlib绘制直方图"><a href="#matplotlib绘制直方图" class="headerlink" title="matplotlib绘制直方图"></a><font size="4">matplotlib绘制直方图</font></h2><h3 id="hist方法"><a href="#hist方法" class="headerlink" title="hist方法"></a><font size="3">hist方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np.random.seed(1)</span><br><span class="line"></span><br><span class="line">x = np.random.randn(10000)</span><br><span class="line"></span><br><span class="line"># plt.hist(x, bins, color, normed=False) 将x绘制直方图，bins为直方图分组的个数，color为直方图的颜色，normed为是否标准化</span><br><span class="line">plt.hist(x, 100)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib13.png" alt="13"></p><h2 id="matplotlib绘制饼状图"><a href="#matplotlib绘制饼状图" class="headerlink" title="matplotlib绘制饼状图"></a><font size="4">matplotlib绘制饼状图</font></h2><h3 id="pie方法"><a href="#pie方法" class="headerlink" title="pie方法"></a><font size="4">pie方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = [0.2236, 0.2664, 0.3218, 0.1882]</span><br><span class="line">label = ['first quarter', 'second quarter', 'third quarter', 'fourth quarter']</span><br><span class="line"></span><br><span class="line"># plt.pie(x, labels, autopct, explode, shadow=False) 将x绘制饼状图，labels为每个部分的标签，autopct为百分数的格式，explode指是否突出显示，shadow为是否添加阴影</span><br><span class="line">plt.pie(x, labels=label, autopct='%.2f%%', explode=[0, 0, 0.1, 0], shadow=True)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib14.png" alt="14"></p><h2 id="matplotlib填充图形"><a href="#matplotlib填充图形" class="headerlink" title="matplotlib填充图形"></a><font size="4">matplotlib填充图形</font></h2><h3 id="fill，fill-between方法"><a href="#fill，fill-between方法" class="headerlink" title="fill，fill_between方法"></a><font size="3">fill，fill_between方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x=np.linspace(0,4*np.pi,100)</span><br><span class="line">y_1 =np.sin(x)</span><br><span class="line">y_2 =np.sin(2 * x)</span><br><span class="line"></span><br><span class="line"># plt.fill(x, y, color, alpha, interpolate=False) 对x，y图形与x轴进行填充，颜色为color，透明度为alpha，interpolate为是否精确填充</span><br><span class="line">plt.figure(1)</span><br><span class="line">plt.fill(x, y_1, color='r', alpha=0.5)</span><br><span class="line"></span><br><span class="line"># plt.fill_between(x, y_1, y_2, where, facecolor, interpolate=False) 对x，y_1图形与x，y_2图形进行填充，填充方式为where，颜色为color，透明度为alpha，interpolate为是否精确填充</span><br><span class="line">plt.figure(2)</span><br><span class="line">plt.plot(x, y_1)</span><br><span class="line">plt.plot(x, y_2)</span><br><span class="line">plt.fill_between(x, y_1, y_2, where=y_1 &gt; y_2, facecolor='red', interpolate=True)</span><br><span class="line">plt.fill_between(x, y_1, y_2, where=y_1 &lt; y_2, facecolor='blue', interpolate=True)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib15.png" alt="15"></p><h2 id="matplotlib绘制几何图形"><a href="#matplotlib绘制几何图形" class="headerlink" title="matplotlib绘制几何图形"></a><font size="4">matplotlib绘制几何图形</font></h2><h3 id="Cicle，Rectangle，Polygon，Ellipse方法"><a href="#Cicle，Rectangle，Polygon，Ellipse方法" class="headerlink" title="Cicle，Rectangle，Polygon，Ellipse方法"></a><font size="3">Cicle，Rectangle，Polygon，Ellipse方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.patches as mpatches</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">fig, ax=plt.subplots()</span><br><span class="line"></span><br><span class="line">clc=[.2,.2]</span><br><span class="line">rec=[.2,.8]</span><br><span class="line">pol=np.array([0.7,0.1,0.8,0.3,0.9,0.3,0.6,0.5]).reshape(4,2)</span><br><span class="line">eli=[.8,.8]</span><br><span class="line"></span><br><span class="line"># mpatches.Circle(c, r, color) 产生c为圆心，r为半径，颜色为color的圆形</span><br><span class="line">cicle=mpatches.Circle(clc,0.1,color='blue')</span><br><span class="line">ax.add_patch(cicle)</span><br><span class="line"></span><br><span class="line"># mpatches.Rectangle(c, length, width, color) 产生左下角坐标为c，长为length，宽为width，颜色为color的矩形</span><br><span class="line">rectangle=mpatches.Rectangle(rec,0.2,0.1,color='red')</span><br><span class="line">ax.add_patch(rectangle)</span><br><span class="line"></span><br><span class="line"># mpatches.Polygon(pol, color) 产生各个定点为pol的多边形</span><br><span class="line">polygon=mpatches.Polygon(pol,color='green')</span><br><span class="line">ax.add_patch(polygon)</span><br><span class="line"></span><br><span class="line"># mpatches.Ellipse(c, a, b,color) 产生圆心为c，长轴为a，短轴为b的椭圆</span><br><span class="line">ellipse=mpatches.Ellipse(eli,0.4,0.2,color='yellow')</span><br><span class="line">ax.add_patch(ellipse)</span><br><span class="line"></span><br><span class="line">plt.axis('equal')</span><br><span class="line">plt.grid(True)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib16.png" alt="16"></p><h2 id="matplotlib绘制二维数据"><a href="#matplotlib绘制二维数据" class="headerlink" title="matplotlib绘制二维数据"></a><font size="4">matplotlib绘制二维数据</font></h2><h3 id="imshow，colorbar方法"><a href="#imshow，colorbar方法" class="headerlink" title="imshow，colorbar方法"></a><font size="3">imshow，colorbar方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np.random.seed(1)</span><br><span class="line"></span><br><span class="line">x = np.random.uniform(0, 255, (5,5))</span><br><span class="line">x</span><br><span class="line"># plt.imshow(array, interpolation='nearest', cmap='hot', origin='upper') 显示二维数据，interpolation为插值方式，默认为最近邻，cmap为颜色显示方式(默认为hot)，可以为'hot'，'cool'，'rainbow'，'bone'，origin为图形绘制的方向，upper从上到下，lower自下而上。</span><br><span class="line">plt.imshow(x, cmap='bone')</span><br><span class="line"></span><br><span class="line"># plt.colorbar() 添加颜色棒</span><br><span class="line">plt.colorbar()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib17.png" alt="17"></p><h2 id="matplotlib绘制3D图形"><a href="#matplotlib绘制3D图形" class="headerlink" title="matplotlib绘制3D图形"></a><font size="4">matplotlib绘制3D图形</font></h2><h3 id="Axes3D，plot-surface方法"><a href="#Axes3D，plot-surface方法" class="headerlink" title="Axes3D，plot_surface方法"></a><font size="3">Axes3D，plot_surface方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import mpl_toolkits.mplot3d as mp</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">x = np.arange(-4, 4, 0.25)</span><br><span class="line">y = np.arange(-4, 4, 0.25)</span><br><span class="line">x, y = np.meshgrid(x, y)</span><br><span class="line">z = np.sqrt(x**2 + y**2)</span><br><span class="line">z = np.sin(z)</span><br><span class="line"></span><br><span class="line"># mp.Axes3D(fig) 生成3D窗口</span><br><span class="line">ax = mp.Axes3D(fig)</span><br><span class="line"></span><br><span class="line"># obj.plot_surface(x, y, z, rstride=1, cstride=1, cmap=None) 画出3D表面图形，rstride为行跨度，cstride为列跨度，cmap为颜色显示方式同plt.imshow</span><br><span class="line">ax.plot_surface(x, y, z, cmap='rainbow')</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib18.png" alt="18"></p><h2 id="matplotlib极坐标系"><a href="#matplotlib极坐标系" class="headerlink" title="matplotlib极坐标系"></a><font size="4">matplotlib极坐标系</font></h2><h3 id="subplot方法-1"><a href="#subplot方法-1" class="headerlink" title="subplot方法"></a><font size="3">subplot方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">theta = np.linspace(-np.pi, np.pi, 100)</span><br><span class="line">r = 0.5 * (1 + np.cos(theta))</span><br><span class="line"></span><br><span class="line"># plt.subplot(111, projection='polar') 绘制极坐标</span><br><span class="line">fig = plt.subplot(111, projection='polar')</span><br><span class="line"></span><br><span class="line">fig.plot(theta, r)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/matplotlib19.png" alt="19"></p><h1 id="Matplotlib小结"><a href="#Matplotlib小结" class="headerlink" title="Matplotlib小结"></a><font size="5" color="red">Matplotlib小结</font></h1><p>  通过Matplotlib，开发者可以仅需要几行代码，便可以生成绘图。在数据分析，机器学习，深度学习中，要对当前的数据进行实时的显示或者对准确率有直观的展示，需要借助Matplotlib的帮助，因此Matplotlib也作为机器学习三剑客之一。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Matplotlib&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="常用库" scheme="https://USTCcoder.github.io/categories/python/%E5%B8%B8%E7%94%A8%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>Pandas</title>
    <link href="https://USTCcoder.github.io/2019/08/13/library%20Pandas/"/>
    <id>https://USTCcoder.github.io/2019/08/13/library Pandas/</id>
    <published>2019-08-13T11:28:32.000Z</published>
    <updated>2020-07-27T06:13:24.057Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LIBRARY/pandas.jpg" alt="0"></p><h1 id="Pandas介绍"><a href="#Pandas介绍" class="headerlink" title="Pandas介绍"></a><font size="5" color="red">Pandas介绍</font></h1><p>  pandas是基于Numpy的一种工具，该工具纳入了大量库和一些标准的数据模型，提供了大量能使我们快速便捷地处理数据的函数和方法。<br><a id="more"></a></p><h1 id="Pandas特点"><a href="#Pandas特点" class="headerlink" title="Pandas特点"></a><font size="5" color="red">Pandas特点</font></h1><p>  <font size="3">Pandas解决了Numpy不利于处理数据结构的问题</font><br>  <font size="3">Pandas能够合并处理常见数据库中的关系型运算</font><br>  <font size="3">Pandas更贴近于日常的生活使用，即表格化的数据形式</font><br>  <font size="3">Pandas具备数据对齐功能，且集成时间序列，既能处理时间序列数据，也能处理非时间序列数据</font></p><h1 id="Pandas应用"><a href="#Pandas应用" class="headerlink" title="Pandas应用"></a><font size="5" color="red">Pandas应用</font></h1><h2 id="Pandas创建表格"><a href="#Pandas创建表格" class="headerlink" title="Pandas创建表格"></a><font size="4">Pandas创建表格</font></h2><h3 id="series方法"><a href="#series方法" class="headerlink" title="series方法"></a><font size="3">series方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line"># pd.Series(list, index) 将list转换为一维表格型数据结构，index为标签名称，默认从0开始0,1, ...</span><br><span class="line">a = pd.Series([1, 'hello', 3.1415, True])</span><br><span class="line">b = pd.Series([1, 'hello', 3.1415, True], index=['int', 'string', 'float', 'bool'])</span><br><span class="line"></span><br><span class="line"># pd.Series(dict) 将dict转换为一维表格型数据结构</span><br><span class="line">c = pd.Series({'int':1, 'string':'hello', 'float':3.1415, 'bool':True})</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas1.png" alt="1"></p><h3 id="DataFrame方法"><a href="#DataFrame方法" class="headerlink" title="DataFrame方法"></a><font size="3">DataFrame方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># pd.DataFrame(array, index, columns) 生成行标签为index，列标签为columns，数据为array的二维表格型数据结构</span><br><span class="line">a = pd.DataFrame(np.arange(12).reshape(3, 4), index=['row_0', 'row_1', 'row_2'], columns=['columns_0', 'columns_1', 'columns_2', 'columns_3']) </span><br><span class="line"></span><br><span class="line"># pd.DataFrame(dict, index) 生成行标签为index，数据为dict的二维表格型数据结构，dict中的每一个key是每一列的列标签，value是每一列的数据</span><br><span class="line">b = pd.DataFrame({'columns_0':[0, 4, 8], 'columns_1':[1, 5, 9], 'columns_2':[2, 6, 10], 'columns_3':[3, 7, 11]}, index=['row_0', 'row_1', 'row_2'])</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas2.png" alt="2"></p><h2 id="Pandas属性"><a href="#Pandas属性" class="headerlink" title="Pandas属性"></a><font size="4">Pandas属性</font></h2><h3 id="dtypes，index，columns，values属性"><a href="#dtypes，index，columns，values属性" class="headerlink" title="dtypes，index，columns，values属性"></a><font size="3">dtypes，index，columns，values属性</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">a = pd.DataFrame({'col_0':[0, 4, 8], 'col_1':[2.71, 3.14, 5], 'col_2':[True, False, False]}, index=['row_0', 'row_1', 'row_2']) </span><br><span class="line"></span><br><span class="line"># obj.dtypes 查看每一列的数据形式</span><br><span class="line">a.dtypes</span><br><span class="line"></span><br><span class="line"># obj.index 查看列的序号</span><br><span class="line">a.index</span><br><span class="line"></span><br><span class="line"># obj.columns 查看行的序号</span><br><span class="line">a.columns</span><br><span class="line"></span><br><span class="line"># obj.values 查看列表中的数据内容</span><br><span class="line">a.values</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas3.png" alt="3"></p><h3 id="describe，head，tail属性"><a href="#describe，head，tail属性" class="headerlink" title="describe，head，tail属性"></a><font size="3">describe，head，tail属性</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">a = pd.DataFrame({'col_0':[0, 4, 8], 'col_1':[2.71, 3.14, 5], 'col_2':[True, False, False]}, index=['row_0', 'row_1', 'row_2']) </span><br><span class="line"></span><br><span class="line"># obj.describe() 查看内容统计，只统计数字内容</span><br><span class="line">a.describe()</span><br><span class="line"></span><br><span class="line"># obj.head(n) 查看前n行</span><br><span class="line">a.head(2)</span><br><span class="line"></span><br><span class="line"># obj.tail(n) 查看后n行</span><br><span class="line">a.tail(2)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas4.png" alt="4"></p><h2 id="Pandas表格排序"><a href="#Pandas表格排序" class="headerlink" title="Pandas表格排序"></a><font size="4">Pandas表格排序</font></h2><h3 id="sort-index方法"><a href="#sort-index方法" class="headerlink" title="sort_index方法"></a><font size="3">sort_index方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np.random.seed(1)</span><br><span class="line"></span><br><span class="line">a = pd.DataFrame(np.random.randint(1, 6, (3, 4)), index=['row_2', 'row_1', 'row_3'], columns=['col_2', 'col_0', 'col_3', 'col_1'])</span><br><span class="line"></span><br><span class="line"># obj.sort_index(axis=0, ascending=True) 对obj的标签进行排序，axis=0(默认)为行标签，axis=1为列标签，ascending=True(默认)为递增顺序，ascending=False为递减顺序</span><br><span class="line">b = a.sort_index()</span><br><span class="line">c = a.sort_index(axis=1, ascending=False)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas5.png" alt="5"></p><h3 id="sort-values方法"><a href="#sort-values方法" class="headerlink" title="sort_values方法"></a><font size="3">sort_values方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">np.random.seed(1)</span><br><span class="line"></span><br><span class="line">a = pd.DataFrame(np.random.randint(1, 6, (3, 4)), index=['row_2', 'row_1', 'row_3'], columns=['col_2', 'col_0', 'col_3', 'col_1'])</span><br><span class="line"></span><br><span class="line"># obj.sort_values(by, axis=0, ascending) 对obj的数据内容进行排序，by指定要排序的行或者列，axis=0(默认)为行标签，axis=1为列标签，ascending=True(默认)为递增顺序，ascending=False为递减顺序</span><br><span class="line">b = a.sort_values(by='col_0')</span><br><span class="line">c = a.sort_values(by='row_1', axis=1, ascending=False)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas6.png" alt="6"></p><h2 id="pandas切片与索引"><a href="#pandas切片与索引" class="headerlink" title="pandas切片与索引"></a><font size="4">pandas切片与索引</font></h2><h3 id="方法"><a href="#方法" class="headerlink" title="[]方法"></a><font size="3">[]方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = pd.DataFrame(np.arange(12).reshape(3, 4), index=['row_0', 'row_1', 'row_2'], columns=['col_0', 'col_1', 'col_2', 'col_3']) </span><br><span class="line"></span><br><span class="line"># obj[col_name] 索引obj中的col_name列</span><br><span class="line">b = a[['col_3', 'col_0']]</span><br><span class="line"></span><br><span class="line"># obj[m:n] 索引obj中的[m, n)行</span><br><span class="line">c = a[1:3]</span><br><span class="line"></span><br><span class="line"># obj[obj.col_name op x] 索引obj的col_name列中对x操作后为True的行</span><br><span class="line">d = a[a.col_0  &gt; 3]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas7.png" alt="7"></p><h3 id="loc，iloc方法"><a href="#loc，iloc方法" class="headerlink" title="loc，iloc方法"></a><font size="3">loc，iloc方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = pd.DataFrame(np.arange(12).reshape(3, 4), index=['row_0', 'row_1', 'row_2'], columns=['col_0', 'col_1', 'col_2', 'col_3']) </span><br><span class="line"></span><br><span class="line"># obj.loc[row_name，col_name=None] 索引obj中行标签为row_name,列标签为col_name(默认是全部列)的所有数据</span><br><span class="line">b = a.loc[['row_1', 'row_2']]</span><br><span class="line"></span><br><span class="line"># obj.iloc[row_index, col_index] 索引obj中的第row_index行和第col_index列</span><br><span class="line">c = a.iloc[[1, 2], [1, 2]]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas8.png" alt="8"></p><h2 id="pandas修改内容"><a href="#pandas修改内容" class="headerlink" title="pandas修改内容"></a><font size="4">pandas修改内容</font></h2><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="[]方法"></a><font size="3">[]方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = pd.DataFrame(np.arange(12).reshape(3, 4), index=['row_0', 'row_1', 'row_2'], columns=['col_0', 'col_1', 'col_2', 'col_3']) </span><br><span class="line"></span><br><span class="line"># obj[col_name] 将obj中的col_name列中的数据改为x</span><br><span class="line">a[['col_3','col_2']] = -1</span><br><span class="line"></span><br><span class="line"># obj[m:n] = x 将obj中的[m, n)行中的所有数据改为x</span><br><span class="line">a[2:3] = -2</span><br><span class="line"></span><br><span class="line"># obj[obj[col_name] op x] = x 将obj的col_name列中对x操作后为True的行中的所有数据改为x</span><br><span class="line">a[a['col_0']  &gt; 3] = -3</span><br><span class="line"></span><br><span class="line"># obj.col_name[obj.col_name op x] = x 将obj的col_name列中对x操作后为True的所有数据改为x</span><br><span class="line">a.col_0[a['col_0']  != -3] =-4</span><br><span class="line"></span><br><span class="line"># obj[col_name] = x 将obj的col_name列修改为x，如果没有该列则增加一列</span><br><span class="line">a['col_4'] =-5</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas9.png" alt="9"></p><h3 id="dropna，fillna方法"><a href="#dropna，fillna方法" class="headerlink" title="dropna，fillna方法"></a><font size="3">dropna，fillna方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = pd.DataFrame({'col_0':[0, 4, np.nan], 'col_1':[2.71, np.nan, np.nan], 'col_2':[np.nan, np.nan, np.nan]}, index=['row_0', 'row_1', 'row_2']) </span><br><span class="line">b = pd.DataFrame({'col_0':[0, 4, 2.5], 'col_1':[2.71, np.nan, np.nan], 'col_2':[np.nan, np.nan, np.nan]}, index=['row_0', 'row_1', 'row_2']) </span><br><span class="line"></span><br><span class="line"># obj.dropna(axis=0, how='any') 将obj中的nan删除，axis=0(默认)为删除行，axis=1为删除列，how='any'(默认)为只要存在nan就删除，how='all'为全部为nan才删除</span><br><span class="line">c = a.dropna()</span><br><span class="line">d = b.dropna(1, 'all')</span><br><span class="line"></span><br><span class="line"># obj.fillna(value) 将obj中的nan用value填充,value可以为数字或者字典，如果是字典则按照字典的对应关系按列填充</span><br><span class="line">e = a.fillna(-1)</span><br><span class="line">f = a.fillna({'col_0':-1, 'col_1':-2, 'col_2':-3})</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas10.png" alt="10"></p><h2 id="pandas合并"><a href="#pandas合并" class="headerlink" title="pandas合并"></a><font size="4">pandas合并</font></h2><h3 id="concat方法"><a href="#concat方法" class="headerlink" title="concat方法"></a><font size="3">concat方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">index_a = ['A', 'B']</span><br><span class="line">index_b = ['B', 'C']</span><br><span class="line">col_a = ['a', 'b']</span><br><span class="line">col_b = ['b', 'c']</span><br><span class="line">a = pd.DataFrame(np.arange(4).reshape(2, 2), index=index_a, columns=col_a) </span><br><span class="line">b = pd.DataFrame(np.arange(4).reshape(2, 2), index=index_b, columns=col_b) </span><br><span class="line"></span><br><span class="line"># pd.concat([obj1, obj2, ...], axis=0, join='outer', ignore_index=False) 将多个对象按照轴进行连接，join为连接方式，'outer'代表将没有相应标签的对象补NaN，'inner'代表只保留共有的标签，ignore_index为是否重新开始排列标签</span><br><span class="line">c = pd.concat([a, b])</span><br><span class="line">d = pd.concat([a, b], axis=1)</span><br><span class="line">e = pd.concat([a, b], join='inner')</span><br><span class="line">f = pd.concat([a, b],ignore_index=True)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas11.png" alt="11"></p><h3 id="append方法"><a href="#append方法" class="headerlink" title="append方法"></a><font size="3">append方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">index_a = ['A', 'B']</span><br><span class="line">index_b = ['B', 'C']</span><br><span class="line">col_a = ['a', 'b']</span><br><span class="line">col_b = ['b', 'c']</span><br><span class="line">a = pd.DataFrame(np.arange(4).reshape(2, 2), index=index_a, columns=col_a) </span><br><span class="line">b = pd.DataFrame(np.arange(4).reshape(2, 2), index=index_b, columns=col_b) </span><br><span class="line"></span><br><span class="line"># obj.append([obj1, obj2, ...], ignore_index=False) 在纵向在obj后追加obj1, obj2,......，如果没有对应的列标签，则补NaN</span><br><span class="line">c = a.append([a])</span><br><span class="line">d = a.append([b])</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas12.png" alt="12"></p><h3 id="merge方法"><a href="#merge方法" class="headerlink" title="merge方法"></a><font size="3">merge方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">index_a = ['A', 'B']</span><br><span class="line">index_b = ['B', 'C']</span><br><span class="line">col_a = ['a', 'b']</span><br><span class="line">col_b = ['b', 'c']</span><br><span class="line">a = pd.DataFrame(np.array([1, 3, 5, 7]).reshape(2, 2), index=index_a, columns=col_a) </span><br><span class="line">b = pd.DataFrame(np.array([3, 4, 5, 6]).reshape(2, 2), index=index_b, columns=col_b) </span><br><span class="line"></span><br><span class="line"># pd.merge(left, right, how='inner', on='col_name',left_index=False,right_index=False) 按照on进行合并两个表格,how='inner'，行标签为两个表格行标签的交集，'outer'，行标签为两个表格行标签的并集，将不相交的部分取NaN，'left'，行标签为left的行标签，'right'，行标签为left的行标签。left_index和right_index指是否按照标签合并，为False根据值合并表格，True根据标签合并表格</span><br><span class="line">c = pd.merge(a, b, on='b')</span><br><span class="line">d = pd.merge(a, b, how='outer', on='b')</span><br><span class="line">e = pd.merge(a, b, how='left', on='b')</span><br><span class="line">f = pd.merge(a, b, how='right', on='b')</span><br><span class="line">g = pd.merge(a, b, how='outer', left_index=True,right_index=True)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas13.png" alt="13"></p><h2 id="Pandas修改行列名"><a href="#Pandas修改行列名" class="headerlink" title="Pandas修改行列名"></a><font size="4">Pandas修改行列名</font></h2><h3 id="replace，rename方法"><a href="#replace，rename方法" class="headerlink" title="replace，rename方法"></a><font size="4">replace，rename方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">a = pd.DataFrame([[1, 3], [5, 7]], index=['row_0', 'row_1'], columns=['col_0', 'col_1']) </span><br><span class="line"></span><br><span class="line"># obj.columns = list，obj.index = list 将列标签改为list，将行标签改为list</span><br><span class="line">a.columns = ['new_col_0', 'new_col_1']</span><br><span class="line">a.index = ['new_row_0', 'new_row_1']</span><br><span class="line"></span><br><span class="line"># obj.replace(list1, list2) 将obj中list1中的数值替换为list2</span><br><span class="line">b = a.replace([1, 7], [2, 6])</span><br><span class="line"></span><br><span class="line"># obj.rename(columns=dict/func, index=dict/func) 用字典或函数来更改行列名</span><br><span class="line">c = a.rename(columns={'new_col_0':'A', 'new_col_1':'B'}, index={'new_row_0':'a', 'new_row_1':'b'})</span><br><span class="line">d = a.rename(columns=lambda x: x[-1], index=lambda x: x[-1])</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas14.png" alt="14"></p><h2 id="pandas数理统计"><a href="#pandas数理统计" class="headerlink" title="pandas数理统计"></a><font size="4">pandas数理统计</font></h2><h3 id="notnull，isnull方法"><a href="#notnull，isnull方法" class="headerlink" title="notnull，isnull方法"></a><font size="3">notnull，isnull方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = pd.DataFrame([[1, np.nan], [5, 7]], index=['row_0', 'row_1'], columns=['col_0', 'col_1']) </span><br><span class="line"></span><br><span class="line"># obj.isnull() 判断obj中每一项是否为NaN</span><br><span class="line">a.isnull()</span><br><span class="line"></span><br><span class="line"># obj.notnull() 判断obj中每一项是否不为NaN</span><br><span class="line">a.notnull()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas15.png" alt="15"></p><h3 id="统计方法"><a href="#统计方法" class="headerlink" title="统计方法"></a><font size="3">统计方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = pd.DataFrame([[1, 4, 7], [2, 5, 8], [5, 4, 1]], index=['r_0', 'r_1', 'r_2'], columns=['c_0', 'c_1', 'c_2']) </span><br><span class="line">b = pd.DataFrame([[1, 4, 7], [2, np.nan, 8], [5, np.nan, np.nan]], index=['r_0', 'r_1', 'r_2'], columns=['c_0', 'c_1', 'c_2']) </span><br><span class="line"></span><br><span class="line"># obj.max/min(axis=0) 统计最大值/最小者，axis=0为列，axis=1为行</span><br><span class="line">a.max(0)</span><br><span class="line">a.min(1)</span><br><span class="line"></span><br><span class="line"># obj.count(axis=0) 统计非空个数，axis用法同pd.max</span><br><span class="line">b.count()</span><br><span class="line"></span><br><span class="line"># obj.mean(axis=0) 统计均值，axis用法同pd.max</span><br><span class="line">a.mean()</span><br><span class="line"></span><br><span class="line"># obj.median(axis=0) 统计中位数，axis用法同pd.max</span><br><span class="line">a.median()</span><br><span class="line"></span><br><span class="line"># obj.std(axis=0) 统计标准差，axis用法同pd.max</span><br><span class="line">a.std()</span><br><span class="line"></span><br><span class="line"># obj.var(obj, axis=None) 统计方差，axis用法同pd.max</span><br><span class="line">a.var()</span><br><span class="line"></span><br><span class="line"># obj.corr(obj, axis=None) 统计相关系数，axis用法同pd.max</span><br><span class="line">a.corr()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas16.png" alt="16"></p><h2 id="pandas数据保存"><a href="#pandas数据保存" class="headerlink" title="pandas数据保存"></a><font size="4">pandas数据保存</font></h2><h3 id="to-csv，read-csv方法"><a href="#to-csv，read-csv方法" class="headerlink" title="to_csv，read_csv方法"></a><font size="3">to_csv，read_csv方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = pd.DataFrame(np.arange(12).reshape(3, 4), index=['row_0', 'row_1', 'row_2'], columns=['col_0', 'col_1', 'col_2', 'col_3']) </span><br><span class="line"></span><br><span class="line"># obj.to_csv/pickle(filename) 将obj保存在文件名为filename的.csv/pickle文件中(要加.csv/pickle扩展名)</span><br><span class="line">a.to_csv('save1.csv')</span><br><span class="line">a.to_pickle('save2.pickle')</span><br><span class="line"></span><br><span class="line"># pd.read_csv(filename) 读取文件名为filename的数组数据(要加.csv/pickle扩展名)</span><br><span class="line">b = pd.read_csv('save1.csv')</span><br><span class="line">c = pd.read_pickle('save2.pickle')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas17.png" alt="17"></p><h2 id="pandas数据显示"><a href="#pandas数据显示" class="headerlink" title="pandas数据显示"></a><font size="4">pandas数据显示</font></h2><h3 id="plot，plot-scatter方法"><a href="#plot，plot-scatter方法" class="headerlink" title="plot，plot.scatter方法"></a><font size="3">plot，plot.scatter方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = np. arange(10)</span><br><span class="line">y = np.log(x)</span><br><span class="line">a = pd.DataFrame({'x':x, 'y':y}) </span><br><span class="line"></span><br><span class="line"># obj.plot.scatter(x, y) 画出x-y对应的散点图，可参考matplotlib</span><br><span class="line">a.plot.scatter('x', 'y')</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/pandas18.png" alt="18"></p><h1 id="Pandas小结"><a href="#Pandas小结" class="headerlink" title="Pandas小结"></a><font size="5" color="red">Pandas小结</font></h1><p>  Pandas可处理的数据更接近来源于生活中的数据，在数据分析，机器学习中，大量的数据都是具有标签的，不只是纯数字的数据，需要借助Pandas的帮助，因此pandas也作为机器学习三剑客之一。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Pandas&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="常用库" scheme="https://USTCcoder.github.io/categories/python/%E5%B8%B8%E7%94%A8%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>Scikit-Learn</title>
    <link href="https://USTCcoder.github.io/2019/08/12/library%20Sklearn/"/>
    <id>https://USTCcoder.github.io/2019/08/12/library Sklearn/</id>
    <published>2019-08-12T12:29:15.000Z</published>
    <updated>2020-07-27T06:13:36.940Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LIBRARY/sklearn.jpg" alt="0"></p><h1 id="Scikit-Learn介绍"><a href="#Scikit-Learn介绍" class="headerlink" title="Scikit-Learn介绍"></a><font size="5" color="red">Scikit-Learn介绍</font></h1><p>  Scikit-Learn库自2007年发布以来，已经称为最受欢迎的机器学习库之一，基于广受欢迎的Numpy和Scipy库构建，能够提供用于机器学习的算法，数据预处理等功能。<br><a id="more"></a></p><h1 id="Scikit-Learn应用"><a href="#Scikit-Learn应用" class="headerlink" title="Scikit-Learn应用"></a><font size="5" color="red">Scikit-Learn应用</font></h1><h2 id="Scikit-Learn线性回归"><a href="#Scikit-Learn线性回归" class="headerlink" title="Scikit-Learn线性回归"></a><font size="4">Scikit-Learn线性回归</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">x_train = np.array([[6], [8], [10], [14], [18]]).reshape(-1, 1)</span><br><span class="line">y_train = np.array([7, 9, 13, 17.5, 18])</span><br><span class="line"></span><br><span class="line"># model = LinearRegression() 创建线性回归模型</span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"># model.fit(X, y) 用训练数据X，y拟合模型</span><br><span class="line">model.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">x_test = np.array([[12]]).reshape(-1, 1)</span><br><span class="line"></span><br><span class="line"># model.predict(X) 用训练后的模型预测数据X</span><br><span class="line">y_test = model.predict(x_test)[0]</span><br><span class="line"></span><br><span class="line">x_max, x_min = max(x_train), min(x_train)</span><br><span class="line">y_max, y_min = model.predict([x_max, x_min])</span><br><span class="line"></span><br><span class="line"># model.coef_ 获取模型的权值系数</span><br><span class="line">k = model.coef_[0]</span><br><span class="line">x_mean, y_mean = np.mean(x_train), np.mean(y_train)</span><br><span class="line">b = y_mean - k * x_mean</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title('Pizza price against diameter:\n' + str(k) + 'x + ' + str(b) + '= y')</span><br><span class="line">plt.xlabel('Pizza diamter')</span><br><span class="line">plt.ylabel('Pizza price')</span><br><span class="line">plt.plot(x_train, y_train, 'k.', label='train_dot')</span><br><span class="line">plt.plot(x_test, y_test, 'ro', label='predict_dot')</span><br><span class="line">plt.plot([x_max, x_min], [y_max, y_min])</span><br><span class="line">plt.text(x_test, y_test, '(' + str(x_test[0][0]) + ',' + str(y_test) + ')')</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(True)</span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn1.png" alt="1"></p><h2 id="Scikit-LearnK近邻分类算法"><a href="#Scikit-LearnK近邻分类算法" class="headerlink" title="Scikit-LearnK近邻分类算法"></a><font size="4">Scikit-LearnK近邻分类算法</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.preprocessing import LabelBinarizer</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line">from sklearn.metrics import precision_score</span><br><span class="line">from sklearn.metrics import recall_score</span><br><span class="line">from sklearn.metrics import f1_score</span><br><span class="line">from sklearn.metrics import matthews_corrcoef</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line"></span><br><span class="line">def classify(x, y):</span><br><span class="line">    male_height, male_weight, female_height, female_weight = [], [], [], []</span><br><span class="line">    for i in range(len(x)):</span><br><span class="line">        if y[i] == 'male':</span><br><span class="line">            male_height.append(x[i][0])</span><br><span class="line">            male_weight.append(x[i][1])</span><br><span class="line">        else:</span><br><span class="line">            female_height.append(x[i][0])</span><br><span class="line">            female_weight.append(x[i][1])</span><br><span class="line">    return male_height, male_weight, female_height, female_weight</span><br><span class="line"></span><br><span class="line">x_train = np.array([[158, 64], [170, 66], [183, 84], [191, 80], [155, 49], [163, 59], [180, 67], [158, 54], [178, 77]])</span><br><span class="line">y_train = ['male', 'male', 'male', 'male', 'female', 'female', 'female', 'female', 'female'] </span><br><span class="line"></span><br><span class="line"># lb = LabelBinarizer() 创建一个标签转换器接口，将标签变成整数</span><br><span class="line">lb = LabelBinarizer()</span><br><span class="line"></span><br><span class="line"># lb.fit_transform(y) 在训练数据集上对标签进行拟合并转换为整数</span><br><span class="line">y_train_binarized = lb.fit_transform(y_train)</span><br><span class="line"></span><br><span class="line">k = 3</span><br><span class="line"></span><br><span class="line"># clf = KNeighborsClassifier(n_neighbors=k) 创建KNN分类器模型</span><br><span class="line">clf = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line"></span><br><span class="line"># clf.fit(X, y) 用训练数据X，y拟合模型</span><br><span class="line">clf.fit(x_train,y_train_binarized.reshape(-1))</span><br><span class="line"></span><br><span class="line">x_test = np.array([[168, 65], [170, 61], [160, 52], [169, 67]])</span><br><span class="line">y_test = ['male', 'male', 'female', 'female']</span><br><span class="line"></span><br><span class="line"># clf.predict(X) 用训练后的模型预测数据X</span><br><span class="line">y_pre_binarized = clf.predict(x_test)</span><br><span class="line"></span><br><span class="line"># lb.inverse_transform(Y) 将预测后的整数转换成标签</span><br><span class="line">y_pre = lb.inverse_transform(y_pre_binarized)</span><br><span class="line"></span><br><span class="line">male_height_train, male_weight_train, female_height_train, female_weight_train = classify(x_train, y_train)</span><br><span class="line">male_height_pre, male_weight_pre, female_height_pre, female_weight_pre = classify(x_test, y_pre)</span><br><span class="line"></span><br><span class="line">y_test_binarized = lb.transform(y_test).T[0]</span><br><span class="line">print(y_test_binarized)</span><br><span class="line">print(y_pre_binarized)</span><br><span class="line"></span><br><span class="line"># accuracy_score(y_test, y_predict) 求真实值与预测值的准确率</span><br><span class="line">print('预测准确率为:%.2f' %accuracy_score(y_test_binarized, y_pre_binarized))</span><br><span class="line"></span><br><span class="line"># precision_score(y_test, y_predict) 求真实值与预测值的精准率</span><br><span class="line">print('预测精准率为:%.2f' %precision_score(y_test_binarized, y_pre_binarized))</span><br><span class="line"></span><br><span class="line"># recall_score(y_test, y_predict) 求真实值与预测值的召回率</span><br><span class="line">print('预测召回率为:%.2f' %recall_score(y_test_binarized, y_pre_binarized))</span><br><span class="line"></span><br><span class="line"># f1_score(y_test, y_predict) 求真实值与预测值的F1得分</span><br><span class="line">print('预测F1得分为:%.2f' %f1_score(y_test_binarized, y_pre_binarized))</span><br><span class="line"></span><br><span class="line"># matthews_corrcoef(y_test, y_predict) 求真实值与预测值的马修斯系数</span><br><span class="line">print('马修斯系数为:%.2f' %matthews_corrcoef(y_test_binarized, y_pre_binarized))</span><br><span class="line"></span><br><span class="line"># classification_report(y_test, y_predict, target_names=None, labels=None) 同时生成真实值与预测值的精准率，召回率和F1得分，目标标签为target_name，对应的值为label</span><br><span class="line">print(classification_report(y_test_binarized, y_pre_binarized, target_names={'male'}, labels=[1]))</span><br><span class="line"></span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line">plt.figure()</span><br><span class="line">plt.title('Human Height and Weight By Sex:')</span><br><span class="line">plt.xlabel('Height')</span><br><span class="line">plt.ylabel('Weight')</span><br><span class="line">plt.grid(True)</span><br><span class="line">plt.scatter(male_height_train, male_weight_train, color='b', marker='o', label='train_male')</span><br><span class="line">plt.scatter(female_height_train, female_weight_train, color='r', marker='o', label='train_female')</span><br><span class="line">plt.scatter(male_height_pre, male_weight_pre, color='b', marker='*', label='pre_male')</span><br><span class="line">plt.scatter(female_height_pre, female_weight_pre, color='r', marker='*', label='pre_female')</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn2.png" alt="2"></p><h2 id="Scikit-LearnK近邻回归算法"><a href="#Scikit-LearnK近邻回归算法" class="headerlink" title="Scikit-LearnK近邻回归算法"></a><font size="4">Scikit-LearnK近邻回归算法</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.neighbors import KNeighborsRegressor</span><br><span class="line">from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line"></span><br><span class="line">x_train = np.array([[158, 1], [170, 1], [183, 1], [191, 1], [155, 0], [163, 0], [180, 0], [158, 0], [170, 0]])</span><br><span class="line">y_train = np.array([64, 86, 84, 80, 49, 59, 67, 54, 67])</span><br><span class="line">x_test = np.array([[168, 1], [180, 1], [160, 0], [169, 0]])</span><br><span class="line">y_test = [65, 96, 52, 67]</span><br><span class="line">k = 3</span><br><span class="line"></span><br><span class="line"># clf = KNeighborsRegressor(n_neighbors=k) 创建KNN回归模型</span><br><span class="line">clf = KNeighborsRegressor(n_neighbors=k)</span><br><span class="line"></span><br><span class="line"># clf.fit(X, y) 用训练数据X，y拟合模型</span><br><span class="line">clf.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"># clf.predict(X) 用训练后的模型预测数据X</span><br><span class="line">pre = clf.predict(x_test)</span><br><span class="line"></span><br><span class="line">print([np.around(x, 2) for x in pre])</span><br><span class="line"></span><br><span class="line"># r2_score(y_test, y_predict) 求真实值与预测值的决定系数</span><br><span class="line">print('Coefficiet of determination: %.2f' %r2_score(y_test, pre))</span><br><span class="line"></span><br><span class="line"># mean_absolute_erro(y_test, y_predict) 求真实值与预测值的平均绝对误差MAE</span><br><span class="line">print('Mean absolute error: %.2f' %mean_absolute_error(y_test, pre))</span><br><span class="line"></span><br><span class="line"># mean_squared_error(y_test, y_predict) 求真实值与预测值的均方误差MSE</span><br><span class="line">print('Mean squared error: %.2f' %mean_squared_error(y_test, pre))</span><br><span class="line"></span><br><span class="line">print('\n' + 'Scaled Processing'.center(30, '~') + '\n')</span><br><span class="line"></span><br><span class="line"># ss = StandardScaler() 创建一个特征缩放转换接口</span><br><span class="line">ss = StandardScaler()</span><br><span class="line"></span><br><span class="line"># ss.fit_transform(x) 在训练数据集上对数据特征进行缩放</span><br><span class="line">x_train_scaled = ss.fit_transform(x_train)</span><br><span class="line"></span><br><span class="line"># ss.transform(x) 在测试数据集上对数据特征进行缩放</span><br><span class="line">x_test_scaled = ss.transform(x_test) </span><br><span class="line"></span><br><span class="line">clf.fit(x_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line">pre_scaled = clf.predict(x_test_scaled)</span><br><span class="line"></span><br><span class="line">print([np.around(x, 2) for x in pre_scaled])</span><br><span class="line">print('Coefficiet of determination: %.2f' %r2_score(y_test, pre_scaled))</span><br><span class="line">print('Mean absolute error: %.2f' %mean_absolute_error(y_test, pre_scaled))</span><br><span class="line">print('Mean squared error: %.2f' %mean_squared_error(y_test, pre_scaled))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn3.png" alt="3"></p><h2 id="Scikit-Learn独热编码"><a href="#Scikit-Learn独热编码" class="headerlink" title="Scikit-Learn独热编码"></a><font size="4">Scikit-Learn独热编码</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction import DictVectorizer</span><br><span class="line"></span><br><span class="line"># onehot_encoder = DictVectorizer() 创建独热编码转换器</span><br><span class="line">onehot_encoder = DictVectorizer()</span><br><span class="line"></span><br><span class="line">x = [{'city': 'New York'}, {'city': 'San Francisco'}, {'city:': 'Chapel Hill'}]</span><br><span class="line"></span><br><span class="line"># onehot_encoder.fit_transform(x).toarray() 将字典的值value进行独热编码</span><br><span class="line">onehot_x = onehot_encoder.fit_transform(x).toarray()</span><br><span class="line"></span><br><span class="line">print(onehot_x)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn4.png" alt="4"></p><h2 id="Scikit-Learn特征标准化"><a href="#Scikit-Learn特征标准化" class="headerlink" title="Scikit-Learn特征标准化"></a><font size="4">Scikit-Learn特征标准化</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn import preprocessing</span><br><span class="line"></span><br><span class="line">x = np.array([[0., 0., 5., 13., 9., 1.], [0., 0., 13., 15., 10., 15.], [0., 3., 15., 2., 0., 11.]])</span><br><span class="line"></span><br><span class="line"># preprocessing.StandardScaler().fit_transform(x) 使用标准化转换器类函数</span><br><span class="line">x_standard_scaled = preprocessing.StandardScaler().fit_transform(x)</span><br><span class="line">print(x_standard_scaled)</span><br><span class="line"></span><br><span class="line"># preprocessing.scale(x) 使用标准化函数scale</span><br><span class="line">x_scaled = preprocessing.scale(x)</span><br><span class="line">print(x_scaled)</span><br><span class="line"></span><br><span class="line"># preprocessing.robust_scale(x) 使用鲁棒性标准化函数robust_scale</span><br><span class="line">x_robust_scaled = preprocessing.robust_scale(x)</span><br><span class="line">print(x_robust_scaled)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn5.png" alt="5"></p><h2 id="Scikit-Learn多元线性回归"><a href="#Scikit-Learn多元线性回归" class="headerlink" title="Scikit-Learn多元线性回归"></a><font size="4">Scikit-Learn多元线性回归</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">x = [[6, 2], [8, 1], [10, 0], [14, 2], [18, 0]]</span><br><span class="line">y = [[7], [9], [13], [17.5], [18]]</span><br><span class="line"></span><br><span class="line"># model = LinearRegression() 创建线性回归模型</span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"># model.fit(X, y) 用训练数据X，y拟合模型</span><br><span class="line">model.fit(x, y)</span><br><span class="line"></span><br><span class="line">x_test = [[8, 2], [9, 0], [11, 2], [16, 2], [12, 0]]</span><br><span class="line">y_test = [[11], [8.5], [15], [18], [11]]</span><br><span class="line"></span><br><span class="line"># model.predict(X) 用训练后的模型预测数据X</span><br><span class="line">predictions = model.predict(x_test)</span><br><span class="line"></span><br><span class="line">for i, prediction in enumerate(predictions):</span><br><span class="line">    print('prediction: %s, truth: %s' %(prediction, y_test[i]))</span><br><span class="line"></span><br><span class="line"># model.score(x, y) 求模型的决定系数</span><br><span class="line">print('R-squared: %.2f' %model.score(x_test, y_test))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn6.png" alt="6"></p><h2 id="Scikit-Learn多项式回归"><a href="#Scikit-Learn多项式回归" class="headerlink" title="Scikit-Learn多项式回归"></a><font size="4">Scikit-Learn多项式回归</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">plt.style.use('ggplot')</span><br><span class="line"></span><br><span class="line">x_train = [[6], [8], [10], [14], [18]]</span><br><span class="line">y_train = [[7], [9], [13], [17.5], [18]]</span><br><span class="line">x_test = [[6], [8], [11], [16]]</span><br><span class="line">y_test = [[8], [12], [15], [18]]</span><br><span class="line"></span><br><span class="line"># model = LinearRegression() 创建线性回归模型</span><br><span class="line">regressor = LinearRegression()</span><br><span class="line"></span><br><span class="line"># model.fit(X, y) 用训练数据X，y拟合模型</span><br><span class="line">regressor.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">xx = np.linspace(0, 26, 100)</span><br><span class="line"></span><br><span class="line"># model.predict(X) 用训练后的模型预测数据X</span><br><span class="line">yy = regressor.predict(xx.reshape(xx.shape[0], 1))</span><br><span class="line"></span><br><span class="line">plt.plot(xx, yy, c='b', label='Linear_poly')</span><br><span class="line"></span><br><span class="line"># quadratic_featurizer = PolynomialFeatures(degree=n) 创建n阶多项式转换器</span><br><span class="line">quadratic_featurizer = PolynomialFeatures(degree=2)</span><br><span class="line"></span><br><span class="line"># quadratic_featurizer.fit_transform(x_train) 在训练数据集上对数据特征进行多项式变换</span><br><span class="line">x_train_quadratic = quadratic_featurizer.fit_transform(x_train)</span><br><span class="line"></span><br><span class="line"># ss.fit_transform(x) 在训练数据集上对数据特征进行多项式变换</span><br><span class="line">x_test_quadratic = quadratic_featurizer.transform(x_test)</span><br><span class="line"></span><br><span class="line"># model = LinearRegression() 创建线性回归模型</span><br><span class="line">regressor_quadratic = LinearRegression()</span><br><span class="line"></span><br><span class="line"># model.fit(X, y) 用训练数据X，y拟合模型</span><br><span class="line">regressor_quadratic.fit(x_train_quadratic, y_train)</span><br><span class="line"></span><br><span class="line"># ss.fit_transform(x) 对数据特征进行多项式变换</span><br><span class="line">xx_quadratic = quadratic_featurizer.transform(xx.reshape(xx.shape[0], 1))</span><br><span class="line"></span><br><span class="line"># model.predict(X) 用训练后的模型预测数据X</span><br><span class="line">yy_quadratic = regressor_quadratic.predict(xx_quadratic)</span><br><span class="line"></span><br><span class="line">print('linear regression r-squared', regressor.score(x_test, y_test))</span><br><span class="line">print('quadratic regression r-squared', regressor_quadratic.score(x_test_quadratic, y_test))</span><br><span class="line"></span><br><span class="line">plt.plot(xx, yy_quadratic, c='r', label='square_poly')</span><br><span class="line">plt.scatter(x_train, y_train, label='data')</span><br><span class="line">plt.axis([0, 25, 0, 25])</span><br><span class="line">plt.title('Pizza price and diameter')</span><br><span class="line">plt.xlabel('Diameter in inches')</span><br><span class="line">plt.ylabel('Pizza Price')</span><br><span class="line">plt.grid(True)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn7.png" alt="7"></p><h2 id="Scikit-Learn逻辑回归和朴素贝叶斯"><a href="#Scikit-Learn逻辑回归和朴素贝叶斯" class="headerlink" title="Scikit-Learn逻辑回归和朴素贝叶斯"></a><font size="4">Scikit-Learn逻辑回归和朴素贝叶斯</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># from sklearn.datasets import load_breast_cancer 从sklearn数据集中导入胸部癌症的数据</span><br><span class="line">x, y = load_breast_cancer(return_X_y=True)</span><br><span class="line"></span><br><span class="line"># train_test_split(x, y, stratify=y, test_size=n) 将x和y按照test_size划分成数据集和测试集，stratify=y按照y中的比例分配</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=31)</span><br><span class="line"></span><br><span class="line"># lr = LogisticRegression() 创建逻辑回归模型</span><br><span class="line">lr = LogisticRegression(solver='liblinear')</span><br><span class="line"></span><br><span class="line"># nb = GaussianNB() 创建朴素贝叶斯模型</span><br><span class="line">nb = GaussianNB()</span><br><span class="line"></span><br><span class="line">lr_scores = []</span><br><span class="line">nb_scores = []</span><br><span class="line"></span><br><span class="line">train_sizes = range(10, len(x_train), 25)</span><br><span class="line">for train_size in train_sizes:</span><br><span class="line">    # train_test_split(x, y, stratify=y, train_size=n) 将x和y按照test_size划分成数据集和测试集，stratify=y按照y中的比例分配</span><br><span class="line">    x_slice, _, y_slice, _ = train_test_split(x_train, y_train, train_size=train_size, stratify=y_train, random_state=31)</span><br><span class="line"></span><br><span class="line">    # nb.fit(X, y) 用训练数据X，y拟合朴素贝叶斯模型</span><br><span class="line">    nb.fit(x_slice, y_slice)</span><br><span class="line"></span><br><span class="line">    # nb.score(x, y) 求朴素贝叶斯模型的决定系数</span><br><span class="line">    nb_scores.append(nb.score(x_test, y_test))</span><br><span class="line"></span><br><span class="line">    # lr.fit(X, y) 用训练数据X，y拟合逻辑回归模型</span><br><span class="line">    lr.fit(x_slice, y_slice)</span><br><span class="line"></span><br><span class="line">    # lr.score(x, y) 求逻辑回归模型的决定系数</span><br><span class="line">    lr_scores.append(lr.score(x_test, y_test))</span><br><span class="line"></span><br><span class="line">plt.plot(train_sizes, nb_scores, label='Naive Bayes')</span><br><span class="line">plt.plot(train_sizes, lr_scores, linestyle='--', label='Logistic Regression')</span><br><span class="line">plt.title('Naive Bayes and Logistic Regression accuracies')</span><br><span class="line">plt.xlabel('Number of training instances')</span><br><span class="line">plt.ylabel('Test set accuracy')</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn8.png" alt="8"></p><h2 id="Scikit-Learn决策树和袋装集成学习"><a href="#Scikit-Learn决策树和袋装集成学习" class="headerlink" title="Scikit-Learn决策树和袋装集成学习"></a><font size="4">Scikit-Learn决策树和袋装集成学习</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line">from sklearn.datasets import make_classification</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line"></span><br><span class="line"># from sklearn.datasets import make_classification 从sklearn数据集中导入make_classification用于创建分类数据集，样本数为n_samples，特征数为n_features，有用的特征数为n_informative，每一类的簇的个数为n_clusters_per_class</span><br><span class="line">x, y = make_classification(n_samples=1000, n_features=100, n_informative=20, n_clusters_per_class=2, random_state=11)</span><br><span class="line"></span><br><span class="line"># train_test_split(x, y) 将x和y划分成数据集和测试集</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=11)</span><br><span class="line"></span><br><span class="line"># clf = DecisionTreeClassifier() 创建决策树模型</span><br><span class="line">clf = DecisionTreeClassifier(random_state=11)</span><br><span class="line"></span><br><span class="line"># clf.fit(X, y) 用训练数据X，y拟合模型</span><br><span class="line">clf.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"># clf.predict(X) 用训练后的模型预测数据X</span><br><span class="line">predictions = clf.predict(x_test)</span><br><span class="line"></span><br><span class="line"># classification_report(y_test, y_predict, target_names=None, labels=None) 同时生成真实值与预测值的精准率，召回率和F1得分，目标标签为target_name，对应的值为label</span><br><span class="line">print(classification_report(y_test, predictions))</span><br><span class="line"></span><br><span class="line"># clf = RandomForestClassifier(n_estimators=n) 创建包含n个树的随机森林分类器</span><br><span class="line">clf = RandomForestClassifier(n_estimators=10, random_state=11)</span><br><span class="line"></span><br><span class="line"># clf.fit(X, y) 用训练数据X，y拟合模型</span><br><span class="line">clf.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"># clf.predict(X) 用训练后的模型预测数据X</span><br><span class="line">predictions = clf.predict(x_test)</span><br><span class="line"></span><br><span class="line"># classification_report(y_test, y_predict, target_names=None, labels=None) 同时生成真实值与预测值的精准率，召回率和F1得分，目标标签为target_name，对应的值为label</span><br><span class="line">print(classification_report(y_test, predictions))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn9.png" alt="9"></p><h2 id="Scikit-Learn推进集成学习"><a href="#Scikit-Learn推进集成学习" class="headerlink" title="Scikit-Learn推进集成学习"></a><font size="4">Scikit-Learn推进集成学习</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import AdaBoostClassifier</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.datasets import make_classification</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># from sklearn.datasets import make_classification 从sklearn数据集中导入make_classification用于创建分类数据集，样本数为n_samples，特征数为n_features，有用的特征数为n_informative，每一类的簇的个数为n_clusters_per_class</span><br><span class="line">x, y = make_classification(n_samples=1000, n_features=50, n_informative=30, n_clusters_per_class=3, random_state=11)</span><br><span class="line"></span><br><span class="line"># train_test_split(x, y) 将x和y划分成数据集和测试集</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=11)</span><br><span class="line"></span><br><span class="line"># clf = DecisionTreeClassifier() 创建决策树模型</span><br><span class="line">clf = DecisionTreeClassifier()</span><br><span class="line"></span><br><span class="line"># clf.fit(X, y) 用训练数据X，y拟合模型</span><br><span class="line">clf.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"># clf.score(x, y) 求模型的决定系数</span><br><span class="line">print('DecisionTree accuracy:%s' %clf.score(x_test, y_test))</span><br><span class="line"></span><br><span class="line"># clf = AdaBoostClassifier(n_estimators=n) 创建具有n个弱学习器的AdaBoost模型</span><br><span class="line">clf = AdaBoostClassifier(n_estimators=50, random_state=11)</span><br><span class="line"></span><br><span class="line"># clf.fit(X, y) 用训练数据X，y拟合模型</span><br><span class="line">clf.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">plt.title('Ensemble Accuracy')</span><br><span class="line">plt.xlabel('Accuracy')</span><br><span class="line">plt.ylabel('Number of base estimators in ensemble')</span><br><span class="line"></span><br><span class="line"># clf.staged_score(x, y) 求AdaBoost模型的弱分类器个数的决定系数</span><br><span class="line">plt.plot(range(1, 51), [accuracy for accuracy in clf.staged_score(x_test, y_test)])</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn10.png" alt="10"></p><h2 id="Scikit-Learn感知机"><a href="#Scikit-Learn感知机" class="headerlink" title="Scikit-Learn感知机"></a><font size="4">Scikit-Learn感知机</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_classification</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line">from sklearn.linear_model import Perceptron</span><br><span class="line"></span><br><span class="line"># from sklearn.datasets import make_classification 从sklearn数据集中导入make_classification用于创建分类数据集，样本数为n_samples，特征数为n_features，有用的特征数为n_informative，每一类的簇的个数为n_clusters_per_class</span><br><span class="line">x, y = make_classification(n_samples=1000, n_features=100, n_informative=20, n_clusters_per_class=2, random_state=11)</span><br><span class="line"></span><br><span class="line"># train_test_split(x, y) 将x和y划分成数据集和测试集</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=11)</span><br><span class="line"></span><br><span class="line"># clf = Perceptron() 创建感知机模型</span><br><span class="line">clf = Perceptron(random_state=11)</span><br><span class="line"></span><br><span class="line"># clf.fit(X, y) 用训练数据X，y拟合模型</span><br><span class="line">clf.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"># clf.predict(X) 用训练后的模型预测数据X</span><br><span class="line">predictions = clf.predict(x_test)</span><br><span class="line"></span><br><span class="line"># classification_report(y_test, y_predict, target_names=None, labels=None) 同时生成真实值与预测值的精准率，召回率和F1得分，目标标签为target_name，对应的值为label</span><br><span class="line">print(classification_report(y_test, predictions))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn11.png" alt="11"></p><h2 id="Scikit-Learn支持向量机"><a href="#Scikit-Learn支持向量机" class="headerlink" title="Scikit-Learn支持向量机"></a><font size="4">Scikit-Learn支持向量机</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_classification</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line"></span><br><span class="line"># from sklearn.datasets import make_classification 从sklearn数据集中导入make_classification用于创建分类数据集，样本数为n_samples，特征数为n_features，有用的特征数为n_informative，每一类的簇的个数为n_clusters_per_class</span><br><span class="line">x, y = make_classification(n_samples=1000, n_features=100, n_informative=20, n_clusters_per_class=2, random_state=11)</span><br><span class="line"></span><br><span class="line"># train_test_split(x, y) 将x和y划分成数据集和测试集</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=11)</span><br><span class="line"></span><br><span class="line"># clf = SVC(kernel='rbf', gamma='auto deprecated', C=1.0) 创建支持向量机模型，核函数默认为rbf高斯核，正则化参数C默认为1.0，核系数参数gamma默认为不使用</span><br><span class="line">clf = SVC(kernel='rbf', gamma=0.01, C=100, random_state=11)</span><br><span class="line"></span><br><span class="line"># clf.fit(X, y) 用训练数据X，y拟合模型</span><br><span class="line">clf.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"># clf.predict(X) 用训练后的模型预测数据X</span><br><span class="line">predictions = clf.predict(x_test)</span><br><span class="line"></span><br><span class="line"># classification_report(y_test, y_predict, target_names=None, labels=None) 同时生成真实值与预测值的精准率，召回率和F1得分，目标标签为target_name，对应的值为label</span><br><span class="line">print(classification_report(y_test, predictions))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn12.png" alt="12"></p><h2 id="Scikit-Learn多层感知机"><a href="#Scikit-Learn多层感知机" class="headerlink" title="Scikit-Learn多层感知机"></a><font size="4">Scikit-Learn多层感知机</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.neural_network.multilayer_perceptron import MLPClassifier</span><br><span class="line"></span><br><span class="line"># from sklearn.datasets import load_digits 从sklearn数据集中导入手写数字的数据</span><br><span class="line">digits = load_digits()</span><br><span class="line">x = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line"></span><br><span class="line"># clf = MLPClassifier(hidden_layer_sizes=(100,), alpha=0.0001, max_iter=200, random_state=20) 创建多层感知机模型，每一层的神经元个数为hidden_layer_sizes，正则化参数alpha默认为0.0001，最大迭代次数默认为200</span><br><span class="line">clf = MLPClassifier(hidden_layer_sizes=(150, 100), alpha=0.1, max_iter=500)</span><br><span class="line"></span><br><span class="line"># cross_val_score(estimator, X, y, n_jobs=None, cv=n) n折交叉验证，估计器为estimator，数据为X和y，同时工作的CPU个数为1</span><br><span class="line">print(cross_val_score(clf, x, y, n_jobs=-1, cv=5))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn13.png" alt="13"></p><h2 id="Scikit-LearnKmeans聚类"><a href="#Scikit-LearnKmeans聚类" class="headerlink" title="Scikit-LearnKmeans聚类"></a><font size="4">Scikit-LearnKmeans聚类</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">plt.subplot(3, 2, 1)</span><br><span class="line">x1 = np.array([1, 2, 3, 1, 5, 6, 5, 5, 6, 7, 8, 9, 7, 9])</span><br><span class="line">x2 = np.array([1, 3, 2, 2, 8, 6, 7, 6, 7, 1, 2, 1, 1, 3])</span><br><span class="line">x = np.vstack((x1, x2)).T</span><br><span class="line">plt.xlim([0, 10])</span><br><span class="line">plt.ylim([0, 10])</span><br><span class="line">plt.title('Instances')</span><br><span class="line">plt.scatter(x1, x2)</span><br><span class="line"></span><br><span class="line">colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'b']</span><br><span class="line">markers = ['o', 's', 'D', 'v', '^', 'p', '*', '+']</span><br><span class="line">tests = [2, 3, 4, 5, 8]</span><br><span class="line">subplot_counter = 1</span><br><span class="line"></span><br><span class="line">for t in tests:</span><br><span class="line">    subplot_counter +=1</span><br><span class="line">    plt.subplot(3, 2, subplot_counter)</span><br><span class="line"></span><br><span class="line">    # kmeans_model = KMeans(n_clusters=n) 创建Kmeans模型，类别个数为n</span><br><span class="line">    kmeans_model = KMeans(n_clusters=t)</span><br><span class="line"></span><br><span class="line">    # kmeans_model.fit(x) 用训练数据X拟合模型</span><br><span class="line">    kmeans_model.fit(x)</span><br><span class="line"></span><br><span class="line">    for i, l in enumerate(kmeans_model.labels_):</span><br><span class="line">        plt.plot(x1[i], x2[i], color=colors[l], marker=markers[l])</span><br><span class="line"></span><br><span class="line">    plt.xlim([0, 10])</span><br><span class="line">    plt.ylim([0, 10])</span><br><span class="line">    plt.title('k=%s' %t)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn14.png" alt="14"></p><h3 id="Scikit-LearnPCA降维"><a href="#Scikit-LearnPCA降维" class="headerlink" title="Scikit-LearnPCA降维"></a><font size="3">Scikit-LearnPCA降维</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line"></span><br><span class="line">data = load_iris()</span><br><span class="line">x = data.data</span><br><span class="line">y = data.target</span><br><span class="line"></span><br><span class="line"># pca = PCA(n_components=n) 创建n维PCA转换器</span><br><span class="line">pca = PCA(n_components=2)</span><br><span class="line"></span><br><span class="line"># pca.fit_transform(X) 在训练数据集进行PCA降维</span><br><span class="line">reduced_x = pca.fit_transform(x) </span><br><span class="line"></span><br><span class="line">red_x, red_y = [], []</span><br><span class="line">blue_x, blue_y = [], []</span><br><span class="line">green_x, green_y = [], []</span><br><span class="line"></span><br><span class="line">for i in range(len(reduced_x)):</span><br><span class="line">    if y[i] == 0:</span><br><span class="line">        red_x.append(reduced_x[i][0])</span><br><span class="line">        red_y.append(reduced_x[i][1])</span><br><span class="line">    elif y[i] == 1:</span><br><span class="line">        blue_x.append(reduced_x[i][0])</span><br><span class="line">        blue_y.append(reduced_x[i][1])</span><br><span class="line">    else:</span><br><span class="line">        green_x.append(reduced_x[i][0])</span><br><span class="line">        green_y.append(reduced_x[i][1])</span><br><span class="line"></span><br><span class="line">plt.scatter(red_x, red_y, c='r', marker='x')</span><br><span class="line">plt.scatter(blue_x, blue_y, c='b', marker='D')</span><br><span class="line">plt.scatter(green_x, green_y, c='g', marker='.')</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/sklearn15.png" alt="15"></p><h1 id="Scikit-Learn小结"><a href="#Scikit-Learn小结" class="headerlink" title="Scikit-Learn小结"></a><font size="5" color="red">Scikit-Learn小结</font></h1><p>  由于Scikit-Learn集成了许多常用的机器学习算法，如决策树，SVM，多层感知机，Kmeans等，可以让使用者节约大量的时间。而且其拥有很好的官方文档，让开发者，研究者可以方便的入门和使用。因此Scikit-Learn在机器学习领域受到广大使用者的喜爱。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Scikit-Learn&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="常用库" scheme="https://USTCcoder.github.io/categories/python/%E5%B8%B8%E7%94%A8%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>Numpy</title>
    <link href="https://USTCcoder.github.io/2019/08/12/library%20Numpy/"/>
    <id>https://USTCcoder.github.io/2019/08/12/library Numpy/</id>
    <published>2019-08-12T12:29:15.000Z</published>
    <updated>2020-07-27T06:13:03.307Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/LIBRARY/numpy.jpg" alt="0"></p><h1 id="Numpy介绍"><a href="#Numpy介绍" class="headerlink" title="Numpy介绍"></a><font size="5" color="red">Numpy介绍</font></h1><p>  NumPy是Python的一种开源的数值计算扩展，这种工具可用来存储和处理大型矩阵，专为进行严格的数字处理而产生。<br><a id="more"></a></p><h1 id="Numpy特点"><a href="#Numpy特点" class="headerlink" title="Numpy特点"></a><font size="5" color="red">Numpy特点</font></h1><p>  <font size="3">NumPy提供了一个N维数组类型ndarray，它描述了相同类型的的集合。</font><br>  <font size="3">numpy内置了并行运算功能，当系统有多个核心时，做某种计算时，numpy会自动做并行计算。</font><br>  <font size="3">Numpy底层使用C语言编写，内部解除了GIL（全局解释器锁），其对数组的操作速度不受Python解释器的限制，效率远高于纯Python代码。</font></p><h1 id="Numpy应用"><a href="#Numpy应用" class="headerlink" title="Numpy应用"></a><font size="5" color="red">Numpy应用</font></h1><h2 id="Numpy创建数组"><a href="#Numpy创建数组" class="headerlink" title="Numpy创建数组"></a><font size="4">Numpy创建数组</font></h2><h3 id="array方法"><a href="#array方法" class="headerlink" title="array方法"></a><font size="3">array方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># np.array(list) 将list转换为数组类型</span><br><span class="line">a = np.array([[1, 2, 3], [4, 5, 6]])</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy1.png" alt="1"></p><h3 id="zeros，ones，eye方法"><a href="#zeros，ones，eye方法" class="headerlink" title="zeros，ones，eye方法"></a><font size="3">zeros，ones，eye方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># np.zeros(shape, dtype=None) 生成形状为shape的全0数组(默认为float64)</span><br><span class="line">a = np.zeros((2,3))</span><br><span class="line"></span><br><span class="line"># np.ones(shape, dtype=None) 生成形状为shape的全1数组(默认为float64)</span><br><span class="line">b = np.ones((2,3))</span><br><span class="line"></span><br><span class="line"># np.zeros_like(array, dtype=None) 生成形状与array相同的全0数组(默认为float64)</span><br><span class="line">c = np.zeros_like(b)</span><br><span class="line"></span><br><span class="line"># np.ones_like(array, dtype=None) 生成形状与array相同的全1数组(默认为float64)</span><br><span class="line">d = np.ones_like(a)</span><br><span class="line"></span><br><span class="line"># np.eye(m, n, k=0) 生成m行n列的单位矩阵，n默认等于m，k为上下的偏移量，默认为0(默认为float64)</span><br><span class="line">e = np.eye(3, 4, 1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy3.png" alt="3"></p><h3 id="arange方法"><a href="#arange方法" class="headerlink" title="arange方法"></a><font size="3">arange方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># np.arange(start, stop, step, dtype=None) 生成start到stop，步长为step的数组</span><br><span class="line">a = np.arange(10, 20, 2)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy4.png" alt="4"></p><h3 id="linspace，logspace方法"><a href="#linspace，logspace方法" class="headerlink" title="linspace，logspace方法"></a><font size="3">linspace，logspace方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># np.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None) 将start到stop等分成num个点，endpoint=True代表包括stop</span><br><span class="line">a = np.linspace(10, 20, 6)</span><br><span class="line"></span><br><span class="line"># np.logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None) 将start到stop等分成num个点，每一个点i的值为base的i次幂</span><br><span class="line">b = np.logspace(1, 2, 10)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy5.png" alt="5"></p><h3 id="random方法"><a href="#random方法" class="headerlink" title="random方法"></a><font size="3">random方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># np.random.seed(n) 设定随机种子，以备后面的结果可以复现</span><br><span class="line">np.random.seed(1)</span><br><span class="line"></span><br><span class="line"># np.random.randint(low, high, shape=None) 产生形状为shape的分布区间为[low, high)的随机整数</span><br><span class="line">a = np.random.randint(0, 10, (3, 3))</span><br><span class="line"></span><br><span class="line"># np.random.rand(shape=None) 产生形状为shape的[0-1)均匀随机数</span><br><span class="line">b = np.random.rand(3,3)</span><br><span class="line"></span><br><span class="line"># np.random.uniform(low=0.0, high=1.0, shape=None) 产生形状为shape的分布区间为[low, high)的均匀随机数</span><br><span class="line">c = np.random.uniform(0, 10, (3, 3))</span><br><span class="line"></span><br><span class="line"># np.random.randn(shape=None) 产生形状为shape的均值为0，方差为1的高斯随机数</span><br><span class="line">d = np.random.rand(3,3)</span><br><span class="line"></span><br><span class="line"># np.random.normal(loc=0.0, scale=1.0, shape=None) 产生形状为shape的均值为loc，标准差为scale的高斯随机数</span><br><span class="line">e = np.random.normal(5, 5, (3,3))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy6.png" alt="6"></p><h2 id="Numpy属性"><a href="#Numpy属性" class="headerlink" title="Numpy属性"></a><font size="4">Numpy属性</font></h2><h3 id="ndim，shape，size，dtype属性"><a href="#ndim，shape，size，dtype属性" class="headerlink" title="ndim，shape，size，dtype属性"></a><font size="3">ndim，shape，size，dtype属性</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line"></span><br><span class="line"># obj.ndim 查看对象的维度</span><br><span class="line">a.ndim</span><br><span class="line"></span><br><span class="line"># obj.shape 查看对象的形状</span><br><span class="line">a.shape</span><br><span class="line"></span><br><span class="line"># obj.size 查看对象的元素个数</span><br><span class="line">a.size</span><br><span class="line"></span><br><span class="line"># obj.dtype 查看对象的类型(整数默认为int32，浮点数默认为float64)</span><br><span class="line">a.dtype</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy2.png" alt="2"></p><h2 id="Numpy切片与索引"><a href="#Numpy切片与索引" class="headerlink" title="Numpy切片与索引"></a><font size="4">Numpy切片与索引</font></h2><h3 id="冒号-方法"><a href="#冒号-方法" class="headerlink" title=":(冒号)方法"></a><font size="3">:(冒号)方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(12).reshape((3, 4))</span><br><span class="line"></span><br><span class="line"># obj[start:stop:step, ...]，指在某一维度上，从start开始，到stop结束，不包括stop，间隔为step，start省略为从第一个元素开始，stop省略为到最后一个元素结束，step省略为间隔为1</span><br><span class="line">b = a[1:, 0:4:2]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy8.png" alt="8"></p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a><font size="3"><a href="列表"></a>方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(12).reshape((3, 4))</span><br><span class="line"></span><br><span class="line"># obj[[x1, x2, ...], ...]，指在某一维度上，取出x1,x2,...所在位置的元素</span><br><span class="line">b = a[[[1, 1], [2, 2]], [[0, 2], [0, 2]]]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy9.png" alt="9"></p><h3 id="nonzero方法"><a href="#nonzero方法" class="headerlink" title="nonzero方法"></a><font size="3">nonzero方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.random.randint(0, 3, (3, 3))</span><br><span class="line"></span><br><span class="line"># np.nonzero(obj) 返回obj中非0的索引</span><br><span class="line">b = a &gt; 1</span><br><span class="line">c = np.nonzero(a)</span><br><span class="line">d = np.nonzero(b)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy34.png" alt="34"></p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a><font size="3"><a href="逻辑值索引"></a>方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(12).reshape((3, 4))</span><br><span class="line"></span><br><span class="line"># obj1[obj1 op obj2] 返回obj1对obj2操作后值为True的值，并用一维数组保存</span><br><span class="line">b = a[a &gt; 5]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy26.png" alt="26"></p><h3 id="ix-方法"><a href="#ix-方法" class="headerlink" title="ix_方法"></a><font size="3">ix_方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(12).reshape((3, 4))</span><br><span class="line"></span><br><span class="line"># obj[np.ix_(array1, array2, ...)] 按照array1,array2,...的顺序取出元素</span><br><span class="line">b = a[np.ix_([1, 2], [0, 2])]</span><br><span class="line"></span><br><span class="line"># 与列表操作对比</span><br><span class="line">c = a[[1, 2], [0, 2]]</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy10.png" alt="10"></p><h3 id="split方法"><a href="#split方法" class="headerlink" title="split方法"></a><font size="3">split方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(12).reshape((2, 6))</span><br><span class="line"></span><br><span class="line"># np.split(obj, indices, axis=0) 将obj按照axis的方向(0代表横向，1代表纵向)切分。indices为整数指平均切分成indices份，为列表指按照列表进行切分。</span><br><span class="line">b = np.split(a, 3, axis=1)</span><br><span class="line">c = np.split(a, [1, 3], axis=1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy11.png" alt="11"></p><h2 id="Numpy插入，连接与删除"><a href="#Numpy插入，连接与删除" class="headerlink" title="Numpy插入，连接与删除"></a><font size="4">Numpy插入，连接与删除</font></h2><h3 id="append方法"><a href="#append方法" class="headerlink" title="append方法"></a><font size="3">append方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(9).reshape((3, 3))</span><br><span class="line"></span><br><span class="line"># np.append(obj, array, axis=None) axis=None时，将obj展开为一维数组，然后再与array连接，axis=0，在纵向连接在下方，axis=1，在横向连接在右方</span><br><span class="line">b = np.append(a, [[9], [10], [11]])</span><br><span class="line">c = np.append(a, [[9,10,11]], axis=0)</span><br><span class="line">d = np.append(a, [[9], [10], [11]], axis=1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy12.png" alt="12"></p><h3 id="insert方法"><a href="#insert方法" class="headerlink" title="insert方法"></a><font size="3">insert方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(9).reshape((3, 3))</span><br><span class="line"></span><br><span class="line"># np.insert(obj, index, array, axis=None) axis的用法同append一样，多了index项，可以插入到任意位置,且位数不相同时可以进行广播操作</span><br><span class="line">b = np.insert(a, 2, 9)</span><br><span class="line">c = np.insert(a, 2, 9, axis=0)</span><br><span class="line">d = np.insert(a, 2, 9, axis=1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy13.png" alt="13"></p><h3 id="concatenate方法"><a href="#concatenate方法" class="headerlink" title="concatenate方法"></a><font size="3">concatenate方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([[1, 2], [3, 4]])</span><br><span class="line">b = np.array([[5, 6], [7, 8]])</span><br><span class="line"></span><br><span class="line"># np.concatenate((obj1, obj2, ...), axis=0) 在axis轴上连接两个数组，默认为axis=0</span><br><span class="line">c = np.concatenate((a, b))</span><br><span class="line">d = np.concatenate((a, b), axis=1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy15.png" alt="15"></p><h3 id="stack方法"><a href="#stack方法" class="headerlink" title="stack方法"></a><font size="3">stack方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([[1, 2], [3, 4]])</span><br><span class="line">b = np.array([[5, 6], [7, 8]])</span><br><span class="line"></span><br><span class="line"># np.stack((obj1, obj2, ...), axis=0) 在axis维度上新建一个轴，并在此轴上连接数组，默认为axis=0</span><br><span class="line">c = np.stack((a, b))</span><br><span class="line">d = np.stack((a, b), axis=1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy16.png" alt="16"></p><h3 id="delete方法"><a href="#delete方法" class="headerlink" title="delete方法"></a><font size="3">delete方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(9).reshape((3, 3))</span><br><span class="line"></span><br><span class="line"># np.delete(obj, index, axis=None) axis的用法同append一样，多了index项，可以删除任意位置的元素</span><br><span class="line">b = np.delete(a, 1)</span><br><span class="line">c = np.delete(a, 1, axis=0)</span><br><span class="line">d = np.delete(a, 1, axis=1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy14.png" alt="14"></p><h2 id="Numpy广播与复制"><a href="#Numpy广播与复制" class="headerlink" title="Numpy广播与复制"></a><font size="4">Numpy广播与复制</font></h2><h3 id="broadcast-to方法"><a href="#broadcast-to方法" class="headerlink" title="broadcast_to方法"></a><font size="3">broadcast_to方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([[1, 2, 3]])</span><br><span class="line">b = np.array([[1], [2], [3]])</span><br><span class="line">c = np.array([1])</span><br><span class="line"></span><br><span class="line"># np.broadcast_to(obj, shape) 将obj广播至shape形状</span><br><span class="line"># 原理是如果obj的维度数小于shape，则在对应维度位置上补1，扩展成相同维度，然后将所有的1维度复制i次，i为shape中相应的维度</span><br><span class="line">d = np.broadcast_to(a, (3, 3)) # a的维度为(1, 3),维度与(3, 3)数量相同，则将1复制3次，变成(3, 3)</span><br><span class="line">e = np.broadcast_to(b, (3, 3)) # b的维度为(3, 1),维度与(3, 3)数量相同，则将1复制3次，变成(3, 3)</span><br><span class="line">f = np.broadcast_to(c, (3, 3)) # c的维度为(1),维度与(3, 3)数量不相同，则在对应维度位置上补1，扩展成相同维度，变成(1, 1)，然后将1复制3次，变成(3, 3)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy17.png" alt="17"></p><h3 id="tile方法"><a href="#tile方法" class="headerlink" title="tile方法"></a><font size="3">tile方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a=np.array([[1, 2], [3, 4]])</span><br><span class="line"></span><br><span class="line"># np.tile(obj, (m, n, ...)) 将obj的维度复制(m, n, ...)次</span><br><span class="line">b = np.tile(a, (2, 3))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy18.png" alt="18"></p><h2 id="Numpy改变数组形状"><a href="#Numpy改变数组形状" class="headerlink" title="Numpy改变数组形状"></a><font size="4">Numpy改变数组形状</font></h2><h3 id="reshape方法"><a href="#reshape方法" class="headerlink" title="reshape方法"></a><font size="3">reshape方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a=np.array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line"></span><br><span class="line"># obj.reshape(shape) 将obj的形状改变为shape</span><br><span class="line">b = a.reshape((3, 2))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy7.png" alt="7"></p><h3 id="resize方法"><a href="#resize方法" class="headerlink" title="resize方法"></a><font size="3">resize方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(9).reshape(3, 3)</span><br><span class="line"></span><br><span class="line"># np.resize(obj, shape) 将obj的大小调整为shape，先按顺序读取，少则从头补入数据，多则删除多余数据</span><br><span class="line">b = np.resize(a, (2, 2))</span><br><span class="line">c = np.resize(a, (4, 4))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy19.png" alt="19"></p><h3 id="T-transpose-方法"><a href="#T-transpose-方法" class="headerlink" title="T(transpose)方法"></a><font size="3">T(transpose)方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line"></span><br><span class="line"># obj.T 将obj转置，等价于np.transpose(obj)</span><br><span class="line">b = a.T</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy20.png" alt="20"></p><h3 id="swapaxes方法"><a href="#swapaxes方法" class="headerlink" title="swapaxes方法"></a><font size="3">swapaxes方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(8).reshape(2, 2, 2)</span><br><span class="line"></span><br><span class="line"># np.swapaxes(obj, axis1, axis2) 交换obj的两个轴</span><br><span class="line">b = np.swapaxes(a, 0, 2)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy21.png" alt="21"></p><h3 id="expand-dims方法"><a href="#expand-dims方法" class="headerlink" title="expand_dims方法"></a><font size="3">expand_dims方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([1, 2, 3])</span><br><span class="line"></span><br><span class="line"># np.expand_dims(obj, axis) 在指定axis插入一个新的轴</span><br><span class="line">b = np.expand_dims(a, 0)</span><br><span class="line">c = np.expand_dims(a, 1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy22.png" alt="22"></p><h3 id="squeeze方法"><a href="#squeeze方法" class="headerlink" title="squeeze方法"></a><font size="3">squeeze方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([[1, 2, 3]])</span><br><span class="line">b = np.array([[1], [2], [3]])</span><br><span class="line"></span><br><span class="line"># np.squeeze(obj, axis) 在指定axis删除轴，如果该轴的大小不为1，则无法删除报错</span><br><span class="line">c = np.squeeze(a, 0)</span><br><span class="line">d = np.squeeze(b, 1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy23.png" alt="23"></p><h3 id="ravel，flatten方法"><a href="#ravel，flatten方法" class="headerlink" title="ravel，flatten方法"></a><font size="3">ravel，flatten方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(9).reshape(3, 3)</span><br><span class="line">b = np.arange(9).reshape(3, 3)</span><br><span class="line"></span><br><span class="line"># obj.ravel(order='C') 将obj展平为一位数组，且修改按行展平后的数据，原数据受到改变。order='C'(按行展平元素)，'F' (按列展平元素)，'A' (按原顺序展平元素)，'K'(按内存中的出现顺序展平元素)</span><br><span class="line">c = a.ravel()</span><br><span class="line">d = a.ravel('F')</span><br><span class="line">c[1] += 1</span><br><span class="line"></span><br><span class="line"># obj.flatten(order='C') 将obj展平为一位数组，且修改按行展平后的数据，原数据不受到改变。order用法同ravel</span><br><span class="line">e = b.flatten()</span><br><span class="line">e[1] += 1</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy24.png" alt="24"></p><h2 id="Numpy数学运算"><a href="#Numpy数学运算" class="headerlink" title="Numpy数学运算"></a><font size="4">Numpy数学运算</font></h2><h3 id="运算符方法"><a href="#运算符方法" class="headerlink" title="运算符方法"></a><font size="3">运算符方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(2, 10, 2).reshape(2, 2)</span><br><span class="line">b = np.arange(1, 5).reshape(2, 2)</span><br><span class="line"></span><br><span class="line"># 算术运算(+，-，*，/，//，%，**，&gt;，&gt;=，&lt;，&lt;=，==，!=，&amp;，|，^，~，&gt;&gt;，&lt;&lt;)要求两个数组具有同样的形状或者可广播为同样形状，逻辑运算返回值为True或者False</span><br><span class="line">c = a + b</span><br><span class="line">d = a - b</span><br><span class="line">e = a * b</span><br><span class="line">f = a / b</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy25.png" alt="25"></p><h3 id="特殊值，对数函数，三角函数方法"><a href="#特殊值，对数函数，三角函数方法" class="headerlink" title="特殊值，对数函数，三角函数方法"></a><font size="3">特殊值，对数函数，三角函数方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 特殊值(π，e，∞，Nan)</span><br><span class="line">a = np.pi / 2</span><br><span class="line">b = np.e</span><br><span class="line">c = np.inf</span><br><span class="line">d = np.nan</span><br><span class="line"></span><br><span class="line"># np.log(obj)，np.log2(obj)，np.log10(obj) 对obj求以e为底，2为底，10为底的对数</span><br><span class="line">e = np.log([b, 2, 10])</span><br><span class="line">f = np.log2([b, 2, 10])</span><br><span class="line">g = np.log10([b, 2, 10])</span><br><span class="line"></span><br><span class="line"># np.sin(obj)，np.arcsin(obj) 对obj求sin值和arcsin值，所用的为弧度值</span><br><span class="line">h = np.sin(a)</span><br><span class="line">i = np.arcsin(h)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy27.png" alt="27"></p><h3 id="around，floor，ceil方法"><a href="#around，floor，ceil方法" class="headerlink" title="around，floor，ceil方法"></a><font size="3">around，floor，ceil方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([3.33, 5.55, 88.88])</span><br><span class="line"></span><br><span class="line"># np.around(obj, decimals=0) 四舍五入操作，decimals大于0，四舍五入到小数点右侧，小于0，四舍五入到小数点左侧</span><br><span class="line">b = np.around(a)</span><br><span class="line">c = np.around(a, 1)</span><br><span class="line">d = np.around(a, -1)</span><br><span class="line"></span><br><span class="line"># np.floor(obj) 向下取整</span><br><span class="line">e = np.floor(a)</span><br><span class="line"></span><br><span class="line"># np.ceil(obj) 向上取整</span><br><span class="line">f = np.ceil(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy28.png" alt="28"></p><h3 id="sum，cumsum，cumprod方法"><a href="#sum，cumsum，cumprod方法" class="headerlink" title="sum，cumsum，cumprod方法"></a><font size="3">sum，cumsum，cumprod方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.random.randint(0, 5, (3, 3))</span><br><span class="line"></span><br><span class="line"># np.sum(obj, axis=None) 将obj元素累加，axis=None代表全局，axis=0代表每列，axis=1代表每行</span><br><span class="line">b = np.sum(a)</span><br><span class="line"></span><br><span class="line"># np.cumsum(obj, axis=None) 将obj逐项累加，axis用法同np.sum</span><br><span class="line">c = np.cumsum(a)</span><br><span class="line">d = np.cumsum(a, 0)</span><br><span class="line"></span><br><span class="line"># np.cumprod(obj, axis=None) 将obj逐项累乘，axis用法同np.sum</span><br><span class="line">e = np.cumprod(a)</span><br><span class="line">f = np.cumprod(a, 1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy32.png" alt="32"></p><h3 id="ptp方法"><a href="#ptp方法" class="headerlink" title="ptp方法"></a><font size="3">ptp方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.random.randint(0, 5, (3, 3))</span><br><span class="line"></span><br><span class="line"># np.ptp(obj, axis=None) 计算axis轴上最大值减最小值的结果，axis用法同np.sum</span><br><span class="line">b = np.ptp(a)</span><br><span class="line">c = np.ptp(a, 0)</span><br><span class="line">d = np.ptp(a, 1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy33.png" alt="33"></p><h3 id="diff方法"><a href="#diff方法" class="headerlink" title="diff方法"></a><font size="3">diff方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.random.randint(0, 10, (5, 5))</span><br><span class="line"></span><br><span class="line"># np.diff(obj, n, axis) 计算n阶差分运算，axis默认为最后一个维度</span><br><span class="line">b = np.diff(a, 1)</span><br><span class="line">c = np.diff(a, 2)</span><br><span class="line">d = np.diff(a, 1, 0)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy36.png" alt="36"></p><h3 id="clip方法"><a href="#clip方法" class="headerlink" title="clip方法"></a><font size="3">clip方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.random.randint(0, 10, (5, 5))</span><br><span class="line"></span><br><span class="line"># np.clip(obj, min_, max_) 将obj中小于min_的值赋值为min_，将obj中大于max_的值赋值为max_</span><br><span class="line">b = np.clip(a, 3, 7)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy37.png" alt="37"></p><h2 id="Numpy数理统计"><a href="#Numpy数理统计" class="headerlink" title="Numpy数理统计"></a><font size="4">Numpy数理统计</font></h2><h3 id="unique方法"><a href="#unique方法" class="headerlink" title="unique方法"></a><font size="3">unique方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.random.randint(0, 10, (5, 5))</span><br><span class="line"></span><br><span class="line"># np.unique(obj, return_index, return_inverse, return_counts) 统计得到不重复元素，如果不是一维数组，会将数组展开。return_index=True返回不重复元素在原数组中第一次出现的索引，return_inverse=返回原数组在不重复元素中的索引, return_counts=返回每个不重复元素在原数组中出现的次数</span><br><span class="line">b, index, inverse, counts = np.unique(a, True, True, True)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy29.png" alt="29"></p><h3 id="any，all方法"><a href="#any，all方法" class="headerlink" title="any，all方法"></a><font size="3">any，all方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.random.randint(0, 10, (5, 5))</span><br><span class="line"></span><br><span class="line"># np.any(obj) 判断obj中是否存在True</span><br><span class="line">b = np.any(a &gt; 5)</span><br><span class="line">c = np.any(a &gt; 10)</span><br><span class="line"></span><br><span class="line"># np.all(obj) 判断obj中是否全都是True</span><br><span class="line">d = np.all(a &gt;= 0)</span><br><span class="line">e = np.all(a &gt; 5)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy35.png" alt="35"></p><h3 id="统计方法"><a href="#统计方法" class="headerlink" title="统计方法"></a><font size="3">统计方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.random.randint(0, 10, (5, 5))</span><br><span class="line"></span><br><span class="line"># np.amax(obj, axis=None)，np.amin(obj, axis=None) 统计最大最小值，axis用法同np.sum</span><br><span class="line">np.amax(a)</span><br><span class="line">np.amax(a,0)</span><br><span class="line">np.amax(a,1)</span><br><span class="line"></span><br><span class="line"># np.argmax(obj, axis=None)，np.argmin(obj, axis=None) 统计最大最小值的索引，axis用法同np.sum</span><br><span class="line">np.argmax(a)</span><br><span class="line">np.argmax(a,0)</span><br><span class="line">np.argmax(a,1)</span><br><span class="line"></span><br><span class="line"># np.mean(obj, axis=None) 统计中位数，axis用法同np.sum</span><br><span class="line">np.median(a)</span><br><span class="line"></span><br><span class="line"># np.mean(obj, axis=None) 统计均值，axis用法同np.sum</span><br><span class="line">np.mean(a)</span><br><span class="line"></span><br><span class="line"># np.mean(obj, axis=None, weights=None) 统计加权平均值，axis用法同np.sum</span><br><span class="line">np.average(a, None, a)</span><br><span class="line"></span><br><span class="line"># np.mean(obj, axis=None) 统计标准差，axis用法同np.sum</span><br><span class="line">np.std(a)</span><br><span class="line"></span><br><span class="line"># np.mean(obj, axis=None) 统计方差，axis用法同np.sum</span><br><span class="line">np.var(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy30.png" alt="30"></p><h3 id="sort，argsort方法"><a href="#sort，argsort方法" class="headerlink" title="sort，argsort方法"></a><font size="3">sort，argsort方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.random.randint(0, 5, (3, 3))</span><br><span class="line"></span><br><span class="line"># np.sort(obj, axis, kind='quicksort', order=None) 对obj进行从小到达排序，axis为指定要排序的轴，axis=0按列排序，axis=1按行排序。kind为排序算法，可以选择'quicksort'(快速排序，默认)，'mergesort'(归并排序)，'heapsort'(堆排序)，order为要排序的字段，一般数学运算不用</span><br><span class="line">b = np.sort(a, 0)</span><br><span class="line">c = np.sort(a, 1)</span><br><span class="line"></span><br><span class="line"># np.argsort(obj, axis, kind='quicksort', order=None) 返回排序后的数组在原数组的索引，用法同sort</span><br><span class="line">d = np.argsort(a, 0)</span><br><span class="line">e = np.argsort(a, 1)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy31.png" alt="31"></p><h2 id="Numpy线性代数"><a href="#Numpy线性代数" class="headerlink" title="Numpy线性代数"></a><font size="4">Numpy线性代数</font></h2><h3 id="dot，matmul方法"><a href="#dot，matmul方法" class="headerlink" title="dot，matmul方法"></a><font size="3">dot，matmul方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(6).reshape(2, 3)</span><br><span class="line">b = np.arange(6).reshape(3, 2)</span><br><span class="line"></span><br><span class="line"># np.dot(obj1, obj2) 等价于np.matmul(obj1, obj2)，矩阵乘法</span><br><span class="line">c = np.dot(a, b)</span><br><span class="line">d = np.matmul(a, b)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy38.png" alt="38"></p><h3 id="det，eig，pinv方法"><a href="#det，eig，pinv方法" class="headerlink" title="det，eig，pinv方法"></a><font size="3">det，eig，pinv方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([[-2, 1, 1], [0, 2, 0], [-4, 1, 3]])</span><br><span class="line"></span><br><span class="line"># np.linalg.det(obj) 计算obj的行列式</span><br><span class="line">b = np.linalg.det(a)</span><br><span class="line"></span><br><span class="line"># np.linalg.eig(obj) 计算obj的特征值和特征向量</span><br><span class="line">c = np.linalg.eig(a)</span><br><span class="line"></span><br><span class="line"># np.linalg.pinv(obj) 计算obj的伪逆矩阵</span><br><span class="line">d = np.linalg.pinv(a)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy39.png" alt="39"></p><h3 id="solve方法"><a href="#solve方法" class="headerlink" title="solve方法"></a><font size="3">solve方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([[2, 2, -1], [1, -2, 4], [5, 8, -1]])</span><br><span class="line">b = np.array([[6], [3], [27]])</span><br><span class="line"></span><br><span class="line"># np.linalg.solve(A, b) 求线性方程组Ax = b的解</span><br><span class="line">c = np.linalg.solve(a, b)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy31.png" alt="31"></p><h2 id="Numpy数据保存"><a href="#Numpy数据保存" class="headerlink" title="Numpy数据保存"></a><font size="4">Numpy数据保存</font></h2><h3 id="save，load方法"><a href="#save，load方法" class="headerlink" title="save，load方法"></a><font size="3">save，load方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(12).reshape(3, 4)</span><br><span class="line"></span><br><span class="line"># np.save(filename, obj) 将obj保存在文件名为filename的.npy文件中(可以不加.npy扩展名)</span><br><span class="line">np.save('save1', a)</span><br><span class="line"></span><br><span class="line"># np.load(filename) 读取文件名为filename的数组数据(要加.npy扩展名)</span><br><span class="line">b = np.load('save1.npy')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy41.png" alt="41"></p><h3 id="savez方法"><a href="#savez方法" class="headerlink" title="savez方法"></a><font size="3">savez方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(12).reshape(3, 4)</span><br><span class="line">b = np.arange(6).reshape(2, 3)</span><br><span class="line"></span><br><span class="line"># np.savez(filename, name1=obj1, name2=obj2, ...) 将多个数据保存在文件名为filename的.npz文件中(可以不加.npz扩展名)，obj1的变量名为name1，obj2的变量名为name2，……(变量的默认名称为arr_0，arr_1，……)</span><br><span class="line">np.savez('save2', no_1=a, no_2=b)</span><br><span class="line">np.savez('save3', a, b)</span><br><span class="line"></span><br><span class="line"># np.load(filename) 读取文件名为filename的数组数据(要加.npz扩展名)</span><br><span class="line">c = np.load('save2.npz')</span><br><span class="line">d = np.load('save3.npz')</span><br><span class="line"></span><br><span class="line"># 提取数据时要使用数组的名称</span><br><span class="line">e = c['no_1']</span><br><span class="line">f = c['no_2']</span><br><span class="line">g = d['arr_0']</span><br><span class="line">h = d['arr_1']</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy42.png" alt="42"></p><h3 id="savetxt，loadtxt方法"><a href="#savetxt，loadtxt方法" class="headerlink" title="savetxt，loadtxt方法"></a><font size="3">savetxt，loadtxt方法</font></h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.arange(12).reshape(3, 4)</span><br><span class="line">b = np.array([1,2,3,4])</span><br><span class="line">c = np.array([5,6,7,8])</span><br><span class="line"></span><br><span class="line"># np.savetxt(filename, obj, fmt='%f', delimiter=' ') 将多个一维数据(相同大小)或者一个二维维数据保存在文件名为filename的.txt文件中(要加.txt扩展名)，格式为fmt(默认为浮点型)，分隔符为delimiter(默认为' ')，保存时会将多个一维数组转化成一个二维数组</span><br><span class="line">np.savetxt('save4.txt', a)</span><br><span class="line">np.savetxt('save5.txt', (b, c))</span><br><span class="line"></span><br><span class="line"># np.loadtxt(filename, dtype='float', delimiter=' ') 读取文件名为filename的数组数据(要加.txt扩展名)，类型为dtypefloat(默认为浮点型)，分隔符为delimiter(默认为' ')</span><br><span class="line">d = np.loadtxt('save4.txt')</span><br><span class="line">e = np.loadtxt('save5.txt')</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/LIBRARY/numpy43.png" alt="43"></p><h1 id="Numpy小结"><a href="#Numpy小结" class="headerlink" title="Numpy小结"></a><font size="5" color="red">Numpy小结</font></h1><p>  由于numpy支持各种矩阵运算，且运算效率非常高，因此numpy库广泛应用于数据分析，机器学习，深度学习等各个领域，其作为机器学习三剑客之一，也受到广大使用者的喜爱。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Numpy&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="python" scheme="https://USTCcoder.github.io/categories/python/"/>
    
      <category term="常用库" scheme="https://USTCcoder.github.io/categories/python/%E5%B8%B8%E7%94%A8%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>线性规划(Linear Programming)</title>
    <link href="https://USTCcoder.github.io/2019/08/08/algorithm%20linear%20programming/"/>
    <id>https://USTCcoder.github.io/2019/08/08/algorithm linear programming/</id>
    <published>2019-08-08T12:09:25.000Z</published>
    <updated>2019-08-07T15:58:18.760Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">线性规划</font></strong></center><p></p><h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a><font size="5" color="red">原理介绍</font></h1><p>   <strong>Linear Programming:线性规划</strong>，是<strong>运筹学</strong>中的一个重要分支，<strong>研究线性约束条件下线性目标函数的极值问题</strong>的数学理论和方法，能从各种限制条件的组合中，选择出最为合理的计算方法，建立线性规划模型，从而求得最佳结果。广泛应用于军事作战、经济分析、经营管理和工程技术等方面。为合理地利用有限的人力、物力、财力等资源作出的<strong>最优决策</strong>，提供科学的依据。<br><a id="more"></a></p><h1 id="算法基础"><a href="#算法基础" class="headerlink" title="算法基础"></a><font size="5" color="red">算法基础</font></h1><h2 id="线性规划标准型"><a href="#线性规划标准型" class="headerlink" title="线性规划标准型"></a><font size="4">线性规划标准型</font></h2><p>  <font size="3">对于复杂的线性规划问题，很难采用初中数学的画图法解决，一般要把问题转化为线性规划标准型。</font><br><img src="/images/ALGORITHM/line1.png" alt="1"></p><h2 id="线性规划标准型转化方法"><a href="#线性规划标准型转化方法" class="headerlink" title="线性规划标准型转化方法"></a><font size="4">线性规划标准型转化方法</font></h2><p>  <font size="3">(1)一般线性规划形式中目标函数如果求最小值，即$\min z = \sum<em>{i=1}^n c_ix_i$，那么令$z’ = -z$，则求解$\max z’ = \sum</em>{i=1}^n c_ix_i$，得到最优解后加负号即可。</font><br>  <font size="3">(2)右端常数项小于零时，则不等式两边同时乘-1，将其变为大于零，并改变不等式方向，保证恒等变形。</font><br>  <font size="3">(3)约束条件大于等于约束时，则在不等式左边减去一个新的非负变量，将不等式约束改为等式约束。</font><br>  <font size="3">(4)约束条件小于等于约束时，则在不等式左边加上一个新的非负变量，将不等式约束改为等式约束。</font><br>  <font size="3">(5)无约束的决策变量x，则引入两个新的非负变量x’，x’’，令$x=x’-x’’, \ x’ \ge 0, \ x’’ \ge 0$，将x’，x’’带入模型</font><br>  <font size="3">(6)决策变量x小于等于0时，令x’=-x，将x’带入模型</font></p><script type="math/tex; mode=display">\min z = x_2 - 3 \ x_3 + 2 \ x_4</script><script type="math/tex; mode=display">\begin{cases} x_1 + 3 \ x_2 - x_3 + 2 \ x_4 =7 \\ -2 \ x_2 + 4 \ x_3 \le 12 \\ -4 \ x_2 + 3 \ x_3 + 8 \ x_4 \le 10 \\ x_i \ge 0 \ (i = 1, \ 2, \ 3, \ 4) \end{cases}</script><p>  <font size="3">将其转化为线性规划标准型:z’=-z</font></p><script type="math/tex; mode=display">\min z' = -x_2 + 3 \ x_3 - 2 \ x_4</script><script type="math/tex; mode=display">\begin{cases} x_1 + 3 \ x_2 - x_3 + 2 \ x_4 =7 \\ -2 \ x_2 + 4 \ x_3 + x_5 = 12 \\ -4 \ x_2 + 3 \ x_3 + 8 \ x_4 + x_6 = 10 \\ x_i \ge 0 \ (i = 1, \ 2, \ 3, \ 4, \ 5, \ 6) \end{cases}</script><h2 id="单纯行算法"><a href="#单纯行算法" class="headerlink" title="单纯行算法"></a><font size="4">单纯行算法</font></h2><p>  <font size="3">基本变量：每个约束条件中的系数为正且只出现在一个约束条件中的变量</font><br>  <font size="3">非基本变量：除基本变量外的变量全部为非基本变量</font><br>  <font size="3">基本可行解：满足标准形式约束条件的可行解称为基本可行解</font><br>  <font size="3">检验数：目标函数中非基本变量的系数</font></p><h2 id="最优解的判别"><a href="#最优解的判别" class="headerlink" title="最优解的判别"></a><font size="4">最优解的判别</font></h2><p>  <font size="3">(1)若目标函数中关于非基本变量的所有系数小于等于0，则当前基本可行解就是最优解。</font><br>  <font size="3">(2)若目标函数中关于非基本变量的所有系数小于等于0，同时存在某个非基本变量的检验数等于0，则线性规划问题有无穷多个最优解。</font><br>  <font size="3">(3)如果某个非基本变量的系数大于0，而该变量对应的列向量的各个分量都小于等于0，则该线性规划问题有无界解。</font></p><h1 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a><font size="5" color="red">算法步骤</font></h1><h2 id="建立初始单纯形表"><a href="#建立初始单纯形表" class="headerlink" title="建立初始单纯形表"></a><font size="4">建立初始单纯形表</font></h2><p>  <font size="3">(1)从构建出的线性规划标准型中找出基本变量和非基本变量，且让目标函数由非基本变量表示。</font><br>  <font size="3">(2)基本变量的系数要缩放到1，基本变量做列，非基本变量做行。</font><br>  <font size="3">(2)检验数放第一行，常数项放第一列，约束条件中的非基本变量的系数作为值，构造初始单纯形表。</font><br><img src="/images/ALGORITHM/line2.png" alt="2"></p><h2 id="根据单纯形表判断是否得到最优解"><a href="#根据单纯形表判断是否得到最优解" class="headerlink" title="根据单纯形表判断是否得到最优解"></a><font size="4">根据单纯形表判断是否得到最优解</font></h2><p>  <font size="3">(1)如果所有的检验数都小于等于0，则已获得最优解，算法结束，取出左上角的值即为最优解。</font><br>  <font size="3">(2)如果所有的检验数有些为正数，但其中某一正的检验数对应的列向量的所有分量均小于等于0，则线性规划问题无解，算法结束。</font><br>  <font size="3">(3)如果所有的检验数有些为正数，但其中某一正的检验数对应的列向量中有正的分量，则继续下一步。</font></p><h2 id="选择入基变量"><a href="#选择入基变量" class="headerlink" title="选择入基变量"></a><font size="4">选择入基变量</font></h2><p>  <font size="3">选取检验数中最大的一个，其对应的非基本变量称为入基变量，该列称为入基列</font></p><h2 id="选择离基变量"><a href="#选择离基变量" class="headerlink" title="选择离基变量"></a><font size="4">选择离基变量</font></h2><p>  <font size="3">选取常数列元素与入基列元素的比值中，正数的最小者所对应的基本变量为离基变量。</font></p><h2 id="换基变换"><a href="#换基变换" class="headerlink" title="换基变换"></a><font size="4">换基变换</font></h2><p>  <font size="3">将单纯形表上的入基变量和离基变量互换位置。</font><br><img src="/images/ALGORITHM/line3.png" alt="3"></p><h2 id="计算新的单纯形表"><a href="#计算新的单纯形表" class="headerlink" title="计算新的单纯形表"></a><font size="4">计算新的单纯形表</font></h2><p>  <font size="3">入基列=-原值/交叉位值。</font><br>  <font size="3">离基行=原值/交叉位值。</font><br>  <font size="3">交叉位=原值去倒数。</font><br>  <font size="3">左上角值=原值+同行入基列元素值*同列离基行元素值/交叉位值。</font></p><p><img src="/images/ALGORITHM/line4.png" alt="4"></p><p>  <font size="3">其余值=原值-同行入基列元素值*同列离基行元素值/交叉位值。</font></p><p><img src="/images/ALGORITHM/line5.png" alt="5"></p><p>  <font size="3">得到新的单纯形表再返回第二步重新判断。直到满足终止条件。</font><br>  <font size="3">本题的最终的单纯形表如下，可知最优解为z’=11，由于此题要求最小值，即z=-z=-11。</font><br><img src="/images/ALGORITHM/line6.png" alt="6"><br>  <font size="3">其最优解为基本变量对应的常数项组成，非基本变量全部置为0，即解为</font></p><script type="math/tex; mode=display">\begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6 \end{pmatrix} = \begin{pmatrix} 0 \\ 4 \\ 5 \\ 0 \\ 0 \\ 11 \end{pmatrix}</script><h1 id="经典例题-最大利润"><a href="#经典例题-最大利润" class="headerlink" title="经典例题(最大利润)"></a><font size="5" color="red">经典例题(最大利润)</font></h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  某工厂有3个车间，第一个车间用1个单位的原料N可以加工5个单位的产品A和2个单位的产品B。<br>  如果产品A直接售出，售价为10元，如果在第二个车间继续加工，则需要加工费5元，加工后售价为19元。<br>  如果产品B直接售出，售价为16元，如果在第三个车间继续加工，则需要加工费4元，加工后售价为24元。<br>  原材料N的单位购入价为5元，每工时的工资是15元，第一个车间加工一个单位的N需要0.05个工时，第二个车间加工一个单位需要0.1个工时，第三个车间加工一个单位需要0.08个工时。<br>  每个月最多能得到12000单位的原材料N，工时最多为1000工时，问如何安排生产才能得到最高的收益？</p><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a><font size="4">问题分析</font></h2><p>  假设A直接卖出的数量为x<sub>1</sub>，收获的利润为$10 \ x_1$<br>  假设A在第二车间加工后的出售量为x<sub>2</sub>，收获的利润为$(19-5-0.1 \times 15) \ x_2 = 12.5 \ x_2$<br>  假设B直接卖出的数量为x<sub>3</sub>，收获的利润为$16 \ x_3$<br>  假设B在第三车间加工后的出售量为x<sub>4</sub>，收获的利润为$(24-4-0.08 \times 15) \ x_4 = 18.8 \ x_4$<br>  假设所用的原材料数量为x<sub>5</sub>，所用的成本为$(5+0.05 \times 15) \ x_5 = 5.75 \ x_5$<br>  根据分析可得目标函数和约束条件如下:</p><script type="math/tex; mode=display">\max z = 10 \ x_1 + 12.5 \ x_2 + 16 \ x_4 +18.8 \ x_4 - 5.75 \ x_5</script><script type="math/tex; mode=display">\begin{cases} x_1 + x_2 - 5 \ x_5 =0 \\ x_3 + x_4 - 2 \ x_5 = 0 \\  x_5 \le 12000 \\ 0.1 \ x_1 + 0.08 \ x_4 +0.05 \ x_5 \le 1000 \\ x_i \ge 0 \ (i = 1, \ 2, \ 3, \ 4, \ 5) \end{cases}</script><p>  将其转换为标准型可知:</p><script type="math/tex; mode=display">\max z = 10 \ x_1 + 12.5 \ x_2 + 16 \ x_4 +18.8 \ x_4 - 5.75 \ x_5</script><script type="math/tex; mode=display">\begin{cases} x_1 + x_2 - 5 \ x_5 =0 \\ x_3 + x_4 - 2 \ x_5 = 0 \\  x_5 + x_6 = 12000 \\ 0.1 \ x_1 + 0.08 \ x_4 +0.05 \ x_5 + x_7 = 1000 \\ x_i \ge 0 \ (i = 1, \ 2, \ 3, \ 4, \ 5, \ 6, \ 7) \end{cases}</script><p>  找出基本变量$x_1, \ x_3, \ x_6, \ x_7$和非基本变量$x_2, \ x_4, \ x_5$<br>  将目标函数由非基本变量表示，即用$x_1 = 5 \ x_5 - x_2, \ x_3 = 2 \ x_5 - x_4$替换，目标函数转化为:</p><script type="math/tex; mode=display">\begin{align} z & =10(5 \ x_5 - x_2) + 12.5 \ x_2 + 16(2 \ x_5 - x_4) + 18.8 \ x_4 - 5.75 \ x_5 \\ & = 2.5 \ x_2 + 2.8 \ x_4 + 76.25 \ x_5 \\ \end{align}</script><p>  构造初始单纯形表<br><img src="/images/ALGORITHM/line7.png" alt="7"></p><p>  第一行输入基本变量的下标，第二行输入非基本变量的下标，然后输入初始单纯形表。</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1 3 6 7 # 基本变量下标</span><br><span class="line">2 4 5 # 非基本变量下标</span><br><span class="line">0 2.5 2.8 76.25 # 初始单纯形表</span><br><span class="line">0 1 0 -5</span><br><span class="line">0 0 1 -2</span><br><span class="line">12000 0 0 1</span><br><span class="line">1000 0.1 0.08 0.05</span><br></pre></td></tr></tbody></table></figure><h2 id="python代码实战"><a href="#python代码实战" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def display_simplex_table(simplex_table):</span><br><span class="line">    print('-----单纯形表如下-----')</span><br><span class="line">    print(' '.join([' '] + nonbase_subscript))</span><br><span class="line">    for i in range(base_num):</span><br><span class="line">        print(' '.join([base_subscript[i]] + [str(x) for x in simplex_table[i]]))</span><br><span class="line"></span><br><span class="line">def judge_simplex_table(simplex_table):</span><br><span class="line">    global solve</span><br><span class="line">    display_simplex_table(simplex_table)</span><br><span class="line">    all_negative_j_flag = True</span><br><span class="line">    for j in range(1, nonbase_num):</span><br><span class="line">        if simplex_table[0][j] &gt; 0:</span><br><span class="line">            all_negative_j_flag = False</span><br><span class="line">            all_negative_i_flag = True</span><br><span class="line">            for i in range(1, base_num):</span><br><span class="line">                if simplex_table[i][j] &gt; 0:</span><br><span class="line">                    all_negative_i_flag = False</span><br><span class="line">            if all_negative_i_flag:</span><br><span class="line">                print('该线性规划问题无界，无法求得最优解')</span><br><span class="line">                return</span><br><span class="line">    if all_negative_j_flag:</span><br><span class="line">        for i in range(1, base_num):</span><br><span class="line">            solve.append(base_subscript[i] + '=' + str(simplex_table[i][0]))</span><br><span class="line">        for j in range(1, nonbase_num):</span><br><span class="line">            solve.append(nonbase_subscript[j] + '=' + str(simplex_table[0][j]))</span><br><span class="line">        print('该问题的最优解为:', simplex_table[0][0])</span><br><span class="line">        print('该问题的解向量为:', ', '.join(solve))</span><br><span class="line">        return</span><br><span class="line">    else:</span><br><span class="line">        update_simplex_table(simplex_table)</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">def update_simplex_table(simplex_table):</span><br><span class="line">    global base_subscript, nonbase_subscript</span><br><span class="line">    in_base_var = simplex_table[0][1:].index(max(simplex_table[0][1:])) + 1</span><br><span class="line">    ratio = []</span><br><span class="line">    for i in range(1, base_num):</span><br><span class="line">        ratio = ratio + [0]  if simplex_table[i][in_base_var] == 0 else ratio + [simplex_table[i][0]/simplex_table[i][in_base_var]]</span><br><span class="line">    out_base_value = max(ratio) + 1</span><br><span class="line">    out_base_var = 0</span><br><span class="line">    for i in range(len(ratio)):</span><br><span class="line">        out_base_value, out_base_var = [ratio[i], i] if 0 &lt; ratio[i] &lt; out_base_value else [out_base_value, out_base_var]</span><br><span class="line">    out_base_var += 1</span><br><span class="line">    tmp_table = [[0 for i in range(nonbase_num)] for j in range(base_num)]</span><br><span class="line">    for i in range(base_num):</span><br><span class="line">        for j in range(nonbase_num):</span><br><span class="line">            if i == 0 and j == 0:</span><br><span class="line">                tmp_table[i][j] = simplex_table[i][j] + simplex_table[out_base_var][j] * simplex_table[i][in_base_var] / simplex_table[out_base_var][in_base_var]</span><br><span class="line">                continue</span><br><span class="line">            if i != out_base_var and j != in_base_var:</span><br><span class="line">                tmp_table[i][j] = simplex_table[i][j] - simplex_table[out_base_var][j] * simplex_table[i][in_base_var] / simplex_table[out_base_var][in_base_var]</span><br><span class="line">                continue</span><br><span class="line">            if i != out_base_var and j == in_base_var:</span><br><span class="line">                tmp_table[i][j] = -1 * simplex_table[i][j] / simplex_table[out_base_var][in_base_var]</span><br><span class="line">                continue</span><br><span class="line">            if i == out_base_var and j != in_base_var:</span><br><span class="line">                tmp_table[i][j] = simplex_table[i][j] / simplex_table[out_base_var][in_base_var]</span><br><span class="line">                continue</span><br><span class="line">            if i == out_base_var and j == in_base_var:</span><br><span class="line">                tmp_table[i][j] = 1 / simplex_table[i][j]</span><br><span class="line">                continue</span><br><span class="line">    simplex_table = [x[:] for x in tmp_table]</span><br><span class="line">    base_subscript[out_base_var], nonbase_subscript[in_base_var] = nonbase_subscript[in_base_var], base_subscript[out_base_var]</span><br><span class="line">    judge_simplex_table(simplex_table)</span><br><span class="line"></span><br><span class="line">print('输入基本变量下标:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    base_subscript = ['c'] + ['x' + x for x in line.strip().split()]</span><br><span class="line">    print('输入非基本变量下标:')</span><br><span class="line">    nonbase_subscript = ['b'] + ['x' + x for x in sys.stdin.readline().strip().split()]</span><br><span class="line">    base_num, nonbase_num = len(base_subscript), len(nonbase_subscript)</span><br><span class="line">    simplex_table, solve = [], []</span><br><span class="line">    print('请输入初始单纯形表:')</span><br><span class="line">    for i in range(base_num):</span><br><span class="line">        simplex_table.append([float(x) for x in sys.stdin.readline().strip().split()])</span><br><span class="line">    judge_simplex_table(simplex_table)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果"><a href="#代码运行结果" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/line8.png" alt="8"></p><h1 id="算法总结"><a href="#算法总结" class="headerlink" title="算法总结"></a><font size="5" color="red">算法总结</font></h1><p>  线性规划问题的难点不在于算法的设计，而是在于如何从文字描述中寻找到合适的模型，如何建立线性规划方程组。线性规划在实际的生产生活中有着重要的应用，虽然该算法理解起来较为复杂，但是记住其求解形式，遇到此类问题直接仿照使用即可。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Linear Programming&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="常用算法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>并查集(Union Find)</title>
    <link href="https://USTCcoder.github.io/2019/08/07/algorithm%20union%20find/"/>
    <id>https://USTCcoder.github.io/2019/08/07/algorithm union find/</id>
    <published>2019-08-07T06:30:18.000Z</published>
    <updated>2019-08-07T15:58:23.253Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">并查集</font></strong></center><p></p><h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a><font size="5" color="red">原理介绍</font></h1><p>   <strong>Union Find:并查集</strong>，是一种<strong>树型</strong>的数据结构，用于处理一些<strong>不相交集合的合并及查询</strong>问题。在一些有N个元素的集合应用问题中，我们通常是在开始时让每个元素构成一个单元素的集合，然后按一定顺序将属于<strong>同一组</strong>的元素所在的<strong>集合合并</strong>。<br><a id="more"></a></p><p><img src="/images/ALGORITHM/union1.jpg" alt="1"></p><h1 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a><font size="5" color="red">算法步骤</font></h1><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a><font size="4">初始化</font></h2><p>  把每个点所在集合初始化为其自身，通常来说，这个步骤在每次使用该数据结构时只需要执行一次。</p><h2 id="查找根结点"><a href="#查找根结点" class="headerlink" title="查找根结点"></a><font size="4">查找根结点</font></h2><p>  查找元素所在的集合，即根节点。为了以后的查找方便，可以在查询时将该结点以及该结点的所有父节点都直接指向根结点，再次查询时即可直接查找到根结点。</p><h2 id="合并"><a href="#合并" class="headerlink" title="合并"></a><font size="4">合并</font></h2><p>  将两个元素所在的集合合并为一个集合，合并之前，应先判断两个元素是否属于同一集合，这可用上面的“查找根结点”操作实现，判断两个根结点是否相同来判断是否属于同一集合。</p><h1 id="经典例题-岛屿数量"><a href="#经典例题-岛屿数量" class="headerlink" title="经典例题(岛屿数量)"></a><font size="5" color="red">经典例题(岛屿数量)</font></h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  有一个二维的网格地图，其中1代表陆地0代表水，并且该网格的四周全部由水包围。我们对岛屿的定义是四面环水，由相邻的陆地水平或垂直连接形成，现在需要统计岛屿的数量。<br>  输入一行数据，使用空格分隔二维地图的每一行，使用逗号分隔一行中的每一项。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1,1,0,0,0 1,1,0,0,0 0,0,1,0,0 0,0,0,1,1 # 输入4×5的地图</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/ALGORITHM/union2.png" alt="2"></p><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  初始时将每个值为1的点都指向自己(即单独一个点作为一个岛)，count等于值为1的点的个数。然后遍历整个地图，如果该点上下左右有值为1的点则查找两个点的根结点，如果根结点相同说明已经在同一个岛上，否则合并两个岛，count值减1。<br>  将所有点都遍历以后，此时相邻的点都具有同样的根结点，此时的count个数即为岛屿的数量。</p><h2 id="python代码实战"><a href="#python代码实战" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">class Union_find:</span><br><span class="line">    def __init__(self, grid):</span><br><span class="line">        row_num, col_num = len(grid), len(grid[0])</span><br><span class="line">        self.count = 0</span><br><span class="line">        self.parent = [-1] * (row_num * col_num)</span><br><span class="line">        self.rank = [0] * (row_num * col_num)</span><br><span class="line">        for i in range(row_num):</span><br><span class="line">            for j in range(col_num):</span><br><span class="line">                if grid[i][j] == '1':</span><br><span class="line">                    self.parent[i * col_num + j] = i * col_num + j</span><br><span class="line">                    self.count += 1</span><br><span class="line"></span><br><span class="line">    def find(self, i):</span><br><span class="line">        root = i</span><br><span class="line">        while self.parent[root] != root: </span><br><span class="line">            root = self.parent[root]</span><br><span class="line">        while self.parent[i] != root:</span><br><span class="line">            i, self.parent[i] = self.parent[i], root</span><br><span class="line">        return root</span><br><span class="line"></span><br><span class="line">    def connection(self, p, q):</span><br><span class="line">        return self.find(p) == self.find(q)</span><br><span class="line"></span><br><span class="line">    def union(self, p, q):</span><br><span class="line">        proot = self.find(p)         </span><br><span class="line">        qroot = self.find(q)</span><br><span class="line">        if qroot != proot:</span><br><span class="line">            self.parent[proot] = qroot</span><br><span class="line">            self.count -= 1</span><br><span class="line"></span><br><span class="line">print('请输入要查询的地图:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    line = line.strip().split()</span><br><span class="line">    row_num, col_num, grid, direction = len(line), (len(line[0]) + 1) // 2, [], [[1, 0], [0, 1]]</span><br><span class="line">    for tmp in line:</span><br><span class="line">        grid.append(tmp.split(','))</span><br><span class="line">    uf = Union_find(grid)</span><br><span class="line">    for i in range(row_num):</span><br><span class="line">        for j in range(col_num):</span><br><span class="line">            if grid[i][j] == '1':</span><br><span class="line">                for x, y in direction:</span><br><span class="line">                    new_i, new_j = i + x, j + y</span><br><span class="line">                    if new_i &lt; row_num and new_j &lt; col_num and grid[new_i][new_j] == '1':</span><br><span class="line">                        uf.union(i * col_num + j, new_i * col_num + new_j)</span><br><span class="line">    print('该地图中岛屿的数量为:', uf.count)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果"><a href="#代码运行结果" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/union.png" alt="0"></p><h1 id="算法总结"><a href="#算法总结" class="headerlink" title="算法总结"></a><font size="5" color="red">算法总结</font></h1><p>  并查集是一个较为复杂且不太常用的算法，但是可以解决一些特定问题，尤其是解决一些集合关系的问题。该算法可以使具有某些特定关系的点作为一个群体，然后统计整体的群体个数即为整体的类别个数，某个群体中个体的数量即为整体中某一类别的数量。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Union Find&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="常用算法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>位运算(Bit Operation)</title>
    <link href="https://USTCcoder.github.io/2019/08/02/algorithm%20bit%20operation/"/>
    <id>https://USTCcoder.github.io/2019/08/02/algorithm bit operation/</id>
    <published>2019-08-02T12:18:25.000Z</published>
    <updated>2019-08-07T15:57:54.910Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">位运算</font></strong></center><p></p><h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a><font size="5" color="red">原理介绍</font></h1><p>   <strong>Bit Operation:位运算</strong>，程序中的所有数在计算机内存中都是以<strong>二进制的形式储存</strong>的，位运算就是直接对整数在内存中的二进制位进行操作，所以运算速度相对较快。位运算主要包括<strong>按位与(&amp;)</strong>、<strong>按位或(|)</strong>、<strong>按位异或(^)</strong>、<strong>取反(~)</strong>、<strong>左移(&lt;&lt;)</strong>、<strong>右移(&gt;&gt;)</strong>这几种，其中除了取反(~)以外，其他的都是二目运算符，即要求运算符左右两侧均有一个运算量。<br><a id="more"></a></p><h1 id="算法基础"><a href="#算法基础" class="headerlink" title="算法基础"></a><font size="5" color="red">算法基础</font></h1><h2 id="原码"><a href="#原码" class="headerlink" title="原码"></a><font size="4">原码</font></h2><p>  <font size="3">原码是二进制的一种表现方式。取该整数的绝对值的二进制，再加上符号位。该原码只是为了让我们看二进制更直观，直接看出正负数和比较大小。但原码不是计算机保存的二进制，所以不能直接参与计算。</font><br><img src="/images/ALGORITHM/bit1.png" alt="1"></p><h2 id="反码"><a href="#反码" class="headerlink" title="反码"></a><font size="4">反码</font></h2><p>  <font size="3">反码主要是针对负数的处理。非负数的反码等于其原码，负数的反码在原码的基础上，符号位不变，其他数值位取反，即把1变成0，把0变成1。反码是为了在计算机中存储二进制，但非真正的二进制值，所以也不直接参与计算。</font><br><img src="/images/ALGORITHM/bit2.png" alt="2"></p><h2 id="补码"><a href="#补码" class="headerlink" title="补码"></a><font size="4">补码</font></h2><p>  <font size="3">补码是真正的二进制值了，主要也是针对负数。非负数不变，而负数是在反码的基础上加1，为了方便正数和负数之间进行运算。</font><br><img src="/images/ALGORITHM/bit3.png" alt="3"></p><h2 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a><font size="4">位运算</font></h2><p><img src="/images/ALGORITHM/bit.jpg" alt="4"></p><h2 id="位运算技巧"><a href="#位运算技巧" class="headerlink" title="位运算技巧"></a><font size="4">位运算技巧</font></h2><script type="math/tex; mode=display">x \ >> \ n \iff \left \lfloor x \div \ 2^n \right \rfloor \ , \ x的二进制值右移n位</script><script type="math/tex; mode=display">x \ << \ n \iff x \ \times \ 2^n \ , \ x的二进制值右移n位</script><script type="math/tex; mode=display">x \ \& \ 1 \ == \ 1 \iff x \ % \ 2 \ == \ 1 \ , \ 判断x是否为奇数</script><script type="math/tex; mode=display">x \ \& \ (x -1) \ , \ 清除x最后一位的1</script><script type="math/tex; mode=display">x \ \& \ (-x) \ , \ 得到x最后一位的1</script><h1 id="经典例题-二进制中1的个数"><a href="#经典例题-二进制中1的个数" class="headerlink" title="经典例题(二进制中1的个数)"></a><font size="5" color="red">经典例题(二进制中1的个数)</font></h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  给定一个正整数n，输出从0到n的每个数的二进制中有多少个1？<br>  输入正整数n</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10 # 输入正整数10</span><br></pre></td></tr></tbody></table></figure><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  分析一个数的二进制中有多少个1，可以使用传统的方法，一直模2(mod 2)然后再除以2，知道结果为0即可。这样做虽然也不慢，但是如果二进制中1的个数很少，这样做效率就很低。<br>  可以采用$x \ \&amp; \ (x -1)$的方法，每次清除x最后一位的1，清除了多少次即有多少个1。并且使用动态规划的思想，保存之前做过的记录，即求6的二进制位(110)有多少1，将做后一位的1去掉之后为(100)，即求4的二进制位有多少1，然后加1即可。</p><h2 id="python代码实战"><a href="#python代码实战" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">print('请输入一个正整数:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    number = int(line.strip())</span><br><span class="line">    number_one_bit = [0] * (number + 1)</span><br><span class="line">    for i in range(1, number + 1):</span><br><span class="line">        number_one_bit[i] = number_one_bit[i &amp; (i - 1)] + 1</span><br><span class="line">    print('从0到' + str(number) + '的二进制表示中1的个数为:', number_one_bit)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果"><a href="#代码运行结果" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/bit4.png" alt="4"></p><h1 id="经典例题-N皇后问题"><a href="#经典例题-N皇后问题" class="headerlink" title="经典例题(N皇后问题)"></a><font size="5" color="red">经典例题(N皇后问题)</font></h1><h2 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  在n×n的国际棋盘上放置彼此不受攻击的n个皇后，按照规则，皇后可以攻击与之在同一行、同一列、统一斜线上的棋子。现在已知又n个皇后，问有多少种不同的放法？<br>  输入皇后的个数n</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6 # 皇后的个数</span><br></pre></td></tr></tbody></table></figure><h2 id="算法分析-1"><a href="#算法分析-1" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  之前再深度优先搜索中提到过N皇后的一般解法，确实深度优先是最容易想到的一种做法，但是并不是最快的一种做法，可以尝试采用深度优先+位运算提高效率。<br>  以4皇后为例，每个皇后有四个格子可以放置，可以当作二进制的四个bit。如8(1000)代表皇后放在第1个格子，4(0100)代表皇后放在第二个格子。某一个皇后可以放置的位置由列，斜线和反斜线三个方向限制。<br>  设第i个皇后放置的行数为row，被攻击的列数为col，被攻击的斜线为pie，被攻击的反斜线为na。因此所有被攻击的点为$col \ | \ pie \ | \ na$，因此可以放置的位置为$~(col \ | \ pie \ | \ na) \ \&amp; \ ((1 &lt;&lt; queen_num) - 1)$，保证高位都为0，不可以放置。<br>  上述操作之后说明该数中二进制位的1就是当前可以放置的位置。每次$x \ \&amp; \ (-x)$得到x最后一位的1，并将皇后放置于该位置p，并使用$x \ \&amp; \ (x -1)$并将此位的1清除，并进入下一行，下一行被攻击的列为$col \ | \ p$，下一行被攻击的斜线为$(pie \ | \ p) \ &lt;&lt; \ 1$，下一行被攻击的反斜线为$(na \ | \ p) \ &gt;&gt; \ 1$</p><h2 id="python代码实战-1"><a href="#python代码实战-1" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def dfs(row, col, pie, na):</span><br><span class="line">    global res_num</span><br><span class="line">    if row &gt;= queen_num:</span><br><span class="line">        res_num += 1</span><br><span class="line">        return</span><br><span class="line">    bits = (~(col | pie | na) &amp; ((1 &lt;&lt; queen_num) - 1))</span><br><span class="line">    while bits &gt; 0:</span><br><span class="line">        p = bits &amp; -bits</span><br><span class="line">        dfs(row + 1, col | p, (pie | p) &lt;&lt; 1, (na | p) &gt;&gt; 1)</span><br><span class="line">        bits &amp;= bits - 1</span><br><span class="line"></span><br><span class="line">print('请输入皇后的个数:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    queen_num, res_num = int(line.strip()), 0</span><br><span class="line">    chess = [[0 for i in range(queen_num)] for j in range(queen_num)]</span><br><span class="line">    dfs(0, 0, 0, 0)</span><br><span class="line">    print('一共有' + str(res_num) + '种皇后放置方法')</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-1"><a href="#代码运行结果-1" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/bit5.png" alt="5"></p><h1 id="经典例题-斐波那契数列"><a href="#经典例题-斐波那契数列" class="headerlink" title="经典例题(斐波那契数列)"></a><font size="5" color="red">经典例题(斐波那契数列)</font></h1><h2 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  假设第一个月有一对刚诞生的兔子，第二个月进入成熟期，第三个月开始生育兔子，而一对成熟的兔子每月回生一对兔子，如果兔子永不死去，那么n个月后有多少对兔子？<br>  输入月份数n</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10 # 求第10个月的兔子数</span><br></pre></td></tr></tbody></table></figure><h2 id="算法分析-2"><a href="#算法分析-2" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  斐波那契数列是一个典型的算法问题，有多个不同版本的解法，也代表着不同的思想。<br>  首先就是递归解法，根据$f(n)=f(n-1)+f(n-2), \ f(1)=f(2)=1$求解，不过这种解法的时间复杂度为$O(({\frac{\sqrt5 + 1}{2}})^n)$，算f(10)还是非常快的，但是算f(100)简直是天方夜谭。<br>  其次是动态规划解法，建立一个大小为n+1的矩阵，每次计算的值存放于矩阵中此时计算$f(n)=f(n-1)+f(n-2)$时，f(n-1)和f(n-2)就不需要递归计算，只要查表即可，时间复杂度为O(n)。<br>  最快的解法为矩阵解法，根据$\begin{pmatrix} f(n-1) \ f(n) \end{pmatrix} = \begin{pmatrix} 1 &amp; 1 \ 1 &amp; 2 \end{pmatrix} \ \begin{pmatrix} f(n-2) \ f(n-3) \end{pmatrix}$可得</p><script type="math/tex; mode=display">\begin{pmatrix} f(\left \lfloor \frac{n}{2} \right \rfloor \times 2) \\ f(\left \lfloor \frac{n}{2} \right \rfloor \times 2+1) \end{pmatrix} = {\begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix}}^{\left \lfloor \frac{n}{2} \right \rfloor} \ \begin{pmatrix} 0 \\ 1 \end{pmatrix}</script><p>  即问题转化为求矩阵${\begin{pmatrix} 1 &amp; 1 \ 1 &amp; 2 \end{pmatrix}}^{\left \lfloor \frac{n}{2} \right \rfloor}$，时间复杂度为O(n)。乘方问题可以用位运算提高效率，时间复杂度可以提升到O(log(n))</p><h2 id="python代码实战-2"><a href="#python代码实战-2" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def matrix_mul(array_1, array_2):</span><br><span class="line">    row_1, mid, col_2 = len(array_1), len(array_2), len(array_2[0])</span><br><span class="line">    array=[[0 for i in range(col_2)] for j in range(row_1)]</span><br><span class="line">    for i in range(row_1):</span><br><span class="line">        for j in range(col_2):</span><br><span class="line">                array[i][j] = array_1[i][0] * array_2[0][j] + array_1[i][1] * array_2[1][j]</span><br><span class="line">    return array</span><br><span class="line"></span><br><span class="line">def matrix_pow(array, m):</span><br><span class="line">    binary, n = [int(x) for x in bin(m)[2:]], len(array)</span><br><span class="line">    res, temp = [[1, 0], [0, 1]], [x[:] for x in array]</span><br><span class="line">    while binary:</span><br><span class="line">        if binary.pop() == 1:</span><br><span class="line">            res = matrix_mul(res, temp)</span><br><span class="line">        temp = matrix_mul(temp, temp)</span><br><span class="line">    return res</span><br><span class="line"></span><br><span class="line">print('请输入月份数:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    month = int(line.strip())</span><br><span class="line">    print('第' + str(month) + '月的兔子数量为:', matrix_mul(matrix_pow([[1, 1], [1, 2]], month//2), [[0], [1]])[month % 2][0])</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-2"><a href="#代码运行结果-2" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/bit6.png" alt="6"></p><h1 id="算法总结"><a href="#算法总结" class="headerlink" title="算法总结"></a><font size="5" color="red">算法总结</font></h1><p>  位运算说是一种算法，实际上应该说是一种方法。许多问题都可以用位运算来提高效率，位运算很少单独使用，往往同其他的算法一起使用，作为其中的一个步骤，能够在特定的情况下发挥出特殊的效果。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Bit Operation&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="常用算法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>广度优先搜索(Breadth-First-Search)</title>
    <link href="https://USTCcoder.github.io/2019/07/31/algorithm%20BFS/"/>
    <id>https://USTCcoder.github.io/2019/07/31/algorithm BFS/</id>
    <published>2019-07-31T12:38:40.000Z</published>
    <updated>2020-05-21T00:20:56.465Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Breadth-First-Search</font></strong></center><p></p><h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a><font size="5" color="red">原理介绍</font></h1><p>   <strong>Breadth-First-Search(BFS):深度优先搜索</strong>，属于<strong>图算法</strong>的一种，是一种盲目搜寻法，目的是系统地<strong>展开并检查图中的所有节点</strong>，以找寻结果。在树搜索算法中，从上到下为纵，从左向右为横，纵向搜索为深度优先，而横向搜索是广度优先。简言之就是<strong>先访问图的顶点</strong>，然后<strong>优先访问其邻接点</strong>，然后再依次<strong>进行被访问点的邻接点</strong>，<strong>一层一层</strong>访问，直至访问完所有点，遍历结束，通常根据<strong>队列的先进先出</strong>性质将各结点遍历。<br><a id="more"></a></p><p><img src="/images/ALGORITHM/bfs1.jpg" alt="bfs"></p><h1 id="算法条件"><a href="#算法条件" class="headerlink" title="算法条件"></a><font size="5" color="red">算法条件</font></h1><h2 id="解空间"><a href="#解空间" class="headerlink" title="解空间"></a><font size="4">解空间</font></h2><p>  <font size="3">解的组织形式可以规范为一个n元组${x_1,x_2,\ldots,x_n}$，并且对解有取值范围的限定，一般为有穷个，解的个数代表一个结点的分支个数。解空间越小，搜索效率越高，解空间大犹如大海捞针，搜索效率很低。</font></p><h2 id="剪枝函数"><a href="#剪枝函数" class="headerlink" title="剪枝函数"></a><font size="4">剪枝函数</font></h2><p>  <font size="3">剪枝函数又称为隐约束，对能否得到问题的可行解的约束称为约束函数，对能否得到问题的最优解的约束称为限界函数。有了剪枝函数，我们就可以剪掉得不到可行解或最优解的分支，避免了无效搜索，提高搜索的效率。因此剪枝函数的设计是十分重要的。</font></p><h1 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a><font size="5" color="red">算法步骤</font></h1><p>  <font size="3">(1)分析题意，了解题目要求</font><br>  <font size="3">(2)根据问题分析解空间的形式</font><br>  <font size="3">(3)根据解空间设计合适的剪枝函数</font></p><h1 id="经典例题-0-1背包"><a href="#经典例题-0-1背包" class="headerlink" title="经典例题(0-1背包)"></a><font size="5" color="red">经典例题(0-1背包)</font></h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  假设山洞里有n个宝物，每种宝物有一定重量w和相应的价值v，背包的装载能力有限，只能运走重量为m的宝物，宝物不可以分割，如何使背包运走物品的价值最大？<br>  第一行先输入宝物的数量n，和背包的承载重量m，然后每一行输出一个宝物对应的重量w和价值v(用空格分开)<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">5 10 # 宝物数n和背包能装载的重量m</span><br><span class="line">2 6 #每个宝物的重量w和价值v</span><br><span class="line">5 3</span><br><span class="line">4 5</span><br><span class="line">2 4</span><br><span class="line">3 6</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  0-1背包问题和普通背包问题不同的是其解空间为{0，1}，即每一个物品都有两种状态，装入或者不装入，因此满足解空间的条件。<br>  分析剪枝函数，如果剩余的价值加上当前的价值都没有已经搜索到的最大价值高，则没有必要继续搜索。</p><h2 id="python代码实战"><a href="#python代码实战" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def bfs(treasure_queue):</span><br><span class="line">    global max_value, max_plan</span><br><span class="line">    while treasure_queue:</span><br><span class="line">        n, out_node = treasure_queue[0][0], treasure_queue.pop(0)</span><br><span class="line">        if n &gt;= count:</span><br><span class="line">            max_value, max_plan = [out_node[2], out_node[3][:]] if out_node[2] &gt; max_value else [max_value, max_plan[:]]</span><br><span class="line">            continue</span><br><span class="line">        if out_node[1] - treasure[n][1] &gt;= 0:</span><br><span class="line">            treasure_queue.append([n + 1, out_node[1] - treasure[n][1], out_node[2] + treasure[n][2], out_node[3][:n] + [True] + out_node[3][n + 1:]])</span><br><span class="line">        if out_node[2] + leave_value[n] &gt; max_value:</span><br><span class="line">            treasure_queue.append([n + 1, out_node[1], out_node[2], out_node[3][:]])</span><br><span class="line"></span><br><span class="line">print('请输入宝物数量和驴子承载重量:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    count, weight = line.strip().split()</span><br><span class="line">    count, weight, treasure, max_plan, max_value, leave_value, res = int(count), float(weight), [], [False] * int(count), 0, [0], []</span><br><span class="line">    print('请输入每个宝物的重量和价值')</span><br><span class="line">    for i in range(count):</span><br><span class="line">        tmp = [float(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">        treasure.append([i + 1] + tmp + [tmp[1] / tmp[0]])</span><br><span class="line">    treasure.sort(key=lambda x: (-x[3]))</span><br><span class="line">    for i in reversed(range(1, count)):</span><br><span class="line">        leave_value = [leave_value[0] + treasure[i][2]] + leave_value</span><br><span class="line">    bfs([[0, weight, 0, [False] * count]])</span><br><span class="line">    print('最优的方案为:\n' + ''.join(['' + ('装入第' + str(j) + '个宝物\n') * (j != 0) for j in sorted([treasure[i][0] * max_plan[i] for i in range(count)])]) + '装入宝物的最大价值为:', max_value)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果"><a href="#代码运行结果" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/bfs2.png" alt="2"></p><h1 id="经典例题-旅行商问题"><a href="#经典例题-旅行商问题" class="headerlink" title="经典例题(旅行商问题)"></a><font size="5" color="red">经典例题(旅行商问题)</font></h1><h2 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  现有n个景点，从第一个景点出发，两个景点之间有数字代表可以直接到达，问如何找到一个路径能够不重复的走遍所有景点回到出发点，而且所经过的路径长度是最短的。<br>  第一行输入景点的个数，第二行输入两地之间可以直接到达的数量，然后每行输入两地和之间的距离</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">5 # 景点个数</span><br><span class="line">9 # 景点之间的连接数</span><br><span class="line">1 2 3 # 景点1和景点2之间的距离为3</span><br><span class="line">1 4 8</span><br><span class="line">1 5 9</span><br><span class="line">2 3 3</span><br><span class="line">2 4 10</span><br><span class="line">2 5 5</span><br><span class="line">3 4 4</span><br><span class="line">3 5 3</span><br><span class="line">4 5 20</span><br></pre></td></tr></tbody></table></figure><h2 id="算法分析-1"><a href="#算法分析-1" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  旅行商问题(TSP)是一个典型的问题，此问题的解空间为n，每一个景点都可以到与之相连的所有点，因此当景点数很多时，最优解的搜索是十分缓慢的。<br>  分析剪枝函数，剪枝函数容易看出，由于不是任何两个景点都是相连的，而且走过的景点不能再走一次，所以这也大大减少了解空间的个数。</p><h2 id="python代码实战-1"><a href="#python代码实战-1" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def bfs(tour_queue):</span><br><span class="line">    global  min_dist, min_route</span><br><span class="line">    while tour_queue:</span><br><span class="line">        n, out_node = tour_queue[0][0], tour_queue.pop(0)</span><br><span class="line">        if n &gt;= scenic_spot_num:</span><br><span class="line">            min_dist, min_route = [out_node[1] + connection[out_node[2][-1]][0], out_node[2][:] + [0]] if out_node[1] + connection[out_node[2][-1]][0] &lt; min_dist else [min_dist, min_route[:]]</span><br><span class="line">        for i in range(scenic_spot_num):</span><br><span class="line">            if i not in out_node[2] and connection[out_node[2][-1]][i] != 0 and out_node[1] + connection[out_node[2][-1]][i] + connection[i][0] &lt; min_dist:</span><br><span class="line">                tour_queue.append([n + 1, out_node[1] + connection[out_node[2][-1]][i], out_node[2] + [i]])</span><br><span class="line"></span><br><span class="line">print('请输入景点数:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    scenic_spot_num, min_dist, min_route = int(line.strip()), 65535, []</span><br><span class="line">    print('请输入连通的景点数:')</span><br><span class="line">    connection_num, connection = int(sys.stdin.readline().strip()), [[0 for i in range(scenic_spot_num)] for j in range(scenic_spot_num)]</span><br><span class="line">    print('请依次输入两个景点之间的距离:')</span><br><span class="line">    for i in range(connection_num):</span><br><span class="line">        begin, end, weight = [int(i) for i in sys.stdin.readline().strip().split()]</span><br><span class="line">        connection[begin - 1][end - 1], connection[end - 1][begin - 1] = weight, weight</span><br><span class="line">    bfs([[1, 0, [0]]])</span><br><span class="line">    print('最短的路径为:' + '-&gt;'.join([str(x + 1) for x in min_route]) + '\n最短的路径长度为:', min_dist)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-1"><a href="#代码运行结果-1" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/bfs3.png" alt="3"></p><h1 id="经典例题-迷宫问题"><a href="#经典例题-迷宫问题" class="headerlink" title="经典例题(迷宫问题)"></a><font size="5" color="red">经典例题(迷宫问题)</font></h1><h2 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  在一个m×n的地图上，有许多障碍物，给定起始点坐标和目的地坐标，问从起始点开始通过上下左右四个方向移动如何找到一条最短路径能够到达目的地？<br>  第一行输入地图的大小m和n，然后每一行输入障碍物的坐标，输入0，0时结束，接着输入起始点坐标和目的地坐标。</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">5 6 # 地图的大小m,n</span><br><span class="line">1 6 # 障碍物的坐标</span><br><span class="line">2 3 </span><br><span class="line">3 4</span><br><span class="line">3 5</span><br><span class="line">5 1</span><br><span class="line">0 0 #输入0,0结束</span><br><span class="line">2 1 #起始点坐标</span><br><span class="line">4 6 #目的地坐标</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/ALGORITHM/bfs5.png" alt="5"></p><h2 id="算法分析-2"><a href="#算法分析-2" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  迷宫问题是一个典型的搜索问题，每个点都有四个移动方向，因此每一个结点都有四个子节点，可以通过广度优先算法来求解此问题。<br>  分析剪枝函数，此题比较特殊，特别适合于用广度优先，广度优先是一层一层遍历，后面访问的结点的层数一定不小于前面结点的层数。判断新加入的坐标是否为目的地坐标，如果是则为最优解，不需要再搜索其他路径。</p><h2 id="python代码实战-2"><a href="#python代码实战-2" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def bfs(circuit_board_queue):</span><br><span class="line">    global min_spend</span><br><span class="line">    while circuit_board_queue:</span><br><span class="line">        out_node_value, out_node_site = circuit_board_queue[0][0], circuit_board_queue.pop(0)[1]</span><br><span class="line">        if [end_x - 1, end_y - 1] == out_node_site[-1]:</span><br><span class="line">            return out_node_value, [x[:] for x in out_node_site]</span><br><span class="line">        for x, y in direction:</span><br><span class="line">            if 0 &lt;= out_node_site[-1][0] + x &lt; m_size and 0 &lt;= out_node_site[-1][1] + y &lt; n_size and out_node_value + 1 &lt; min_spend[out_node_site[-1][0] + x][out_node_site[-1][1] + y]:</span><br><span class="line">                min_spend[out_node_site[-1][0] + x][out_node_site[-1][1] + y] = out_node_value + 1</span><br><span class="line">                circuit_board_queue.append([out_node_value + 1, out_node_site + [[out_node_site[-1][0] + x, out_node_site[-1][1] + y]]])</span><br><span class="line"></span><br><span class="line">print('请输入地图大小:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    m_size, n_size = [int(x) for x in line.strip().split()]</span><br><span class="line">    circuit_board, min_spend, direction = [[1 for i in range(n_size)] for j in range(m_size)], [[65535 for i in range(n_size)] for j in range(m_size)], [[1, 0], [0, 1], [-1, 0], [0, -1]]</span><br><span class="line">    while True:</span><br><span class="line">        print('请输入障碍物坐标:')</span><br><span class="line">        x, y = [int(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">        if x == 0 and y == 0:</span><br><span class="line">            break</span><br><span class="line">        circuit_board[x - 1][y - 1] = 0</span><br><span class="line">    print('请输入起点坐标')</span><br><span class="line">    begin_x, begin_y = [int(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">    min_spend[begin_x - 1][begin_y - 1] = 0</span><br><span class="line">    print('请输入终点坐标')</span><br><span class="line">    end_x, end_y = [int(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">    min_dist, min_route = bfs([[0, [[begin_x - 1, begin_y - 1]]]])</span><br><span class="line">    print('这条最短路径的长度为:', min_dist, '\n最佳的路径为:' + '-&gt;'.join([str(tuple([y + 1 for y in x])) for x in min_route]))</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-2"><a href="#代码运行结果-2" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/bfs4.png" alt="4"></p><h1 id="算法总结"><a href="#算法总结" class="headerlink" title="算法总结"></a><font size="5" color="red">算法总结</font></h1><p>  广度优先搜索是一个基本搜索方法，和深度优先有异曲同工之妙，对于许多问题都可以同时用这两种方法解决。和深度优先相同，都是指数级的时间复杂度，但是对于有些问题不得不使用广度优先进行遍历，因此寻找合适的约束条件可以大大减少时间的开销。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Breadth-First-Search&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="常用算法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>深度优先搜索(Depth-First-Search)</title>
    <link href="https://USTCcoder.github.io/2019/07/27/algorithm%20DFS/"/>
    <id>https://USTCcoder.github.io/2019/07/27/algorithm DFS/</id>
    <published>2019-07-27T12:13:40.000Z</published>
    <updated>2020-05-21T00:21:09.893Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">Depth-First-Search</font></strong></center><p></p><h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a><font size="5" color="red">原理介绍</font></h1><p>   <strong>Depth-First-Search(DFS):深度优先搜索</strong>，属于<strong>图算法</strong>的一种，是一种盲目搜寻法，从<strong>初始状态出发</strong>，当搜索到某一步时，发现原先选择并不是最优或达不到目标，就<strong>退回一步重新选择</strong>。根据产生子节点的条件约束，搜索问题的最优解。因此又名<strong>回溯法</strong>，是一种<strong>能进则进，进不了则换，换不了则退</strong>的搜索方法。<br><a id="more"></a></p><p><img src="/images/ALGORITHM/dfs1.jpg" alt="dfs"></p><h1 id="算法条件"><a href="#算法条件" class="headerlink" title="算法条件"></a><font size="5" color="red">算法条件</font></h1><h2 id="解空间"><a href="#解空间" class="headerlink" title="解空间"></a><font size="4">解空间</font></h2><p>  <font size="3">解的组织形式可以规范为一个n元组${x_1,x_2,\ldots,x_n}$，并且对解有取值范围的限定，一般为有穷个，解的个数代表一个结点的分支个数。解空间越小，搜索效率越高，解空间大犹如大海捞针，搜索效率很低。</font></p><h2 id="剪枝函数"><a href="#剪枝函数" class="headerlink" title="剪枝函数"></a><font size="4">剪枝函数</font></h2><p>  <font size="3">剪枝函数又称为隐约束，对能否得到问题的可行解的约束称为约束函数，对能否得到问题的最优解的约束称为限界函数。有了剪枝函数，我们就可以剪掉得不到可行解或最优解的分支，避免了无效搜索，提高搜索的效率。因此剪枝函数的设计是十分重要的。</font></p><h1 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a><font size="5" color="red">算法步骤</font></h1><p>  <font size="3">(1)分析题意，了解题目要求</font><br>  <font size="3">(2)根据问题分析解空间的形式</font><br>  <font size="3">(3)根据解空间设计合适的剪枝函数</font></p><h1 id="经典例题-0-1背包"><a href="#经典例题-0-1背包" class="headerlink" title="经典例题(0-1背包)"></a><font size="5" color="red">经典例题(0-1背包)</font></h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  假设山洞里有n个宝物，每种宝物有一定重量w和相应的价值v，背包的装载能力有限，只能运走重量为m的宝物，宝物不可以分割，如何使背包运走物品的价值最大？<br>  第一行先输入宝物的数量n，和背包的承载重量m，然后每一行输出一个宝物对应的重量w和价值v(用空格分开)<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">5 10 # 宝物数n和背包能装载的重量m</span><br><span class="line">2 6 #每个宝物的重量w和价值v</span><br><span class="line">5 3</span><br><span class="line">4 5</span><br><span class="line">2 4</span><br><span class="line">3 6</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  0-1背包问题和普通背包问题不同的是其解空间为{0，1}，即每一个物品都有两种状态，装入或者不装入，因此满足解空间的条件。<br>  分析剪枝函数，如果剩余的价值加上当前的价值都没有已经搜索到的最大价值高，则没有必要继续搜索。</p><h2 id="python代码实战"><a href="#python代码实战" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def dfs(n, current_plan, leave_weight, current_value):</span><br><span class="line">    global  max_value, max_plan</span><br><span class="line">    if n &gt;= count:</span><br><span class="line">        max_value, max_plan = [current_value, current_plan[:]] if current_value &gt; max_value else [max_value, max_plan[:]]</span><br><span class="line">        return</span><br><span class="line">    if leave_weight - treasure[n][1] &gt;= 0:</span><br><span class="line">        leave_weight, current_value, current_plan[n] = [leave_weight - treasure[n][1], current_value + treasure[n][2], True]</span><br><span class="line">        dfs(n + 1, current_plan, leave_weight, current_value)</span><br><span class="line">        leave_weight, current_value, current_plan[n] = [leave_weight + treasure[n][1], current_value - treasure[n][2], False]</span><br><span class="line">    if leave_value[n] + current_value &gt; max_value:</span><br><span class="line">        dfs(n + 1, current_plan, leave_weight, current_value)</span><br><span class="line"></span><br><span class="line">print('请输入宝物数量和驴子承载重量:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    count, weight = line.strip().split()</span><br><span class="line">    count, weight, treasure, max_plan, max_value, leave_value, res = int(count), float(weight), [], [False] * int(count), 0, [0], []</span><br><span class="line">    print('请输入每个宝物的重量和价值')</span><br><span class="line">    for i in range(count):</span><br><span class="line">        tmp = [float(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">        treasure.append([i + 1] + tmp + [tmp[1] / tmp[0]])</span><br><span class="line">    treasure.sort(key=lambda x: (-x[3]))</span><br><span class="line">    for i in reversed(range(1, count)):</span><br><span class="line">        leave_value = [leave_value[0] + treasure[i][2]] + leave_value</span><br><span class="line">    dfs(0, [False] * count, weight, 0)</span><br><span class="line">    print('最优的方案为:\n' + ''.join(['' + ('装入第' + str(j) + '个宝物\n') * (j != 0) for j in sorted([treasure[i][0] * max_plan[i] for i in range(count)])]) + '装入宝物的最大价值为:', max_value)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果"><a href="#代码运行结果" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/dfs2.png" alt="2"></p><h1 id="经典例题-n皇后问题"><a href="#经典例题-n皇后问题" class="headerlink" title="经典例题(n皇后问题)"></a><font size="5" color="red">经典例题(n皇后问题)</font></h1><h2 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  在n×n的国际棋盘上放置彼此不受攻击的n个皇后，按照规则，皇后可以攻击与之在同一行、同一列、统一斜线上的棋子。现在已知又n个皇后，问有多少种不同的放法？<br>  输入皇后的个数n</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4 # 皇后的个数</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/ALGORITHM/dfs2.jpg" alt="7"></p><h2 id="算法分析-1"><a href="#算法分析-1" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  n皇后问题不同于0-1背包问题，n皇后的解空间为n，每一个皇后都有n种放法，因此当n很大时，解法的搜索非常缓慢。<br>  分析剪枝函数，已经放置了k个皇后之后，就没有n种不同的放法了，可以通过判断和以前的皇后放法是否冲突来缩小解空间的搜索。</p><h2 id="python代码实战-1"><a href="#python代码实战-1" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def judge_n(n, col, current_queen):</span><br><span class="line">    for i in range(len(current_queen)):</span><br><span class="line">        if current_queen[i] == col or (n - i) == col - current_queen[i] or (n - i) == current_queen[i] - col:</span><br><span class="line">            return False</span><br><span class="line">    return True</span><br><span class="line"></span><br><span class="line">def dfs(n, current_queen):</span><br><span class="line">    global  res, res_num</span><br><span class="line">    if n &gt;= queen_num:</span><br><span class="line">        res, res_num = [res + '第' + str(res_num + 1) + '种皇后放置方法为:' + ''.join(str([(x + 1) for x in current_queen])) + '\n', res_num + 1]</span><br><span class="line">        return</span><br><span class="line">    for i in range(queen_num):</span><br><span class="line">        if judge_n(n, i, current_queen):</span><br><span class="line">            dfs(n + 1, current_queen + [i])</span><br><span class="line"></span><br><span class="line">print('请输入皇后的个数:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    queen_num, res, res_num = int(line.strip()), '', 0</span><br><span class="line">    chess = [[0 for i in range(queen_num)] for j in range(queen_num)]</span><br><span class="line">    dfs(0, [])</span><br><span class="line">    print('一共有' + str(res_num) + '种皇后放置方法\n' + res)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-1"><a href="#代码运行结果-1" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/dfs3.png" alt="3"></p><h1 id="经典例题-旅行商问题TSP"><a href="#经典例题-旅行商问题TSP" class="headerlink" title="经典例题(旅行商问题TSP)"></a><font size="5" color="red">经典例题(旅行商问题TSP)</font></h1><h2 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  现有n个景点，从第一个景点出发，两个景点之间有数字代表可以直接到达，问如何找到一个路径能够不重复的走遍所有景点回到出发点，而且所经过的路径长度是最短的。<br>  第一行输入景点的个数，第二行输入两地之间可以直接到达的数量，然后每行输入两地和之间的距离</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">5 # 景点个数</span><br><span class="line">9 # 景点之间的连接数</span><br><span class="line">1 2 3 # 景点1和景点2之间的距离为3</span><br><span class="line">1 4 8</span><br><span class="line">1 5 9</span><br><span class="line">2 3 3</span><br><span class="line">2 4 10</span><br><span class="line">2 5 5</span><br><span class="line">3 4 4</span><br><span class="line">3 5 3</span><br><span class="line">4 5 20</span><br></pre></td></tr></tbody></table></figure><h2 id="算法分析-2"><a href="#算法分析-2" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  旅行商问题(TSP)是一个典型的问题，此问题的解空间为n，每一个景点都可以到与之相连的所有点，因此当景点数很多时，最优解的搜索是十分缓慢的。<br>  分析剪枝函数，剪枝函数容易看出，由于不是任何两个景点都是相连的，而且走过的景点不能再走一次，所以这也大大减少了解空间的个数。</p><h2 id="python代码实战-2"><a href="#python代码实战-2" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def dfs(n, current_dist, current_route):</span><br><span class="line">    global min_dist, min_route</span><br><span class="line">    if n &gt;= scenic_spot_num:</span><br><span class="line">        min_dist, min_route = [current_dist + connection[current_route[-1]][0], current_route[:] + [0]] if current_dist + connection[current_route[-1]][0] &lt; min_dist else [min_dist, min_route[:]]</span><br><span class="line">        return</span><br><span class="line">    for i in range(scenic_spot_num):</span><br><span class="line">        if i not in current_route and connection[current_route[-1]][i] != 0 and current_dist + connection[current_route[-1]][i] + connection[i][0] &lt; min_dist:</span><br><span class="line">            dfs(n + 1, current_dist + connection[current_route[-1]][i], current_route + [i])</span><br><span class="line"></span><br><span class="line">print('请输入景点数:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    scenic_spot_num, min_dist, min_route = int(line.strip()), 65535, []</span><br><span class="line">    print('请输入连通的景点数:')</span><br><span class="line">    connection_num, connection = int(sys.stdin.readline().strip()), [[0 for i in range(scenic_spot_num)] for j in range(scenic_spot_num)]</span><br><span class="line">    print('请依次输入两个景点之间的距离:')</span><br><span class="line">    for i in range(connection_num):</span><br><span class="line">        begin, end, weight = [int(i) for i in sys.stdin.readline().strip().split()]</span><br><span class="line">        connection[begin - 1][end - 1], connection[end - 1][begin - 1] = weight, weight</span><br><span class="line">    dfs(1, 0, [0])</span><br><span class="line">    print('最短的路径为:' + '-&gt;'.join([str(x + 1) for x in min_route]) + '\n最短的路径长度为:', min_dist)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-2"><a href="#代码运行结果-2" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/dfs4.png" alt="4"></p><h1 id="算法总结"><a href="#算法总结" class="headerlink" title="算法总结"></a><font size="5" color="red">算法总结</font></h1><p>  深度优先搜索是一个基本搜索方法，对于很多问题来说都可以用深度优先搜索来解决。但不一定是最优的解法，因为深度优先搜索是指数级的时间复杂度，但是对于有些问题不得不使用深度优先进行遍历，因此寻找合适的约束条件可以大大减少时间的开销。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Depth-First-Search&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="常用算法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>动态规划(Dynamic Programming)</title>
    <link href="https://USTCcoder.github.io/2019/07/25/algorithm%20Dynamic_Programming/"/>
    <id>https://USTCcoder.github.io/2019/07/25/algorithm Dynamic_Programming/</id>
    <published>2019-07-25T11:38:40.000Z</published>
    <updated>2020-08-31T14:12:39.297Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">动态规划</font></strong></center><p></p><h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a><font size="5" color="red">原理介绍</font></h1><p>   <strong>Dynamic Programming(DP):动态规划</strong>，是运筹学的一个分支，是求解决策过程最优化的数学方法。于1957年被Richard Bellman(理查德·贝尔曼)提出。其中的Programming不是编程的意思，而是指一种表格处理法，把每一步得到的子问题的结果<strong>存储在表格</strong>里，每次遇到子问题时不需要再求解一遍。其本质也是一种分治算法，其不同点在于分治算法将原问题分解为若干子问题，自顶向下求解各子问题，再合并子问题的解。动态规划也是把原问题分解为若干子问题，然后<strong>自底向上</strong>，先求解最小的子问题，把结果存储起来，求解大问题时直接查询小问题的解，<strong>避免了大量的重复计算</strong>，提高了算法效率。<strong>其本质为：递归+记忆化</strong><br><a id="more"></a></p><p><img src="/images/ALGORITHM/dynamic.png" alt="dynamic"></p><h1 id="问题条件"><a href="#问题条件" class="headerlink" title="问题条件"></a><font size="5" color="red">问题条件</font></h1><h2 id="最优子结构"><a href="#最优子结构" class="headerlink" title="最优子结构"></a><font size="4">最优子结构</font></h2><p>  <font size="3">最优子结构性质是指问题的最优解包含其子问题的最优解。最优子结构是使用动态规划的最基本条件，如果问题不具有最优子结构性质，就不可以使用动态规划解决。</font></p><h2 id="子问题重叠"><a href="#子问题重叠" class="headerlink" title="子问题重叠"></a><font size="4">子问题重叠</font></h2><p>  <font size="3">子问题重叠是指再求解子问题的过程中，有大量的子问题是重复的，那么只需要求解依次，然后存储在表格中，以便使用时查询。这不是动态规划的必要条件，但是可以充分体现动态规划的优势。</font></p><h1 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a><font size="5" color="red">算法步骤</font></h1><p>  <font size="3">(1)分析最优解的结构特征</font><br>  <font size="3">(2)定义状态转移方程(递归式)</font><br>  <font size="3">(3)自底向上计算最优解并记录</font></p><h1 id="经典例题-0-1背包"><a href="#经典例题-0-1背包" class="headerlink" title="经典例题(0-1背包)"></a><font size="5" color="red">经典例题(0-1背包)</font></h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  假设山洞里有n个宝物，每种宝物有一定重量w和相应的价值v，背包的装载能力有限，只能运走重量为m的宝物，宝物不可以分割，如何使背包运走物品的价值最大？<br>  第一行先输入宝物的数量n，和背包的承载重量m，然后每一行输出一个宝物对应的重量w和价值v(用空格分开)<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">5 10 # 宝物数n和背包能装载的重量m</span><br><span class="line">2 6 #每个宝物的重量w和价值v</span><br><span class="line">5 3</span><br><span class="line">4 5</span><br><span class="line">2 4</span><br><span class="line">3 6</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  和普通背包问题不同，如果盲目的装入当前单位重量价值最高的物品，可能导致背包剩余空间较大，达不到最优解。首先分析问题的结构特征，如果第i个宝物装入背包是问题的最优解，背包总重量$m-w_i$一定是问题$\lbrace a_1, a_2, \ldots, a_n \rbrace$的最优解。因此该问题具有最优子结构性质。<br>  根据分析可知，判断第i个宝物装入重量为j的背包时会转化为两种可能，装入或者不装入，不放入时，问题变为前i-1个宝物装入重量为j的背包的最大价值。放入时，问题变为前i-1个宝物装入重量为j-w<sub>i</sub>的背包的最大价值加上第i个宝物的价值v<sub>i</sub>。即比较两者的最大值，用donkey[i][j]表示第i个宝物装入容量为j的背包里的最大价值。<br>$donkey[i][j] = \begin{cases} donkey[i-1][j], &amp; j&lt;w_i \ \max{donkey[i-1][j], donkey[i-1][j-w[i]]+v[i]}, &amp; j \ge w_i \end{cases}$</p><h2 id="0-1背包图解"><a href="#0-1背包图解" class="headerlink" title="0-1背包图解"></a><font size="4">0-1背包图解</font></h2><p><img src="/images/ALGORITHM/dynamic1.png" alt="1"></p><h2 id="python代码实战"><a href="#python代码实战" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">print('请输入宝物数量和驴子承载重量:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    count, weight = line.strip().split()</span><br><span class="line">    count, weight, treasure, donkey, i, j, plan = int(count), int(weight), [], [[0 for i in range(int(weight) + 1)] for j in range(int(count) + 1)], 0, 0, []</span><br><span class="line">    print('请输入每个宝物的重量和价值')</span><br><span class="line">    for i in range(count):</span><br><span class="line">        treasure.append([int(x) for x in sys.stdin.readline().strip().split()])</span><br><span class="line">    for i in range(1, count + 1):</span><br><span class="line">        for j in range(1, weight + 1):</span><br><span class="line">                donkey[i][j] = max(donkey[i - 1][j - treasure[i - 1][0]] + treasure[i - 1][1], donkey[i - 1][j]) if j &gt;= treasure[i - 1][0] else donkey[i - 1][j]</span><br><span class="line">    while donkey[i][j] != 0:</span><br><span class="line">        plan, i, j = [['放入第' + str(i) + '个宝物'] + ['\n'] + plan, i - 1, j - treasure[i - 1][0]] if donkey[i][j] != donkey[i - 1][j] else [plan, i - 1, j]</span><br><span class="line">    print('无法放入任何一个宝物') if donkey[-1][-1] == 0 else print(''.join(plan) + '装入宝物的最大价值为:', donkey[-1][-1])</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果"><a href="#代码运行结果" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/dynamic6.png" alt="6"></p><h1 id="经典例题-最长公共序列"><a href="#经典例题-最长公共序列" class="headerlink" title="经典例题(最长公共序列)"></a><font size="5" color="red">经典例题(最长公共序列)</font></h1><h2 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  给定两个序列，如何找出最长公共子序列<br>  第一行输入字符串s1，第二行输入字符串s2</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ABCADAB # 字符串s1</span><br><span class="line">BACDBA # 字符串s2</span><br></pre></td></tr></tbody></table></figure><h2 id="算法分析-1"><a href="#算法分析-1" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  首先分析问题是否具有最优子结构的性质，假设$Z<em>k={z_1,z_2,z_3,\ldots,z_k}$是$X_m={x_1,x_2,x_3,\ldots,x_m}$和$Y_n={y_1,y_2,y_3,\ldots,y_n}$的最长公共组序列，可以有三种情况讨论：<br>  (1)当$z_k=x_m=y_n$时，则$Z_k={z_1,z_2,z_3,\ldots,z</em>{k-1}}$是$X<em>m={x_1,x_2,x_3,\ldots,x</em>{m-1}}$和$Y<em>n={y_1,y_2,y_3,\ldots,y</em>{n-1}}$的最长公共组序列。<br>  (2)当$z<em>k \neq x_m,x_m \neq y_n$时，则$Z_k={z_1,z_2,z_3,\ldots,z_k}$是$X_m={x_1,x_2,x_3,\ldots,x</em>{m-1}}$和$Y<em>n={y_1,y_2,y_3,\ldots,y_n}$的最长公共组序列。<br>  (3)当$z_k \neq y_n,x_m \neq y_n$时，则$Z_k={z_1,z_2,z_3,\ldots,z_k}$是$X_m={x_1,x_2,x_3,\ldots,x_m}$和$Y_n={y_1,y_2,y_3,\ldots,y</em>{n-1}}$的最长公共组序列。<br>  因此问题满足最优子结构性质，用char[i][j]表示X<sub>i</sub>和Y<sub>j</sub>的最长公共子序列长度。<br>$char[i][j] = \begin{cases} char[i-1][j-1]+1, &amp; x_i=y_j \ \max{char[i][j-1], char[i-1][j]}, &amp; x_i \neq y_j  \end{cases}$</p><h2 id="公共子序列图解"><a href="#公共子序列图解" class="headerlink" title="公共子序列图解"></a><font size="4">公共子序列图解</font></h2><p><img src="/images/ALGORITHM/dynamic2.png" alt="2"></p><h2 id="python代码实战-1"><a href="#python代码实战-1" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">print('输入字符串s1:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    s1 = line.strip()</span><br><span class="line">    print('输入字符串s2:')</span><br><span class="line">    s2 = sys.stdin.readline().strip()</span><br><span class="line">    s1_length, s2_length, i, j, route = len(s1), len(s2), 0, 0, ''</span><br><span class="line">    compare_array, route_array = [[0 for m in range(s2_length + 1)] for n in range(s1_length + 1)], [[0 for m in range(s2_length + 1)] for n in range(s1_length + 1)]</span><br><span class="line">    for i in range(1, s1_length + 1):</span><br><span class="line">        for j in range(1, s2_length + 1):</span><br><span class="line">            compare_array[i][j], route_array[i][j] = [compare_array[i - 1][j - 1] + 1, 1] if s1[i - 1] == s2[j - 1] else([compare_array[i][j - 1], 2] if compare_array[i][j - 1] &gt;= compare_array[i - 1][j] else [compare_array[i - 1][j], 3])</span><br><span class="line">    while i &gt; 0 and j &gt; 0:</span><br><span class="line">        route, i, j = [route + s1[i - 1], i - 1, j - 1] if route_array[i][j] == 1 else ([route, i, j - 1] if route_array[i][j] == 2 else [route , i - 1, j])</span><br><span class="line">    print(s1 + '和' + s2 + '无公共序列') if len(route) == 0 else print(s1 + '和' + s2 + '的最长公共序列的长度为:', len(route), '\n' + s1 + '和' + s2 +'的最长公共序列是:' + route[::-1])</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-1"><a href="#代码运行结果-1" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/dynamic4.png" alt="4"></p><h1 id="经典例题-字符串编辑距离"><a href="#经典例题-字符串编辑距离" class="headerlink" title="经典例题(字符串编辑距离)"></a><font size="5" color="red">经典例题(字符串编辑距离)</font></h1><h2 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  给定两个序列，如何使用最少的增加字符，删除字符，替换字符操作，使两个序列相同？<br>  第一行输入字符串str1，第二行输入字符串str2</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">family # 字符串str1</span><br><span class="line">frame # 字符串str2</span><br></pre></td></tr></tbody></table></figure><h2 id="算法分析-2"><a href="#算法分析-2" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  分析此题是否具有最优子结构性质，假设char[i][j]是$X_i={x_1,x_2,x_3,\ldots,x_i}$和$Y_j={y_1,y_2,y_3,\ldots,y_j}$的编辑距离的最优解，可以有两种情况讨论：<br>  (1)当两个字符串满足$x_i=y_j$时，则char[i-1][j-1]是$X_m={x_1,x_2,x_3,\ldots,x_i}$和$Y_n={y_1,y_2,y_3,\ldots,y_j}$的编辑距离的最优解。<br>  (2)当两个字符串满足$x_i \neq y_j$时，则可以删除字符x<sub>i</sub>或删除字符y<sub>j</sub>或将字符x<sub>i</sub>替换为字符y<sub>j</sub>，即满足下列条件$\max(char[i-1][j],char[i][j-1],char[i-1][j-1])+1$是$X_m={x_1,x_2,x_3,\ldots,x_i}$和$Y_n={y_1,y_2,y_3,\ldots,y_j}$的编辑距离的最优解。<br>  因此问题满足最优子结构性质，写出其状态转移方程。<br>$char[i][j] = \begin{cases} char[i-1][j-1], &amp; x_i=y_j \ \max{char[i][j-1], char[i-1][j],char[i-1][j-1]}+1, &amp; x_i \neq y_j  \end{cases}$</p><h2 id="字符串距离图解"><a href="#字符串距离图解" class="headerlink" title="字符串距离图解"></a><font size="4">字符串距离图解</font></h2><p><img src="/images/ALGORITHM/dynamic3.png" alt="3"></p><h2 id="python代码实战-2"><a href="#python代码实战-2" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">print('输入字符串str1:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    str1 = line.strip()</span><br><span class="line">    print('输入字符串str2:')</span><br><span class="line">    str2 = sys.stdin.readline().strip()</span><br><span class="line">    str1_length, str2_length, i, j, route = len(str1), len(str2), 0 ,0, []</span><br><span class="line">    compare_array = [list(range(str2_length + 1))] + [[x] + [0] * str2_length for x in range(1,str1_length + 1)]</span><br><span class="line">    for i in range(1, str1_length + 1):</span><br><span class="line">        for j in range(1, str2_length + 1):</span><br><span class="line">            compare_array[i][j] = compare_array[i - 1][j - 1] if str1[i - 1] == str2[j - 1] else min(compare_array[i - 1][j - 1], compare_array[i][j - 1], compare_array[i - 1][j]) + 1</span><br><span class="line">    while i &gt; 0 and j &gt; 0:</span><br><span class="line">        route, i, j = [route, i - 1, j - 1] if str1[i - 1] == str2[j - 1] else ([['将' + str1 + '中' + str(i) + '处的' + str1[i - 1] + '替换为' + str2[j - 1]] + ['\n'] + route, i - 1, j - 1] if compare_array[i - 1][j - 1] + 1 == compare_array[i][j] else([['将' + str1 + '中' + str(i) + '处插入' + str2[j - 1]] + ['\n'] + route, i, j - 1] if compare_array[i][j - 1] + 1 == compare_array[i][j] else [['将' + str1 + '中' + str(i) + '处的' + str1[i - 1] + '删除'] + ['\n'] + route, i - 1, j]))</span><br><span class="line">    print(str1 + '和' + str2 + '的编辑距离是:', compare_array[-1][-1], '\n' + '将' + str1 + '转换为' + str2 + '的操作为:' + '\n' + ''.join(route[:-1]))</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-2"><a href="#代码运行结果-2" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/dynamic5.png" alt="5"></p><h1 id="算法总结"><a href="#算法总结" class="headerlink" title="算法总结"></a><font size="5" color="red">算法总结</font></h1><p>  由于动态规划不局限于眼前的最优状态，而是记录了以前的所有状态，因此动态规划具有很强的大局观，可以较为容易地得到全局最优解，因此在实际的生产生活中使用较广。动态规划的关键是分析问题是否具有最优子结构，如果问题具有该性质，说明可以使用动态规划来解决问题。然后是找到其状态转移方程，这也是最难考虑的一个问题。得到了状态转移方程，此问题迎刃而解。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Dynamic Programming&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="常用算法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>分治算法(Divide and Conquer)</title>
    <link href="https://USTCcoder.github.io/2019/07/21/algorithm%20divide_and_conquer/"/>
    <id>https://USTCcoder.github.io/2019/07/21/algorithm divide_and_conquer/</id>
    <published>2019-07-21T11:38:40.000Z</published>
    <updated>2019-08-07T15:58:05.502Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">分治算法</font></strong></center><p></p><h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a><font size="5" color="red">原理介绍</font></h1><p>   <strong>Divide and Conquer:分治算法</strong>，分而治之。山高皇帝远，治理国家，不可能所有的事情都由皇帝解决，国家分省、市、县、镇、村，层层管理，最终汇总合并到皇帝。借鉴于这种思想，将一个规模为n的问题<strong>分解</strong>为k个规模较小的子问题，这些子问题<strong>互相独立</strong>且<strong>与原问题相同</strong>（如果子问题的规模仍然不够小，则再继续划分），然后<strong>递归求解</strong>这些问题，最好用适当的方法将各子问题的解<strong>合并</strong>成原问题的解。<br><a id="more"></a></p><p><img src="/images/ALGORITHM/divide.jpg" alt="divide"></p><h1 id="解题步骤"><a href="#解题步骤" class="headerlink" title="解题步骤"></a><font size="5" color="red">解题步骤</font></h1><h2 id="分解"><a href="#分解" class="headerlink" title="分解"></a><font size="4">分解</font></h2><p>  <font size="3">将要解决的问题分解为若干个规模较小，相互独立，与原问题形式相同的子问题。</font></p><h2 id="治理"><a href="#治理" class="headerlink" title="治理"></a><font size="4">治理</font></h2><p>  <font size="3">求解各个子问题，由于子问题的形式与原问题相同，只是规模较小而已，而当子问题划分得足够小时，就可以用简单得方法解决。</font></p><h2 id="合并"><a href="#合并" class="headerlink" title="合并"></a><font size="4">合并</font></h2><p>  <font size="3">按照原问题的要求，将各个子问题的解逐层合并，构成原问题的解。</font></p><h1 id="经典例题-合并排序"><a href="#经典例题-合并排序" class="headerlink" title="经典例题(合并排序)"></a><font size="5" color="red">经典例题(合并排序)</font></h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  给定一个无序数列，将其排成有序数列。<br>  第一行输出元素的个数n，第二行依次输出数列中的元素(用空格分开)<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">8 # 元素的个数</span><br><span class="line">42 15 20 6 8 38 50 12 # 数列中的元素</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  在数列排序中，数越少越容易排序，基于这个思想，可以考虑将长序列分成短序列，当序列分为只有一个元素时，其本身即为有序。<br>  然后执行合并操作，将两个有序序列合并为一个有序序列也是较为容易的。</p><h2 id="归并图解"><a href="#归并图解" class="headerlink" title="归并图解"></a><font size="4">归并图解</font></h2><p><img src="/images/ALGORITHM/divide4.png" alt="4"></p><h2 id="python代码实战"><a href="#python代码实战" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def merge_sort(list_, begin, end):</span><br><span class="line">    if begin &lt; end:</span><br><span class="line">        merge_sort(list_, begin, (begin + end) // 2)</span><br><span class="line">        merge_sort(list_, (begin + end) // 2 + 1, end)</span><br><span class="line">        merge(list_, begin, (begin + end) // 2, end)</span><br><span class="line"></span><br><span class="line">def merge(list_, begin, mid, end):</span><br><span class="line">    new_list = []</span><br><span class="line">    p_1, p_2 = begin, mid + 1</span><br><span class="line">    while p_1&lt;=mid and p_2 &lt;=end:</span><br><span class="line">        new_list, p_1, p_2 = [new_list + [list_[p_1]], p_1 + 1, p_2 + 0] if list_[p_1] &lt;= list_[p_2] else [new_list + [list_[p_2]], p_1 + 0, p_2 + 1]</span><br><span class="line">    new_list += list_[p_2:end + 1] if p_1 &gt; mid else list_[p_1:mid + 1]</span><br><span class="line">    list_[begin:end + 1] = new_list</span><br><span class="line"></span><br><span class="line">print('请输入数列中元素的个数')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    num_number = int(line.strip().split()[0])</span><br><span class="line">    print('请依次输入数列中的元素，并用空格分开')</span><br><span class="line">    num_list = [int(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">    merge_sort(num_list, 0, num_number - 1)</span><br><span class="line">    print('合并排序的结果为:', num_list)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果"><a href="#代码运行结果" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/divide1.png" alt="1"></p><h1 id="经典例题-快速排序"><a href="#经典例题-快速排序" class="headerlink" title="经典例题(快速排序)"></a><font size="5" color="red">经典例题(快速排序)</font></h1><h2 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  给定一个无序数列，将其排成有序数列。<br>  第一行输出元素的个数n，第二行依次输出数列中的元素(用空格分开)</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">9 # 元素的个数</span><br><span class="line">30 24 5 58 18 36 12 42 39 # 数列中的元素</span><br></pre></td></tr></tbody></table></figure><h2 id="算法分析-1"><a href="#算法分析-1" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  快速排序的思想和合并排序类似，都是采用分治算法，区别之处在于，合并排序通过先分裂，再合并，合并的同时进行排序。而快速排序是先进行排序，生成两段有序的数列，然后找到分裂点再进行分裂，最后再合并。<br>  如何找到分裂点是快速排序的难点<br>(1)首先取数组的第一个元素作为基准元素base，建立一个头指针和一个尾指针。<br>(2)从左向右进行扫描，如果找到大于base的元素，头指针停留在此处，进入步骤(3)。<br>(3)从右向左进行扫描，如果找到小于等于base的元素，尾指针停留在此处，然后交换头尾指针的值，并且头指针向右移动一个距离，尾指针向左移动一个距离。<br>(4)重复(2)和(3)，直到头指针大于等于尾指针的位置。此时头指针之前的元素都是小于等于基准元素的，头指针之后的元素都是大于基准元素的。<br>  找到分裂点以后，根据分裂点将长数列分解成短数列重复上述方法进行分裂，最终将分裂的结果按顺序合并即可。</p><h2 id="快排图解"><a href="#快排图解" class="headerlink" title="快排图解"></a><font size="4">快排图解</font></h2><p><img src="/images/ALGORITHM/divide5.png" alt="5"></p><h2 id="python代码实战-1"><a href="#python代码实战-1" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def quick_sort(list_, begin, end):</span><br><span class="line">    if begin &lt; end:</span><br><span class="line">        base_element, head, tail = list_[begin], begin, end</span><br><span class="line">        while head &lt; tail:</span><br><span class="line">            while head &lt; tail and list_[head] &lt;= base_element:</span><br><span class="line">                head += 1</span><br><span class="line">            while head &lt; tail and list_[tail] &gt; base_element:</span><br><span class="line">                tail -= 1</span><br><span class="line">            list_[head], list_[tail], head, tail = [list_[tail], list_[head], head + 1, tail - 1] if head &lt; tail else [list_[head], list_[tail], head, tail]</span><br><span class="line">        if list_[head] &lt; list_[begin]:</span><br><span class="line">            list_[head], list_[begin] = list_[begin], list_[head]</span><br><span class="line">            quick_sort(list_, begin, head - 1)</span><br><span class="line">            quick_sort(list_, head + 1, end)</span><br><span class="line">        else:</span><br><span class="line">            list_[head - 1], list_[begin] = list_[begin], list_[head - 1]</span><br><span class="line">            quick_sort(list_, begin, head - 2)</span><br><span class="line">            quick_sort(list_, head, end)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print('请输入要排序的数据个数:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    num_number = int(line.strip().split()[0])</span><br><span class="line">    print('请输入要排序的数据，并用空格分开')</span><br><span class="line">    list_number = [int(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">    quick_sort(list_number, 0, num_number - 1)</span><br><span class="line">    print('快速排序的结果为:', list_number)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-1"><a href="#代码运行结果-1" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/divide2.png" alt="2"></p><h1 id="经典例题-大数乘法"><a href="#经典例题-大数乘法" class="headerlink" title="经典例题(大数乘法)"></a><font size="5" color="red">经典例题(大数乘法)</font></h1><h2 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  现有两个很大的数据，由于计算机硬件的限制，无法用乘法直接进行求解，如何设计算法求解出正确的结果？<br>  第一行输入乘数a，第二行输入乘数b</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1122334455667788998877665544332211 #乘数 a</span><br><span class="line">9988776655443322112233445566778899 # 乘数 b</span><br></pre></td></tr></tbody></table></figure><h2 id="算法分析-2"><a href="#算法分析-2" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  由于计算机的硬件限制，无法对大值数据进行操作，因此需要根据运算的法则对数据进行分解。<br>  假设要计算$3278 * 41926$，可以将两个数进行分解，将3278分解为$(32*10^2)+(78*10^0)$，将41926分解为$(419*10^2)+(26*10^0)$，然后根据乘法的运算性质$(a+b)*(c+d)=ac+ad+bc+bd$可得原式为$(32*419*10^4)+(32*26*10^2)+(78*419*10^2)+(78*26*10^0)$。<br>  然后发现上式的第一项$(32*419*10^4)$还可以进行分解，将32分解为$(3*10^1)+(2*10^0)$，将419分解为$(41*10^1)+(9*10^0)$，……，直到分解出的两个数中有一个为一位数则不需要分解，因为一位数的乘法很简单。</p><h2 id="乘法图解"><a href="#乘法图解" class="headerlink" title="乘法图解"></a><font size="4">乘法图解</font></h2><p><img src="/images/ALGORITHM/divide6.png" alt="6"></p><h2 id="python代码实战-2"><a href="#python代码实战-2" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">class Number:</span><br><span class="line">    def __init__(self, value = None, length = 0, ten = 0):</span><br><span class="line">        self.value = value</span><br><span class="line">        self.length = length</span><br><span class="line">        self.ten = ten</span><br><span class="line"></span><br><span class="line">def big_add(number_a, number_b, ans):</span><br><span class="line">    number_a, number_b = [number_b, number_a] if number_a.ten &lt; number_b.ten else [number_a, number_b]</span><br><span class="line">    ans.ten, tmp, temp, number_a_len, number_b_len = number_b.ten, 0, number_a.ten - number_b.ten, number_a.length + number_a.ten, number_b.length + number_b.ten</span><br><span class="line">    ans_length = max(number_a_len, number_b_len)</span><br><span class="line">    for i in range(ans_length - ans.ten):</span><br><span class="line">        ta = 0 if i &lt; temp or i &gt;= number_a.length + temp else number_a.value[i - temp]</span><br><span class="line">        tb = number_b.value[i] if i &lt; number_b.length else 0</span><br><span class="line">        ans.value[i], tmp = (ta + tb + tmp) % 10, (ta + tb + tmp) // 10</span><br><span class="line">    ans.length = ans_length - ans.ten</span><br><span class="line">    if tmp &gt; 0:</span><br><span class="line">        ans.value[ans_length - ans.ten], ans.length = [tmp, ans_length - ans.ten + 1]</span><br><span class="line"></span><br><span class="line">def big_mul(number_a, number_b, ans):</span><br><span class="line">    mid_a, mid_b = [number_a.length &gt;&gt; 1, number_b.length &gt;&gt; 1]</span><br><span class="line">    if number_a.length == 1 or number_b.length == 1:</span><br><span class="line">        if number_a.length == 1:</span><br><span class="line">            number_a, number_b = number_b, number_a</span><br><span class="line">        ans.ten, w, tmp = number_a.ten + number_b.ten, number_b.value[0], 0</span><br><span class="line">        for i in range(number_a.length):</span><br><span class="line">            ans.value[i], tmp = (w * number_a.value[i] + tmp) % 10, (w * number_a.value[i] + tmp) // 10</span><br><span class="line">        ans.length = number_a.length</span><br><span class="line">        if tmp &gt; 0:</span><br><span class="line">            ans.value[number_a.length], ans.length = [tmp, number_a.length + 1]</span><br><span class="line">        return</span><br><span class="line">    high_a, low_a, high_b, low_b = [Number(number_a.value[mid_a:], number_a.length - mid_a, number_a.ten + mid_a), Number(number_a.value[:mid_a], mid_a, number_a.ten), Number(number_b.value[mid_b:], number_b.length - mid_b, number_b.ten + mid_b), Number(number_b.value[:mid_b], mid_b, number_b.ten)]</span><br><span class="line">    t_1, t_2, t_3, t_4, tmp_ans = [Number([0] * (high_a.length + high_a.ten + high_b.length + high_b.ten)), Number([0] * (high_a.length + high_a.ten + low_b.length + low_b.ten)), Number([0] * (low_a.length + low_a.ten + high_b.length + high_b.ten)),Number([0] * (low_a.length + low_a.ten + low_b.length + low_b.ten)), Number([0] * (len(ans.value) + ans.ten))]</span><br><span class="line">    big_mul(high_a, high_b, t_1)</span><br><span class="line">    big_mul(high_a, low_b, t_2)</span><br><span class="line">    big_mul(low_a, high_b, t_3)</span><br><span class="line">    big_mul(low_a, low_b, t_4)</span><br><span class="line">    big_add(t_1, t_2, ans)</span><br><span class="line">    big_add(t_3, ans, tmp_ans)</span><br><span class="line">    big_add(t_4, tmp_ans, ans)</span><br><span class="line"></span><br><span class="line">print('输入大整数a:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    number_a = list(line.strip().split()[0])</span><br><span class="line">    print('输入大整数b:')</span><br><span class="line">    number_b = list(sys.stdin.readline().strip().split()[0])</span><br><span class="line">    char, number_a = [-1, number_a[1:]] if number_a[0] == '-' else [1, number_a]</span><br><span class="line">    char, number_b = [char * -1, number_b[1:]] if number_b[0] =='-' else [char, number_b]</span><br><span class="line">    number_a, number_b, ans = Number(list(reversed([int(x) for x in number_a])), len(number_a), 0), Number(list(reversed([int(x) for x in number_b])), len(number_b), 0), Number([0] * (len(number_a) + len(number_b)))</span><br><span class="line">    big_mul(number_a, number_b, ans)</span><br><span class="line">    print(''.join([str(x) for x in ans.value[::-1]]).lstrip('0'))</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-2"><a href="#代码运行结果-2" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/divide3.png" alt="3"></p><h1 id="算法总结"><a href="#算法总结" class="headerlink" title="算法总结"></a><font size="5" color="red">算法总结</font></h1><p>  分治算法的难点是如何进行分解，就像盗梦空间的梦境一样，层层深入，却要清醒在每一层应该做什么事情。是应该先分裂再做事情还是先做事情在分裂也是需要考虑的。最重要的一点是梦境不能永远深入，一定要在某一个时机回到现实，即要拥有截止条件，判断是否已经达到需要的深度。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Divide and Conquer&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="常用算法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>贪心算法(Greedy)</title>
    <link href="https://USTCcoder.github.io/2019/07/19/algorithm%20greedy/"/>
    <id>https://USTCcoder.github.io/2019/07/19/algorithm greedy/</id>
    <published>2019-07-19T11:29:40.000Z</published>
    <updated>2019-08-07T15:58:14.589Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">贪心算法</font></strong></center><p></p><h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a><font size="5" color="red">原理介绍</font></h1><p>   <strong>Greedy:贪心算法</strong>，又称贪婪算法，人要活在当下，看清楚眼前，这种思想就是贪心思想。是指在对问题求解时，总是做出<strong>当前最好</strong>的选择，不从整体最优上加以考虑。也就是说，它期望通过<strong>局部最优选择</strong>从而得到全局最优的解决方案。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><h2 id="贪心选择"><a href="#贪心选择" class="headerlink" title="贪心选择"></a><font size="4">贪心选择</font></h2><p>  贪心选择指原问题的整体最优解可以通过一系列局部最优的选择得到。应用同一规则，将原问题变为一个相似的但规模更小的子问题，而后的每一步都是当前最佳的选择。这种选择依赖于已做出的选择，但不依赖于未做出的选择。</p><h2 id="最优子结构"><a href="#最优子结构" class="headerlink" title="最优子结构"></a><font size="4">最优子结构</font></h2><p>  当一个问题的最优解包含其子问题的最优解时，称此问题具有最优子结构性质。问题的最优子结构性质是该问题能否可用贪心算法求解的关键。如原问题$S=\lbrace a_1, a_2, a_3, \ldots, a_n \rbrace$,通过贪心选择出一个当前最优解${a_i}$之后，转化为求解子问题$S- \lbrace a_i \rbrace $，如果原问题的最优解包含子问题的最优解，则说明该问题满足最优子结构性质。</p><h1 id="经典例题-背包问题"><a href="#经典例题-背包问题" class="headerlink" title="经典例题(背包问题)"></a><font size="5" color="red">经典例题(背包问题)</font></h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  假设山洞里有n个宝物，每种宝物有一定重量w和相应的价值v，背包的装载能力有限，只能运走重量为m的宝物，宝物可以分割，如何使背包运走物品的价值最大？<br>  第一行先输入宝物的数量n，和背包的承载重量m，然后每一行输出一个宝物对应的重量w和价值v(用空格分开)<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">6 19 # 宝物数n和背包能装载的重量m</span><br><span class="line">2 8 #每个宝物的重量w和价值v</span><br><span class="line">6 1</span><br><span class="line">7 9</span><br><span class="line">4 3</span><br><span class="line">10 2</span><br><span class="line">3 4</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  对于普通背包问题，宝物可以分割，当前取得的宝物不会受到后面宝物的影响，如果是0-1背包，即物品不可以分割，当前宝物就会受到后面宝物的影响。假设该问题的最优解为S，拿走当前最优解第i个宝物，现在转换成一个新问题，有n-1个宝物，背包重量为$m-w_i$，可以得到最优解为$S- \lbrace a_i \rbrace$。因此可以使用贪心算法。<br>  该问题有三个思考方向<br>(1)选择价值最大的宝物装入背包<br>(2)选择重量最小的宝物装入背包<br>(3)选择单位重量价值最大的宝物装入背包<br>选择价值最大的宝物，如果重量也很大，则不是最优解；选择重量最小的宝物，如果价值也很小，则也不是最优解；因此应该选择单位重量价值最大的宝物。</p><h2 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a><font size="4">流程图</font></h2><p><img src="/images/ALGORITHM/greedy8.png" alt="8"></p><h2 id="python代码实战"><a href="#python代码实战" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">print('请输入宝物数量和驴子承载重量:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    count, weight = line.strip().split()</span><br><span class="line">    count, weight, treasure, price, plan = int(count), float(weight), [], 0, ''</span><br><span class="line">    print('请输入每个宝物的重量和价值')</span><br><span class="line">    for i in range(count):</span><br><span class="line">        tmp = [float(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">        treasure.append([i + 1] + tmp + [tmp[1] / tmp[0]])</span><br><span class="line">    treasure.sort(key=lambda x: (-x[3]))</span><br><span class="line">    for i in treasure:</span><br><span class="line">        if weight &gt; i[1]:</span><br><span class="line">            price, weight, plan = price + i[2], weight - i[1], plan + '将第' + str(i[0]) +'个宝物全部装入\n'</span><br><span class="line">        else:</span><br><span class="line">            price, plan = price + weight * i[3], plan + '剩余重量全部装入第' + str(i[0]) + '个宝物\n'</span><br><span class="line">            break</span><br><span class="line">    print('最优的方案为:\n' + plan + '装入的最大价值为:', price)</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果"><a href="#代码运行结果" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/greedy1.png" alt="1"></p><h1 id="经典例题-最短路径"><a href="#经典例题-最短路径" class="headerlink" title="经典例题(最短路径)"></a><font size="5" color="red">经典例题(最短路径)</font></h1><h2 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  现有一个景点地图，有n个城市，m条路径(路径是有向的，即来回的距离不同)，假设从某一结点出发，求到其他各个结点的最短路径。<br>  第一行先输入城市数n，第二行输入总路径数m，然后每一行输入A市，B市，以及A到B的距离(用空格分开)，最后输入起始位置和目的地位置。<br><img src="/images/ALGORITHM/greedy4.png" alt="4"><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">5 # 城市数n</span><br><span class="line">8 # 路径数m</span><br><span class="line">1 2 2 #从1号城市到2号城市的距离为2</span><br><span class="line">1 3 5</span><br><span class="line">2 3 2</span><br><span class="line">2 4 6</span><br><span class="line">3 4 7</span><br><span class="line">3 5 1</span><br><span class="line">4 3 2</span><br><span class="line">4 5 4</span><br><span class="line">1 #起始位置</span><br><span class="line">5 #目的地位置</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/ALGORITHM/greedy2.png" alt="2"></p><h2 id="算法分析-1"><a href="#算法分析-1" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  最短路径算法是1959年由荷兰图灵奖得主Edsger Wybe Dijkstra(艾兹格·迪科斯彻)提出的，从起始点开始，逐渐增加结点数，依次求出源点到各个定点的最短路径，直到求出目标结点。<br>  首先建立一个数组dist，存放从源点到各点的最短路径，列表S，代表已经包含的结点数，初始值为空。S中的结点，代表从源点到该点的最短路径已确定。列表V,代表没有包含的结点数，初始值为全部结点，然后从这些结点中寻找距离源点最近的结点(贪心算法)。假如找到某一点i，说明从源点通过S，到达i的最短路径为dist[i]，将i加入S，并从V中剔除。更新V中其余的点到源点的最短路径，即检查其余各点是否可以通过i点到达源点。</p><h2 id="流程图-1"><a href="#流程图-1" class="headerlink" title="流程图"></a><font size="4">流程图</font></h2><p><img src="/images/ALGORITHM/greedy9.png" alt="9"></p><h2 id="python代码实战-1"><a href="#python代码实战-1" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">print('请输入城市个数:')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    num_city = int(line.strip().split()[0])</span><br><span class="line">    print('请输入路线个数:')</span><br><span class="line">    num_route = int(sys.stdin.readline().strip().split()[0])</span><br><span class="line">    print('请输入城市之间的路线及距离')</span><br><span class="line">    map = [[65535*(i!=j) for i in range(num_city+1)] for j in range(num_city+1)]</span><br><span class="line">    for i in range(num_route):</span><br><span class="line">        begin, end, length = [int(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">        map[begin][end] = length</span><br><span class="line">    print('请输入小明所在的位置:')</span><br><span class="line">    begin = int(sys.stdin.readline().strip().split()[0])</span><br><span class="line">    print('请输入终点的位置:')</span><br><span class="line">    end = int(sys.stdin.readline().strip().split()[0])</span><br><span class="line">    dist, route = [0 | x for x in map[begin]], [0 for i in range(num_city+1)]</span><br><span class="line">    un_used = list(range(num_city+1))</span><br><span class="line">    while len(un_used) &gt; 1:</span><br><span class="line">        tmp_loc, tmp_length = [0, 65535]</span><br><span class="line">        for i in un_used[1:]:</span><br><span class="line">            tmp_length, tmp_loc = [dist[i], i] if dist[i] &lt; tmp_length else [tmp_length, tmp_loc]</span><br><span class="line">        if tmp_loc == end or tmp_loc == 0:</span><br><span class="line">            break</span><br><span class="line">        un_used.remove(tmp_loc)</span><br><span class="line">        for j in un_used[1:]:</span><br><span class="line">            dist[j], route[j] = [dist[tmp_loc] + map[tmp_loc][j], tmp_loc] if dist[tmp_loc] + map[tmp_loc][j] &lt; dist[j] else [dist[j], route[j]]</span><br><span class="line">    if route[end] == 0:</span><br><span class="line">        print('目的地不可达')</span><br><span class="line">    else:</span><br><span class="line">        terminal, res = end, str(end)</span><br><span class="line">        while route[terminal] != 0:</span><br><span class="line">            terminal = route[terminal]</span><br><span class="line">            res = str(terminal) + '-&gt;' + res</span><br><span class="line">        print('所走过的路径为:' + str(begin) + '-&gt;' + res + ' 最短距离为:',dist[end])</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-1"><a href="#代码运行结果-1" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/greedy3.png" alt="3"></p><h1 id="经典例题-最小生成树"><a href="#经典例题-最小生成树" class="headerlink" title="经典例题(最小生成树)"></a><font size="5" color="red">经典例题(最小生成树)</font></h1><h2 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a><font size="4">问题描述</font></h2><p>  现有一个学校，下面有n个学院，m条路经(路径是无向的，即来回的距离相同)现在设计一条网络布线，将各个学院联通起来，问如何设计可以使费用最少。<br>  第一行先输入结点数n和边数，然后每一行输入A，B，以及A到B的距离(用空格分开)，最后输入起始位置。</p><p><img src="/images/ALGORITHM/greedy5.png" alt="5"></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">7 12 #输入结点数和边数</span><br><span class="line">1 2 23 # 从1号学院到2号学院的距离为23</span><br><span class="line">1 6 28</span><br><span class="line">1 7 36</span><br><span class="line">2 3 20</span><br><span class="line">2 7 1</span><br><span class="line">3 4 15</span><br><span class="line">3 7 4</span><br><span class="line">4 5 3</span><br><span class="line">4 7 9</span><br><span class="line">5 6 17</span><br><span class="line">5 7 16</span><br><span class="line">6 7 25</span><br><span class="line">1 #起始位置</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/ALGORITHM/greedy6.png" alt="6"></p><h2 id="算法分析-2"><a href="#算法分析-2" class="headerlink" title="算法分析"></a><font size="4">算法分析</font></h2><p>  最小生成树算法是1957年由Robert C. Prim(普里姆)提出的，和dijkstra算法类似，从起始点开始，逐渐增加结点数，依次求出每一步的最小生成树，直到包含所有结点。<br>  首先建立一个列表S，代表已经包含的结点数，初始值为空。S中的结点，代表从源点到该点的最短路径已确定。列表V,代表没有包含的结点数，初始值为全部结点。建立一个数组cost，存放从V中结点到S中最近邻点的距离，然后从这些距离中寻找最短的距离(贪心算法)。假如找到某一点i，将i加入S中，说明此时S是最小的生成树，并将i从V中剔除。更新V中其余的结点通过i结点到达S中结点的最近邻的路径。</p><h2 id="流程图-2"><a href="#流程图-2" class="headerlink" title="流程图"></a><font size="4">流程图</font></h2><p><img src="/images/ALGORITHM/greedy10.png" alt="10"></p><h2 id="python代码实战-2"><a href="#python代码实战-2" class="headerlink" title="python代码实战"></a><font size="4">python代码实战</font></h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">print('输入结点数和边数')</span><br><span class="line">for line in sys.stdin:</span><br><span class="line">    num_node, num_side = [int(x) for x in line.strip().split()]</span><br><span class="line">    map_node = [[65535*(i!=j) for i in range(num_node + 1)] for j in range(num_node + 1)]</span><br><span class="line">    print('输入结点及之间的边值')</span><br><span class="line">    for i in range(num_side):</span><br><span class="line">        begin, end, weight = [int(x) for x in sys.stdin.readline().strip().split()]</span><br><span class="line">        map_node[begin][end], map_node[end][begin] = weight, weight</span><br><span class="line">    print('请输入起始结点')</span><br><span class="line">    begin_node = int(sys.stdin.readline().strip().split()[0])</span><br><span class="line">    un_used, low_cost, close_node = list(range(num_node + 1)), [0 | x for x in map_node[begin_node]], [0 for i in range(num_node + 1)]</span><br><span class="line">    while len(un_used) &gt; 1:</span><br><span class="line">        tmp_loc, tmp_length = 0, 65535</span><br><span class="line">        for i in un_used[1:]:</span><br><span class="line">            tmp_loc, tmp_length = [i, low_cost[i]] if low_cost[i] &lt; tmp_length else [tmp_loc, tmp_length]</span><br><span class="line">        if tmp_loc == 0:</span><br><span class="line">            break</span><br><span class="line">        un_used.remove(tmp_loc)</span><br><span class="line">        for i in un_used[1:]:</span><br><span class="line">            low_cost[i], close_node[i] = [map_node[tmp_loc][i], tmp_loc] if map_node[tmp_loc][i] &lt; low_cost[i] else [low_cost[i], close_node[i]]</span><br><span class="line">    if len(un_used) &gt; 1:</span><br><span class="line">        print('无法将所有的边连接')</span><br><span class="line">    else:</span><br><span class="line">        for i in range(1,len(close_node)):</span><br><span class="line">            if i == begin_node:</span><br><span class="line">                continue</span><br><span class="line">            print(str(begin_node) + '-&gt;' + str(i)) if close_node[i] == 0 else print(str(close_node[i]) + '-&gt;' + str(i))</span><br><span class="line">        print('最小的花费为:' + str(sum(low_cost[1:])))</span><br></pre></td></tr></tbody></table></figure><h2 id="代码运行结果-2"><a href="#代码运行结果-2" class="headerlink" title="代码运行结果"></a><font size="4">代码运行结果</font></h2><p><img src="/images/ALGORITHM/greedy7.png" alt="7"></p><h1 id="算法总结"><a href="#算法总结" class="headerlink" title="算法总结"></a><font size="5" color="red">算法总结</font></h1><p>  贪心算法相对简单，代码易于实现。但是由于其只关注于当前的最优解，很难得到全局最优解，不符合人类的思维习惯，因此在日常生活中，很少见到贪心算法处理的问题。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Greedy&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="常用算法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>MLP(Multi-Layer Perceptron)</title>
    <link href="https://USTCcoder.github.io/2019/07/12/classfication%20MLP/"/>
    <id>https://USTCcoder.github.io/2019/07/12/classfication MLP/</id>
    <published>2019-07-12T01:25:16.000Z</published>
    <updated>2020-09-03T08:47:52.525Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/MACHINE/mlp.png" alt="mlp"></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  <strong>MLP(Multi-Layer Perceptron)</strong>:多层感知机<strong>本质上是一种全连接的深度神经网络(DNN)</strong>，感知机只有一层功能神经元进行学习和训练，其能力非常有限，难以解决非线性可分的问题。为了解决这个问题，需要考虑使用多层神经元进行学习。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><h2 id="反向传播-BP-BackPropagation"><a href="#反向传播-BP-BackPropagation" class="headerlink" title="反向传播(BP, BackPropagation)"></a>反向传播(BP, BackPropagation)</h2><p>神经网络的学习过程可真是太牛B了，人类的学习过程是，从小学到大学，学习知识以后，都需要考试，然后根据得分修正以往的错误部分。BP的思想也是一样，一开始神经网络什么都不知道，给予网络随机的权重，然后进行学习，每次得到一个结果，我们将其与标准结果进行对比(这类似于考试成绩与标准答案进行对比)。然后根据差异寻找问题的根源。</p><p>BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整。<br>下面我举一个简单的例子，小伙伴们就可以清晰的知道BP的原理。<br>假设有N个样本，样本的特征数为F，因此输入矩阵的大小为[F, N]，只有一个隐藏层，且神经元的个数为F1，因此w1的大小为[F1, F]，b1的大小为[F, 1]，经过计算后z1的大小为[F1, N]，激活函数为ReLu，第一层的输出a1的大小为[F1, N]，对于一个二分类问题，输出神经元的个数为1，因此w2的大小为[1, F1]，b2的大小为[1, 1]，经过计算后z2的大小为[1, N]，激活函数为Sigmoid，第二层的输出$\hat{y}$的大小为[1, N]。</p><p>我们使用以下记号表示前向计算过程</p><script type="math/tex; mode=display">z1 = w1 \cdot x + b1</script><script type="math/tex; mode=display">a1 = ReLu(z1)</script><script type="math/tex; mode=display">z2 = w2 \cdot a1 + b2</script><script type="math/tex; mode=display">\hat{y} = Sigmoid(z2)</script><script type="math/tex; mode=display">J(\hat{y}, y) = -[y \cdot log(\hat{y}) + (1 - y) \cdot log(1 - \hat{y})]</script><p>下面推导误差反向传播过程，我们只要计算出各个参数的梯度即可</p><script type="math/tex; mode=display">\begin{align} \frac{\partial J}{\partial \hat{y}} & = -(\frac{y}{\hat{y}} - \frac{1 - y}{1 - \hat{y}}) \\ \frac{\partial J}{\partial z2} & = \frac{\partial J}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z2} \\ & = -(\frac{y}{\hat{y}} \hat{y}(1 - \hat{y}) - \frac{1 - y}{1 - \hat{y}} \hat{y}(1 - \hat{y})) \\ & = \hat{y} - y \\ \frac{\partial J}{\partial w2} & = \frac{\partial J}{\partial z2} \cdot \frac{\partial z2}{\partial w2} \\ & = \frac{\partial J}{\partial z2} \cdot a2^T \\ \frac{\partial J}{\partial b2} & = \frac{\partial J}{\partial z2} \cdot \frac{\partial z2}{\partial b2} \\ & = \frac{1}{N}\sum_{1}^{N}\frac{\partial J}{\partial z2} \\ \frac{\partial J}{\partial a1} & = \frac{\partial J}{\partial z2} \cdot \frac{\partial z2}{\partial a1} \\ & = w2^T \cdot \frac{\partial J}{\partial z2} \\ \frac{\partial J}{\partial z1} & = \frac{\partial J}{\partial a1} \cdot \frac{\partial a1}{\partial z1} \\ & = \frac{\partial J}{\partial a1} * dReLu(z1) \\ \frac{\partial J}{\partial w1} & = \frac{\partial J}{\partial z1} \cdot \frac{\partial z1}{\partial w1} \\ & = \frac{\partial J}{\partial z1} \cdot a1^T \\ \frac{\partial J}{\partial b1} & = \frac{\partial J}{\partial z1} \cdot \frac{\partial z1}{\partial b1} \\ & = \frac{1}{N}\sum_{1}^{N}\frac{\partial J}{\partial z1} \end{align}</script><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><h2 id="mlp-train-m"><a href="#mlp-train-m" class="headerlink" title="mlp_train.m"></a>mlp_train.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;</span><br><span class="line">%激活函数</span><br><span class="line">f_leaky_relu=@(x)max(0.01*x,x);</span><br><span class="line">f_sigmod=@(x)1./(1+exp(-x));</span><br><span class="line">df_leaky_relu=@(x)max(0.01*x,x)./x;</span><br><span class="line">%学习率</span><br><span class="line">learn_rate=0.01;</span><br><span class="line">%输入x的矩阵</span><br><span class="line">train_x=[0.6,0.1,0.1,0.4,0.8,0.5,0.9,0.1,0.5,0.9,0.5,0.4,0.5,0.6;...</span><br><span class="line">    0.4,0.1,0.8,0.6,0.1,0.6,0.9,0.5,0.1,0.5,0.9,0.4,0.4,0.6];</span><br><span class="line">%  x=[0.8,0.1,0.4;...</span><br><span class="line">%      0.6,0.1,0.8];</span><br><span class="line">%输入y的矩阵,y为行向量</span><br><span class="line">train_y=[1,0,0,1,0,1,0,0,0,0,0,1,1,1];</span><br><span class="line">% y=[1,0,1];</span><br><span class="line">%样本数</span><br><span class="line">train_num=length(train_y);</span><br><span class="line">%特征数目</span><br><span class="line">feat_num=size(train_x,1);</span><br><span class="line">%各层的节点数</span><br><span class="line">node=[200,1];</span><br><span class="line">%网络层数</span><br><span class="line">network_num=length(node);</span><br><span class="line">%w的矩阵大小,wi,i+1=A 维度为node(i)*node(i-1)</span><br><span class="line">max_col=max([feat_num,node(1:end-1)]);</span><br><span class="line">max_row=max(node);</span><br><span class="line">w(:,:,:)=zeros(max_row,max_col,network_num);</span><br><span class="line">for i=1:network_num</span><br><span class="line">    if i==1</span><br><span class="line">        w(1:node(i),1:feat_num,i)=rand(node(i),feat_num)*0.1;</span><br><span class="line">    else</span><br><span class="line">        w(1:node(i),1:node(i-1),i)=rand(node(i),node(i-1))*0.1;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">dw(:,:,:)=zeros(max_row,max_col,network_num);</span><br><span class="line">%b的矩阵大小，bi,i+1=A 维度为node(i)*1</span><br><span class="line">b(:,:,:)=zeros(max_row,1,network_num);</span><br><span class="line">db(:,:,:)=zeros(max_row,1,network_num);</span><br><span class="line">%z的矩阵大小,zi,i+1=A 维度为node(i)*sample_num</span><br><span class="line">z(max_row,train_num,network_num)=0;</span><br><span class="line">dz(max_row,train_num,network_num)=0;</span><br><span class="line">%a的矩阵大小,ai,i+1=A 维度为node(i)*sample_num</span><br><span class="line">a(max_row,train_num,network_num)=0;</span><br><span class="line">fprintf('开始训练...\n\n');</span><br><span class="line">for times=1:5000</span><br><span class="line">    %% %forward propagation</span><br><span class="line">    for i=1:network_num</span><br><span class="line">        b_broadcast=zeros(node(i),train_num);</span><br><span class="line">        for j=1:train_num</span><br><span class="line">            b_broadcast(:,j)=b(1:node(i),1,i);</span><br><span class="line">        end</span><br><span class="line">        if i==1</span><br><span class="line">            %z1=w[1]x+b[1]</span><br><span class="line">            z(1:node(i),1:train_num,i)=w(1:node(i),1:feat_num,i)*train_x+b_broadcast;</span><br><span class="line">        else</span><br><span class="line">            %z[n]=w[n]a[n-1]+b[n]</span><br><span class="line">            z(1:node(i),1:train_num,i)=w(1:node(i),1:node(i-1),i)*a(1:node(i-1),1:train_num,i-1)+b_broadcast;</span><br><span class="line">        end</span><br><span class="line">        if i==network_num</span><br><span class="line">            %a[network_num]=sigmod(z[network_num])</span><br><span class="line">            a(1:node(i),1:train_num,i)=f_sigmod(z(1:node(i),1:train_num,i));</span><br><span class="line">        else</span><br><span class="line">            %a[n]=leaky_relu(z[n])</span><br><span class="line">            a(1:node(i),1:train_num,i)=f_leaky_relu(z(1:node(i),1:train_num,i));</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    %% %error</span><br><span class="line">    e=0;</span><br><span class="line">    for k=1:train_num</span><br><span class="line">        %损失函数e=-1/m∑(ylog(y_hat)+(1-y)log(1-y_hat))</span><br><span class="line">        e=e-(train_y(k)*log(a(1,k,network_num))+(1-train_y(k))*log(1-a(1,k,network_num)));</span><br><span class="line">    end</span><br><span class="line">    e=e/train_num;</span><br><span class="line">    </span><br><span class="line">    %% %back propagation</span><br><span class="line">    for i=network_num:-1:1</span><br><span class="line">        if i==network_num</span><br><span class="line">            %dz[network_num]=y_hat-y</span><br><span class="line">            dz(1:node(i),1:train_num,i)=a(1,:,network_num)-train_y;</span><br><span class="line">        else</span><br><span class="line">            %dz[n]=w[n+1]'dz[n+1].*df_leaky_relu(z[n])</span><br><span class="line">            dz(1:node(i),1:train_num,i)=w(1:node(i+1),1:node(i),i+1)'*dz(1:node(i+1),1:train_num,i+1).*df_leaky_relu(z(1:node(i),1:train_num,i));</span><br><span class="line">        end</span><br><span class="line">        if i==1</span><br><span class="line">            %dw[1]=dz[1]*x'</span><br><span class="line">            dw(1:node(i),1:feat_num,i)=dz(1:node(i),1:train_num,i)*train_x';</span><br><span class="line">        else</span><br><span class="line">            %dw[n]=dz[n]*a[n-1]'</span><br><span class="line">            dw(1:node(i),1:node(i-1),i)=dz(1:node(i),1:train_num,i)*a(1:node(i-1),1:train_num,i-1)';</span><br><span class="line">        end</span><br><span class="line">        db(1:node(i),1,i)=sum(dz(1:node(i),1:train_num,i),2)/train_num;</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    %% %weight</span><br><span class="line">    %利用梯度下降法更新权值</span><br><span class="line">    for i=1:network_num</span><br><span class="line">        if i==1</span><br><span class="line">            w(1:node(i),1:feat_num,i)=w(1:node(i),1:feat_num,i)-learn_rate*dw(1:node(i),1:feat_num,i);</span><br><span class="line">        else</span><br><span class="line">            w(1:node(i),1:node(i-1),i)=w(1:node(i),1:node(i-1),i)-learn_rate*dw(1:node(i),1:node(i-1),i);</span><br><span class="line">        end</span><br><span class="line">        b(1:node(i),1,i)=b(1:node(i),1,i)-learn_rate*db(1:node(i),1,i);</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">%% %判断训练是否完成</span><br><span class="line">if e&gt;0.01</span><br><span class="line">    disp('请调整参数重新训练');</span><br><span class="line">else</span><br><span class="line">    disp('训练完成,运行bp_test开始测试');</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="mlp-test-m"><a href="#mlp-test-m" class="headerlink" title="mlp_test.m"></a>mlp_test.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">clc;close all;</span><br><span class="line">%输入测试集</span><br><span class="line">test_x=rand(2,500);</span><br><span class="line">%测试集样本数</span><br><span class="line">test_num=size(test_x,2);</span><br><span class="line">%z的矩阵大小,zi,i+1=A node(i)*sample_num,eg z1,2=A3*50</span><br><span class="line">test_z(max_row,test_num,network_num)=0;</span><br><span class="line">%a的矩阵大小,ai,i+1=A node(i)*sample_num,eg a1,2=A3*50</span><br><span class="line">test_a(max_row,test_num,network_num)=0;</span><br><span class="line"></span><br><span class="line">for i=1:network_num</span><br><span class="line">    b_broadcast=zeros(node(i),test_num);</span><br><span class="line">    for j=1:test_num</span><br><span class="line">        b_broadcast(:,j)=b(1:node(i),1,i);</span><br><span class="line">    end</span><br><span class="line">    if i==1</span><br><span class="line">        %z1=w[1]x+b[1]</span><br><span class="line">        test_z(1:node(i),1:test_num,i)=w(1:node(i),1:feat_num,i)*test_x+b_broadcast;</span><br><span class="line">    else</span><br><span class="line">        %z[n]=w[n]a[n-1]+b[n]</span><br><span class="line">        test_z(1:node(i),1:test_num,i)=w(1:node(i),1:node(i-1),i)*test_a(1:node(i-1),1:test_num,i-1)+b_broadcast;</span><br><span class="line">    end</span><br><span class="line">    if i==network_num</span><br><span class="line">        %a[network_num]=sigmod(z[network_num])</span><br><span class="line">        test_a(1:node(i),1:test_num,i)=f_sigmod(test_z(1:node(i),1:test_num,i));</span><br><span class="line">    else</span><br><span class="line">        %a[n]=leaky_relu(z[n])</span><br><span class="line">        test_a(1:node(i),1:test_num,i)=f_leaky_relu(test_z(1:node(i),1:test_num,i));</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">%test_y为BP的输出</span><br><span class="line">test_y=test_a(1,:,network_num);</span><br><span class="line">%小于0.5则为第一类，大于0.5则为第二类,否则拒绝判决</span><br><span class="line">for i=1:test_num</span><br><span class="line">    if test_y(i)&gt;0.5</span><br><span class="line">        fprintf('第%d个是第二类，概率为:%f\n',i,test_y(i));</span><br><span class="line">    elseif test_y(i)&lt;0.5</span><br><span class="line">        fprintf('第%d个是第一类，概率为:%f\n',i,1-test_y(i));</span><br><span class="line">    else</span><br><span class="line">        fprintf('拒绝判决\n');</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">%如果数据的特征是二维的，可以绘图表示</span><br><span class="line">if feat_num==2</span><br><span class="line">    hold on;</span><br><span class="line">    %画出训练集，用*表示</span><br><span class="line">    for i=1:train_num</span><br><span class="line">        if train_y(i)==1</span><br><span class="line">            plot(train_x(1,i),train_x(2,i),'r*')</span><br><span class="line">        else</span><br><span class="line">            plot(train_x(1,i),train_x(2,i),'b*')</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    %画出测试集，用o表示</span><br><span class="line">    for i=1:test_num</span><br><span class="line">        if test_y(i)&gt;0.5</span><br><span class="line">            plot(test_x(1,i),test_x(2,i),'ro')</span><br><span class="line">        elseif test_y(i)&lt;0.5</span><br><span class="line">            plot(test_x(1,i),test_x(2,i),'bo')</span><br><span class="line">        else</span><br><span class="line">            plot(test_x(1,i),test_x(2,i),'go')</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    hold off;</span><br><span class="line">else</span><br><span class="line">    disp('The Feature Is Not Two-Dimensional ')</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/mlp1.png" alt="mlp"></p><h1 id="MLP-Multi-Layer-Perceptron-优缺点"><a href="#MLP-Multi-Layer-Perceptron-优缺点" class="headerlink" title="MLP(Multi-Layer Perceptron)优缺点"></a><font size="5" color="red">MLP(Multi-Layer Perceptron)优缺点</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>可以实现多分类任务。</li><li>参数量和迭代次数满足条件时，可以拟合线性不可分任务。</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>计算量较大，训练时间较长。</li><li>算法可能陷入局部最优解，导致训练失败。</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;MLP(Multi-Layer Perceptron)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="有监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>逻辑回归(Logistics Regression)</title>
    <link href="https://USTCcoder.github.io/2019/07/02/classification%20LR/"/>
    <id>https://USTCcoder.github.io/2019/07/02/classification LR/</id>
    <published>2019-07-02T05:25:30.000Z</published>
    <updated>2020-08-31T14:10:52.651Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/MACHINE/lr.jpg" alt="lr"></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  逻辑回归(Logistics Regression):是一种<strong>广义线性回归</strong>，都具有 w’x+b，其中w和b是待求参数，其区别在于他们的<strong>因变量不同</strong>，多重线性回归直接将w’x+b作为因变量，即y =w’x+b，而logistic回归则通过函数g将w’x+b对应一个隐状态p，p =g(w’x+b),然后<strong>根据p 与1-p的大小决定因变量的值</strong>。如果g是logistic函数，就是logistic回归。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><h2 id="预测函数"><a href="#预测函数" class="headerlink" title="预测函数"></a><font size="4">预测函数</font></h2><p>对于二分类问题，$y \in \lbrace 0,1 \rbrace$，1表示正例，0表示负例。逻辑回归是在线性函数$W^Tx$输出预测实际值的基础上，寻找一个假设函数函数$h_W(x)=g(W^Tx)$，将实际值映射到到0，1之间，如果</p><script type="math/tex; mode=display">y =\begin{cases} 1 & h_W(x) \ge 0.5 \\ 0 & h_W(x) <0.5 \end{cases}</script><p><strong>逻辑回归中选择对数几率函数(logistic function)</strong></p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^(-z)}</script><p><img src="/images/MACHINE/lr1.png" alt="sigmod"><br><strong>有一个非常重要的性质，可以方便我们的计算</strong>。</p><script type="math/tex; mode=display">g(z)'=g(z) \cdot (1 - g(z))</script><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a><font size="4">损失函数</font></h2><p>我们假设</p><script type="math/tex; mode=display">\begin{cases}P(Y=1|X) = p(x) \\ P(Y=0|X) = 1 - p(x) \end{cases}</script><p>根据<strong>极大似然估计</strong>可知</p><script type="math/tex; mode=display">\hat{w} = \underset{w}{argmax} \ L(w) = \underset{w}{argmax} \prod_{i = 1}^{N} [p(x_i)]^{y_i} \cdot [1 - p(x_i)]^{1 - y_i}</script><p><strong>两边同时取对数，计算对数似然函数</strong>。</p><script type="math/tex; mode=display">\hat{w} = \underset{w}{argmax} \ ln(L(w)) = \underset{w}{argmax} \sum_{i = 1}^{N} [y_i \cdot ln(p(x_i)) + (1 - y_i) \cdot ln(1 - p(x_i))]</script><p><strong>因为我们在计算时常常求最小值，因此我们设计损失函数</strong>，令$J(w) = -\frac{1}{N} ln(L(w))$</p><script type="math/tex; mode=display">\hat{w} = \underset{w}{argmin} \ J(w)=-\frac{1}{N} \underset{w}{argmax} \ ln(L(w))</script><script type="math/tex; mode=display">\hat{w} = -\underset{w}{argmin} \frac{1}{N} \sum_{i = 1}^{N} [y_i \cdot ln(p(x_i)) + (1 - y_i) \cdot ln(1 - p(x_i))]</script><p>下面转化为求$\underset{w}{argmin} \ J(w)$，<strong>使用常见的梯度下降法进行求解</strong>。</p><script type="math/tex; mode=display">g_i = \frac{\partial J(w)}{\partial w_i} = (p(x_i) - y_i) \cdot x_i</script><script type="math/tex; mode=display">w_i^{k + 1} = w_i^k - \alpha \cdot g_i</script><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><h2 id="LR-train-m"><a href="#LR-train-m" class="headerlink" title="LR_train.m"></a>LR_train.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;</span><br><span class="line">f_sigmod=@(x)1./(1+exp(-x));</span><br><span class="line">%x为样本，行数代表特征数，列数代表样本数,以列向量的形式输出，label为样本所对应的类别</span><br><span class="line">train_x=[0.5,0.5,0.8,0.2,0.1;...</span><br><span class="line">    0.8,0.7,0.6,0.2,0.4];</span><br><span class="line">%1为第一类，2为第二类</span><br><span class="line">train_y=[1,1,1,0,0];</span><br><span class="line">%特征数</span><br><span class="line">feat_num=size(train_x,1);</span><br><span class="line">%样本数</span><br><span class="line">train_num=size(train_x,2);</span><br><span class="line">%设置最大迭代次数</span><br><span class="line">times=10000;</span><br><span class="line">%学习率</span><br><span class="line">a=0.1;</span><br><span class="line">%权向量</span><br><span class="line">w=rand(1,feat_num+1);</span><br><span class="line">%增广矩阵</span><br><span class="line">x_expend=[train_x;ones(1,train_num)];</span><br><span class="line">for i=1:times</span><br><span class="line">    tem=w;</span><br><span class="line">    %w=w-a/m*Σ(sigmod(wx)-y)x</span><br><span class="line">    w=w-a/train_num*sum(repmat((f_sigmod(w*x_expend)-train_y),feat_num+1,1).*x_expend,2)';</span><br><span class="line">    if sum(abs(w-tem))&lt;5e-3</span><br><span class="line">        break;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">disp(i)</span><br><span class="line">if i&gt;=times</span><br><span class="line">    disp('The question is not Linearly Separable');</span><br><span class="line">%否则线性可分，写出函数表达式</span><br><span class="line">else</span><br><span class="line">    express=[];</span><br><span class="line">    %输出表达式</span><br><span class="line">    for i=1:feat_num</span><br><span class="line">        if w(i)&gt;0</span><br><span class="line">            express=[express,num2str(w(i)),'x',num2str(i),'+'];</span><br><span class="line">        elseif w(i)&lt;0</span><br><span class="line">            express=[express(1:end-1),num2str(w(i)),'x',num2str(i),'+'];</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    if w(feat_num+1)==0</span><br><span class="line">        express=[express(1:end-1),'=0'];</span><br><span class="line">    elseif w(feat_num+1)&gt;0</span><br><span class="line">        express=[express(1:end),num2str(w(feat_num+1)),'=0'];</span><br><span class="line">    else</span><br><span class="line">        express=[express(1:end-1),num2str(w(feat_num+1)),'=0'];</span><br><span class="line">    end</span><br><span class="line">    fprintf(['决策面函数为:',express,'\n\n']);</span><br><span class="line">    disp('运行LR_test开始测试');</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="LR-test-m"><a href="#LR-test-m" class="headerlink" title="LR_test.m"></a>LR_test.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">close all;</span><br><span class="line">%输入测试集</span><br><span class="line">test_x=rand(2,10);</span><br><span class="line">%测试集样本数</span><br><span class="line">test_num=size(test_x,2);</span><br><span class="line">%如果误差不能满足条件，此为线性不可分问题</span><br><span class="line">if i&gt;=times</span><br><span class="line">    disp('The question is not Linearly Separable');</span><br><span class="line">%如果满足条件可以找到一个超平面</span><br><span class="line">else</span><br><span class="line">    %如果是二维特征，可以用平面坐标系绘图表示</span><br><span class="line">    if feat_num==2</span><br><span class="line">        hold on;</span><br><span class="line">        axis([0,1,0,1])</span><br><span class="line">        if w(1)==0</span><br><span class="line">            line_x=[0,1];</span><br><span class="line">            line_y=[-w(3)/w(2),-w(3)/w(2)];</span><br><span class="line">        elseif w(2)==0</span><br><span class="line">            line_x=[-w(3)/w(1),-w(3)/w(1)];</span><br><span class="line">            line_y=[0,1];</span><br><span class="line">        else</span><br><span class="line">            line_x=[0,1];</span><br><span class="line">            line_y=[-w(3)/w(2),(-w(3)-w(1))/w(2)];</span><br><span class="line">        end</span><br><span class="line">        %用黑色绘制分界线</span><br><span class="line">        plot(line_x,line_y,'k');</span><br><span class="line">        %绘制测试集样本点</span><br><span class="line">        test_y=w*[test_x;ones(1,test_num)];</span><br><span class="line">        for i=1:test_num</span><br><span class="line">            %如果大于0，则归为一类，用红色的圈表示</span><br><span class="line">            if test_y(i)&gt;0</span><br><span class="line">                plot(test_x(1,i),test_x(2,i),'ro');</span><br><span class="line">            %如果小于0，则归为一类，用蓝色的圈表示</span><br><span class="line">            elseif test_y(i)&lt;0</span><br><span class="line">                plot(test_x(1,i),test_x(2,i),'bo');</span><br><span class="line">            %否则用绿色的圈表示</span><br><span class="line">            else</span><br><span class="line">                plot(test_x(1,i),test_x(2,i),'go');</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        %说明1类对应红色，2类对应蓝色</span><br><span class="line">        if (train_x(1,1)*w(1)+train_x(2,1)*w(2)+w(3)&gt;=0&amp;&amp;train_y(1)==1)||(train_x(1,i)*w(1)+train_x(2,i)*w(2)+w(3)&lt;0&amp;&amp;train_y(1)==0)</span><br><span class="line">            %绘制训练集样本点</span><br><span class="line">            for i=1:train_num</span><br><span class="line">                %如果大于0，则归为一类，用红色的*表示</span><br><span class="line">                if train_y(i)==1</span><br><span class="line">                    plot(train_x(1,i),train_x(2,i),'r*')</span><br><span class="line">                %如果小于0，则归为一类，用蓝色的*表示</span><br><span class="line">                else</span><br><span class="line">                    plot(train_x(1,i),train_x(2,i),'b*')</span><br><span class="line">                end</span><br><span class="line">            end</span><br><span class="line">        %说明1类对应蓝色，2类对应红色</span><br><span class="line">        else</span><br><span class="line">            for i=1:train_num</span><br><span class="line">                %如果大于0，则归为一类，用红色的*表示</span><br><span class="line">                if train_y(i)==1</span><br><span class="line">                    plot(train_x(1,i),train_x(2,i),'b*')</span><br><span class="line">                %如果小于0，则归为一类，用蓝色的*表示</span><br><span class="line">                else</span><br><span class="line">                    plot(train_x(1,i),train_x(2,i),'r*')</span><br><span class="line">                end</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        hold off</span><br><span class="line">    %如果不是两个特征不能用平面坐标系表示</span><br><span class="line">    else</span><br><span class="line">        disp('The Feature Is Not Two-Dimensional');</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/lr2.png" alt="LR"></p><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>算法简单，容易理解</li><li>适合于大多数线性分类的任务</li><li>鲁棒性较好，能够抵挡轻微噪声的影响</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>容易欠拟合</li><li>在多分类任务或者非线性任务上难以使用</li><li>特征空间较大或者特征缺失情况下表现较差</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;逻辑回归(Logistics Regression)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="有监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯</title>
    <link href="https://USTCcoder.github.io/2019/06/28/classfication%20Bayes/"/>
    <id>https://USTCcoder.github.io/2019/06/28/classfication Bayes/</id>
    <published>2019-06-28T14:13:59.000Z</published>
    <updated>2020-09-01T01:22:59.144Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/MACHINE/bayes.png" alt="bayes"></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  <strong>朴素贝叶斯</strong>:基于<strong>贝叶斯定理</strong>，采用了<strong>属性条件独立性假设</strong>，对已知类别，假设所有属性相互独立，也就是假设每个属性独立地对分类结果发生影响。发源于古典数学理论，而且所需估计的参数很少，算法也较为简单，但是<strong>正是因为其假设属性之间相互独立，因此在实际应用中往往是不成立的，所以给模型的正确性带来了一定的影响</strong>。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><h2 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h2><script type="math/tex; mode=display">P(c|x) = \frac{P(c)P(x|c)}{P(x)}</script><p>介绍一些非常重要的概念，<strong>$P(c)$称为先验概率</strong>，<strong>$P(x|c)$是样本x相对于类标记c的类条件概率</strong>，<strong>$P(x)$是用于归一化的因子</strong>，给定样本x，归一化因子$P(x)$与类标记无关。因此<strong>估计$P(c|x)$就转化为如何基于训练数据集来估计先验$P(c)$和类条件概率$P(x|c)$</strong>。</p><h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>朴素贝叶斯模型的训练过程就是一个参数估计的过程，对于这个问题，机器学习分成了两派，频率派和贝叶斯派。<br><strong>频率派认为：参数虽然未知，但是却存在着固定值，可以通过优化似然函数等准则确定参数值</strong>。<br><strong>贝叶斯派认为：参数是未观察到的随机变量，其本身也有分布，因此可以假设参数服从一个先验分布，然后基于观测到的数据计算参数的后验分布</strong>。</p><p><strong>极大似然估计(MLE, Maximum Likelihood Estimation)是根据数据采样来估计概率分布的经典方法</strong>。令$D_c$表示训练集D中第c类样本组成的集合，假设这些样本是独立同分布的，则参数$\theta_c$对于数据集$D_c$的似然是</p><script type="math/tex; mode=display">P(D_c|\theta_c) = \prod_{x \in D_c} P(x|\theta_c)</script><p><strong>对$\theta_c$进行极大似然估计，就是去寻找能最大化似然$P(D_c|\theta_c)$的参数值$\hat{\theta}_c$。通常为了计算方便，使用对数似然</strong>。</p><script type="math/tex; mode=display">\begin{align} LL(\theta_c) & = log(P(D_c|\theta_c)) \\ & = \sum_{x \in D_c} log(P(x|\theta_c)) \end{align}</script><p>此时参数$\theta_c$的极大似然估计$\hat{\theta}_c$为</p><script type="math/tex; mode=display">\hat{\theta}_c = \underset{\theta_c}{argmax} \ LL(\theta_c)</script><p>在连续属性情形下，假设概率密度函数$p(x|c) ~ N(\mu_c, \sigma_c^2)$，则参数$\mu_c$和$\sigma_c^2$的极大似然估计为</p><script type="math/tex; mode=display">\hat{\mu}_c = \frac{1}{|D_c|}\sum_{x \in D_c}x</script><script type="math/tex; mode=display">\hat{\sigma}_c^2 = \frac{1}{|D_c|}\sum_{x \in D_c}(x - \hat{\mu}_c)(x - \hat{\mu}_c)^T</script><h2 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h2><p>基于条件独立性假设，因此贝叶斯定理可以重写为</p><script type="math/tex; mode=display">\begin{align} P(c|x) & = \frac{P(c)P(x|c)}{P(x)} \\ & = \frac{P(c)}{P(x)}\prod_{i = 1}^{d}P(x_i|c) \end{align}</script><p>因为对所有类别来说，$P(x)$相同，可以看出朴素贝叶斯分类器的训练过程就是基于训练集D来估计先验概率$P(c)$和类条件概率$P(x_i|c)$。</p><p>令$D_c$表示训练集D中第c类样本组成的集合，则<strong>先验概率为</strong></p><script type="math/tex; mode=display">P(c) = \frac{|D_c|}{D}</script><p>令$D_{c, x_i}$表示$D_c$中第i个属性取值为$x_i$的样本组成的集合，则<strong>离散属性类条件概率为</strong></p><script type="math/tex; mode=display">P(x_i|c) = \frac{|D_{c, x_i}|}{|D_c|}</script><p>对于连续属性的类条件概率求解时，假定$P(x<em>i|c) ~ N(\mu</em>{c, i}, \sigma_{c, i}^2)$，则<strong>连续属性类条件概率为</strong></p><script type="math/tex; mode=display">P(x_i|c) = \frac{1}{\sqrt{2 \pi}\sigma_{c, i}}exp(-\frac{(x - \mu_{c, i})^2}{2 \sigma_{c, i}^2})</script><p>计算好上面这两个参数后，直接代入公式</p><script type="math/tex; mode=display">\hat{c} = \underset{c \in \gamma}{argmax} \ P(c) \prod_{i = 1}^{d}P(x_i|c)</script><p>其中$\gamma$为所有类别数，<strong>对每一个类别都进行计算，哪一个类别得到的值最大，则将样本分到哪一个类别中去即可</strong>。</p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><h2 id="BAYES-train-m"><a href="#BAYES-train-m" class="headerlink" title="BAYES_train.m"></a>BAYES_train.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;</span><br><span class="line">%x为样本，行数代表特征数，列数代表样本数,以列向量的形式输出</span><br><span class="line">train_x=[0.8,0.9,0.1,0.2,0.1,0.2;...</span><br><span class="line">    0.8,0.9,0.9,0.2,0.1,0.8];</span><br><span class="line">%1为第一类，2为第二类，3为第三类</span><br><span class="line">train_y=[1,1,3,2,2,3];</span><br><span class="line">%特征数</span><br><span class="line">feat_num=size(train_x,1);</span><br><span class="line">%样本数</span><br><span class="line">train_num=size(train_x,2);</span><br><span class="line">tem=tabulate(train_y);</span><br><span class="line">%p_y为先验概率</span><br><span class="line">p_y=tem(:,3)/100;</span><br><span class="line">u_sigma=zeros(feat_num,2,length(p_y));</span><br><span class="line">for i=1:length(p_y)</span><br><span class="line">   for j=1:feat_num</span><br><span class="line">       %u_sigma(j,:,i)代表第i类，第j个特征的均值和方差</span><br><span class="line">       u_sigma(j,:,i)=[mean(train_x(j,train_y==i)),std(train_x(j,train_y==i))];</span><br><span class="line">   end</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="BAYES-test-m"><a href="#BAYES-test-m" class="headerlink" title="BAYES_test.m"></a>BAYES_test.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">close all;</span><br><span class="line">%输入测试集</span><br><span class="line">test_x=rand(2,100);</span><br><span class="line">%测试集样本数</span><br><span class="line">test_num=size(test_x,2);</span><br><span class="line">%temp保存类条件概率</span><br><span class="line">temp=zeros(feat_num,size(u_sigma,3));</span><br><span class="line">test_y=zeros(1,test_num);</span><br><span class="line">for i=1:test_num</span><br><span class="line">    for j=1:feat_num</span><br><span class="line">        for k=1:size(u_sigma,3)</span><br><span class="line">            %第j行第k列代表第i个样本第k类第j个特征的概率，即类条件概率P(x_j|y=k)</span><br><span class="line">            temp(j,k)=normpdf(test_x(j,i),u_sigma(j,1,k),u_sigma(j,2,k));</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    %根据条件独立性假设，将类条件概率相乘得到后验概率</span><br><span class="line">    p=prod(temp);</span><br><span class="line">    %求出最大后验概率的类别</span><br><span class="line">    [~,test_y(i)]=max(p);</span><br><span class="line">end</span><br><span class="line">if feat_num==2</span><br><span class="line">    hold on;</span><br><span class="line">    color_bar=rand(size(u_sigma,3),3);</span><br><span class="line">    for i=1:train_num</span><br><span class="line">        plot(train_x(1,i),train_x(2,i),'color',color_bar(train_y(i),:),'marker','*');</span><br><span class="line">    end</span><br><span class="line">    for i=1:test_num</span><br><span class="line">        plot(test_x(1,i),test_x(2,i),'color',color_bar(test_y(i),:),'marker','o');</span><br><span class="line">    end</span><br><span class="line">    hold off;</span><br><span class="line">    %如果不是两个特征不能用平面坐标系表示</span><br><span class="line">else</span><br><span class="line">    disp('The Feature Is Not Two-Dimensional');</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/bayes1.png" alt="bayes"></p><h1 id="朴素贝叶斯优缺点"><a href="#朴素贝叶斯优缺点" class="headerlink" title="朴素贝叶斯优缺点"></a><font size="5" color="red">朴素贝叶斯优缺点</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>算法简单，易于理解和实现，且模型无需训练。</li><li>计算量小，能够达到实时的效率。</li><li>同样适合于对于多分类问题。</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>因为假设各属性独立，因此实际生活中可能出现问题。</li><li>样本不平衡时，尤其是一类样本多，其他类样本少时会产生严重的问题。</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;朴素贝叶斯&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="有监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>决策树(Decision Tree)</title>
    <link href="https://USTCcoder.github.io/2019/06/24/classification%20DT/"/>
    <id>https://USTCcoder.github.io/2019/06/24/classification DT/</id>
    <published>2019-06-24T06:45:20.000Z</published>
    <updated>2020-09-01T01:24:42.349Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/MACHINE/dt.jpg" alt="dt"></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  决策树(Decision Tree):是在已知各种情况发生概率的基础上，直观运用概率分析的一种<strong>图解法</strong>。决策树是一种<strong>树形结构</strong>，其中每个<strong>内部节点表示一个属性上的测试</strong>，每个<strong>分支代表一个测试输出</strong>，每个<strong>叶节点代表一种类别</strong>。由于这种决策分支画成图形很像一棵树的枝干，故称决策树。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><h2 id="树的构建"><a href="#树的构建" class="headerlink" title="树的构建"></a><font size="4">树的构建</font></h2><ul><li>步骤1：<strong>将所有的数据看成是一个节点（根节点），进入步骤2</strong></li><li>步骤2：<strong>根据划分准则，从所有属性中挑选一个对节点进行分割，进入步骤3</strong></li><li>步骤3：<strong>生成若干个子节点，对每一个子节点进行判断，如果满足停止分裂的条件，进入步骤4；否则，进入步骤2</strong></li><li>步骤4：<strong>设置该节点是叶子节点，其输出的结果为该节点数量占比最大的类别</strong></li></ul><h2 id="划分准则"><a href="#划分准则" class="headerlink" title="划分准则"></a><font size="4">划分准则</font></h2><h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><p>信息熵：假设样本集合D中第k类样本所占的比例为$p_k(k=1,2,\ldots,y)$，则D的信息熵定义为：</p><script type="math/tex; mode=display">Ent(D)=-\displaystyle \sum_{k=1}^y p_klog_2p_k</script><p><strong>Ent(D)的值越小，则D的纯度越高</strong>。</p><h3 id="信息增益-ID3"><a href="#信息增益-ID3" class="headerlink" title="信息增益(ID3)"></a>信息增益(ID3)</h3><p>假设离散属性a有V个不同的取值$\lbrace a^1,a^2,\ldots,a^V \rbrace$，若使用a来对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为$a^v$的样本，记为$D^v$。我们可以计算出$D^v$的信息熵，再给分支结点赋予权重$\frac{\left| D^v \right|}{\left| D \right|}$<br>则可以计算出用属性a对样本集D进行划分所获得的”信息增益”。</p><script type="math/tex; mode=display">Gain(D,a)=Ent(D)-\displaystyle \sum_{v=1}^V \frac{\left| D^v \right|}{\left| D \right|}Ent(D^v)</script><p><strong>一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的纯度提升越大，ID3决策树学习算法就是以信息增益为准则来划分属性</strong>。</p><h3 id="增益率-C4-5"><a href="#增益率-C4-5" class="headerlink" title="增益率(C4.5)"></a>增益率(C4.5)</h3><p>实际上，信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的C4.5决策树算法不直接使用信息增益，而是使用”增益率”来选择最优划分属性。<br>增益率定义为</p><script type="math/tex; mode=display">Gain\_ratio=\frac{Gain(D,a)}{IV(a)}</script><script type="math/tex; mode=display">IV(a)=-\displaystyle \sum_{v=1}^V \frac{\left| D^v \right|}{\left| D \right|} log_2 \frac{\left| D^v \right|}{\left| D \right|}</script><p><strong>需要注意的是，增益率准则对可取值数目较少的属性有所偏好，因此C4.5算法先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的</strong>。</p><h3 id="基尼指数-CART"><a href="#基尼指数-CART" class="headerlink" title="基尼指数(CART)"></a>基尼指数(CART)</h3><p><strong>基尼指数反应了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此基尼指数越小，则数据集D的纯度越高</strong>。</p><script type="math/tex; mode=display">Gini(D)=1-\displaystyle \sum_{k=1}^y {p_k}^2</script><p>属性a的基尼指数定义为</p><script type="math/tex; mode=display">Gini\_index(D,a)=\displaystyle \sum_{v=1}^V \frac{\left| D^v \right|}{\left| D \right|} Gini(D^v)</script><p><strong>在选择属性集合时，选择使划分后基尼指数最小的属性作为最优划分属性</strong>。</p><h2 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a><font size="4">剪枝处理</font></h2><p><strong>剪枝(pruning)是决策树学习算法对付”过拟合”的主要手段</strong>。<br>决策树剪枝的基本策略有”预剪枝(prepruning)”和”后剪枝(postpruning)”。</p><h3 id="预剪枝-prepruning"><a href="#预剪枝-prepruning" class="headerlink" title="预剪枝(prepruning)"></a>预剪枝(prepruning)</h3><p><strong>预剪枝：是在决策树生长过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶子结点</strong>。<br>预剪枝优点：</p><ul><li><strong>降低过拟合风险</strong></li><li><strong>显著减少决策树的训练时间开销和测试时间开销</strong><br>预剪枝缺点：</li><li><strong>因为”贪心”本质，可能带来欠拟合的风险</strong></li></ul><h3 id="后剪枝-postpruning"><a href="#后剪枝-postpruning" class="headerlink" title="后剪枝(postpruning)"></a>后剪枝(postpruning)</h3><p><strong>后剪枝：先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶节点</strong>。<br>后剪枝优点：</p><ul><li><strong>泛化性能较好</strong></li><li><strong>欠拟合风险较小</strong><br>后剪枝缺点：</li><li><strong>生成完全决策树后进行，并且自底向上对所有非叶结点进行逐一考察，时间开销大</strong></li></ul><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/MACHINE/dt.png" alt="DT"></p><p><br><br></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用数据为罗斯.昆兰(Ross Quinlan)当年所用的高尔夫模型</font><br>其实这里使用的算法并不是ID3，因为老师上课讲错了，误把ID3讲成最小熵，ID3应该是最小熵增益。但是作业要求是实现最小熵的ID3算法，所以我起名为ID3。但是<strong>标准的ID3，还需要一个比较的过程，确定哪一种分割是最小熵增益</strong>。<br><img src="/images/MACHINE/dt2.png" alt="DATA"></p><h2 id="ID3-main-m"><a href="#ID3-main-m" class="headerlink" title="ID3_main.m"></a>ID3_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">%% %获取基本信息</span><br><span class="line">clear;clc;close all;</span><br><span class="line">%设置类别和标签最长为100字符</span><br><span class="line">char_len=100;</span><br><span class="line">%生成一个长度为100的空串</span><br><span class="line">space(1:char_len)=' ';</span><br><span class="line">%读取文本文档</span><br><span class="line">fo=fopen('data1.txt','rt');</span><br><span class="line">txt=textscan(fo,'%s');</span><br><span class="line">fclose(fo);</span><br><span class="line">%class_name为类别名称，如outlook，temperature等等</span><br><span class="line">class_name=strsplit(txt{1}{1},',');</span><br><span class="line">class_name=class_name(1:end-1);</span><br><span class="line">%class_num为类别数</span><br><span class="line">class_num=length(class_name);</span><br><span class="line">%sample_num为样本数</span><br><span class="line">sample_num=length(txt{1})-1;</span><br><span class="line">data{sample_num,class_num+1}=[];</span><br><span class="line">%读入数据</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    temp=strsplit(txt{1}{i+1},',');</span><br><span class="line">    for j=1:class_num</span><br><span class="line">        data{i,j}=[temp{j},'_',class_name{1,j}];</span><br><span class="line">    end</span><br><span class="line">    data{i,j+1}=temp{j+1};</span><br><span class="line">end</span><br><span class="line">%class_info存放每一个类别的标签信息，如第一个元胞中存放rain，sunny，overcast</span><br><span class="line">class_info{1,class_num}=[];</span><br><span class="line">for i=1:class_num</span><br><span class="line">    temp=unique(data(:,i));</span><br><span class="line">    class_info{i}=temp;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">%% %生成树</span><br><span class="line">No=1;</span><br><span class="line">%创建100*100的字符矩阵存放树的信息。</span><br><span class="line">tree(1:char_len,1:char_len)=' ';</span><br><span class="line">[tree,No] = ID3_creat(data,class_name,tree,No);</span><br><span class="line"></span><br><span class="line">%% %构建连接矩阵</span><br><span class="line">%获取树的节点</span><br><span class="line">tree_node=tree(1:2:No,:);</span><br><span class="line">%获取树的标签</span><br><span class="line">tree_label=tree(2:2:No,:);</span><br><span class="line">%创建连接矩阵</span><br><span class="line">vect{size(tree_node,1),size(tree_node,1)}=[];</span><br><span class="line">%vect元胞记录父子关系</span><br><span class="line">for i=1:size(tree_node,1)</span><br><span class="line">    tem=find(ismember(class_name,deblank(tree_node(i,:))));</span><br><span class="line">    if isempty(tem)</span><br><span class="line">        continue;</span><br><span class="line">    end</span><br><span class="line">    num=size(class_info{1,tem},1);</span><br><span class="line">    for j=1:num</span><br><span class="line">        temp=space;</span><br><span class="line">        temp(1:length(class_info{1,tem}{j}))=class_info{1,tem}{j};</span><br><span class="line">        for k=1:size(tree_label,1)</span><br><span class="line">            if isequal(tree_label(k,:),temp)</span><br><span class="line">                break;</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        vect{i,k+1}=temp;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">%% %绘制树图</span><br><span class="line">node=zeros(1,size(tree_node,1));</span><br><span class="line">%根据vect元胞中的父子关系画出树图</span><br><span class="line">for i=1:size(vect,2)</span><br><span class="line">    tem=vect(:,i);</span><br><span class="line">    for j=1:size(tem,1)</span><br><span class="line">        if ~isempty(tem{j})</span><br><span class="line">            node(i)=j;</span><br><span class="line">            break;</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">treeplot(node);</span><br><span class="line"></span><br><span class="line">%% %写树的类别（节点）</span><br><span class="line">[x,y]=treelayout(node);</span><br><span class="line">x=x';</span><br><span class="line">y=y';</span><br><span class="line">text(x(:,1),y(:,1),tree_node);</span><br><span class="line"></span><br><span class="line">%% %写树的标签（枝条）</span><br><span class="line">x1=zeros(size(tree_label,1));</span><br><span class="line">y1=zeros(size(tree_label,1));</span><br><span class="line">%根据父子关系在父子节点中点写入标签</span><br><span class="line">for i=2:length(node)</span><br><span class="line">    x1(i-1,1)=(x(i,1)+x(node(i),1))/2;</span><br><span class="line">    y1(i-1,1)=(y(i,1)+y(node(i),1))/2;</span><br><span class="line">end</span><br><span class="line">for i=1:size(tree_label)</span><br><span class="line">    temp=strsplit(tree_label(i,:),'_');</span><br><span class="line">    tree_label(i,:)=[temp{1},space(length(temp{1})+1:end)];</span><br><span class="line">end</span><br><span class="line">text(x1(:,1),y1(:,1),tree_label);</span><br></pre></td></tr></tbody></table></figure><h2 id="ID3-split-m"><a href="#ID3-split-m" class="headerlink" title="ID3_split.m"></a>ID3_split.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">function bestfeature=ID3_split(data)</span><br><span class="line">%求最小熵的分割算法</span><br><span class="line">numfeatures = size(data,2) -1 ;</span><br><span class="line">bestent = log2(numfeatures);</span><br><span class="line">bestfeature = -1;</span><br><span class="line">for i =1:numfeatures</span><br><span class="line">    ent = ID3_ent(data,i);</span><br><span class="line">    if ent &lt; bestent</span><br><span class="line">        bestent = ent;</span><br><span class="line">        bestfeature = i;</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="ID3-ent-m"><a href="#ID3-ent-m" class="headerlink" title="ID3_ent.m"></a>ID3_ent.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">function ent=ID3_ent(data,i)</span><br><span class="line">%求最小熵</span><br><span class="line">info=tabulate(data(:,i));</span><br><span class="line">ent=0;</span><br><span class="line">for k=1:size(info,1)</span><br><span class="line">    loc=ismember(data(:,i),info{k,1});</span><br><span class="line">    info1=tabulate(data(loc,end));</span><br><span class="line">    temp=0;</span><br><span class="line">    for n=1:size(info1,1)</span><br><span class="line">        temp=temp-info1{n,3}/100*log2(info1{n,3}/100);</span><br><span class="line">    end</span><br><span class="line">    ent=ent+info{k,3}/100*temp;</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="ID3-creat-m"><a href="#ID3-creat-m" class="headerlink" title="ID3_creat.m"></a>ID3_creat.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">function [tree,No]=ID3_creat(data,class_name,tree,No)</span><br><span class="line">classlist=data(:,end);</span><br><span class="line">%如果标签全为yes或no则已经分完，返回</span><br><span class="line">if size(tabulate(classlist),1)==1</span><br><span class="line">    tree(No,1:length(classlist{1}))=classlist{1};</span><br><span class="line">    return</span><br><span class="line">end</span><br><span class="line">%如果没有分完找到最好的特征，递归生成树</span><br><span class="line">bestfeature_loc = ID3_split(data);</span><br><span class="line">bestfeature=class_name{1,bestfeature_loc};</span><br><span class="line">tree(No,1:length(bestfeature))=bestfeature;</span><br><span class="line">featureValues=tabulate(data(:,bestfeature_loc));</span><br><span class="line">for m=1:size(featureValues,1)</span><br><span class="line">    tree(No+1,1:length(featureValues{m,1}))=featureValues{m,1};</span><br><span class="line">    loc=ismember(data(:,bestfeature_loc),featureValues{m,1});</span><br><span class="line">    data1=data(loc,[1:bestfeature_loc-1,bestfeature_loc+1:end]);</span><br><span class="line">    [tree,No] = ID3_creat(data1,class_name(:,[1:bestfeature_loc-1,bestfeature_loc+1:end]),tree,No+2);</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/dt1.png" alt="DT"></p><h1 id="ID3-C4-5-CART性能比较"><a href="#ID3-C4-5-CART性能比较" class="headerlink" title="ID3, C4.5, CART性能比较"></a><font size="5" color="red">ID3, C4.5, CART性能比较</font></h1><script type="math/tex; mode=display">\begin{array}{|c|c|c|c|c|} 算法 & 结构 & 特征选择 & 连续值 & 缺失值  \\ \hline ID3&多叉树&信息增益&不支持&不支持\\ C4.5&多叉树&信息增益比&支持&支持\\ CART&二叉树&基尼系数&支持&支持\\  \end{array}</script><h1 id="决策树分类优缺点"><a href="#决策树分类优缺点" class="headerlink" title="决策树分类优缺点"></a><font size="5" color="red">决策树分类优缺点</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>数据量一般不会太大</li><li>具有很强的可解释性</li><li>生成的决策树简单直观</li><li>可以处理多维度输出的分类问题。</li><li>既可以处理离散值也可以处理连续值</li><li>可以通过剪枝来权衡欠拟合和过拟合</li><li>基本不需要预处理，不需要提前归一化，处理缺失值。</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>树结构受样本影响较大</li><li>复杂的模型很难用决策树解决</li><li>寻找最优的决策树是一个NP难的问题</li><li>如果某些特征的样本比例过大，生成决策树容易偏向于这些特征，这个可以通过调节样本权重来改善</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;决策树(Decision Tree)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="有监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Fisher线性判别</title>
    <link href="https://USTCcoder.github.io/2019/06/20/classfication%20FISHER/"/>
    <id>https://USTCcoder.github.io/2019/06/20/classfication FISHER/</id>
    <published>2019-06-20T14:07:20.000Z</published>
    <updated>2020-09-01T01:24:25.025Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/MACHINE/fisher.png" alt="fisher"></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  <strong>Fisher线性判别</strong>:由<strong>Fisher于1936年提出</strong>，是一种经典的线性学习方法，其根据<strong>方差分析的思想建立起来的一种线性判别法</strong>，该判别方法<strong>对总体的分布不做任何要求</strong>。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>核心思想非常朴素，用周志华老师的话就是：<strong>给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近，异类样例的投影点尽可能远离。在对新样本进行分类时，将其投影到同样的这条直线上，根据投影点的位置来确定新样本的类别</strong>。</p><p>设$\mu_i, \Sigma_i, i \in {0, 1}$分别表示第i类样本的均值向量和协方差矩阵。<br>设投影直线的表达式为$y = \omega x$，则将样本点投影到直线上，两类样本的投影中心分别为$\omega^T \mu_0, \omega^T \mu_0$，两类样本的协方差分别为$\omega^T \Sigma_0 \omega, \omega^T \Sigma_1 \omega$</p><p><strong>为了寻找一个最优的投影直线，因此我们要尽量使得类内距离越小越好，类间距离越大越好。可以认为是一种高内聚，低耦合的思想</strong>。</p><p><strong>要使同类样例投影点的协方差尽可能小，即$\omega^T \Sigma_0 \omega + \omega^T \Sigma_1 \omega$尽可能小，要使不同样例投影中心的距离尽可能大，即$||\omega^T \mu_0 - \omega^T \mu_1||_2^2$尽可能大</strong>。</p><p>因此得到了我们的<strong>目标函数</strong></p><script type="math/tex; mode=display">\begin{align} J & = \frac{||\omega^T \mu_0 - \omega^T \mu_1||_2^2}{\omega^T \Sigma_0 \omega + \omega^T \Sigma_1 \omega}  \\ & = \frac{\omega^T (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T \omega}{\omega^T (\Sigma_0 + \Sigma_1) \omega} \end{align}</script><p>我们定义<strong>类内散度矩阵</strong></p><script type="math/tex; mode=display">S_{\omega} = \Sigma_0 + \Sigma_1 = \sum_{x \in X_0}(x - \mu_0)(x - \mu_0)^T + \sum_{x \in X_1}(x - \mu_1)(x - \mu_1)^T</script><p>我们定义<strong>类间散度矩阵</strong></p><script type="math/tex; mode=display">S_b = (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T</script><p>则目标函数可以重写为</p><script type="math/tex; mode=display">J = \frac{\omega^T S_b \omega}{\omega^T S_{\omega} \omega}</script><h1 id="求解过程"><a href="#求解过程" class="headerlink" title="求解过程"></a><font size="5" color="red">求解过程</font></h1><p><strong>因为分子和分母都是关于$\omega$的二次项，因此解与$\omega$的长度无关，仅仅与其方向有关</strong>，也就是说，若$\omega$是这个问题的一个解，那么对于任意常数$\alpha$，有$\alpha \omega$也是这个问题的解。因此令$\omega^T S_{\omega} \omega = 1$，则目标函数等价于</p><script type="math/tex; mode=display">\underset{\omega}{argmin} \ -\omega^T S_b \omega, \ s.t. \ \omega^T S_{\omega} \omega = 1</script><p>使用拉格朗日乘子法可求得</p><script type="math/tex; mode=display">S_b \omega = \lambda S_{\omega} \omega</script><p>因为$S_b \omega = (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T \omega, \ (\mu_0 - \mu_1)^T \omega$是标量，因此可以设</p><script type="math/tex; mode=display">S_b \omega = \lambda (\mu_0 - \mu_1)</script><p>综合上面两个式子可以得到</p><script type="math/tex; mode=display">\omega = S_{\omega}^{-1}(\mu_0 - \mu_1)</script><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><h2 id="FISHER-train-m"><a href="#FISHER-train-m" class="headerlink" title="FISHER_train.m"></a>FISHER_train.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">% x为样本，行数代表特征数，列数代表样本数,以列向量的形式输出，label为样本所对应的类别</span><br><span class="line">train_x=[0.5,0.5,0.8,0.2,0.1;...</span><br><span class="line">    0.8,0.7,0.6,0.2,0.4];</span><br><span class="line">% 1为第一类，0为第二类</span><br><span class="line">train_y=[1,1,1,0,0];</span><br><span class="line">%样本数</span><br><span class="line">train_num=size(train_x,2);</span><br><span class="line">%特征数</span><br><span class="line">feat_num=size(train_x,1);</span><br><span class="line">x1=mean(train_x(:,train_y==1),2);</span><br><span class="line">x2=mean(train_x(:,train_y==0),2);</span><br><span class="line">sw=(train_x(:,train_y==1)-repmat(x1,1,sum(train_y==1)))*(train_x(:,train_y==1)-repmat(x1,1,sum(train_y==1)))'...</span><br><span class="line">    +(train_x(:,train_y==0)-repmat(x2,1,sum(train_y==0)))*(train_x(:,train_y==0)-repmat(x2,1,sum(train_y==0)))';</span><br><span class="line">%求出权向量</span><br><span class="line">w=(sw\(x1-x2))';</span><br><span class="line">fprintf('权向量为:');</span><br><span class="line">disp(w);</span><br><span class="line">fprintf('\n\n');</span><br><span class="line">disp('运行FISHER_test开始测试');</span><br></pre></td></tr></tbody></table></figure><h2 id="FISHER-test-m"><a href="#FISHER-test-m" class="headerlink" title="FISHER_test.m"></a>FISHER_test.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">close all;</span><br><span class="line">%输入测试集</span><br><span class="line">test_x=rand(2,10);</span><br><span class="line">%测试集样本数</span><br><span class="line">test_num=size(test_x,2);</span><br><span class="line">b_test=w*test_x;</span><br><span class="line">%找到分割平面</span><br><span class="line">tem=w*train_x;</span><br><span class="line">if min(tem(train_y==1))&gt;max(tem(train_y==0))</span><br><span class="line">    mid=(min(tem(train_y==1))+max(tem(train_y==0)))/2;</span><br><span class="line">    test_y=b_test&gt;mid;</span><br><span class="line">else</span><br><span class="line">    mid=(max(tem(train_y==1))+min(tem(train_y==0)))/2;</span><br><span class="line">    test_y=b_test&lt;mid;</span><br><span class="line">end</span><br><span class="line">%如果是二维特征，可以用平面坐标系绘图表示</span><br><span class="line">if feat_num==2</span><br><span class="line">    hold on;</span><br><span class="line">    axis equal;</span><br><span class="line">    k1=w(2)/w(1);</span><br><span class="line">    if w(1)==0</span><br><span class="line">        line1_x=[1,1];</span><br><span class="line">        line1_y=[-1,1];</span><br><span class="line">        x_mid=[-1,1];</span><br><span class="line">        y_mid=[mid,mid];</span><br><span class="line">    elseif w(2)==0</span><br><span class="line">        line1_x=[-1,1];</span><br><span class="line">        line1_y=[1,1];</span><br><span class="line">        x_mid=[mid,mid];</span><br><span class="line">        y_mid=[-1,1];</span><br><span class="line">    else</span><br><span class="line">        line1_x=[-1,1];</span><br><span class="line">        line1_y=k1*line1_x+1;</span><br><span class="line">        x_mid=[-1,1];</span><br><span class="line">        y_mid=(mid-w(1)*x_mid)/w(2);</span><br><span class="line">    end</span><br><span class="line">    %画出投影到一维直线的形状</span><br><span class="line">    plot(line1_x,line1_y,'k');</span><br><span class="line">    plot(x_mid,y_mid,'k');</span><br><span class="line">    </span><br><span class="line">    b_train=w*train_x;</span><br><span class="line">    point_train=zeros(2,train_num);</span><br><span class="line">    %画出训练集投影直线</span><br><span class="line">    for i=1:train_num</span><br><span class="line">        if train_y(i)==1</span><br><span class="line">            point_train(:,i)=[w(1),w(2);k1,-1]\[b_train(i);-1];</span><br><span class="line">            plot([train_x(1,i),point_train(1,i)],[train_x(2,i),point_train(2,i)]','r');</span><br><span class="line">            plot(train_x(1,i),train_x(2,i),'r*');</span><br><span class="line">        else</span><br><span class="line">            point_train(:,i)=[w(1),w(2);k1,-1]\[b_train(i);-1];</span><br><span class="line">            plot([train_x(1,i),point_train(1,i)],[train_x(2,i),point_train(2,i)]','g');</span><br><span class="line">            plot(train_x(1,i),train_x(2,i),'g*')</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    point_test=zeros(2,test_num);</span><br><span class="line">%     画出测试集投影直线</span><br><span class="line">    for i=1:test_num</span><br><span class="line">        if test_y(i)==1</span><br><span class="line">            point_test(:,i)=[w(1),w(2);k1,-1]\[b_test(i);-1];</span><br><span class="line">            plot([test_x(1,i),point_test(1,i)],[test_x(2,i),point_test(2,i)]','r');</span><br><span class="line">            plot(test_x(1,i),test_x(2,i),'ro');</span><br><span class="line">        else</span><br><span class="line">            point_test(:,i)=[w(1),w(2);k1,-1]\[b_test(i);-1];</span><br><span class="line">            plot([test_x(1,i),point_test(1,i)],[test_x(2,i),point_test(2,i)]','g');</span><br><span class="line">            plot(test_x(1,i),test_x(2,i),'go')</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">else</span><br><span class="line">    disp('The Feature Is Not Two-Dimensional');</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/fisher1.png" alt="fisher"></p><h1 id="Fisher优缺点"><a href="#Fisher优缺点" class="headerlink" title="Fisher优缺点"></a><font size="5" color="red">Fisher优缺点</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>无需参数估计，无需训练。</li><li>算法简单，易于理解和实现。</li><li>计算量小，能够达到实时的效率。</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>对于多分类问题，实现起来较为复杂。</li><li>样本不平衡时，尤其是一类样本多，其他类样本少时会产生严重的问题。</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Fisher线性判别&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="有监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>KNN(K Nearest Neighbor)</title>
    <link href="https://USTCcoder.github.io/2019/06/15/classfication%20KNN/"/>
    <id>https://USTCcoder.github.io/2019/06/15/classfication KNN/</id>
    <published>2019-06-15T00:25:20.000Z</published>
    <updated>2020-09-01T01:24:11.550Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/MACHINE/knn.jpg" alt="knn"></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  KNN(K Nearest Neighbor):又称K近邻，是一种基本分类方法，给定测试实例，基于<strong>某种距离度量</strong>找出训练集中与其<strong>最靠近的k个</strong>实例点，然后基于这k个最近邻的信息使用<strong>投票法</strong>，即选择这k个实例中<strong>出现最多</strong>的标记类别作为分类结果。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><h2 id="距离的度量"><a href="#距离的度量" class="headerlink" title="距离的度量"></a><font size="4">距离的度量</font></h2><p>  <font size="4">1. 欧式距离(Euclidean Distance)</font></p><script type="math/tex; mode=display">L_2(x_i,x_j)=\sqrt{\displaystyle \sum_{k=1}^n ({x_i}^{(k)}-{x_j}^{(k)})^2}</script><p>  <font size="4">2. 曼哈顿距离(Manhattan distance)</font></p><script type="math/tex; mode=display">L_1(x_i,x_j)=\displaystyle \sum_{k=1}^n \lvert{x_i}^{(k)}-{x_j}^{(k)} \rvert</script><p>  <font size="4">3.余弦距离(Cosine Distance)</font></p><script type="math/tex; mode=display">L_{cos}(x_i,x_j)=1-\frac{x_i \cdot x_j}{\lVert x_i \rVert_2 \ \lVert x_j \rVert_2}</script><h2 id="K值的选择"><a href="#K值的选择" class="headerlink" title="K值的选择"></a><font size="4">K值的选择</font></h2><p>  K值的选择会对K近邻法的结果产生重大影响，在应用中，k值一般取一个比较小的数值，通常采用交叉验证法来选取最优的k值。<br>  K=1时K近邻算法退化成最近邻，即数据的类别为距离最近的样本的类别。</p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/MACHINE/knn.png" alt="KNN"></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><h2 id="KNN-main-m"><a href="#KNN-main-m" class="headerlink" title="KNN_main.m"></a>KNN_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">%类别数目，请输入大于1的数</span><br><span class="line">class_num=2;</span><br><span class="line">%k近邻数目</span><br><span class="line">knn=3;</span><br><span class="line">%训练集样本</span><br><span class="line">train_x=[0.7,0.8,0.1,0.4,0.2;...</span><br><span class="line">     0.5,0.6,0.1,0.8,0.2];</span><br><span class="line">train_y=[1,1,2,1,2];</span><br><span class="line">%特征数目</span><br><span class="line">feat_num=size(train_x,1);</span><br><span class="line">%测试集样本</span><br><span class="line">test_x=[rand(1,50);rand(1,50)];</span><br><span class="line">%训练集样本数</span><br><span class="line">train_num=size(train_x,2);</span><br><span class="line">%测试集样本数</span><br><span class="line">test_num=size(test_x,2);</span><br><span class="line">%尺度缩放到0-1</span><br><span class="line">train_x_scale=zeros(size(train_x));</span><br><span class="line">test_x_scale=zeros(size(test_x));</span><br><span class="line">for i=1:feat_num</span><br><span class="line">    train_x_scale(i,:)=(train_x(i,:)-min(train_x(i,:)))/(max(train_x(i,:))-min(train_x(i,:)));</span><br><span class="line">    test_x_scale(i,:)=(test_x(i,:)-min(train_x(i,:)))/(max(train_x(i,:))-min(train_x(i,:)));</span><br><span class="line">end</span><br><span class="line">%如果knn大于样本数，则无法判别</span><br><span class="line">if knn&gt;train_num</span><br><span class="line">    disp('Error');</span><br><span class="line">else</span><br><span class="line">    test_y=KNN_classify(train_x_scale,train_y,test_x_scale,train_num,test_num,knn);</span><br><span class="line">    %如果数据的特征是二维的，可以绘图表示</span><br><span class="line">    if feat_num==2</span><br><span class="line">        KNN_display(train_x,train_y,test_x,test_y,train_num,test_num,class_num);</span><br><span class="line">    else</span><br><span class="line">        disp('The Feature Is Not Two-Dimensional');</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="KNN-classify-m"><a href="#KNN-classify-m" class="headerlink" title="KNN_classify.m"></a>KNN_classify.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">function test_y=KNN_classify(train_x_scale,train_y,test_x_scale,train_num,test_num,knn)</span><br><span class="line">test_y=zeros(1,test_num);</span><br><span class="line">distance=zeros(test_num,train_num);</span><br><span class="line">for i=1:test_num</span><br><span class="line">    %distance(i,j)代表第i个测试集到第j个训练集的距离</span><br><span class="line">    distance(i,:)=sum((train_x_scale-repmat(test_x_scale(:,i),1,train_num)).^2);</span><br><span class="line">    temp=sort(distance(i,:));</span><br><span class="line">    %找到最近的knn个数据</span><br><span class="line">    tem=tabulate(train_y(distance(i,:)&lt;=temp(knn)));</span><br><span class="line">    %找到最近的数据中最多的类别</span><br><span class="line">    test_y(i)=tem(find(tem(:,2)==max(tem(:,2)),1),1);</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="KNN-display-m"><a href="#KNN-display-m" class="headerlink" title="KNN_display.m"></a>KNN_display.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">function KNN_display(train_x,train_y,test_x,test_y,train_num,test_num,class_num)</span><br><span class="line">hold on;</span><br><span class="line">color_bar=zeros(class_num,3);</span><br><span class="line">for i=1:class_num</span><br><span class="line">    color_bar(i,:)=[rand(1),rand(1),rand(1)];</span><br><span class="line">end</span><br><span class="line">%画出每一类的训练数据，用*表示</span><br><span class="line">for i=1:train_num</span><br><span class="line">    plot(train_x(1,i),train_x(2,i),'color',color_bar(train_y(i),:),'marker','*');</span><br><span class="line">end</span><br><span class="line">%画出每一类的测试数据，用o表示</span><br><span class="line">for j=1:test_num</span><br><span class="line">    plot(test_x(1,j),test_x(2,j),'color',color_bar(test_y(j),:),'marker','o');</span><br><span class="line">end</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/knn1.png" alt="KNN"></p><h1 id="KNN优缺点"><a href="#KNN优缺点" class="headerlink" title="KNN优缺点"></a><font size="5" color="red">KNN优缺点</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>无需参数估计，无需训练</li><li>算法简单，易于理解和实现</li><li>适合于对稀有数据进行分类</li><li>特别适合于多分类问题，KNN的表现超过SVM</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>无法给出分类规则</li><li>对于高维特征，距离的选择和衡量不准确</li><li>计算量较大，对每一个测试样本都需要计算与其他样本的距离</li><li>样本不平衡时，尤其是一类样本多，其他类样本少时会产生严重的问题</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;KNN(K Nearest Neighbor)&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="有监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>差分进化算法(DE)</title>
    <link href="https://USTCcoder.github.io/2019/05/27/optimization_DE/"/>
    <id>https://USTCcoder.github.io/2019/05/27/optimization_DE/</id>
    <published>2019-05-27T07:28:55.000Z</published>
    <updated>2019-09-26T03:59:57.834Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">差分进化算法</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  DE(Differential Evolution Algorithm):是一种高效的全局优化算法。它也是基于群体的启发式搜索算法，群中的每个个体对应一个解向量。差分进化算法的进化流程则与遗传算法非常类似，都包括变异、杂交和选择操作，但这些操作的具体定义与遗传算法有所不同。</p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 随机产生一些初始种群</font></p><p>  <font size="4">2. 根据适应度对种群采用某种方式进行自然选择</font></p><p>  <font size="4">3. 对选择剩余的种群进行差分遗传，产生新的种群</font></p><p>  <font size="4">4. 对父代和子代留一处理，回到步骤2，直到满足某个终止条件</font></p><p>  <font size="4">5. 此时剩余的是适应度较好的种群，比较可得该算法的最优解</font></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/OPTIMIZATION/de2.png" alt="DE"></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用测试函数可以查看相关文档，<a href="https://ustccoder.github.io/2019/05/19/optimization_Testfunction/">测试函数(Test Function)</a></font></p><h2 id="DE-main-m"><a href="#DE-main-m" class="headerlink" title="DE_main.m"></a>DE_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">%自变量取值范围</span><br><span class="line">range_x=[ones(1,1),-ones(1,1)]*500;</span><br><span class="line">%维度</span><br><span class="line">n=size(range_x,1);</span><br><span class="line">%种群数量</span><br><span class="line">gn=100;</span><br><span class="line">%迭代次数</span><br><span class="line">times=1000;</span><br><span class="line">%交叉概率</span><br><span class="line">cr=0.5;</span><br><span class="line">%随机产生一些种群</span><br><span class="line">group=zeros(n,gn);</span><br><span class="line">for k=1:n</span><br><span class="line">    group(k,:)=(rand(1,gn))*(range_x(k,2)-range_x(k,1))+range_x(k,1);</span><br><span class="line">end</span><br><span class="line">%设置当前最优解</span><br><span class="line">best_value=zeros(1,times);</span><br><span class="line">tic;</span><br><span class="line">for k=1:times</span><br><span class="line">    for i=1:gn</span><br><span class="line">        %基因重组的过程中可能发生染色体变异</span><br><span class="line">        exchange=randperm(gn,3);</span><br><span class="line">        h=group(:,exchange(1))+rand(1)*(group(:,exchange(2))-group(:,exchange(3)));</span><br><span class="line">        h(h&gt;500)=500;</span><br><span class="line">        h(h&lt;-500)=-500;</span><br><span class="line">        v=group(:,i);</span><br><span class="line">        %染色体交换,保留的物种产生后代时发生基因重组</span><br><span class="line">        for j=1:n</span><br><span class="line">            if cr&gt;rand(1)</span><br><span class="line">                v(j)=h(j);</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        if f(v)&lt;f(group(:,i))</span><br><span class="line">            group(:,i)=v;</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    best_value(k)=min(f(group));</span><br><span class="line">    if k&gt;100&amp;&amp;abs(best_value(k)-best_value(k-100))&lt;1e-5</span><br><span class="line">        break;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">time=toc;</span><br><span class="line">disp(['用时：',num2str(time),'秒'])</span><br><span class="line">[mini,index]=min(f(group));</span><br><span class="line">disp(['fmin=',num2str(mini)]);</span><br><span class="line">for k=1:n</span><br><span class="line">    disp(['x',num2str(k),'=',num2str(group(k,index))]);</span><br><span class="line">end</span><br><span class="line">if n==1</span><br><span class="line">    hold on;</span><br><span class="line">    plot(group(index),mini,'ro');</span><br><span class="line">    plot_x=range_x(1):(range_x(2)-range_x(1))/1000:range_x(2);</span><br><span class="line">    plot_y=f(plot_x);</span><br><span class="line">    plot(plot_x,plot_y);</span><br><span class="line">    text((range_x(1)+range_x(2))/2,max(plot_y)+0.1*(max(plot_y)-min(plot_y)),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br><span class="line">if n==2</span><br><span class="line">    %所求最小值的函数</span><br><span class="line">    func=@(x1,x2)x1.*sin(sqrt(abs(x1)))+x2.*sin(sqrt(abs(x2)));</span><br><span class="line">    plot_x=range_x(1,1):(range_x(1,2)-range_x(1,1))/1000:range_x(1,2);</span><br><span class="line">    plot_y=range_x(2,1):(range_x(2,2)-range_x(2,1))/1000:range_x(2,2);</span><br><span class="line">    [plot_x,plot_y] =meshgrid(plot_x,plot_y);</span><br><span class="line">    plot_z=func(plot_x,plot_y);</span><br><span class="line">    surf(plot_x,plot_y,plot_z);</span><br><span class="line">    xlabel('x1');</span><br><span class="line">    ylabel('x2');</span><br><span class="line">    zlabel('y');</span><br><span class="line">    hold on;</span><br><span class="line">    plot3(group(1,index),group(2,index),mini,'ko')</span><br><span class="line">    text((range_x(1,1)+range_x(1,2))/2,(range_x(2,1)+range_x(2,2))/2,max(max(plot_z))+0.5*(max(max(plot_z))-min(min(plot_z))),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="f-m"><a href="#f-m" class="headerlink" title="f.m"></a>f.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">function res=f(x)</span><br><span class="line">func=@(x)(x).*sin(sqrt(abs(x)));</span><br><span class="line">res=zeros(1,size(x,2));</span><br><span class="line">for i=1:size(x,1)</span><br><span class="line">    res=res+func(x(i,:));</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/OPTIMIZATION/de1.png" alt="DE"></p><script type="math/tex; mode=display">f(x)=x \cdot \sin(\sqrt{\lvert x \rvert}) \ , \ x \in [-500,500]</script><script type="math/tex; mode=display">理论值：f(x)_{min}=f(-420.96874592006)=-418.982887272434</script><script type="math/tex; mode=display">所求值：f(x)_{min}=f(-420.975929624477)=-418.982887272434</script><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>受到参数影响较小</li><li>不会产生早熟收敛问题</li><li>适用于多维的最优值求解</li><li>从群体出发，具有并行性</li><li>算法不依赖初始种群的选择</li><li>可用于求解复杂的非线性优化问题</li><li>使用概率机制进行迭代，具有随机性</li><li>具有可扩展性，容易与其他算法结合</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>对问题编码表示较为困难</li><li>因为有大量的比较和选择，可能速度稍慢于遗传算法</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;DE&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="全局搜索方法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%85%A8%E5%B1%80%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>全局搜索算法比较(Global Search Algorithm Comparison)</title>
    <link href="https://USTCcoder.github.io/2019/05/26/optimization_compare/"/>
    <id>https://USTCcoder.github.io/2019/05/26/optimization_compare/</id>
    <published>2019-05-26T10:59:14.000Z</published>
    <updated>2019-09-26T03:59:21.634Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">全局搜索算法比较</font></strong></center><p></p><h1 id="全局搜索"><a href="#全局搜索" class="headerlink" title="全局搜索"></a><font size="5" color="red">全局搜索</font></h1><p>  梯度方法，牛顿法，共轭梯度法，拟牛顿法等，能够从初始点出发，产生一个迭代序列。但是很多时候，往往只能收敛到<strong>局部极小点</strong>。因此为了保证算法能够收敛到<strong>全局最小点</strong>，需要借助于<strong>全局搜索算法</strong>来实现。<br><a id="more"></a></p><h1 id="算法分类"><a href="#算法分类" class="headerlink" title="算法分类"></a><font size="5" color="red">算法分类</font></h1><p><img src="/images/OPTIMIZATION/compare.png" alt="COMPARE"></p><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><p><font size="4">所用测试函数可以查看相关文档，<a href="https://ustccoder.github.io/2019/05/19/optimization_Testfunction/">测试函数(Test Function)</a></font></p><h2 id="模拟退火算法-SA"><a href="#模拟退火算法-SA" class="headerlink" title="模拟退火算法(SA)"></a><a href="https://ustccoder.github.io/2019/05/20/optimization_SA/">模拟退火算法(SA)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>计算过程简单</li><li>可用于求解复杂的非线性优化问题</li><li>相比梯度下降，增加了逃离局部最小的可能</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>参数敏感</li><li>收敛速度慢</li><li>执行时间长</li><li>算法性能与初始值有关</li><li>可能落入其他的局部最小值</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/sa1.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_sa_2.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/compare_sa_3.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_sa_4.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><h2 id="遗传算法-GA"><a href="#遗传算法-GA" class="headerlink" title="遗传算法(GA)"></a><a href="https://ustccoder.github.io/2019/05/21/optimization_GA/">遗传算法(GA)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>从群体出发，具有并行性</li><li>可用于求解复杂的非线性优化问题</li><li>使用概率机制进行迭代，具有随机性</li><li>具有可扩展性，容易与其他算法结合</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>受到参数影响较大</li><li>可能产生早熟收敛问题</li><li>对问题编码表示较为困难</li><li>算法对初始种群的选择有一定的依赖性</li><li>搜索速度比较慢，要得到较精确的解需要较多的训练时间</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/ga1.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_ga_2.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/compare_ga_3.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_ga_4.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><h2 id="免疫算法-IA"><a href="#免疫算法-IA" class="headerlink" title="免疫算法(IA)"></a><a href="https://ustccoder.github.io/2019/05/22/optimization_IA/">免疫算法(IA)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>受到参数影响较小</li><li>解决早熟收敛问题</li><li>从群体出发，具有并行性</li><li>对抗体选择的依赖性降低</li><li>可用于求解复杂的非线性优化问题</li><li>使用概率机制进行迭代，具有随机性</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>对问题编码表示较为困难</li><li>要进行多次免疫应答，因此速度慢于遗传算法</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/ia1.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_ia_2.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/compare_ia_3.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_ia_4.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><h2 id="蚁群算法-ACO"><a href="#蚁群算法-ACO" class="headerlink" title="蚁群算法(ACO)"></a><a href="https://ustccoder.github.io/2019/05/23/optimization_ACO/">蚁群算法(ACO)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>搜索速度较快</li><li>受到参数影响较小</li><li>从群体出发，具有并行性</li><li>可用于求解复杂的非线性优化问题</li><li>具有可扩展性，容易与其他算法结合</li><li><font size="4"><strong>缺点：</strong></font></li><li>对初始蚂蚁的数量有很高的要求</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/aco1.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_aco_2.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/compare_aco_3.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_aco_4.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><h2 id="粒子群算法-PSO"><a href="#粒子群算法-PSO" class="headerlink" title="粒子群算法(PSO)"></a><a href="https://ustccoder.github.io/2019/05/24/optimization_PSO/">粒子群算法(PSO)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>搜索能力最快</li><li>从群体出发，具有并行性</li><li>可用于求解复杂的非线性优化问题</li><li><font size="4"><strong>缺点：</strong></font></li><li>受到参数影响较大</li><li>存在早熟收敛问题</li><li>对初始粒子群的数量有很高的要求</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/pso1.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_pso_2.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/compare_pso_3.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_pso_4.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><h2 id="单纯形法-Nelder-Mead"><a href="#单纯形法-Nelder-Mead" class="headerlink" title="单纯形法(Nelder-Mead)"></a><a href="https://ustccoder.github.io/2019/05/25/optimization_NM/">单纯形法(Nelder-Mead)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>受到参数影响较小</li><li>具有快速随机的搜索能力</li><li>可用于求解复杂的非线性优化问题</li><li>每次迭代都更接近最优解，精度最高</li><li><font size="4"><strong>缺点：</strong></font></li><li>算法性能与初始值有关</li><li>不适用于多维的最优值求解</li><li>可能落入其他的局部最小值</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/nm1.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_nm_2.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/compare_nm_3.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_nm_4.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><h2 id="遗传差分算法-DE"><a href="#遗传差分算法-DE" class="headerlink" title="遗传差分算法(DE)"></a><a href="https://ustccoder.github.io/2019/05/27/optimization_DE/">遗传差分算法(DE)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>受到参数影响较小</li><li>不会产生早熟收敛问题</li><li>适用于多维的最优值求解</li><li>从群体出发，具有并行性</li><li>算法不依赖初始种群的选择</li><li>可用于求解复杂的非线性优化问题</li><li>使用概率机制进行迭代，具有随机性</li><li>具有可扩展性，容易与其他算法结合</li><li><font size="4"><strong>缺点：</strong></font></li><li>对问题编码表示较为困难</li><li>因为有大量的比较和选择，可能速度稍慢于遗传算法</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/de1.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_de_2.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><p></p><center><div style="float:left;margin-left:50px"><img src="/images/OPTIMIZATION/compare_de_3.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/OPTIMIZATION/compare_de_4.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><h2 id="算法对比"><a href="#算法对比" class="headerlink" title="算法对比"></a>算法对比</h2><p></p><center><img src="/images/OPTIMIZATION/compare1.png">搜索精度分析</center><p></p><div style="float:none;clear:both;"></div><center><img src="/images/OPTIMIZATION/compare2.png">所用时间分析</center><div style="float:none;clear:both;"></div><h1 id="特点小结"><a href="#特点小结" class="headerlink" title="特点小结"></a><font size="5" color="red">特点小结</font></h1><h2 id="模拟退火算法-SA-1"><a href="#模拟退火算法-SA-1" class="headerlink" title="模拟退火算法(SA)"></a>模拟退火算法(SA)</h2><ul><li>不同问题对温度要求不同，起始温度和温度变化率都会影响搜索精度和时间</li><li>每次都随机产生一个解，有时很难跳出较深较远的局部最优</li><li>可以重复运算多次，取多次的最小值，这样可以保证得到全局最优</li></ul><h2 id="遗传算法-GA-1"><a href="#遗传算法-GA-1" class="headerlink" title="遗传算法(GA)"></a>遗传算法(GA)</h2><ul><li>不同的问题需要的种群个数不同，种群个数越多，搜索精度越高，用时越长</li><li>迭代次数越高，自然选择越强，保留的结果越好，搜索精度越高，用时越长</li><li>问题的表示影响遗传算法的效率，用二进制基因串表示还是用十进制表示需要考虑</li><li>自然选择的方式很重要，采用精英选择或轮盘赌会产生不同的效果</li><li>遗传算子对算法的影响很大，采用何种交叉方式和多大的变异率最合适</li></ul><h2 id="免疫算法-IA-1"><a href="#免疫算法-IA-1" class="headerlink" title="免疫算法(IA)"></a>免疫算法(IA)</h2><ul><li>免疫算法包含遗传算法的所有特点</li><li>免疫过程从记忆细胞中取出部分，提高搜索效率</li><li>免疫过程随机产生另一部分抗体，给搜索到其他更优点创造了可能性</li><li>每次的免疫过程都相当于一次遗传算法，因此免疫次数越多，精度越高，时间越长</li></ul><h2 id="蚁群算法-ACO-1"><a href="#蚁群算法-ACO-1" class="headerlink" title="蚁群算法(ACO)"></a>蚁群算法(ACO)</h2><ul><li>和自然界中蚁群一样，蚂蚁越多，搜索精度越高，但是计算量大，用时更长</li><li>迭代次数越高，信息素的作用时间越长，所求结果更好，精度更高，用时更长</li></ul><h2 id="粒子群算法-PSO-1"><a href="#粒子群算法-PSO-1" class="headerlink" title="粒子群算法(PSO)"></a>粒子群算法(PSO)</h2><ul><li>和自然界中飞鸟一样，飞鸟越多越容易遇到全局最优解，搜索精度越高，用时更长</li><li>迭代次数越高，飞鸟之间的信息交换越多，所求结果更好，精度更高，用时更长</li><li>粒子群算法可以实现高度的并行计算，因此在搜索函数最值方面速度最快</li></ul><h2 id="单纯形法-Nelder-Mead-1"><a href="#单纯形法-Nelder-Mead-1" class="headerlink" title="单纯形法(Nelder-Mead)"></a>单纯形法(Nelder-Mead)</h2><ul><li>单纯形法是一种收缩算法，如果初始点选择在局部极小值区域，会收缩到局部最小</li><li>和SA相同，可以重复运算多次，取多次的最小值，这样可以保证得到全局最优。</li><li>和其他算法不同，没有随机性，每次迭代都产生比上一次更优的解，搜索精度最高</li></ul><h2 id="差分进化算法-DE"><a href="#差分进化算法-DE" class="headerlink" title="差分进化算法(DE)"></a>差分进化算法(DE)</h2><ul><li>差分进化算法类似于改进版的遗传算法，更加适合于多维最优值求解</li><li>和遗传算法不同的是具有差分性质，能够更容易地跳出当前的局部解</li><li>淘汰机制也十分简单，将子代和父代对比，直接淘汰表现差的解，精英选择</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Global Search Algorithm Comparison&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="全局搜索方法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%85%A8%E5%B1%80%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>单纯形法(Nelder-Mead)</title>
    <link href="https://USTCcoder.github.io/2019/05/25/optimization_NM/"/>
    <id>https://USTCcoder.github.io/2019/05/25/optimization_NM/</id>
    <published>2019-05-25T11:33:42.000Z</published>
    <updated>2019-09-26T03:59:38.043Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">单纯形法</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  Nelder-Mead:单纯形法秉承<strong>保证每一次迭代比前一次更优</strong>的基本思想，先找出一个基本可行解，看是否是最优解，若不是，则按照一定法则转换到另一改进后<strong>更优的基本可行解</strong>，再鉴别，若仍不是，则再转换，按此重复进行。因基本可行解的个数有限，故经<strong>有限次转换必能得出问题的最优解</strong>。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 随机产生N+1个点，构造单纯形，N为所求极值的维度</font></p><p>  <font size="4">2. 对这些点的函数值进行从小到大排序，求出最优N个点的重心p<sub>g</sub></font></p><script type="math/tex; mode=display">f(p_0) \leq f(p_1) \leq \ldots \leq f(p_N)</script><script type="math/tex; mode=display">p_{g}=\displaystyle \sum_{i=0}^{N-1}\frac {p_{i}}{N}</script><p>  <font size="4">3. 对最差的点进行反射得到p<sub>r</sub></font></p><script type="math/tex; mode=display">p_{r}=p_{g}+\rho \cdot (p_{g}-p_{N}) \ , \ 其中 \rho 为反射系数</script><p>  <font size="4">4. 如果f(p<sub>0</sub>)≤f(p<sub>r</sub>)<f(p<sub>N-1&lt;/sub&gt;)，p<sub>r</sub>代替p<sub>N</sub>，回到步骤2</f(p<sub></font></p><p>  <font size="4">5. 如果f(p<sub>r</sub>)<f(p<sub>0&lt;/sub&gt;)，说明p<sub>r</sub>方向有利于函数值下降</f(p<sub></font></p><script type="math/tex; mode=display">p_{e}=p_{g}+ \chi \cdot (p_{r}-p_{g}) \ , \ 其中 \chi 为延伸系数</script><p>  <font size="4">6. 如果f(p<sub>e</sub>)<f(p<sub>r&lt;/sub&gt;)，p<sub>e</sub>代替p<sub>N</sub>，否则p<sub>r</sub>代替p<sub>N</sub>，回到步骤2</f(p<sub></font></p><p>  <font size="4">7. f(p<sub>r</sub>)≥f(p<sub>N-1</sub>)，说明要进行收缩操作</font></p><script type="math/tex; mode=display">p_{c}= \begin{cases} p_{g}+ \gamma \cdot (p_{r}-p_{g}) &  f(p_{r}) < f(p_{N}) \\ p_{g}+ \gamma \cdot (p_{l}-p_{r}) & f(p_{r}) \ge f(p_{N}) \end{cases} \ , \ 其中 \gamma 为收缩系数</script><p>  <font size="4">8. 如果f(p<sub>c</sub>)≤f(p<sub>N</sub>)，p<sub>c</sub>代替p<sub>N</sub>，回到步骤2</font></p><p>  <font size="4">9. f(p<sub>c</sub>)&gt;f(p<sub>r</sub>)，只保留p<sub>0</sub>，其他点到p<sub>0</sub>距离减半，收缩单纯形</font></p><p>  <font size="4">10.如果不满足某个终止条件，回到步骤2</font></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/OPTIMIZATION/nm2.png" alt="NelderMead"></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用测试函数可以查看相关文档，<a href="https://ustccoder.github.io/2019/05/19/optimization_Testfunction/">测试函数(Test Function)</a></font></p><h2 id="NelderMead-main-m"><a href="#NelderMead-main-m" class="headerlink" title="NelderMead_main.m"></a>NelderMead_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">%自变量取值范围</span><br><span class="line">range_x=[ones(1,1),-ones(1,1)]*500;</span><br><span class="line">%维度</span><br><span class="line">n=size(range_x,1);</span><br><span class="line">%反射系数rho</span><br><span class="line">rho=1;</span><br><span class="line">%延伸系数</span><br><span class="line">ka1=2;</span><br><span class="line">%收缩系数</span><br><span class="line">ka2=0.5;</span><br><span class="line">%迭代次数</span><br><span class="line">times=100;</span><br><span class="line">%尝试解次数</span><br><span class="line">num=50;</span><br><span class="line">value=zeros(n,num);</span><br><span class="line">tic;</span><br><span class="line">for i=1:num</span><br><span class="line">    %给x赋初值</span><br><span class="line">    x=zeros(n,n+1);</span><br><span class="line">    for k=1:n</span><br><span class="line">        x(k,:)=(rand(1,n+1))*(range_x(k,2)-range_x(k,1))+range_x(k,1);</span><br><span class="line">    end</span><br><span class="line">    best_value=zeros(1,times);</span><br><span class="line">    for j=1:times</span><br><span class="line">        [~,index]=sort(f(x));</span><br><span class="line">        %将小的值排在前面</span><br><span class="line">        x=x(:,index);</span><br><span class="line">        %求重心pg</span><br><span class="line">        xg=sum(x(:,1:end-1),2)/n;</span><br><span class="line">        %进行反射</span><br><span class="line">        xr=xg+rho*(xg-x(:,n+1));</span><br><span class="line">        %判断自变量是否在范围</span><br><span class="line">        for k=1:n</span><br><span class="line">            if xr(k)&lt;range_x(k,1)</span><br><span class="line">                xr(k)=range_x(k,1);</span><br><span class="line">            end</span><br><span class="line">            if xr(k)&gt;range_x(k,2)</span><br><span class="line">                xr(k)=range_x(k,2);</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        %如果目标函数值在最好和最坏之间</span><br><span class="line">        if f(xr)&gt;=f(x(:,1))&amp;&amp;f(xr)&lt;f(x(:,n))</span><br><span class="line">            x(:,n+1)=xr;</span><br><span class="line">        %如果新产生的点比最小的点还要小，说明这个方向有利于值的减小</span><br><span class="line">        elseif f(xr)&lt;f(x(:,1))</span><br><span class="line">            %进一步向这个方向延伸</span><br><span class="line">            xe=xg+ka1*(xr-xg);</span><br><span class="line">            for k=1:n</span><br><span class="line">                if xe(k)&lt;range_x(k,1)</span><br><span class="line">                    xe(k)=range_x(k,1);</span><br><span class="line">                end</span><br><span class="line">                if xe(k)&gt;range_x(k,2)</span><br><span class="line">                    xe(k)=range_x(k,2);</span><br><span class="line">                end</span><br><span class="line">            end</span><br><span class="line">            %如果第二次延伸后的点比第一次延伸后产生的点小，则用第二次延伸后的点替换原来最大的点</span><br><span class="line">            if f(xe)&lt;f(xr)</span><br><span class="line">                x(:,n+1)=xe;</span><br><span class="line">            %否则用第一次延伸后的点替换原来最大的点</span><br><span class="line">            else</span><br><span class="line">                x(:,n+1)=xr;</span><br><span class="line">            end</span><br><span class="line">        %如果新产生的点比最小的点还要大</span><br><span class="line">        else</span><br><span class="line">            %如果新产生的点比最大的点小，说明要进行外收缩</span><br><span class="line">            if f(xr)&lt;f(x(:,n+1))</span><br><span class="line">                xc=xg+ka2*(xr-xg);</span><br><span class="line">            %如果新产生的点比最大的点大，说明要进行内收缩</span><br><span class="line">            else</span><br><span class="line">                xc=xg+ka2*(x(:,n+1)-xg);</span><br><span class="line">            end</span><br><span class="line">            %如果无论进行内收缩还是外收缩产生的值都比最大值要小，则替换最大值</span><br><span class="line">            if f(xc)&lt;=f(x(:,n+1))</span><br><span class="line">                x(:,n+1)=xc;</span><br><span class="line">            %%如果无论进行内收缩还是外收缩产生的值都比最大值要大，则缩小范围继续搜索</span><br><span class="line">            else</span><br><span class="line">                for k=2:n+1</span><br><span class="line">                    x(:,k)=(x(:,1)+x(:,k))/2;</span><br><span class="line">                end</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        best_value(j)=x(find(f(x)==min(f(x)),1));</span><br><span class="line">        if j&gt;5&amp;&amp;abs(best_value(j)-best_value(j-5))&lt;1e-5</span><br><span class="line">            break;</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    value(:,i)=x(:,find(f(x)==min(f(x)),1));</span><br><span class="line">end</span><br><span class="line">time=toc;</span><br><span class="line">disp(['用时：',num2str(time),'秒'])</span><br><span class="line">[mini,index]=min(f(value));</span><br><span class="line">disp(['fmin=',num2str(mini)]);</span><br><span class="line">for k=1:n</span><br><span class="line">    disp(['x',num2str(k),'=',num2str(value(k,index))]);</span><br><span class="line">end</span><br><span class="line">if n==1</span><br><span class="line">    hold on;</span><br><span class="line">    plot(value(index),mini,'ro');</span><br><span class="line">    plot_x=range_x(1):(range_x(2)-range_x(1))/1000:range_x(2);</span><br><span class="line">    plot_y=f(plot_x);</span><br><span class="line">    plot(plot_x,plot_y);</span><br><span class="line">    text((range_x(1)+range_x(2))/2,max(plot_y)+0.1*(max(plot_y)-min(plot_y)),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br><span class="line">if n==2</span><br><span class="line">    func=@(x1,x2)x1.*sin(sqrt(abs(x1)))+x2.*sin(sqrt(abs(x2)));</span><br><span class="line">    plot_x=range_x(1,1):(range_x(1,2)-range_x(1,1))/1000:range_x(1,2);</span><br><span class="line">    plot_y=range_x(2,1):(range_x(2,2)-range_x(2,1))/1000:range_x(2,2);</span><br><span class="line">    [plot_x,plot_y] =meshgrid(plot_x,plot_y);</span><br><span class="line">    plot_z=func(plot_x,plot_y);</span><br><span class="line">    surf(plot_x,plot_y,plot_z);</span><br><span class="line">    xlabel('x1');</span><br><span class="line">    ylabel('x2');</span><br><span class="line">    zlabel('y');</span><br><span class="line">    hold on;</span><br><span class="line">    plot3(value(1,index),value(2,index),mini,'ko')</span><br><span class="line">    text((range_x(1,1)+range_x(1,2))/2,(range_x(2,1)+range_x(2,2))/2,max(max(plot_z))+0.5*(max(max(plot_z))-min(min(plot_z))),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="f-m"><a href="#f-m" class="headerlink" title="f.m"></a>f.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">function res=f(x)</span><br><span class="line">func=@(x)(x).*sin(sqrt(abs(x)));</span><br><span class="line">res=zeros(1,size(x,2));</span><br><span class="line">for i=1:size(x,1)</span><br><span class="line">    res=res+func(x(i,:));</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/OPTIMIZATION/nm1.png" alt="NelderMead"></p><script type="math/tex; mode=display">f(x)=x \cdot \sin(\sqrt{\lvert x \rvert}) \ , \ x \in [-500,500]</script><script type="math/tex; mode=display">理论值：f(x)_{min}=f(-420.96874592006)=-418.982887272434</script><script type="math/tex; mode=display">所求值：f(x)_{min}=f(-420.968746504328)=-418.982887272434</script><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>受到参数影响较小</li><li>具有快速随机的搜索能力</li><li>可用于求解复杂的非线性优化问题</li><li>每次迭代都更接近最优解，精度最高</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>算法性能与初始值有关</li><li>不适用于多维的最优值求解</li><li>可能落入其他的局部最小值</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Nelder-Mead&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="全局搜索方法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%85%A8%E5%B1%80%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>粒子群算法(PSO)</title>
    <link href="https://USTCcoder.github.io/2019/05/24/optimization_PSO/"/>
    <id>https://USTCcoder.github.io/2019/05/24/optimization_PSO/</id>
    <published>2019-05-24T13:14:15.000Z</published>
    <updated>2019-09-26T02:47:39.894Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">粒子群算法</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  PSO(Particle Swarm Optimization):受到飞鸟集群活动的规律性启发，进而利用群体智能建立的一个简化模型。粒子群算法在对动物集群活动行为观察基础上，利用群体中的个体对信息的<strong>共享</strong>使整个群体的运动在问题求解空间中产生<strong>从无序到有序</strong>的演化过程，从而获得最优解。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 随机产生一些粒子</font></p><p>  <font size="4">2. 找出所有粒子中适应度最优的粒子g<sub>best</sub></font></p><script type="math/tex; mode=display">g_{best}=\underset{x}{arg \ min} \ f(x)</script><p>  <font size="4">3. 更新每一个粒子的飞行速度</font></p><script type="math/tex; mode=display">v_{i}'=w \cdot v_{i}+c1 \cdot r_{i} \cdot (p_{i}-x_{i})+c2 \cdot s_{i} \cdot (g_{best}-x_{i})</script><p>  <font size="4">4. 获得每个粒子当前位置x<sub>i</sub>和该粒子在飞行中到达过的最优位置p<sub>i</sub></font></p><script type="math/tex; mode=display">x_{i}'=x_{i}+v_{i}'</script><script type="math/tex; mode=display">p_{i}'= \begin{cases} x_{i}' &  f(x_{i}') < f(p_{i}) \\ p_{i} & f(x_{i}') \ge f(p_{i}) \end{cases}</script><p>  <font size="4">5. 回到步骤2，直到满足某个终止条件</font></p><p>  <font size="4">6. 此时粒子集群，粒子群位置为极小值，最小的p为算法的最优解</font></p><p>  <font size="4">7. 回到步骤2，直到满足某个终止条件</font></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/OPTIMIZATION/pso2.png" alt="PSO"></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用测试函数可以查看相关文档，<a href="https://ustccoder.github.io/2019/05/19/optimization_Testfunction/">测试函数(Test Function)</a></font></p><h2 id="PSO-main-m"><a href="#PSO-main-m" class="headerlink" title="PSO_main.m"></a>PSO_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">%自变量取值范围</span><br><span class="line">range_x=[ones(1,1),-ones(1,1)]*500;</span><br><span class="line">%维度</span><br><span class="line">n=size(range_x,1);</span><br><span class="line">%迭代次数</span><br><span class="line">times=100;</span><br><span class="line">%w为惯性权重</span><br><span class="line">w=0.8;</span><br><span class="line">%c1为自身认知权重</span><br><span class="line">c1=1;</span><br><span class="line">%c2为社会认知权重</span><br><span class="line">c2=1;</span><br><span class="line">%粒子群的个数</span><br><span class="line">gn=1000;</span><br><span class="line">%粒子群的初始位置</span><br><span class="line">x=zeros(n,gn);</span><br><span class="line">for k=1:n</span><br><span class="line">    x(k,:)=(rand(1,gn))*(range_x(k,2)-range_x(k,1))+range_x(k,1);</span><br><span class="line">end</span><br><span class="line">%个体极值</span><br><span class="line">p=x;</span><br><span class="line">%v代表粒子的速度</span><br><span class="line">v=zeros(n,gn);</span><br><span class="line">%设置当前最优解</span><br><span class="line">best_value=zeros(1,times);</span><br><span class="line">tic;</span><br><span class="line">for i=1:times</span><br><span class="line">    [solve,gbest]=min(f(x));</span><br><span class="line">    for j=1:gn</span><br><span class="line">        %速度分为3个部分，惯性速度，该点最优和全局最优</span><br><span class="line">        v(:,j)=w*v(:,j)+c1*rand(n,1).*(p(:,j)-x(:,j))+c2*rand(n,1).*(x(:,gbest)-x(:,j));</span><br><span class="line">        x(:,j)=x(:,j)+v(:,j);</span><br><span class="line">        %限制解的范围</span><br><span class="line">        for k=1:n</span><br><span class="line">            if x(k,j)&lt;range_x(k,1)</span><br><span class="line">                x(k,j)=range_x(k,1);</span><br><span class="line">            end</span><br><span class="line">            if x(k,j)&gt;range_x(k,2)</span><br><span class="line">                x(k,j)=range_x(k,2);</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        if f(x(:,j))&lt;f(p(:,j))</span><br><span class="line">            p(:,j)=x(:,j);</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    best_value(i)=min(f(p));</span><br><span class="line">    if i&gt;5&amp;&amp;abs(best_value(i)-best_value(i-5))&lt;1e-5</span><br><span class="line">        break;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">time=toc;</span><br><span class="line">disp(['用时：',num2str(time),'秒'])</span><br><span class="line">[mini,index]=min(f(p));</span><br><span class="line">disp(['fmin=',num2str(mini)]);</span><br><span class="line">for k=1:n</span><br><span class="line">    disp(['x',num2str(k),'=',num2str(p(k,index))]);</span><br><span class="line">end</span><br><span class="line">if n==1</span><br><span class="line">    hold on;</span><br><span class="line">    plot(p(index),mini,'ro');</span><br><span class="line">    plot_x=range_x(1):(range_x(2)-range_x(1))/1000:range_x(2);</span><br><span class="line">    plot_y=f(plot_x);</span><br><span class="line">    plot(plot_x,plot_y);</span><br><span class="line">    text((range_x(1)+range_x(2))/2,max(plot_y)+0.1*(max(plot_y)-min(plot_y)),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br><span class="line">if n==2</span><br><span class="line">    %所求最小值的函数</span><br><span class="line">    func=@(x1,x2)x1.*sin(sqrt(abs(x1)))+x2.*sin(sqrt(abs(x2)));</span><br><span class="line">    plot_x=range_x(1,1):(range_x(1,2)-range_x(1,1))/1000:range_x(1,2);</span><br><span class="line">    plot_y=range_x(2,1):(range_x(2,2)-range_x(2,1))/1000:range_x(2,2);</span><br><span class="line">    [plot_x,plot_y] =meshgrid(plot_x,plot_y);</span><br><span class="line">    plot_z=func(plot_x,plot_y);</span><br><span class="line">    surf(plot_x,plot_y,plot_z);</span><br><span class="line">    xlabel('x1');</span><br><span class="line">    ylabel('x2');</span><br><span class="line">    zlabel('y');</span><br><span class="line">    hold on;</span><br><span class="line">    plot3(p(1,index),p(2,index),mini,'ko')</span><br><span class="line">    text((range_x(1,1)+range_x(1,2))/2,(range_x(2,1)+range_x(2,2))/2,max(max(plot_z))+0.5*(max(max(plot_z))-min(min(plot_z))),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="f-m"><a href="#f-m" class="headerlink" title="f.m"></a>f.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">function res=f(x)</span><br><span class="line">func=@(x)(x).*sin(sqrt(abs(x)));</span><br><span class="line">res=zeros(1,size(x,2));</span><br><span class="line">for i=1:size(x,1)</span><br><span class="line">    res=res+func(x(i,:));</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/OPTIMIZATION/pso1.png" alt="PSO"></p><script type="math/tex; mode=display">f(x)=x \cdot \sin(\sqrt{\lvert x \rvert}) \ , \ x \in [-500,500]</script><script type="math/tex; mode=display">理论值：f(x)_{min}=f(-420.96874592006)=-418.982887272434</script><script type="math/tex; mode=display">所求值：f(x)_{min}=f(-420.968750420615)=-418.982887272432</script><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>搜索能力最快</li><li>从群体出发，具有并行性</li><li>可用于求解复杂的非线性优化问题</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>受到参数影响较大</li><li>存在早熟收敛问题</li><li>对初始粒子群的数量有很高的要求</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;PSO&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="全局搜索方法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%85%A8%E5%B1%80%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>蚁群算法(ACO)</title>
    <link href="https://USTCcoder.github.io/2019/05/23/optimization_ACO/"/>
    <id>https://USTCcoder.github.io/2019/05/23/optimization_ACO/</id>
    <published>2019-05-23T12:17:47.000Z</published>
    <updated>2019-09-26T02:45:50.943Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">蚁群算法</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  ACO(ant colony optimization):研究蚂蚁觅食的过程中，发现单个蚂蚁的行为比较简单，但是蚁群整体却可以体现一些智能的行为。例如蚁群可以在不同的环境下，寻找最短到达食物源的路径。蚂蚁会在其经过的路径上释放一种可以称之为<strong>信息素</strong>的物质，蚁群内的蚂蚁对信息素具有感知能力，它们会<strong>沿着信息素浓度较高路径行走</strong>，而每只路过的蚂蚁都会在路上留下信息素，形成一种类似<strong>正反馈</strong>的机制。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 随机产生一些蚂蚁</font></p><p>  <font size="4">2. 判断蚂蚁所在位置的值越小，信息素越多</font></p><p>  <font size="4">3. 如果信息素较多，蚂蚁小幅移动，信息素较少，蚂蚁大幅移动</font></p><p>  <font size="4">4. 如果蚂蚁移动之后值变小，则说明移动方向正确</font></p><p>  <font size="4">5. 回到步骤2，直到满足某个终止条件</font></p><p>  <font size="4">6. 此时蚂蚁集群，蚁群位置为极小值，比较可得该算法的最优解</font></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/OPTIMIZATION/aco2.png" alt="ACO"></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用测试函数可以查看相关文档，<a href="https://ustccoder.github.io/2019/05/19/optimization_Testfunction/">测试函数(Test Function)</a></font></p><h2 id="ACO-main-m"><a href="#ACO-main-m" class="headerlink" title="ACO_main.m"></a>ACO_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">%自变量取值范围</span><br><span class="line">range_x=[ones(1,1),-ones(1,1)]*500;</span><br><span class="line">%维度</span><br><span class="line">n=size(range_x,1);</span><br><span class="line">%蚂蚁数</span><br><span class="line">m=1000;</span><br><span class="line">%迭代次数</span><br><span class="line">times=100;</span><br><span class="line">%信息素挥发系数</span><br><span class="line">rho=0.8;</span><br><span class="line">%转移概率常数</span><br><span class="line">p0=1;</span><br><span class="line">%转移概率</span><br><span class="line">p=zeros(1,m);</span><br><span class="line">%x为蚁群的初始位置</span><br><span class="line">x=zeros(n,m);</span><br><span class="line">for k=1:n</span><br><span class="line">    x(k,:)=(rand(1,m))*(range_x(k,2)-range_x(k,1))+range_x(k,1);</span><br><span class="line">end</span><br><span class="line">%tau为信息素</span><br><span class="line">tau=-f(x);</span><br><span class="line">%设置当前最优解</span><br><span class="line">best_value=zeros(1,times);</span><br><span class="line">tic;</span><br><span class="line">for i=1:times</span><br><span class="line">    [~,bestindex]=max(tau);</span><br><span class="line">    for j=1:m</span><br><span class="line">        %信息素越大越不容易转移</span><br><span class="line">        p(j)=(tau(bestindex)-tau(j))/tau(bestindex);</span><br><span class="line">        if p(j)&lt;p0</span><br><span class="line">            %如果信息素较多，转移步伐就较小</span><br><span class="line">            temp=x(:,j)+(rand(n,1)*2-1)/i;</span><br><span class="line">        else</span><br><span class="line">            temp=zeros(n,1);</span><br><span class="line">            %如果信息素较少，转移步伐就较大</span><br><span class="line">            for k=1:n</span><br><span class="line">                temp(k)=x(k,j)+(rand(1,1)-0.5)*(range(k,2)-range(k,1));</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        %限制边界条件</span><br><span class="line">        for k=1:n</span><br><span class="line">            if temp(k)&lt;range_x(k,1)</span><br><span class="line">                temp(k)=range_x(k,1);</span><br><span class="line">            end</span><br><span class="line">            if temp(k)&gt;range_x(k,2)</span><br><span class="line">                temp(k)=range_x(k,2);</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        %如果移动后值变小，则移动</span><br><span class="line">        if f(temp)&lt;f(x(:,j))</span><br><span class="line">            x(:,j)=temp;</span><br><span class="line">        end</span><br><span class="line">        %更新信息素，函数值越小，信息量越大</span><br><span class="line">        tau(j)=(1-rho)*tau(j)-f(x(:,j));</span><br><span class="line">    end</span><br><span class="line">    best_value(i)=min(f(x));</span><br><span class="line">    if i&gt;5&amp;&amp;abs(best_value(i)-best_value(i-5))&lt;1e-5</span><br><span class="line">        break;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">time=toc;</span><br><span class="line">disp(['用时：',num2str(time),'秒'])</span><br><span class="line">[mini,index]=min(f(x));</span><br><span class="line">disp(['fmin=',num2str(mini)]);</span><br><span class="line">for k=1:n</span><br><span class="line">    disp(['x',num2str(k),'=',num2str(x(k,index))]);</span><br><span class="line">end</span><br><span class="line">if n==1</span><br><span class="line">    hold on;</span><br><span class="line">    plot(x(index),mini,'ro');</span><br><span class="line">    plot_x=range_x(1):(range_x(2)-range_x(1))/1000:range_x(2);</span><br><span class="line">    plot_y=f(plot_x);</span><br><span class="line">    plot(plot_x,plot_y);</span><br><span class="line">    text((range_x(1)+range_x(2))/2,max(plot_y)+0.1*(max(plot_y)-min(plot_y)),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br><span class="line">if n==2</span><br><span class="line">    func=@(x1,x2)x1.*sin(sqrt(abs(x1)))+x2.*sin(sqrt(abs(x2)));</span><br><span class="line">    plot_x=range_x(1,1):(range_x(1,2)-range_x(1,1))/1000:range_x(1,2);</span><br><span class="line">    plot_y=range_x(2,1):(range_x(2,2)-range_x(2,1))/1000:range_x(2,2);</span><br><span class="line">    [plot_x,plot_y] =meshgrid(plot_x,plot_y);</span><br><span class="line">    plot_z=func(plot_x,plot_y);</span><br><span class="line">    surf(plot_x,plot_y,plot_z);</span><br><span class="line">    xlabel('x1');</span><br><span class="line">    ylabel('x2');</span><br><span class="line">    zlabel('y');</span><br><span class="line">    hold on;</span><br><span class="line">    plot3(x(1,index),x(2,index),mini,'ko')</span><br><span class="line">    text((range_x(1,1)+range_x(1,2))/2,(range_x(2,1)+range_x(2,2))/2,max(max(plot_z))+0.5*(max(max(plot_z))-min(min(plot_z))),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="f-m"><a href="#f-m" class="headerlink" title="f.m"></a>f.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">function res=f(x)</span><br><span class="line">func=@(x)(x).*sin(sqrt(abs(x)));</span><br><span class="line">res=zeros(1,size(x,2));</span><br><span class="line">for i=1:size(x,1)</span><br><span class="line">    res=res+func(x(i,:));</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/OPTIMIZATION/aco1.png" alt="ACO"></p><script type="math/tex; mode=display">f(x)=x \cdot \sin(\sqrt{\lvert x \rvert}) \ , \ x \in [-500,500]</script><script type="math/tex; mode=display">理论值：f(x)_{min}=f(-420.96874592006)=-418.982887272434</script><script type="math/tex; mode=display">所求值：f(x)_{min}=f(-420.959294517745)=-418.982875999576</script><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>搜索速度较快</li><li>受到参数影响较小</li><li>从群体出发，具有并行性</li><li>可用于求解复杂的非线性优化问题</li><li>具有可扩展性，容易与其他算法结合</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>对初始蚂蚁的数量有很高的要求</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;ACO&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="全局搜索方法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%85%A8%E5%B1%80%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>免疫算法(IA)</title>
    <link href="https://USTCcoder.github.io/2019/05/22/optimization_IA/"/>
    <id>https://USTCcoder.github.io/2019/05/22/optimization_IA/</id>
    <published>2019-05-22T10:29:47.000Z</published>
    <updated>2019-09-26T02:44:42.414Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">测试函数说明</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  IA(Immune Algorithm):免疫算法基于<strong>生物免疫系统</strong>的基本机制，模仿了人体的免疫系统，解决了遗传算法的<strong>早熟收敛</strong>问题。因为免疫系统具有<strong>辨识记忆</strong>的特点，所以可以更快识别群体，面对待求解问题时，相当于面对各种抗原，可以提前<strong>注射疫苗</strong>抑制退化问题，从而更加保持优胜劣汰的特点。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 随机产生一些记忆细胞</font></p><p>  <font size="4">2. 取出部分记忆细胞，剩下抗体由随机产生</font></p><p>  <font size="4">3. 根据适应度对抗体采用某种方式进行选择</font></p><p>  <font size="4">4. 对选择剩余的抗体进行遗传操作，产生新的抗体</font></p><p>  <font size="4">5. 回到步骤3，直到满足某个终止条件</font></p><p>  <font size="4">6. 将产生的新抗体和记忆细胞比较，产生新的记忆细胞</font></p><p>  <font size="4">7. 回到步骤2，直到满足某个终止条件</font></p><p>  <font size="4">8. 此时得到免疫力最好的记忆细胞，比较可得该算法的最优解</font></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/OPTIMIZATION/ia2.png" alt="IA"></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用测试函数可以查看相关文档，<a href="https://ustccoder.github.io/2019/05/19/optimization_Testfunction/">测试函数(Test Function)</a></font></p><h2 id="IA-main-m"><a href="#IA-main-m" class="headerlink" title="IA_main.m"></a>IA_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">%自变量取值范围</span><br><span class="line">range_x=[ones(1,1),-ones(1,1)]*500;</span><br><span class="line">%维度</span><br><span class="line">n=size(range_x,1);</span><br><span class="line">%种群数量</span><br><span class="line">gn=400;</span><br><span class="line">%遗传迭代时进入下一代的数量</span><br><span class="line">m=50    ;</span><br><span class="line">%从记忆细胞种拿出的个数</span><br><span class="line">l=200;</span><br><span class="line">%迭代次数</span><br><span class="line">times=200;</span><br><span class="line">%免疫作用次数</span><br><span class="line">t=10;</span><br><span class="line">%记忆细胞</span><br><span class="line">remember=zeros(n,gn);</span><br><span class="line">for k=1:n</span><br><span class="line">    remember(k,:)=(rand(1,gn))*(range_x(k,2)-range_x(k,1))+range_x(k,1);</span><br><span class="line">end</span><br><span class="line">tic</span><br><span class="line">for p=1:t</span><br><span class="line">    tem=zeros(n,gn-l);</span><br><span class="line">    for k=1:n</span><br><span class="line">        tem(k,:)=(rand(1,gn-l))*(range_x(k,2)-range_x(k,1))+range_x(k,1);</span><br><span class="line">    end</span><br><span class="line">    group=[remember(:,randperm(gn,l)),tem];</span><br><span class="line">    %设置当前最优解</span><br><span class="line">    best_value=zeros(1,times);</span><br><span class="line">    for k=1:times</span><br><span class="line">        y=f(group);</span><br><span class="line">        %全部变成正值</span><br><span class="line">        if min(y)&lt;0</span><br><span class="line">            tem=y-min(y)*1.0001;</span><br><span class="line">        else</span><br><span class="line">            tem=y+0.1;</span><br><span class="line">        end</span><br><span class="line">        %值越小适应越好</span><br><span class="line">        tem=1./tem;</span><br><span class="line">        child=zeros(n,gn);</span><br><span class="line">        %挑选m个种群进入下一代</span><br><span class="line">        for i=1:m</span><br><span class="line">            %轮盘赌选择，适应大的选择概率大</span><br><span class="line">            temp=zeros(1,gn-i+1);</span><br><span class="line">            for j=1:gn-i+1</span><br><span class="line">                temp(j)=sum(tem(1:j));</span><br><span class="line">            end</span><br><span class="line">            temp=temp/temp(gn-i+1);</span><br><span class="line">            %保留最合适的物种</span><br><span class="line">            choose=find(temp&gt;rand(1),1);</span><br><span class="line">            child(:,i)=group(:,choose);</span><br><span class="line">            group=[group(:,1:choose-1),group(:,choose+1:end)];</span><br><span class="line">            tem=[tem(1:choose-1),tem(choose+1:end)];</span><br><span class="line">        end</span><br><span class="line">        %染色体交换,保留的物种产生后代时发生基因重组</span><br><span class="line">        for i=1:floor((gn-m)/2)</span><br><span class="line">            exchange=randperm(m,2);</span><br><span class="line">            a=rand(n,1);</span><br><span class="line">            child(:,i*2-1+m)=a.*child(:,exchange(1))+(1-a).*child(:,exchange(2));</span><br><span class="line">            child(:,i*2+m)=(1-a).*child(:,exchange(1))+a.*child(:,exchange(2));</span><br><span class="line">        end</span><br><span class="line">        if mod(gn-m,2)==1</span><br><span class="line">            exchange=randperm(m,2);</span><br><span class="line">            child(:,gn)=(child(:,exchange(1))+child(:,exchange(2)))/2;</span><br><span class="line">        end</span><br><span class="line">        %基因重组的过程中可能发生染色体变异</span><br><span class="line">        if rand(1)&lt;0.1</span><br><span class="line">            exchange=randperm(gn-m,1);</span><br><span class="line">            a=rand(1);</span><br><span class="line">            for j=1:n</span><br><span class="line">                child(j,exchange+m)=a.*child(j,exchange+m)+(1-a).*(rand(1)*(range(j,2)-range(j,1))+range(j,1));</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        %重组之后后代变成当前种群</span><br><span class="line">        group=child;</span><br><span class="line">        best_value(k)=min(f(group));</span><br><span class="line">        if k&gt;5&amp;&amp;abs(best_value(k)-best_value(k-5))&lt;1e-5</span><br><span class="line">            break;</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    %将本次免疫过程中最好的和记忆细胞比较，选取最好的作为记忆细胞</span><br><span class="line">    if min(f(group))&lt;=min(f(remember))</span><br><span class="line">        [~,index]=min(f(group));</span><br><span class="line">        remember=ones(n,gn).*repmat(group(:,index),1,gn);</span><br><span class="line">    else</span><br><span class="line">        [~,index]=min(f(remember));</span><br><span class="line">        remember=ones(n,gn).*repmat(group(:,index),1,gn);</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">time=toc;</span><br><span class="line">disp(['用时：',num2str(time),'秒'])</span><br><span class="line">[mini,index]=min(f(remember));</span><br><span class="line">disp(['fmin=',num2str(mini)]);</span><br><span class="line">for k=1:n</span><br><span class="line">    disp(['x',num2str(k),'=',num2str(remember(k,index))]);</span><br><span class="line">end</span><br><span class="line">if n==1</span><br><span class="line">    hold on;</span><br><span class="line">    plot(remember(index),mini,'ro');</span><br><span class="line">    plot_x=range_x(1):(range_x(2)-range_x(1))/1000:range_x(2);</span><br><span class="line">    plot_y=f(plot_x);</span><br><span class="line">    plot(plot_x,plot_y);</span><br><span class="line">    text((range_x(1)+range_x(2))/2,max(plot_y)+0.1*(max(plot_y)-min(plot_y)),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br><span class="line">if n==2</span><br><span class="line">    %所求最小值的函数</span><br><span class="line">    func=@(x1,x2)x1.*sin(sqrt(abs(x1)))+x2.*sin(sqrt(abs(x2)));</span><br><span class="line">    plot_x=range_x(1,1):(range_x(1,2)-range_x(1,1))/1000:range_x(1,2);</span><br><span class="line">    plot_y=range_x(2,1):(range_x(2,2)-range_x(2,1))/1000:range_x(2,2);</span><br><span class="line">    [plot_x,plot_y] =meshgrid(plot_x,plot_y);</span><br><span class="line">    plot_z=func(plot_x,plot_y);</span><br><span class="line">    surf(plot_x,plot_y,plot_z);</span><br><span class="line">    xlabel('x1');</span><br><span class="line">    ylabel('x2');</span><br><span class="line">    zlabel('y');</span><br><span class="line">    hold on;</span><br><span class="line">    plot3(remember(1,index),remember(2,index),mini,'ko')</span><br><span class="line">    text((range_x(1,1)+range_x(1,2))/2,(range_x(2,1)+range_x(2,2))/2,max(max(plot_z))+0.5*(max(max(plot_z))-min(min(plot_z))),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="f-m"><a href="#f-m" class="headerlink" title="f.m"></a>f.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">function res=f(x)</span><br><span class="line">func=@(x)(x).*sin(sqrt(abs(x)));</span><br><span class="line">res=zeros(1,size(x,2));</span><br><span class="line">for i=1:size(x,1)</span><br><span class="line">    res=res+func(x(i,:));</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/OPTIMIZATION/ia1.png" alt="IA"></p><script type="math/tex; mode=display">f(x)=x \cdot \sin(\sqrt{\lvert x \rvert}) \ , \ x \in [-500,500]</script><script type="math/tex; mode=display">理论值：f(x)_{min}=f(-420.96874592006)=-418.982887272434</script><script type="math/tex; mode=display">所求值：f(x)_{min}=f(-420.966448106285)=-418.982886605937</script><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>受到参数影响较小</li><li>解决早熟收敛问题</li><li>从群体出发，具有并行性</li><li>对抗体选择的依赖性降低</li><li>可用于求解复杂的非线性优化问题</li><li>使用概率机制进行迭代，具有随机性</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>对问题编码表示较为困难</li><li>要进行多次免疫应答，因此速度慢于遗传算法</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;IA&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="全局搜索方法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%85%A8%E5%B1%80%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>遗传算法(GA)</title>
    <link href="https://USTCcoder.github.io/2019/05/21/optimization_GA/"/>
    <id>https://USTCcoder.github.io/2019/05/21/optimization_GA/</id>
    <published>2019-05-21T09:59:45.000Z</published>
    <updated>2019-09-26T02:43:33.246Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">遗传算法</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  GA(Genetic Algorithm):是模拟达尔文生物进化论的<strong>自然选择</strong>和<strong>遗传学机理</strong>的生物进化过程的计算模型，不需要确定的规则就能自动获取和指导优化的搜索空间，<strong>自适应</strong>地调整搜索方向，是一种通过模拟自然进化过程搜索最优解的方法。<a id="more"></a><br>初代种群产生之后，按照<strong>适者生存</strong>和<strong>优胜劣汰</strong>的原理，逐代（generation）演化产生出越来越好的近似解，在每一代，根据问题域中个体的<strong>适应度（fitness）</strong>大小选择个体，并借助于自然遗传学的<strong>遗传算子</strong>进行组合<strong>交叉（crossover）</strong>和<strong>变异（mutation）</strong>，产生出代表新的解集的种群。</p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 随机产生一些初始种群</font></p><p>  <font size="4">2. 根据适应度对种群采用某种方式进行自然选择</font></p><p>  <font size="4">3. 对选择剩余的种群进行遗传操作，产生新的种群</font></p><p>  <font size="4">4. 回到步骤2，直到满足某个终止条件</font></p><p>  <font size="4">5. 此时剩余的是适应度较好的种群，比较可得该算法的最优解</font></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/OPTIMIZATION/ga2.png" alt="GA"></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用测试函数可以查看相关文档，<a href="https://ustccoder.github.io/2019/05/19/optimization_Testfunction/">测试函数(Test Function)</a></font></p><h2 id="GA-main-m"><a href="#GA-main-m" class="headerlink" title="GA_main.m"></a>GA_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">%自变量取值范围</span><br><span class="line">range_x=[ones(1,1),-ones(1,1)]*500;</span><br><span class="line">%维度</span><br><span class="line">n=size(range_x,1);</span><br><span class="line">%种群数量</span><br><span class="line">gn=400;</span><br><span class="line">%进入下一代的数量</span><br><span class="line">m=50;</span><br><span class="line">%迭代次数</span><br><span class="line">times=200;</span><br><span class="line">%随机产生一些种群</span><br><span class="line">group=zeros(n,gn);</span><br><span class="line">for k=1:n</span><br><span class="line">    group(k,:)=(rand(1,gn))*(range_x(k,2)-range_x(k,1))+range_x(k,1);</span><br><span class="line">end</span><br><span class="line">%设置当前最优解</span><br><span class="line">best_value=zeros(1,times);</span><br><span class="line">tic;</span><br><span class="line">for k=1:times</span><br><span class="line">    y=f(group);</span><br><span class="line">    %全部变成正值</span><br><span class="line">    if min(y)&lt;0</span><br><span class="line">        tem=y-min(y)*1.0001;</span><br><span class="line">    else</span><br><span class="line">        tem=y+0.1;</span><br><span class="line">    end</span><br><span class="line">    %值越小适应越好</span><br><span class="line">    tem=1./tem;</span><br><span class="line">    child=zeros(n,gn);</span><br><span class="line">    %挑选m个种群进入下一代</span><br><span class="line">    for i=1:m</span><br><span class="line">        %轮盘赌选择，适应大的选择概率大</span><br><span class="line">        temp=zeros(1,gn-i+1);</span><br><span class="line">        for j=1:gn-i+1</span><br><span class="line">            temp(j)=sum(tem(1:j));</span><br><span class="line">        end</span><br><span class="line">        temp=temp/temp(gn-i+1);</span><br><span class="line">        %保留最合适的物种</span><br><span class="line">        choose=find(temp&gt;rand(1),1);</span><br><span class="line">        child(:,i)=group(:,choose);</span><br><span class="line">        group=[group(:,1:choose-1),group(:,choose+1:end)];</span><br><span class="line">        tem=[tem(1:choose-1),tem(choose+1:end)];</span><br><span class="line">    end</span><br><span class="line">    %染色体交换,保留的物种产生后代时发生基因重组</span><br><span class="line">    for i=1:floor((gn-m)/2)</span><br><span class="line">        exchange=randperm(m,2);</span><br><span class="line">        a=rand(n,1);</span><br><span class="line">        child(:,i*2-1+m)=a.*child(:,exchange(1))+(1-a).*child(:,exchange(2));</span><br><span class="line">        child(:,i*2+m)=(1-a).*child(:,exchange(1))+a.*child(:,exchange(2));</span><br><span class="line">    end</span><br><span class="line">    if mod(gn-m,2)==1</span><br><span class="line">        exchange=randperm(m,2);</span><br><span class="line">        child(:,gn)=(child(:,exchange(1))+child(:,exchange(2)))/2;</span><br><span class="line">    end</span><br><span class="line">    %基因重组的过程中可能发生染色体变异</span><br><span class="line">    if rand(1)&lt;0.1</span><br><span class="line">        exchange=randperm(gn-m,1);</span><br><span class="line">        a=rand(1);</span><br><span class="line">        for j=1:n</span><br><span class="line">            child(j,exchange+m)=a.*child(j,exchange+m)+(1-a).*(rand(1)*(range(j,2)-range(j,1))+range(j,1));</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    %重组之后后代变成当前种群</span><br><span class="line">    group=child;</span><br><span class="line">    best_value(k)=min(f(group));</span><br><span class="line">    if k&gt;5&amp;&amp;abs(best_value(k)-best_value(k-5))&lt;1e-5</span><br><span class="line">        break;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">time=toc;</span><br><span class="line">disp(['用时：',num2str(time),'秒'])</span><br><span class="line">[mini,index]=min(f(group));</span><br><span class="line">disp(['fmin=',num2str(mini)]);</span><br><span class="line">for k=1:n</span><br><span class="line">    disp(['x',num2str(k),'=',num2str(group(k,index))]);</span><br><span class="line">end</span><br><span class="line">if n==1</span><br><span class="line">    hold on;</span><br><span class="line">    plot(group(index),mini,'ro');</span><br><span class="line">    plot_x=range_x(1):(range_x(2)-range_x(1))/1000:range_x(2);</span><br><span class="line">    plot_y=f(plot_x);</span><br><span class="line">    plot(plot_x,plot_y);</span><br><span class="line">    text((range_x(1)+range_x(2))/2,max(plot_y)+0.1*(max(plot_y)-min(plot_y)),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br><span class="line">if n==2</span><br><span class="line">    %所求最小值的函数</span><br><span class="line">    func=@(x1,x2)x1.*sin(sqrt(abs(x1)))+x2.*sin(sqrt(abs(x2)));</span><br><span class="line">    plot_x=range_x(1,1):(range_x(1,2)-range_x(1,1))/1000:range_x(1,2);</span><br><span class="line">    plot_y=range_x(2,1):(range_x(2,2)-range_x(2,1))/1000:range_x(2,2);</span><br><span class="line">    [plot_x,plot_y] =meshgrid(plot_x,plot_y);</span><br><span class="line">    plot_z=func(plot_x,plot_y);</span><br><span class="line">    surf(plot_x,plot_y,plot_z);</span><br><span class="line">    xlabel('x1');</span><br><span class="line">    ylabel('x2');</span><br><span class="line">    zlabel('y');</span><br><span class="line">    hold on;</span><br><span class="line">    plot3(group(1,index),group(2,index),mini,'ko')</span><br><span class="line">    text((range_x(1,1)+range_x(1,2))/2,(range_x(2,1)+range_x(2,2))/2,max(max(plot_z))+0.5*(max(max(plot_z))-min(min(plot_z))),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="f-m"><a href="#f-m" class="headerlink" title="f.m"></a>f.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">function res=f(x)</span><br><span class="line">func=@(x)(x).*sin(sqrt(abs(x)));</span><br><span class="line">res=zeros(1,size(x,2));</span><br><span class="line">for i=1:size(x,1)</span><br><span class="line">    res=res+func(x(i,:));</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/OPTIMIZATION/ga1.png" alt="GA"></p><script type="math/tex; mode=display">f(x)=x \cdot \sin(\sqrt{\lvert x \rvert}) \ , \ x \in [-500,500]</script><script type="math/tex; mode=display">理论值：f(x)_{min}=f(-420.96874592006)=-418.982887272434</script><script type="math/tex; mode=display">所求值：f(x)_{min}=f(-420.975929624477)=-418.982880761435</script><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>从群体出发，具有并行性</li><li>可用于求解复杂的非线性优化问题</li><li>使用概率机制进行迭代，具有随机性</li><li>具有可扩展性，容易与其他算法结合</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>受到参数影响较大</li><li>可能产生早熟收敛问题</li><li>对问题编码表示较为困难</li><li>算法对初始种群的选择有一定的依赖性</li><li>搜索速度比较慢，要得到较精确的解需要较多的训练时间</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;GA&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="全局搜索方法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%85%A8%E5%B1%80%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>模拟退火算法(SA)</title>
    <link href="https://USTCcoder.github.io/2019/05/20/optimization_SA/"/>
    <id>https://USTCcoder.github.io/2019/05/20/optimization_SA/</id>
    <published>2019-05-20T12:12:20.000Z</published>
    <updated>2019-09-26T03:11:19.533Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">模拟退火算法</font></strong></center><p></p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a><font size="5" color="red">背景介绍</font></h1><p>  SA(Simulate Anneal):是一种基于<strong>Mentcarlo迭代求解法</strong>的一种启发式随机搜索方法，基于物理中固体物质的退火过程与一般组合优化问题之间的相似性，通过<strong>模拟退火过程</strong>，用来在一个大的搜寻空间内找寻命题的最优解（或近似最优解）。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 任选一初始状态S<sub>0</sub>作为当前解，设置初始温度T<sub>0</sub></font></p><p>  <font size="4">2. 对该温度下的状态S<sub>0</sub>产生一个扰动S’，并按概率接收</font></p><script type="math/tex; mode=display">\Delta C=f(S')-f(S)</script><script type="math/tex; mode=display">P= \begin{cases} 1 &  \Delta C \leq 0 \\ e^{\frac {- \Delta C}{T} } & \Delta C > 0 \end{cases}</script><p>  <font size="4">3. 按照某种方式降温T=T-ΔT，回到步骤2，直到满足某个终止条件</font></p><p>  <font size="4">4. 此时达到的状态S即为该算法的最优解</font></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/OPTIMIZATION/sa2.png" alt="SA"></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用测试函数可以查看相关文档，<a href="https://ustccoder.github.io/2019/05/19/optimization_Testfunction/">测试函数(Test Function)</a></font></p><h2 id="SA-ap-m"><a href="#SA-ap-m" class="headerlink" title="SA_ap.m"></a>SA_ap.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">%自变量取值范围</span><br><span class="line">range_x=[ones(1,1),-ones(1,1)]*500;</span><br><span class="line">%维度</span><br><span class="line">n=size(range_x,1);</span><br><span class="line">%尝试解次数</span><br><span class="line">num=10;</span><br><span class="line">value=zeros(n,num);</span><br><span class="line">delta_t=0.2;</span><br><span class="line">tic;</span><br><span class="line">for i=1:num</span><br><span class="line">    %给x赋初值</span><br><span class="line">    x=zeros(n,1);</span><br><span class="line">    for k=1:n</span><br><span class="line">        x(k)=(rand(1))*(range_x(k,2)-range_x(k,1))+range_x(k,1);</span><br><span class="line">    end</span><br><span class="line">    %初始温度t</span><br><span class="line">    t=100;</span><br><span class="line">    while t&gt;1e-5</span><br><span class="line">        x=SA_metripolis(range_x,t,x,n);</span><br><span class="line">        %温度每次下降delta_t</span><br><span class="line">        t=t-delta_t;</span><br><span class="line">    end</span><br><span class="line">    value(:,i)=x;</span><br><span class="line">end</span><br><span class="line">time=toc;</span><br><span class="line">disp(['用时：',num2str(time),'秒'])</span><br><span class="line">[mini,index]=min(f(value));</span><br><span class="line">disp(['fmin=',num2str(mini)]);</span><br><span class="line">for k=1:n</span><br><span class="line">    disp(['x',num2str(k),'=',num2str(value(k,index))]);</span><br><span class="line">end</span><br><span class="line">if n==1</span><br><span class="line">    hold on;</span><br><span class="line">    plot(value(index),mini,'ro');</span><br><span class="line">    plot_x=range_x(1):(range_x(2)-range_x(1))/1000:range_x(2);</span><br><span class="line">    plot_y=f(plot_x);</span><br><span class="line">    plot(plot_x,plot_y);</span><br><span class="line">    text((range_x(1)+range_x(2))/2,max(plot_y)+0.1*(max(plot_y)-min(plot_y)),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br><span class="line">if n==2</span><br><span class="line">    %所求最小值的函数</span><br><span class="line">    func=@(x1,x2)x1.*sin(sqrt(abs(x1)))+x2.*sin(sqrt(abs(x2)));</span><br><span class="line">    plot_x=range_x(1,1):(range_x(1,2)-range_x(1,1))/1000:range_x(1,2);</span><br><span class="line">    plot_y=range_x(2,1):(range_x(2,2)-range_x(2,1))/1000:range_x(2,2);</span><br><span class="line">    [plot_x,plot_y] =meshgrid(plot_x,plot_y);</span><br><span class="line">    plot_z=func(plot_x,plot_y);</span><br><span class="line">    surf(plot_x,plot_y,plot_z);</span><br><span class="line">    xlabel('x1');</span><br><span class="line">    ylabel('x2');</span><br><span class="line">    zlabel('y');</span><br><span class="line">    hold on;</span><br><span class="line">    plot3(value(1,index),value(2,index),mini,'ko')</span><br><span class="line">    text((range_x(1,1)+range_x(1,2))/2,(range_x(2,1)+range_x(2,2))/2,max(max(plot_z))+0.5*(max(max(plot_z))-min(min(plot_z))),['用时：',num2str(time),'秒']);</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="SA-metripolis-m"><a href="#SA-metripolis-m" class="headerlink" title="SA_metripolis.m"></a>SA_metripolis.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">function x=SA_metripolis(range_x,t,x,n)</span><br><span class="line">delta=1;</span><br><span class="line">for i=1:100</span><br><span class="line">    %产生一个随机扰动</span><br><span class="line">    x_new=(rand(n,1)-0.5)*delta+x;</span><br><span class="line">    %限制解的范围</span><br><span class="line">    for j=1:n</span><br><span class="line">        if x_new(j)&lt;range_x(j,2)</span><br><span class="line">            x_new(j)=range_x(j,2);</span><br><span class="line">        end</span><br><span class="line">        if x_new(j)&gt;range_x(j,1)</span><br><span class="line">            x_new(j)=range_x(j,1);</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    dc=f(x_new)-f(x);</span><br><span class="line">    if dc&lt;0</span><br><span class="line">        x=x_new;</span><br><span class="line">        %如果扰动的结果比原来大，则有概率的保留</span><br><span class="line">    else</span><br><span class="line">        p=exp(-dc/t);</span><br><span class="line">        if rand(1)&lt;=p</span><br><span class="line">            x=x_new;</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="f-m"><a href="#f-m" class="headerlink" title="f.m"></a>f.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">function res=f(x)</span><br><span class="line">func=@(x)(x).*sin(sqrt(abs(x)));</span><br><span class="line">res=zeros(1,size(x,2));</span><br><span class="line">for i=1:size(x,1)</span><br><span class="line">    res=res+func(x(i,:));</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/OPTIMIZATION/sa1.png" alt="SA"></p><script type="math/tex; mode=display">f(x)=x \cdot \sin(\sqrt{\lvert x \rvert}) \ , \ x \in [-500,500]</script><script type="math/tex; mode=display">理论值：f(x)_{min}=f(-420.96874592006)=-418.982887272434</script><script type="math/tex; mode=display">所求值：f(x)_{min}=f(-420.967823415805)=-418.982887164947</script><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>计算过程简单</li><li>可用于求解复杂的非线性优化问题</li><li>相比梯度下降，增加了逃离局部最小的可能</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>参数敏感</li><li>收敛速度慢</li><li>执行时间长</li><li>算法性能与初始值有关</li><li>可能落入其他的局部最小值</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;SA&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="全局搜索方法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%85%A8%E5%B1%80%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>测试函数(Test Function)</title>
    <link href="https://USTCcoder.github.io/2019/05/19/optimization_Testfunction/"/>
    <id>https://USTCcoder.github.io/2019/05/19/optimization_Testfunction/</id>
    <published>2019-05-19T11:29:40.000Z</published>
    <updated>2019-08-07T15:55:53.067Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">测试函数说明</font></strong></center><p></p><h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a><font size="5" color="red">原理介绍</font></h1><p>  Test Function:对于全局最优解来说，测试函数的选择是<strong>至关重要</strong>的，测试函数的好坏往往可以体现出<strong>搜索算法的优劣</strong>。有时性能一般的算法在某个特定的函数下发挥的很好，但是在别的函数下就很难搜索到全局最优解。因此我们需要设计各种测试函数，从<strong>搜索效率，搜索精度，适应程度</strong>多个方面综合比较各个算法，只有这样，在今后的使用中才能得心应手。<br><a id="more"></a></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><h2 id="Function-one-m"><a href="#Function-one-m" class="headerlink" title="Function_one.m"></a>Function_one.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">f=@(x)x.*sin(sqrt(abs(x)));</span><br><span class="line">x=-5000:1:5000;</span><br><span class="line">y=f(x);</span><br><span class="line">plot(x,y)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/OPTIMIZATION/test_function1.png" alt="one"></p><script type="math/tex; mode=display">f(x)=x \cdot \sin(\sqrt{\lvert x \rvert}) \ , \ x \in [-500,500]</script><script type="math/tex; mode=display">f(x)_{min}=f(-420.96874592006)=-418.982887272434</script><h2 id="Function-two-m"><a href="#Function-two-m" class="headerlink" title="Function_two.m"></a>Function_two.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">f=@(x)((x+1).*(x+2).*(x+3).*(x+4).*(x+5)+5);</span><br><span class="line">x=-5:0.01:0;</span><br><span class="line">y=f(x);</span><br><span class="line">plot(x,y)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/OPTIMIZATION/test_function2.png" alt="two"></p><script type="math/tex; mode=display">f(x)=x \cdot (x+1) \cdot (x+2) \cdot (x+3) \cdot (x+4) \cdot (x+5) + 5 \ , \ x \in [-5,0]</script><script type="math/tex; mode=display">f(x)_{min}=f(-1.35556713184173)=1.36856779155116</script><h2 id="Function-three-m"><a href="#Function-three-m" class="headerlink" title="Function_three.m"></a>Function_three.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">f=@(x)((x+2).*cos(9*x)+sin(7*x));</span><br><span class="line">x=0:0.01:4;</span><br><span class="line">y=f(x);</span><br><span class="line">plot(x,y)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/OPTIMIZATION/test_function3.png" alt="three"></p><script type="math/tex; mode=display">f(x)=(x+2) \cdot \cos(9 \ x) + \sin(7 \ x) \ , \ x \in [0,4]</script><script type="math/tex; mode=display">f(x)_{min}=f(2.44888001781347)=-5.43427465397202</script><h2 id="Function-four-m"><a href="#Function-four-m" class="headerlink" title="Function_four.m"></a>Function_four.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">f=@(x)(5*exp(-0.5*x).*sin(30*x)+exp(0.2*x).*sin(20*x)+6);</span><br><span class="line">x=0:0.01:8;</span><br><span class="line">y=f(x);</span><br><span class="line">plot(x,y)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/OPTIMIZATION/test_function4.png" alt="four"></p><script type="math/tex; mode=display">f(x)=5 \ e^{-0.5 \ x} \cdot \sin(30 \ x) + e^{0.2 \ x} \cdot \sin(20 \ x) + 6 \ , \ x \in [0,8]</script><script type="math/tex; mode=display">f(x)_{min}=f(0.5725)=1.2573</script><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Test Function&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
      <category term="全局搜索方法" scheme="https://USTCcoder.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%85%A8%E5%B1%80%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>聚类算法比较(Clustering Algorithms Comparison)</title>
    <link href="https://USTCcoder.github.io/2019/05/17/clustering_compare/"/>
    <id>https://USTCcoder.github.io/2019/05/17/clustering_compare/</id>
    <published>2019-05-17T14:20:18.000Z</published>
    <updated>2019-08-07T15:52:12.139Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">聚类算法比较</font></strong></center><p></p><h1 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a><font size="5" color="red">无监督学习</font></h1><p>  现实生活中常常会<strong>缺乏足够的先验知识</strong>，因此<strong>难以人工标注类别或进行人工类别标注的成本太高</strong>。很自然地，我们希望计算机能代我们完成这些工作，或至少提供一些帮助。根据<strong>类别未知(没有被标记)</strong>的训练样本解决模式识别中的各种问题，称之为<strong>无监督学习</strong>。<br><a id="more"></a></p><h1 id="算法分类"><a href="#算法分类" class="headerlink" title="算法分类"></a><font size="5" color="red">算法分类</font></h1><p><img src="/images/MACHINE/compare5.png" alt="COMPARE"></p><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><p><font size="4">所用数据集可以查看相关文档，<a href="https://ustccoder.github.io/2019/04/25/clustering_Dataset/">数据集(Data Set)</a></font></p><h2 id="凝聚的层次聚类-AGNES"><a href="#凝聚的层次聚类-AGNES" class="headerlink" title="凝聚的层次聚类(AGNES)"></a><a href="https://ustccoder.github.io/2019/05/01/clustering_AGNES/">凝聚的层次聚类(AGNES)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>对噪声数据不敏感</li><li>算法简单，容易理解</li><li>不依赖初始值的选择</li><li>对于类别较多的训练集分类较快</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>合并操作不能撤销</li><li>需要在测试前知道类别的个数</li><li>对于类别较少的训练集分类较慢</li><li>只适合分布呈凸型或者球形的数据集</li><li>对于高维数据，距离的度量并不是很好</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/MACHINE/agnes.png" width="200" height="260"><center>高斯型数据</center>&lt;/div&gt;</div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/MACHINE/compare_agnes2.png" width="200" height="260"><center>圆形数据</center></div><div style="float:none;clear:both;"></div><h2 id="分裂的层次聚类-DIANA"><a href="#分裂的层次聚类-DIANA" class="headerlink" title="分裂的层次聚类(DIANA)"></a><a href="https://ustccoder.github.io/2019/05/02/clustering_DIANA/">分裂的层次聚类(DIANA)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>算法简单，容易理解</li><li>不依赖初始值的选择</li><li>对于类别较少的训练集分类较快</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>对噪声数据敏感</li><li>分裂操作不能撤销</li><li>需要在测试前知道类别的个数</li><li>对于类别较多的训练集分类较慢</li><li>只适合分布呈凸型或者球形的数据集</li><li>对于高维数据，距离的度量并不是很好</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/MACHINE/diana.png" width="200" height="260"><center>高斯型数据</center>&lt;/div&gt;</div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/MACHINE/compare_diana4.png" width="200" height="260"><center>混合型数据</center></div><div style="float:none;clear:both;"></div><h2 id="K均值聚类-K-MEANS"><a href="#K均值聚类-K-MEANS" class="headerlink" title="K均值聚类(K-MEANS)"></a><a href="https://ustccoder.github.io/2019/05/03/clustering_KMEANS/">K均值聚类(K-MEANS)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>算法简单，容易理解</li><li>大数据集时，对噪声数据不敏感</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>对初始中心点敏感</li><li>需要在测试前知道类别的个数</li><li>只适合分布呈凸型或者球形的数据集</li><li>对于高维数据，距离的度量并不是很好</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/MACHINE/kmeans.png" width="200" height="260"><center>高斯型数据</center>&lt;/div&gt;</div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/MACHINE/compare_kmeans3.png" width="200" height="260"><center>直线型数据</center></div><div style="float:none;clear:both;"></div><h2 id="迭代自组织分析聚类-ISODATA"><a href="#迭代自组织分析聚类-ISODATA" class="headerlink" title="迭代自组织分析聚类(ISODATA)"></a><a href="https://ustccoder.github.io/2019/05/04/clustering_ISODATA/">迭代自组织分析聚类(ISODATA)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>大数据集时，对噪声数据不敏感</li><li>可以动态调整类别个数和类别中心</li><li>在先验知识不足的情况下有较好的分类能力</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>对初始中心点敏感</li><li>算法复杂，分类速度较慢</li><li>只适合分布呈凸型或者球形的数据集</li><li>对于高维数据，距离的度量并不是很好</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/MACHINE/isodata.png" width="200" height="260"><center>高斯型数据</center>&lt;/div&gt;</div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/MACHINE/compare_isodata3.png" width="200" height="260"><center>直线型数据</center></div><div style="float:none;clear:both;"></div><h2 id="密度聚类-DBSCAN"><a href="#密度聚类-DBSCAN" class="headerlink" title="密度聚类(DBSCAN)"></a><a href="https://ustccoder.github.io/2019/05/05/clustering_DBSCAN/">密度聚类(DBSCAN)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>算法简单，容易理解</li><li>不依赖初始数据点的选择</li><li>可以完成任意形状的聚类</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>对噪声数据敏感</li><li>需要在测试前确定eps和minPts</li><li>不适合数据集中密度差异较大的情况</li><li>对于高维数据，距离的度量并不是很好</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/MACHINE/dbscan.png" width="200" height="260"><center>圆形数据</center>&lt;/div&gt;</div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/MACHINE/compare_dbscan3.png" width="200" height="260"><center>直线型数据</center></div><div style="float:none;clear:both;"></div><h2 id="密度最大值聚类-MDCA"><a href="#密度最大值聚类-MDCA" class="headerlink" title="密度最大值聚类(MDCA)"></a><a href="https://ustccoder.github.io/2019/05/07/clustering_MDCA/">密度最大值聚类(MDCA)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>对噪声数据不敏感</li><li>不依赖初始数据点的选择</li><li>可以完成任意形状的聚类</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>算法复杂，分类速度较慢</li><li>需要在测试前确定密度阈值</li><li>对于高维数据，距离的度量并不是很好</li><li>不适合数据集密度差异较大或整体密度基本相同的情况</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/MACHINE/mdca.png" width="200" height="260"><center>高斯型数据</center>&lt;/div&gt;</div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/MACHINE/compare_mdca4.png" width="200" height="260"><center>混合型数据</center></div><div style="float:none;clear:both;"></div><h2 id="快速搜索聚类-CFDP"><a href="#快速搜索聚类-CFDP" class="headerlink" title="快速搜索聚类(CFDP)"></a><a href="https://ustccoder.github.io/2019/05/09/clustering_CFDP/">快速搜索聚类(CFDP)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>对噪声数据不敏感</li><li>不依赖初始数据点的选择</li><li>可以完成任意形状的聚类</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>离群点的确定非常复杂</li><li>算法复杂，分类速度较慢</li><li>对于高维数据，距离的度量并不是很好</li><li>不适合数据集整体密度基本相同的情况</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/MACHINE/compare_cfdp1.png" width="200" height="260"><center>高斯型数据</center>&lt;/div&gt;</div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/MACHINE/compare_cfdp4.png" width="200" height="260"><center>混合型数据</center></div><div style="float:none;clear:both;"></div><h2 id="谱聚类-Spectral-Clustering"><a href="#谱聚类-Spectral-Clustering" class="headerlink" title="谱聚类(Spectral Clustering)"></a><a href="https://ustccoder.github.io/2019/05/11/clustering_SPECTRAL/">谱聚类(Spectral Clustering)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>不依赖初始数据点的选择</li><li>使用了降维技术，适合于高维数据的聚类</li><li>建立在谱图理论，能在大部分形状聚类，收敛于全局最优解</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>难以对圆形数据聚类</li><li>对噪声数据非常敏感</li><li>需要在测试前知道类别的个数</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/MACHINE/compare_spectral3.png" width="200" height="260"><center>直线型数据</center>&lt;/div&gt;</div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/MACHINE/spectral.png" width="200" height="260"><center>混合型数据</center></div><div style="float:none;clear:both;"></div><h2 id="高斯混合模型聚类-GMM"><a href="#高斯混合模型聚类-GMM" class="headerlink" title="高斯混合模型聚类(GMM)"></a><a href="https://ustccoder.github.io/2019/05/12/clustering_GMM/">高斯混合模型聚类(GMM)</a></h2><ul><li><font size="4"><strong>优点：</strong><ul><li>可以完成大部分形状的聚类</li><li>大数据集时，对噪声数据不敏感</li><li>对于距离或密度聚类，更适合高维特征</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>计算复杂，速度较慢</li><li>难以对圆形数据聚类</li><li>需要在测试前知道类别的个数</li><li>初始化参数会对聚类结果产生影响</li></ul></font></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/MACHINE/compare_gmm1.png" width="200" height="260"><center>高斯型数据</center>&lt;/div&gt;</div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/MACHINE/compare_gmm3.png" width="200" height="260"><center>直线型数据</center></div><div style="float:none;clear:both;"></div><h1 id="特点小结"><a href="#特点小结" class="headerlink" title="特点小结"></a><font size="5" color="red">特点小结</font></h1><ul><li>凸型或者球形分布的数据集，绝大部分算法都是可以适用的</li><li>圆形分布的数据集，DBSCAN算法最为合适</li><li>线型分布的数据集，DBSCAN，Spectral Clustering，GMM都可以使用</li><li>高维特征最好使用Spectral Clustering或者GMM算法</li><li>密度算法大多适用于各类的密度峰值相差不大的情况</li><li>实际中可以通过已知的某些先验知识尝试去选择合适的算法</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Clustering Algorithms Comparison&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>高斯混合模型聚类(GMM)</title>
    <link href="https://USTCcoder.github.io/2019/05/12/clustering_GMM/"/>
    <id>https://USTCcoder.github.io/2019/05/12/clustering_GMM/</id>
    <published>2019-05-12T12:05:27.000Z</published>
    <updated>2019-08-07T15:53:07.009Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">高斯混合模型聚类方法</font></strong></center><p></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  GMM(Gaussian Mixture Model,):是一个将事物分解为若干的基于<strong>高斯概率密度函数（正态分布曲线）</strong>形成的模型，混合高斯分布( MoG )由多个混合成分组成，每一个混合成分对应一个高斯分布。当聚类问题中<strong>各个类别的尺寸不同</strong>、聚类间<strong>有相关关系</strong>的时候，往往使用 MoG 更合适。<a id="more"></a>对一个样本来说， MoG 得到的是<strong>其属于各个类的概率</strong>(通过计算后验概率得到)，而不是完全的属于某个类，这种聚类方法被成为<strong>软聚类</strong>。一般说来， <strong>任意形状</strong>的概率分布都可以用多个高斯分布函数去近似，因而，MoG 的应用也比较广泛。</p><h1 id="步骤分析"><a href="#步骤分析" class="headerlink" title="步骤分析"></a><font size="5" color="red">步骤分析</font></h1><p>  <font size="4">1. 选择高斯模型个数K，初始化参数</font></p><p>  <font size="4">2. 根据贝叶斯定理，求出z<sub>j</sub>的后验分布概率</font></p><script type="math/tex; mode=display">p(z_j=i | x_j) = \frac{\alpha_i \cdot p(x_j | \mu_i , \Sigma_i)}{\displaystyle \sum_{l=1}^k \alpha_l \cdot p(x_j | \mu_l , \Sigma_l)}</script><p>  <font size="4">3. 使用EM算法进行迭代</font></p><ul><li><p>计算均值向量：</p><script type="math/tex; mode=display">\mu_i '=\frac{\displaystyle \sum_{j=1}^m p(z_j=i | x_j) \cdot x_j}{\displaystyle \sum_{j=1}^m p(z_j=i | x_j)}</script></li><li><p>计算协方差矩阵：</p><script type="math/tex; mode=display">\Sigma_i '=\frac{\displaystyle \sum_{j=1}^m p(z_j=i | x_j) \ (x_j - \mu_i ') \ (x_j - \mu_i ')^T}{\displaystyle \sum_{j=1}^m p(z_j=i | x_j)}</script></li><li><p>计算混合系数：</p><script type="math/tex; mode=display">\alpha_i '=\frac{\displaystyle \sum_{j=1}^m p(z_j=i | x_j)}{m}</script></li></ul><p>  <font size="4">4. 重复步骤1，2，直到满足某个终止条件</font></p><p>  <font size="4">5. 定义高斯混合分布</font><br>  根据所求得的均值向量，协方差矩阵和混合系数可以定义如下函数：</p><script type="math/tex; mode=display">p(x)=\displaystyle \sum_{l=1}^k \alpha_i \cdot p(x | \mu_i , \Sigma_i) \ , \ s.t. \displaystyle \sum_{l=1}^k \alpha_i=1</script><p><img src="/images/MACHINE/gmm5.png" alt="GMM"></p><p>  <font size="4">6. 对样本进行标记</font></p><script type="math/tex; mode=display">\lambda_j=\underset{i \in \{ 1,2, \cdots ,k\}}{arg\ max} \ p(z_j=i | x_j)</script><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/MACHINE/gmm9.png" alt="GMM"></p><p><br><br></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用数据集可以查看相关文档，<a href="https://ustccoder.github.io/2019/04/25/clustering_Dataset/">数据集(Data Set)</a></font></p><h2 id="GMM-main-m"><a href="#GMM-main-m" class="headerlink" title="GMM_main.m"></a>GMM_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">load('..\\cluster_mixture.mat');</span><br><span class="line">%输入x的矩阵</span><br><span class="line">x=data;</span><br><span class="line">%样本数</span><br><span class="line">sample_num=size(x,2);</span><br><span class="line">%混合高斯个数</span><br><span class="line">class_num=3;</span><br><span class="line">%特征数目</span><br><span class="line">feat_num=size(x,1);</span><br><span class="line">%尺度缩放到0-1</span><br><span class="line">x_scale=zeros(size(x));</span><br><span class="line">for i=1:feat_num</span><br><span class="line">    x_scale(i,:)=(x(i,:)-min(x(i,:)))/(max(x(i,:))-min(x(i,:)));</span><br><span class="line">end</span><br><span class="line">y=GMM_classify(x_scale,sample_num,class_num,feat_num);</span><br><span class="line">% 如果数据的特征是二维的，可以绘图表示</span><br><span class="line">if feat_num==2</span><br><span class="line">    GMM_display(x,y,sample_num,class_num);</span><br><span class="line">else</span><br><span class="line">    disp('The Feature Is Not Two-Dimensional');</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="GMM-classify-m"><a href="#GMM-classify-m" class="headerlink" title="GMM_classify.m"></a>GMM_classify.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">function y=GMM_classify(x_scale,sample_num,class_num,feat_num)</span><br><span class="line">%初始化均值向量,协方差矩阵，混合系数</span><br><span class="line">a=ones(1,class_num)/class_num;</span><br><span class="line">u=zeros(feat_num,class_num);</span><br><span class="line">sigma=zeros(feat_num,feat_num,class_num);</span><br><span class="line">randIndex = randperm(size(x_scale,2));</span><br><span class="line">u(:,1:class_num)=x_scale(:,randIndex(1:class_num));</span><br><span class="line">for i=1:class_num</span><br><span class="line">    sigma(:,:,i)=eye(feat_num)/10;</span><br><span class="line">end</span><br><span class="line">for t=1:50</span><br><span class="line">    %判断sigma是否正定</span><br><span class="line">    if sum(sum(sum(isnan(sigma))))&gt;0||sum(sum(sum(isinf(sigma))))&gt;0</span><br><span class="line">        break;</span><br><span class="line">    end</span><br><span class="line">    pm_x=zeros(1,sample_num);</span><br><span class="line">    %计算每个样本的全概率</span><br><span class="line">    for i=1:sample_num</span><br><span class="line">        tem=0;</span><br><span class="line">        for j=1:class_num</span><br><span class="line">            tem=tem+a(j)*mvnpdf(x_scale(:,i), u(:,j), sigma(:,:,j));</span><br><span class="line">        end</span><br><span class="line">        pm_x(i)=tem;</span><br><span class="line">    end</span><br><span class="line">    %计算第i个样本属于第j类的后验概率</span><br><span class="line">    pm=zeros(sample_num,class_num);</span><br><span class="line">    for i=1:sample_num</span><br><span class="line">        for j=1:class_num</span><br><span class="line">            pm(i,j)=a(j)*mvnpdf(x_scale(:,i), u(:,j), sigma(:,:,j))/pm_x(i);</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    %计算均值向量,协方差矩阵，混合系数</span><br><span class="line">    for i=1:class_num</span><br><span class="line">        sum_pm=sum(pm(:,i));</span><br><span class="line">        u(:,i)=sum(repmat(pm(:,i)',feat_num,1).*x_scale,2)/sum_pm;</span><br><span class="line">        for j=1:sample_num</span><br><span class="line">            sigma(:,:,i)=sigma(:,:,i)+pm(j,i)*(x_scale(:,j)-u(:,i))*(x_scale(:,j)-u(:,i))';</span><br><span class="line">        end</span><br><span class="line">        sigma(:,:,i)=sigma(:,:,i)/sum_pm;</span><br><span class="line">        a(i)=sum_pm/sample_num;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">%把每个样本中混合系数最大的一个类别作为其标签</span><br><span class="line">[~,y]=max(pm,[],2);</span><br></pre></td></tr></tbody></table></figure><h2 id="GMM-display-m"><a href="#GMM-display-m" class="headerlink" title="GMM_display.m"></a>GMM_display.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">function GMM_display(x,y,sample_num,class_num)</span><br><span class="line">figure;</span><br><span class="line">hold on;</span><br><span class="line">color_bar=zeros(class_num,3);</span><br><span class="line">for i=1:class_num</span><br><span class="line">    color_bar(i,:)=[rand(1),rand(1),rand(1)];</span><br><span class="line">end</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    if y(i)==0</span><br><span class="line">        %画出噪声点，用*表示</span><br><span class="line">        plot(x(1,i),x(2,i),'k*')</span><br><span class="line">    else</span><br><span class="line">        %画出每一类的样本数据，用o表示</span><br><span class="line">        plot(x(1,i),x(2,i),'color',color_bar(y(i),:),'marker','o');</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/gmm.png" alt="GMM"></p><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>可以完成大部分形状的聚类</li><li>大数据集时，对噪声数据不敏感</li><li>对于距离或密度聚类，更适合高维特征</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>计算复杂，速度较慢</li><li>难以对圆形数据聚类</li><li>需要在测试前知道类别的个数</li><li>初始化参数会对聚类结果产生影响</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;GMM&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>谱聚类(Spectral Clustering)</title>
    <link href="https://USTCcoder.github.io/2019/05/11/clustering_SPECTRAL/"/>
    <id>https://USTCcoder.github.io/2019/05/11/clustering_SPECTRAL/</id>
    <published>2019-05-11T10:52:21.000Z</published>
    <updated>2020-05-09T10:20:44.771Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">谱聚类方法</font></strong></center><p></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  Spectral Clustering:是一种基于<strong>图论</strong>的聚类算法，第一步是<strong>构图</strong>：将数据集中的每个对象看做空间中的点V，将这些点之用边E连接起来，距离较远的两个点之间的边权重值较低、距离较近的两个点之间的边权重值较高，这样就构成了一个<strong>基于相似度的无向权重图G(V,E)</strong>。第二步是<strong>切图</strong>：按照一定的切边规则将图切分为不同的子图，规则是使子图内的边权重和尽可能大，不同子图间的边权重和尽可能小，从而达到聚类目的。</p><a id="more"></a><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 邻接矩阵W的构建</font></p><script type="math/tex; mode=display">w_{ij}=\begin{cases}exp(-\frac{\lVert x_i-x_j \rVert ^2}{2 \sigma ^2})  & i \neq j \\\\[2ex] 0 & i=j \end{cases}</script><p>  <font size="4">2. 度矩阵W的构建</font></p><script type="math/tex; mode=display">d_{ij}=\begin{cases}0  & i \neq j \\\\[2ex] \displaystyle \sum_{j=1} w_{ij} & i=j \end{cases}</script><p>  <font size="4">3. 目标函数</font></p><ul><li><p>权重切图</p><script type="math/tex; mode=display">W(A,B)=\displaystyle \sum_{i \in A, j \in B} w_{ij}</script></li><li><p>Ncut切图<br>\begin{align}<br>Ncut(A<em>1,A_2, \cdots ,A_n) &amp;=\displaystyle \sum</em>{i=1}^n \frac{W(A<em>i, \overline {A_i})}{\displaystyle \sum</em>{j \in A<em>i} \displaystyle \sum</em>{k=1} w_{jk}} \\<br>&amp;=\underset{F}{\underbrace{arg\ min}}\ {tr(F^TD^{-1/2}LD^{-1/2}F)} \ , \ s.t. \ H^TDH=\mathrm{I}<br>\end{align}</p></li></ul><p>  <font size="4">4. 求标准化拉普拉斯矩阵</font></p><script type="math/tex; mode=display">L_{sym}=D^{-1/2}LD^{-1/2}=D^{-1/2}(D-W)D^{-1/2}</script><p>  <font size="4">5. 取前K个特征值对应的特征向量</font></p><p>  <font size="4">6. 用K-Means对归一化的特征向量进行分类</font></p><p>  <font size="4">7. 一个便于理解的实例</font><br><img src="/images/MACHINE/spectral2.png" alt="SPECTRAL"></p><script type="math/tex; mode=display">x=\begin{bmatrix} 0.7 & 0.8 & 0.1 & 0.4 & 0.2 & 0.5 & 0.6 \\\\ 0.5 & 0.6 & 0.1 & 0.8 & 0.2 & 0.8 & 0.7 \end{bmatrix}</script><p><br><br></p><script type="math/tex; mode=display">w=\begin{bmatrix} 0 & 0.990 & 0.771 & 0.914 & 0.844 & 0.937 & 0.975 \\\\ 0.990 & 0 & 0.691 & 0.905 & 0.771 & 0.937 & 0.975 \\\\ 0.771 & 0.691 & 0 & 0.748 & 0.990 & 0.723 & 0.737 \\\\ 0.914 & 0.905 & 0.748 & 0 & 0.819 & 0.995 & 0.975 \\\\ 0.844 & 0.771 & 0.990 & 0.819 & 0 & 0.799 & 0.815 \\\\ 0.937 & 0.937 & 0.723 & 0.995 & 0.799 & 0 & 0.990 \\\\ 0.975 & 0.975 & 0.737 & 0.975 & 0.815 & 0.990 & 0 \end{bmatrix}</script><p><br><br></p><script type="math/tex; mode=display">d=\begin{bmatrix} 5.43 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 5.27 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 4.66 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 5.36 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 5.04 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 5.38 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 5.47 \end{bmatrix}</script><p><br><br></p><script type="math/tex; mode=display">L_{sym}=\begin{bmatrix} 1 & -0.185 & -0.153 & -0.169 & -0.161 & -0.173 & -0.179 \\\\ -0.185 & 1 & -0.139 & -0.170 & -0.150 & -0.176 & -0.182 \\\\ -0.153 & -0.139 & 1 & -0.150 & -0.204 & -0.144 & -0.146 \\\\ -0.169 & -0.170 & -0.150 & 1 & -0.158 & -0.185 & -0.180 \\\\ -0.161 & -0.150 & -0.204 & -0.158 & 1 & -0.153 & -0.155 \\\\ -0.173 & -0.176 & -0.144 & -0.185 & -0.153 & 1 & -0.183 \\\\ -0.179 & -0.182 & -0.146 & -0.180 & -0.155 & -0.183 & 1 \end{bmatrix}</script><p><br><br></p><script type="math/tex; mode=display">feat\_value=\begin{bmatrix} 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1.080 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1.159 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1.205 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1.183 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 1.187 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1.186 \end{bmatrix}</script><p><br><br></p><script type="math/tex; mode=display">feat\_vector=\begin{bmatrix} -0.385 & -0.141 & 0.535 & -0.004 & 0.371 & 0.534 & -0.349 \\\\  -0.379 & -0.302 & 0.470 & -0.040 & 0.042 & -0.691 & 0.249 \\\\  -0.357 & 0.642 & -0.005 & -0.675 & -0.031 & -0.033 & 0.051 \\\\  -0.383 & -0.178 & -0.575 & -0.004 & 0.246 & -0.299 & -0.584 \\\\  -0.371 & 0.558 & 0.010 & 0.736 & -0.036 & -0.053 & 0.070 \\\\  -0.383 & -0.261 & -0.400 & -0.019 & 0.264 & 0.314 & 0.676 \\\\  -0.387 & -0.256 & -0.034 & -0.017 & -0.853 & 0.213 & -0.104 \end{bmatrix}</script><p>  <font size="4">取最小的两个特征值对应的特征向量可得：</font></p><script type="math/tex; mode=display">feature\_vector=\begin{bmatrix} -0.385 & -0.141 \\\\  -0.379 & -0.302 \\\\  -0.357 & 0.642 \\\\  -0.383 & -0.178 \\\\  -0.371 & 0.558 \\\\  -0.383 & -0.261 \\\\  -0.387 & -0.256 \end{bmatrix}</script><p>  <font size="4">特征向量按行归一化：</font></p><script type="math/tex; mode=display">feature\_vector=\begin{bmatrix} -0.939 & -0.344 \\\\ -0.782 & -0.623 \\\\ -0.486 & 0.874 \\\\ -0.907 & -0.422 \\\\ -0.553 & 0.833 \\\\ -0.827 & -0.563 \\\\ -0.834 & -0.552 \end{bmatrix}</script><p>  <font size="4">可以明显看出：${x_1,x_2,x_4,x_6,x_7} \in A_1\ ,\ {x_3,x_5} \in A_2$</font><br><img src="/images/MACHINE/spectral3.png" alt="SPECTRAL"></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/MACHINE/spectral9.png" alt="SPECTRAL"></p><p><br><br></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用数据集可以查看相关文档，<a href="https://ustccoder.github.io/2019/04/25/clustering_Dataset/">数据集(Data Set)</a></font></p><h2 id="SPECTRAL-main-m"><a href="#SPECTRAL-main-m" class="headerlink" title="SPECTRAL_main.m"></a>SPECTRAL_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">load('..\\\\cluster_mixture.mat');</span><br><span class="line">%输入x的矩阵</span><br><span class="line">x=data;</span><br><span class="line">randIndex = randperm(size(x,2));</span><br><span class="line">x=x(:,randIndex);</span><br><span class="line">%希望划分的类别数</span><br><span class="line">class_num=3;</span><br><span class="line">%样本数</span><br><span class="line">sample_num=size(x,2);</span><br><span class="line">%特征数目</span><br><span class="line">feat_num=size(x,1);</span><br><span class="line">%尺度缩放到0-1</span><br><span class="line">x_scale=zeros(size(x));</span><br><span class="line">for i=1:feat_num</span><br><span class="line">    x_scale(i,:)=(x(i,:)-min(x(i,:)))/(max(x(i,:))-min(x(i,:)));</span><br><span class="line">end</span><br><span class="line">y=SPECTRAL_classify(x_scale,sample_num,class_num);</span><br><span class="line">%如果数据的特征是二维的，可以绘图表示</span><br><span class="line">if feat_num==2</span><br><span class="line">    SPECTRAL_display(x,y,sample_num,class_num);</span><br><span class="line">else</span><br><span class="line">    disp('The Feature Is Not Two-Dimensional');</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="SPECTRAL-classify-m"><a href="#SPECTRAL-classify-m" class="headerlink" title="SPECTRAL_classify.m"></a>SPECTRAL_classify.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">function y=SPECTRAL_classify(x_scale,sample_num,class_num)</span><br><span class="line">%w为邻接矩阵</span><br><span class="line">w=zeros(sample_num);</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    w(i,:)=exp(-sum((x_scale-repmat(x_scale(:,i),1,sample_num)).^2)/2);</span><br><span class="line">    w(i,i)=0;</span><br><span class="line">end</span><br><span class="line">%D为度矩阵</span><br><span class="line">d=diag(sum(w,2));</span><br><span class="line">%标准化拉普拉斯矩阵</span><br><span class="line">l=d^(-0.5)*(d-w)*d^(-0.5);</span><br><span class="line">%求特征向量和特征值</span><br><span class="line">[feat_vector,feat_value_temp]=eig(l);</span><br><span class="line">feat_value=diag(feat_value_temp);</span><br><span class="line">temp=sort(feat_value);</span><br><span class="line">loc=feat_value&lt;=temp(class_num);</span><br><span class="line">%求出最小的class_num个特征向量</span><br><span class="line">class_feat_vector=feat_vector(:,loc);</span><br><span class="line">%特征向量归一化</span><br><span class="line">class_feat_vector=class_feat_vector./repmat(sqrt(sum(class_feat_vector.^2,2)),1,class_num);</span><br><span class="line">%利用kmeans进行分类</span><br><span class="line">y=SPECTRAL_kmeans(class_feat_vector',sample_num,class_num);</span><br></pre></td></tr></tbody></table></figure><h2 id="SPECTRAL-display-m"><a href="#SPECTRAL-display-m" class="headerlink" title="SPECTRAL_display.m"></a>SPECTRAL_display.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">function SPECTRAL_display(x,y,sample_num,class_num)</span><br><span class="line">color_bar=zeros(class_num,3);</span><br><span class="line">hold on;</span><br><span class="line">for i=1:class_num</span><br><span class="line">    color_bar(i,:)=[rand(1),rand(1),rand(1)];</span><br><span class="line">end</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    %绘制数据集，用o表示</span><br><span class="line">    plot(x(1,i),x(2,i),'color',color_bar(y(i),:),'marker','o');</span><br><span class="line">end</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><h2 id="SPECTRAL-kmeans-m"><a href="#SPECTRAL-kmeans-m" class="headerlink" title="SPECTRAL_kmeans.m"></a>SPECTRAL_kmeans.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">function y=SPECTRAL_kmeans(class_feat_vector,sample_num,class_num)</span><br><span class="line">%类别中心位置</span><br><span class="line">loc_center=class_feat_vector(:,1:class_num);</span><br><span class="line">%设置迭代次数</span><br><span class="line">k=0;</span><br><span class="line">while 1</span><br><span class="line">    %初始化最新的分类中心</span><br><span class="line">    loc_center_new=zeros(size(loc_center));</span><br><span class="line">    distance=zeros(class_num,sample_num);</span><br><span class="line">    %distance为每一个样本到每一类的距离</span><br><span class="line">    for i=1:class_num</span><br><span class="line">        distance(i,:)=sum((class_feat_vector-repmat(loc_center(:,i),1,sample_num)).^2);</span><br><span class="line">    end</span><br><span class="line">    %求出每个样本到哪一类最近</span><br><span class="line">    [~,y]=min(distance);</span><br><span class="line">    %更新分类中心</span><br><span class="line">    for i=1:class_num</span><br><span class="line">        loc_center_new(:,i)=sum(class_feat_vector(:,y==i),2)/sum(y==i);</span><br><span class="line">    end</span><br><span class="line">    %如果分类中心和上一次分类中心相等则分类完毕</span><br><span class="line">    if isequal(loc_center_new,loc_center)</span><br><span class="line">        break;</span><br><span class="line">    %否则继续分类</span><br><span class="line">    else</span><br><span class="line">        loc_center=loc_center_new;</span><br><span class="line">        k=k+1;</span><br><span class="line">        %如果分类次数达到1000仍然没有结束，则强制分类结束</span><br><span class="line">        if k&gt;=1000</span><br><span class="line">            break;</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/spectral.png" alt="SPECTRAL"></p><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>不依赖初始数据点的选择</li><li>使用了降维技术，适合于高维数据的聚类</li><li>建立在谱图理论，能在大部分形状聚类，收敛于全局最优解</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>难以对圆形数据聚类</li><li>对噪声数据非常敏感</li><li>需要在测试前知道类别的个数</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Spectral Clustering&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>快速搜索聚类(CFDP)</title>
    <link href="https://USTCcoder.github.io/2019/05/09/clustering_CFDP/"/>
    <id>https://USTCcoder.github.io/2019/05/09/clustering_CFDP/</id>
    <published>2019-05-09T12:09:57.000Z</published>
    <updated>2019-08-07T15:51:56.081Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">快速搜索聚类方法</font></strong></center><p></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  CFDP(Clustering By Fast Search And Find Of Density Peaksd):经典的聚类算法K-means不能检测非球面类别的数据分布，DBSCAN必须指定一个密度阈值，CFDP通过对两种方法的改善，选择每个区域密度最大值，根据密度选择周围点的归属。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 求出每个点的密度ρ<sub>i</sub>(多种定义方法)</font></p><ul><li>k近邻均值倒数<script type="math/tex; mode=display">\rho_i = \frac{k}{\displaystyle \sum_{j=1}^k d_{ij}} \ , \ d_{i1} \leq d_{i2} \leq \cdots \leq d_{in}</script></li><li>高斯核相似度<script type="math/tex; mode=display">\rho_i = \underset{d_{ij} \leq d_c}{\sum}exp[-(\frac{d_{ij}}{d_c})^2]</script></li><li>周围点的个数<script type="math/tex; mode=display">\rho_i = \sum_{j=1}^n {\chi(d_{ij}-d_c)} \ , \ \chi(x)= \begin{cases} 1, & x<0 \\[2ex] 0, & otherwise \end{cases} \ , \ 其中d_c为截断距离</script></li></ul><p>  <font size="4">2. 密度从大到小排序，并求出最大密度ρ<sub>max</sub></font></p><script type="math/tex; mode=display">\rho_{x_1} \ge \rho_{x_2} \ge \cdots \ge \rho_{x_n} \ , \ \rho_{max} = \rho_{x_1}</script><ul><li>d<sub>ij</sub>:原序列i，j样本的距离</li><li>d(x<sub>i</sub>,x<sub>j</sub>):密度排序后，x<sub>i</sub>和x<sub>j</sub>样本的距离</li></ul><p>  <font size="4">3. 求出每个点的距离δ<sub>i</sub></font><br>  δ<sub>i</sub>：到密度大于i的最近点j的距离dist(ij)</p><script type="math/tex; mode=display">\delta_{x_i} = \begin{cases} \underset{j<i}{min}\ d(x_i,x_j) &  i \ge 2  \\[2ex] \underset{j \ge 2}{\max}\ \delta_{x_j} & i=1 \end{cases}</script><p>  <font size="4">4. 画出ρ-δ图，找到离群点(代表每一类的中心)</font><br><img src="/images/MACHINE/cfdp1.png" alt="CFDP"></p><p>  <font size="4">5. 按密度从大到小归属于距离最近点的类别</font></p><script type="math/tex; mode=display">x_i \in C_k \ , \ 其中k=\underset{j<i \ , \ x_j \in C_k}{arg \ min}\ d(x_i,x_j)</script><p>  <font size="4">6. 定义两类之间的最小距离d<sub>0</sub></font><br>  两类的最小距离：所有样本之间距离从小到大排序后第2%个称两类的最小距离。</p><p>  <font size="4">7. 定义边缘点，求出边缘点最大密度ρ<sub>b</sub></font><br>  边缘点：在k类数据到非k类数据的最小距离小于dist<sub>0</sub>的点，称为k类数据的边缘点。</p><script type="math/tex; mode=display">E = \{i | d_{ij}<dist_0 , \forall i \in C_k , j \in \overline{C_k} \}</script><script type="math/tex; mode=display">\rho_b = \underset{i \in E}{\max}\ \rho_i</script><p>  <font size="4">8. 判断噪声点</font><br>  噪声点：将k类中密度小于ρ<sub>b</sub>的所有数据记为噪声。</p><script type="math/tex; mode=display">N=\{i | \rho_i<\rho_b , \forall i \in C_k  \}</script><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/MACHINE/cfdp9.png" alt="CFDP"></p><p><br><br></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用数据集可以查看相关文档，<a href="https://ustccoder.github.io/2019/04/25/clustering_Dataset/">数据集(Data Set)</a></font></p><h2 id="CFDP-main-m"><a href="#CFDP-main-m" class="headerlink" title="CFDP_main.m"></a>CFDP_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">load('..\\cluster_line.mat');</span><br><span class="line">%输入x的矩阵</span><br><span class="line">x=data;</span><br><span class="line">randIndex = randperm(size(x,2));</span><br><span class="line">x=x(:,randIndex);</span><br><span class="line">%样本数</span><br><span class="line">sample_num=size(x,2);</span><br><span class="line">%判断密度时检测周围点的个数</span><br><span class="line">k=round(sample_num/10);</span><br><span class="line">%特征数目</span><br><span class="line">feat_num=size(x,1);</span><br><span class="line">%尺度缩放到0-1</span><br><span class="line">x_scale=zeros(size(x));</span><br><span class="line">for i=1:feat_num</span><br><span class="line">    x_scale(i,:)=(x(i,:)-min(x(i,:)))/(max(x(i,:))-min(x(i,:)));</span><br><span class="line">end</span><br><span class="line">[y,density_max]=CFDP_classify(x_scale,sample_num,k);</span><br><span class="line">% 如果数据的特征是二维的，可以绘图表示</span><br><span class="line">if feat_num==2</span><br><span class="line">    CFDP_display(x,y,sample_num,density_max)</span><br><span class="line">else</span><br><span class="line">    disp('The Feature Is Not Two-Dimensional');</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="CFDP-classify-m"><a href="#CFDP-classify-m" class="headerlink" title="CFDP_classify.m"></a>CFDP_classify.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">function [y,density_max]=CFDP_classify(x_scale,sample_num,k)</span><br><span class="line">y=zeros(1,sample_num);</span><br><span class="line">%p为每个样本的密度</span><br><span class="line">p=zeros(1,sample_num);</span><br><span class="line">%deta为高密度距离</span><br><span class="line">deta=zeros(1,sample_num);</span><br><span class="line">%distance(i,j)代表第i个样本到第j个样本的距离</span><br><span class="line">distance=zeros(sample_num);</span><br><span class="line">%利用k近邻均值定义密度较好，不会出现很多密度相同的点。如果采用半径内个数的定义方法，可能一块区域会出现很多的类别</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    distance(i,:)=sum((x_scale-repmat(x_scale(:,i),1,sample_num)).^2);</span><br><span class="line">    temp=sort(distance(i,:));</span><br><span class="line">    p(i)=k./sum(distance(i,distance(i,:)&lt;=temp(k)));</span><br><span class="line">end</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    temp=find(p&gt;p(i));</span><br><span class="line">    %如果有多个最大点</span><br><span class="line">    if isempty(temp)</span><br><span class="line">        deta(i)=distance(i,find(p==max(p),1));</span><br><span class="line">    else</span><br><span class="line">        %找到密度大于该点的且距离该点最近的点</span><br><span class="line">        tem=find(distance(i,p&gt;p(i))==min(distance(i,p&gt;p(i))),1);</span><br><span class="line">        deta(i)=distance(i,temp(tem));</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">%密度最大点赋值无穷   </span><br><span class="line">deta(find(p==max(p),1))=1;</span><br><span class="line">p_judge=max(p)/5;</span><br><span class="line">deta_judge=0.1;</span><br><span class="line">%获取类别的中心</span><br><span class="line">center_loc=p&gt;p_judge&amp;deta&gt;deta_judge;</span><br><span class="line">%找到类别中心所在位置</span><br><span class="line">density_max=find(center_loc);</span><br><span class="line">%将p和deta较大的值作为新的聚类中心</span><br><span class="line">y(center_loc)=1:sum(center_loc);</span><br><span class="line">%对密度从大到小排序</span><br><span class="line">[~,density_loc]=sort(p,'descend');</span><br><span class="line">%将某点划分到密度大于该点并且距离该点最近的一类</span><br><span class="line">for i=2:sample_num</span><br><span class="line">    if y(density_loc(i))==0</span><br><span class="line">        temp=density_loc(1:i-1);</span><br><span class="line">        y(density_loc(i))=y(temp(find(distance(density_loc(i),temp)==min(distance(density_loc(i),temp)),1)));</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">%定义两类之间的最小距离</span><br><span class="line">distance_temp=distance.*triu(ones(sample_num));</span><br><span class="line">temp=sort(distance_temp(distance_temp&gt;0));</span><br><span class="line">%取出前%2的距离作为最小距离</span><br><span class="line">dc=temp(round(length(temp)/50));</span><br><span class="line">for i=1:sum(center_loc)</span><br><span class="line">    %得到边缘点</span><br><span class="line">    edge= min(distance(y==i,y~=i),[],2)&lt;dc;</span><br><span class="line">    if ~(isempty(edge)||isequal(edge,zeros(length(find(y==i)),1)))</span><br><span class="line">        tem=find(y==i);</span><br><span class="line">        %得到边缘区域密度最大的值</span><br><span class="line">        pb=max(p(tem(edge)));</span><br><span class="line">        %将密度小于该最大值的点作为噪声点</span><br><span class="line">        y(tem(p(y==i)&lt;=pb))=0;</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="CFDP-display-m"><a href="#CFDP-display-m" class="headerlink" title="CFDP_display.m"></a>CFDP_display.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">function CFDP_display(x,y,sample_num,density_max)</span><br><span class="line">color_bar=zeros(length(density_max),3);</span><br><span class="line">hold on;</span><br><span class="line">for i=1:length(density_max)</span><br><span class="line">    color_bar(i,:)=[rand(1),rand(1),rand(1)];</span><br><span class="line">end</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    if y(i)==0</span><br><span class="line">        plot(x(1,i),x(2,i),'ko');</span><br><span class="line">    else</span><br><span class="line">        if ismember(i,density_max)</span><br><span class="line">            plot(x(1,i),x(2,i),'color',color_bar(y(i),:),'marker','*');</span><br><span class="line">        else</span><br><span class="line">            plot(x(1,i),x(2,i),'color',color_bar(y(i),:),'marker','o');</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/cfdp.png" alt="CFDP"></p><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>对噪声数据不敏感</li><li>不依赖初始数据点的选择</li><li>可以完成任意形状的聚类</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>离群点的确定非常复杂</li><li>算法复杂，分类速度较慢</li><li>对于高维数据，距离的度量并不是很好</li><li>不适合数据集整体密度基本相同的情况</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;CFDP&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>密度最大值聚类(MDCA)</title>
    <link href="https://USTCcoder.github.io/2019/05/07/clustering_MDCA/"/>
    <id>https://USTCcoder.github.io/2019/05/07/clustering_MDCA/</id>
    <published>2019-05-07T12:05:27.000Z</published>
    <updated>2019-08-07T15:53:29.379Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">密度最大值聚类方法</font></strong></center><p></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  MDCA(Maximum Density Clustering Application):将基于密度的思想引入到划分聚类中，使用<strong>密度</strong>而不是初始质心作为考察簇归属情况的依据，能够<strong>自动确定</strong>簇数量并发现任意形状的簇。MDCA一般不保留噪声，因此也避免了由于阈值选择不当而造成大量对象丢弃情况。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1.最大密度点：可用K近邻距离之和的倒数表示密度</font></p><script type="math/tex; mode=display">\rho_{max}=\{ \rho_{x} | x \in C, \forall x_i \in C, \rho_(x) \ge \rho_(x_i) \} \ , \ 其中C为数据集</script><p>  <font size="4">2. 密度曲线：根据所有对象与x的欧式距离对数据集重新排序</font></p><script type="math/tex; mode=display">S_{\rho_{max}}=\{x_1 , x_2 , \cdots , x_n | d(x,x_1) \leq d(x,x_2) \leq \cdots \leq d(x,x_n) \}</script><p><img src="/images/MACHINE/mdca1.png" alt="MDCA"></p><p>  <font size="4">3. 将密度曲线中第一个谷值之前的数据归为一类，并将其剔除</font></p><p>  <font size="4">4. 重复步骤1，2，3直到所有的点都在ρ<sub>0</sub>之下或者ρ<sub>0</sub>之上</font></p><p>  <font size="4">5. 两个簇C<sub>i</sub>和C<sub>j</sub>，用最近样本距离作为簇间距离</font></p><script type="math/tex; mode=display">d(c_i,c_j)=\underset{x_i \in C_i,x_j \in C_j}{\min}d(x_i,x_j)</script><p><img src="/images/MACHINE/mdca2.png" alt="MDCA"></p><p>  <font size="4">6. 根据簇间距离阈值d<sub>0</sub>，判断是否需要合并两类</font></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/MACHINE/mdca9.png" alt="MDCA"></p><p><br><br></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用数据集可以查看相关文档，<a href="https://ustccoder.github.io/2019/04/25/clustering_Dataset/">数据集(Data Set)</a></font></p><h2 id="MDCA-main-m"><a href="#MDCA-main-m" class="headerlink" title="MDCA_main.m"></a>MDCA_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">load('..\\cluster_gauss.mat');</span><br><span class="line">%输入x的矩阵</span><br><span class="line">x=data;</span><br><span class="line">randIndex = randperm(size(x,2));</span><br><span class="line">x=x(:,randIndex);</span><br><span class="line">%样本数</span><br><span class="line">sample_num=size(x,2);</span><br><span class="line">%判断密度时检测周围点的个数</span><br><span class="line">k=5;</span><br><span class="line">%最小阈值密度</span><br><span class="line">density_min=25000;</span><br><span class="line">%最小阈值距离</span><br><span class="line">distance_min=0.1;</span><br><span class="line">%特征数目</span><br><span class="line">feat_num=size(x,1);</span><br><span class="line">%尺度缩放到0-1</span><br><span class="line">x_scale=zeros(size(x));</span><br><span class="line">for i=1:feat_num</span><br><span class="line">    x_scale(i,:)=(x(i,:)-min(x(i,:)))/(max(x(i,:))-min(x(i,:)));</span><br><span class="line">end</span><br><span class="line">[y,class_num]=MDCA_classify(x_scale,sample_num,k,density_min,distance_min);</span><br><span class="line">% 如果数据的特征是二维的，可以绘图表示</span><br><span class="line">if feat_num==2</span><br><span class="line">    MDCA_display(x,y,sample_num,class_num);</span><br><span class="line">else</span><br><span class="line">    disp('The Feature Is Not Two-Dimensional');</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="MDCA-classify-m"><a href="#MDCA-classify-m" class="headerlink" title="MDCA_classify.m"></a>MDCA_classify.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">function [y,class_num]=MDCA_classify(x_scale,sample_num,k,density_min,distance_min)</span><br><span class="line">class_num=1;</span><br><span class="line">y=zeros(1,sample_num);</span><br><span class="line">%p为每个样本的密度</span><br><span class="line">p=zeros(1,sample_num);</span><br><span class="line">%distance(i,j)代表第i个样本到第j个样本的距离</span><br><span class="line">distance=zeros(sample_num);</span><br><span class="line">%利用k近邻均值定义密度较好，不会出现很多密度相同的点。如果采用半径内个数的定义方法，可能一块区域会出现很多的类别</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    distance(i,:)=sum((x_scale-repmat(x_scale(:,i),1,sample_num)).^2);</span><br><span class="line">    temp=sort(distance(i,:));</span><br><span class="line">    p(i)=k./sum(distance(i,distance(i,:)&lt;=temp(k)));</span><br><span class="line">end        </span><br><span class="line">[y,class_num]=MDCA_findclass(y,p,distance,density_min,class_num);    </span><br><span class="line">%设置两个标置位</span><br><span class="line">while 1</span><br><span class="line">    flag_2=0;</span><br><span class="line">    for i=1:class_num</span><br><span class="line">        flag_1=0;</span><br><span class="line">        for j=i+1:class_num</span><br><span class="line">            if min(min(distance(y==i,y==j)))&lt;=distance_min</span><br><span class="line">                y(y==j)=i;</span><br><span class="line">                y(y&gt;j)=y(y&gt;j)-1;</span><br><span class="line">                class_num=class_num-1;</span><br><span class="line">                flag_1=1;</span><br><span class="line">                break;</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        if flag_1==1;</span><br><span class="line">            break;</span><br><span class="line">        end</span><br><span class="line">        flag_2=1;</span><br><span class="line">    end</span><br><span class="line">    if flag_2==1</span><br><span class="line">        break;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">loc=find(y~=0);</span><br><span class="line">[temp,tem]=min(distance(y==0,y~=0),[],2);</span><br><span class="line">y(y==0)=y(loc(tem)).*(temp&lt;=distance_min)';</span><br></pre></td></tr></tbody></table></figure><h2 id="MDCA-findclass-m"><a href="#MDCA-findclass-m" class="headerlink" title="MDCA_findclass.m"></a>MDCA_findclass.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">function [y,class_num]=MDCA_findclass(y,p,distance,density_min,class_num)</span><br><span class="line">tem=find(y==0);</span><br><span class="line">p_temp=p(tem);</span><br><span class="line">%找到最大的密度点</span><br><span class="line">p_max=tem(find(p_temp==max(p_temp),1));</span><br><span class="line">%按照到最大密度点的距离从小到大排序</span><br><span class="line">[~,b]=sort(distance(p_max,tem));</span><br><span class="line">temp=tem(b);</span><br><span class="line">curve=p(temp);</span><br><span class="line">if max(curve)&gt;density_min</span><br><span class="line">    loc=find(curve&lt;=density_min);</span><br><span class="line">    if ~isempty(loc)&amp;&amp;length(loc)&gt;=2</span><br><span class="line">        for i=1:length(loc)-1</span><br><span class="line">            if loc(i+1)-loc(i)&lt;=2</span><br><span class="line">                break;</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">        y(temp(1:loc(i)))=class_num;</span><br><span class="line">        [y,class_num]=MDCA_findclass(y,p,distance,density_min,class_num+1);</span><br><span class="line">        return;</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="MDCA-display-m"><a href="#MDCA-display-m" class="headerlink" title="MDCA_display.m"></a>MDCA_display.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">function MDCA_display(x,y,sample_num,class_num)</span><br><span class="line">figure;</span><br><span class="line">hold on;</span><br><span class="line">for i=1:class_num</span><br><span class="line">    color_bar(i,:)=[rand(1),rand(1),rand(1)];</span><br><span class="line">end</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    if y(i)==0</span><br><span class="line">        %画出噪声点，用*表示</span><br><span class="line">        plot(x(1,i),x(2,i),'k*')</span><br><span class="line">    else</span><br><span class="line">        %画出每一类的样本数据，用o表示</span><br><span class="line">        plot(x(1,i),x(2,i),'color',color_bar(y(i),:),'marker','o');</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/mdca.png" alt="MDCA"></p><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>对噪声数据不敏感</li><li>不依赖初始数据点的选择</li><li>可以完成任意形状的聚类</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>算法复杂，分类速度较慢</li><li>需要在测试前确定密度阈值</li><li>对于高维数据，距离的度量并不是很好</li><li>不适合数据集密度差异较大或整体密度基本相同的情况</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;MDCA&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>密度聚类(DBSCAN)</title>
    <link href="https://USTCcoder.github.io/2019/05/05/clustering_DBSCAN/"/>
    <id>https://USTCcoder.github.io/2019/05/05/clustering_DBSCAN/</id>
    <published>2019-05-05T11:25:30.000Z</published>
    <updated>2019-08-07T15:52:46.035Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">密度聚类方法</font></strong></center><p></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  DBSCAN(Density-Based Spatial Clustering Of Applications With Noise):DBSCAN需要两个参数，<strong>扫描半径 </strong>(eps)和<strong>最小包含点数</strong>(minPts)。 任选一个未被标记的点开始，找出与其距离在eps之内(包括eps)的所有附近点。如果<strong>附近点的数量大于等于minPts</strong>，则当前点与其附近点形成一个簇，并且出发点被标记。 然后递归，以相同的方法处理该簇内所有未被标记的点，从而对簇进行扩展。如果<strong>附近点的数量小于minPts</strong>，则该点被标记，不作扩展。如果簇充分地被扩展，即簇内的所有点被标记为已访问，然后用同样的算法去处理未被访问的点，直到所有的点都被标记。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 挑选一个未标记样本，置为一类，搜索附近样本</font><br><img src="/images/MACHINE/dbscan1.jpg" alt="DBSCAN"></p><p>  <font size="4">2. 如果附近样本数大于minPts，将这些样本归于该类，在此类中挑选未标记样本，继续搜索附近样本</font><br><img src="/images/MACHINE/dbscan2.png" alt="DBSCAN"></p><p>  <font size="4">3. 重复步骤2，直到该类中所有样本都被标记</font></p><p>  <font size="4">4. 重复步骤1，直到所有样本都被标记</font></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/MACHINE/dbscan9.png" alt="DBSCAN"></p><p><br><br></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用数据集可以查看相关文档，<a href="https://ustccoder.github.io/2019/04/25/clustering_Dataset/">数据集(Data Set)</a></font></p><h2 id="DBSCAN-main-m"><a href="#DBSCAN-main-m" class="headerlink" title="DBSCAN_main.m"></a>DBSCAN_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">load('..\\cluster_cicle.mat');</span><br><span class="line">%输入x的矩阵</span><br><span class="line">x=data;</span><br><span class="line">randIndex = randperm(size(x,2));</span><br><span class="line">x=x(:,randIndex);</span><br><span class="line">%搜索半径</span><br><span class="line">eps=0.05;</span><br><span class="line">%圆内点数</span><br><span class="line">minpts=2;</span><br><span class="line">%样本数</span><br><span class="line">sample_num=size(x,2);</span><br><span class="line">%特征数目</span><br><span class="line">feat_num=size(x,1);</span><br><span class="line">%尺度缩放到0-1</span><br><span class="line">x_scale=zeros(size(x));</span><br><span class="line">for i=1:feat_num</span><br><span class="line">    x_scale(i,:)=(x(i,:)-min(x(i,:)))/(max(x(i,:))-min(x(i,:)));</span><br><span class="line">end</span><br><span class="line">[y,color_bar]=DBSCAN_classify(x_scale,sample_num,eps,minpts);</span><br><span class="line">%如果数据的特征是二维的，可以绘图表示</span><br><span class="line">if feat_num==2</span><br><span class="line">    DBSCAN_display(x,y,color_bar,sample_num)</span><br><span class="line">else</span><br><span class="line">    disp('The Feature Is Not Two-Dimensional');</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="DBSCAN-classify-m"><a href="#DBSCAN-classify-m" class="headerlink" title="DBSCAN_classify.m"></a>DBSCAN_classify.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">function [y,color_bar]=DBSCAN_classify(x_scale,sample_num,eps,minpts)</span><br><span class="line">%标记是否已经分过类</span><br><span class="line">flag=zeros(1,sample_num);</span><br><span class="line">y=zeros(1,sample_num);</span><br><span class="line">color_bar=[];</span><br><span class="line">%类别数目</span><br><span class="line">i=1;</span><br><span class="line">%找到未标记的数据点则继续循环</span><br><span class="line">while ~isempty(find(flag==0,1))</span><br><span class="line">    %求出该点C到其余各点的距离</span><br><span class="line">    distance=sum((x_scale-repmat(x_scale(:,find(flag==0,1)),1,sample_num)).^2);</span><br><span class="line">    %将找到的点标记</span><br><span class="line">    flag(find(flag==0,1))=1;</span><br><span class="line">    %找出距离找到的点小于半径的所有点</span><br><span class="line">    temp=find(distance&lt;eps^2);</span><br><span class="line">    %如果个数大于等于设定的个数</span><br><span class="line">    if length(temp)&gt;=minpts</span><br><span class="line">        %建立一个类别的颜色信息</span><br><span class="line">        color_bar(i,:)=[rand(1),rand(1),rand(1)];</span><br><span class="line">        %将这些点全部分为i类</span><br><span class="line">        y(temp)=i;</span><br><span class="line">        %找出i类中没有标记的点则继续循环</span><br><span class="line">        while ~isempty(find(y==i&amp;flag==0,1))</span><br><span class="line">            %求出该点D到其余各点的距离</span><br><span class="line">            distance_part=sum((x_scale-repmat(x_scale(:,find(y==i&amp;flag==0,1)),1,sample_num)).^2);</span><br><span class="line">            %将找到的点标记</span><br><span class="line">            flag(find(y==i&amp;flag==0,1))=1;</span><br><span class="line">            %找出距离找到的点小于半径的所有点</span><br><span class="line">            temp_part=find(distance_part&lt;eps^2);</span><br><span class="line">            %如果个数大于等于设定的个数,</span><br><span class="line">            if length(temp_part)&gt;=minpts</span><br><span class="line">                %将这些点全部分为i类</span><br><span class="line">                y(temp_part)=i;</span><br><span class="line">            end</span><br><span class="line">        end</span><br><span class="line">    else</span><br><span class="line">        continue;</span><br><span class="line">    end</span><br><span class="line">    %这一类找完时，类别加1</span><br><span class="line">    i=i+1;</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="DBSCAN-display-m"><a href="#DBSCAN-display-m" class="headerlink" title="DBSCAN_display.m"></a>DBSCAN_display.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">function DBSCAN_display(x,y,color_bar,sample_num)</span><br><span class="line">figure;</span><br><span class="line">hold on;</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    if y(i)==0</span><br><span class="line">        %画出噪声点，用*表示</span><br><span class="line">        plot(x(1,i),x(2,i),'k*')</span><br><span class="line">    else</span><br><span class="line">        %画出每一类的样本数据，用o表示</span><br><span class="line">        plot(x(1,i),x(2,i),'color',color_bar(y(i),:),'marker','o');</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/dbscan.png" alt="DBSCAN"></p><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>算法简单，容易理解</li><li>不依赖初始数据点的选择</li><li>可以完成任意形状的聚类</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>对噪声数据敏感</li><li>需要在测试前确定eps和minPts</li><li>不适合数据集中密度差异较大的情况</li><li>对于高维数据，距离的度量并不是很好</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;DBSCAN&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>迭代自组织分析聚类(ISODATA)</title>
    <link href="https://USTCcoder.github.io/2019/05/04/clustering_ISODATA/"/>
    <id>https://USTCcoder.github.io/2019/05/04/clustering_ISODATA/</id>
    <published>2019-05-04T11:28:51.000Z</published>
    <updated>2019-08-07T15:53:15.137Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">迭代自组织分析聚类方法</font></strong></center><p></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  ISODATA(Iterative Selforganizing Data Analysis) :在k-均值算法的基础上，增加对聚类结果的<strong>合并</strong>和<strong>分裂</strong>两个操作，当聚类结果某一类中<strong>样本数太少，或两个类间的距离太近，或样本类别远大于设定类别数</strong>时，进行<strong>合并</strong>，当聚类结果某一类中<strong>样本数太多，或某个类内方差太大，或样本类别远小于设定类别数</strong>时，进行<strong>分裂</strong>。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 初始常量(c<sub>0</sub>,n<sub>0</sub>,d<sub>min</sub>,d<sub>max</sub>,T)</font><br>    c<sub>0</sub>：希望得到的聚类数<br>    n<sub>0</sub>：每类的样本数<br>    d<sub>min</sub>：最小类间距离<br>    d<sub>max</sub>：最大类内距离<br>    T：最大迭代次数</p><p>  <font size="4">2. 最小类间距离</font></p><script type="math/tex; mode=display">d_{min}=\underset{C_i,C_j \subseteq C}{min}{d( \overline {C_i},\overline {C_j} )} \ , \ 其中\overline {C_i}=\frac {1}{\lvert C_i \rvert}\underset{x_i \in C_i}{\sum}{x_i}</script><p>  <font size="4">3. 最大类内距离</font></p><script type="math/tex; mode=display">d_{max}=\underset{C_i \subseteq C}{max} \ \frac {1}{\lvert {C_i} \rvert}\underset{x_i \in C_i}{\sum}{d( x_i,\overline C_i )} \ , \ 其中\overline {C_i}=\frac {1}{\lvert C_i \rvert}\underset{x_i \in C_i}{\sum}{x_i}</script><table>    <tbody><tr><td><center><img src="/images/MACHINE/isodata1.png">初始时刻状态</center></td>    <td><center><img src="/images/MACHINE/isodata2.png">第一次迭代后</center></td></tr></tbody></table><table>    <tbody><tr><td><center><img src="/images/MACHINE/isodata3.png">第二次迭代后</center></td>    <td><center><img src="/images/MACHINE/isodata4.png">第三次迭代后</center></td></tr></tbody></table><table>    <tbody><tr><td><center><img src="/images/MACHINE/isodata5.png">第四次迭代后</center></td>    <td><center><img src="/images/MACHINE/isodata6.png">第五次迭代后</center></td></tr></tbody></table><p>  <font size="4">4. 判断是否达到分裂条件</font></p><ul><li>当前类别数是否小于希望得到聚类数的一半</li><li>当前每一类的数目是否大于每类样本数的二倍</li><li>当前类内距离是否大于最大类内距离</li></ul><p>  <font size="4">5. 分裂不满足条件的类别，回到步骤2，直到满足某个终止条件</font></p><p>  <font size="4">6. 判断是否达到合并条件</font></p><ul><li>当前类别数是否大于希望得到聚类数的二倍</li><li>当前每一类的数目是否小于每类样本数的一半</li><li>当前类间距离是否小于最小类间距离</li></ul><p>  <font size="4">7. 合并不满足条件的类别，回到步骤2，直到满足某个终止条件</font></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/MACHINE/isodata9.png" alt="ISODATA"></p><p><br><br></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用数据集可以查看相关文档，<a href="https://ustccoder.github.io/2019/04/25/clustering_Dataset/">数据集(Data Set)</a></font></p><h2 id="ISODATA-main-m"><a href="#ISODATA-main-m" class="headerlink" title="ISODATA_main.m"></a>ISODATA_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">load('..\\cluster_gauss.mat');</span><br><span class="line">%输入x的矩阵</span><br><span class="line">x=data;</span><br><span class="line">randIndex = randperm(size(x,2));</span><br><span class="line">x=x(:,randIndex);</span><br><span class="line">%希望划分的类别数</span><br><span class="line">hope_class_num=3;</span><br><span class="line">%希望每一类的数目</span><br><span class="line">hope_num=20;</span><br><span class="line">%设定类内的最大距离,小一点</span><br><span class="line">max_class_inner_distance=0.1;</span><br><span class="line">%设定类间的最小距离,小一点</span><br><span class="line">min_class_between_distance=0.1;</span><br><span class="line">%最多迭代次数</span><br><span class="line">times=100;</span><br><span class="line">%样本数</span><br><span class="line">sample_num=size(x,2);</span><br><span class="line">%特征数目</span><br><span class="line">feat_num=size(x,1);</span><br><span class="line">%尺度缩放到0-1</span><br><span class="line">x_scale=zeros(size(x));</span><br><span class="line">for i=1:feat_num</span><br><span class="line">    x_scale(i,:)=(x(i,:)-min(x(i,:)))/(max(x(i,:))-min(x(i,:)));</span><br><span class="line">end</span><br><span class="line">[y,class_num,class_center]=ISODATA_classify(x_scale,sample_num,hope_class_num,hope_num,max_class_inner_distance,min_class_between_distance,times);</span><br><span class="line">%样本中心尺度复原</span><br><span class="line">for i=1:feat_num</span><br><span class="line">    class_center(i,:)=(max(x(i,:))-min(x(i,:)))*class_center(i,:)+min(x(i,:));</span><br><span class="line">end</span><br><span class="line">%如果数据的特征是二维的，可以绘图表示</span><br><span class="line">if feat_num==2</span><br><span class="line">    ISODATA_display(x,y,class_center,sample_num,class_num);</span><br><span class="line">else</span><br><span class="line">    disp('The Feature Is Not Two-Dimensional');</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="ISODATA-classify-m"><a href="#ISODATA-classify-m" class="headerlink" title="ISODATA_classify.m"></a>ISODATA_classify.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br></pre></td><td class="code"><pre><span class="line">function [y,class_num,class_center]=ISODATA_classify(x_scale,sample_num,hope_class_num,hope_num,max_class_inner_distance,min_class_between_distance,times)</span><br><span class="line">%给每一个样本都视为0类</span><br><span class="line">y=zeros(1,sample_num);</span><br><span class="line">%将前class_num个样本分为class_num类</span><br><span class="line">y(1:hope_class_num)=1:hope_class_num;</span><br><span class="line">%目前的类别数</span><br><span class="line">class_num=hope_class_num;</span><br><span class="line">k=0;</span><br><span class="line">while 1</span><br><span class="line">    class_center=zeros(2,class_num);</span><br><span class="line">    for i=1:class_num</span><br><span class="line">        %更新类别的中心</span><br><span class="line">        class_center(:,i)=sum(x_scale(:,y==i),2)/sum(y==i);</span><br><span class="line">    end</span><br><span class="line">    %采用最近邻进行分类</span><br><span class="line">    for i=1:sample_num</span><br><span class="line">        distance=sum((class_center-repmat(x_scale(:,i),1,class_num)).^2);</span><br><span class="line">        y(i)=find(distance==min(distance),1);</span><br><span class="line">    end</span><br><span class="line">    for i=1:class_num</span><br><span class="line">        %更新类别的中心</span><br><span class="line">        class_center(:,i)=sum(x_scale(:,y==i),2)/sum(y==i);</span><br><span class="line">    end</span><br><span class="line">    %如果迭代次数大于times，则停止迭代</span><br><span class="line">    if k&gt;=times</span><br><span class="line">        break;</span><br><span class="line">    end</span><br><span class="line">    each_class_num=zeros(1,class_num);</span><br><span class="line">    distance_between_class=zeros(class_num);</span><br><span class="line">    %统计每一类的个数,并求出类间距离</span><br><span class="line">    for i=1:class_num</span><br><span class="line">        each_class_num(i)=sum(y==i);</span><br><span class="line">        distance_between_class(i,:)=sum((class_center-repmat(class_center(:,i),1,class_num)).^2);</span><br><span class="line">        distance_between_class(i,i)=inf;</span><br><span class="line">    end</span><br><span class="line">    %统计类内距离</span><br><span class="line">    distance_class_inner=zeros(1,class_num);</span><br><span class="line">    for i=1:class_num</span><br><span class="line">        temp=x_scale(:,y==i);</span><br><span class="line">        distance_class_inner(i)=sum(sum((temp-repmat(class_center(:,i),1,sum(y==i))).^2))/sum(y==i);</span><br><span class="line">    end</span><br><span class="line">    %如果类内距离最大的一类的类内距离大于设定值，则分裂两类</span><br><span class="line">    if max(distance_class_inner)&gt;max_class_inner_distance</span><br><span class="line">        tem=find(distance_class_inner==max(distance_class_inner),1);</span><br><span class="line">        %temp为该类别的样本</span><br><span class="line">        temp=x_scale(:,y==tem);</span><br><span class="line">        %distance(i,j)指第i个样本到第j个样本的距离</span><br><span class="line">        distance=zeros(size(temp));</span><br><span class="line">        for i=1:sum(y==tem)</span><br><span class="line">            distance(i,:)=sum((temp-repmat(temp(:,i),1,sum(y==tem))).^2);</span><br><span class="line">        end</span><br><span class="line">        %找到距离最远的两个样本</span><br><span class="line">        [row,col]=find(distance==max(max(distance)),1);</span><br><span class="line">        %分别找到这个两个样本的所在位置</span><br><span class="line">        temp=find(y==tem);</span><br><span class="line">        row=temp(row);</span><br><span class="line">        col=temp(col);</span><br><span class="line">        %类别数+1</span><br><span class="line">        class_num=class_num+1;</span><br><span class="line">        %令该类别撤销</span><br><span class="line">        y(y==tem)=0;</span><br><span class="line">        %添加两个新类别，一个覆盖原类别，另一个类别为原类别数+1</span><br><span class="line">        y(row)=tem;</span><br><span class="line">        y(col)=class_num;</span><br><span class="line">        k=k+1;</span><br><span class="line">        continue;</span><br><span class="line">    end</span><br><span class="line">    %如果两类之间的最小距离小于设定阈值，则合并两类</span><br><span class="line">    if min(min(distance_between_class))&lt;min_class_between_distance</span><br><span class="line">        %找到距离最近的两个类别</span><br><span class="line">        [row,col]=find(distance_between_class==min(min(distance_between_class)),1);</span><br><span class="line">        %类别数-1</span><br><span class="line">        class_num=class_num-1;</span><br><span class="line">        %将col类合并到row类中</span><br><span class="line">        y(y==col)=row;</span><br><span class="line">        %调整类别序号</span><br><span class="line">        y(y&gt;col)=y(y&gt;col)-1;</span><br><span class="line">        k=k+1;</span><br><span class="line">        continue;</span><br><span class="line">    end</span><br><span class="line">    %如果某一类的最小数量小于等于希望数量的一半</span><br><span class="line">    if min(each_class_num)&lt;=hope_num/2</span><br><span class="line">        %找到该类别</span><br><span class="line">        tem=find(each_class_num==min(each_class_num),1);</span><br><span class="line">        %令该类别撤销</span><br><span class="line">        y(y==tem)=0;</span><br><span class="line">        %重新调整类别序号</span><br><span class="line">        y(y&gt;tem)=y(y&gt;tem)-1;</span><br><span class="line">        %类别数-1</span><br><span class="line">        class_num=class_num-1;</span><br><span class="line">        continue;</span><br><span class="line">    end</span><br><span class="line">    %如果某一类的最小数量大于等于希望数量的2倍</span><br><span class="line">    if max(each_class_num)&gt;=hope_num*2</span><br><span class="line">        %找到该类别</span><br><span class="line">        tem=find(each_class_num==max(each_class_num),1);</span><br><span class="line">        %temp为该类别的样本</span><br><span class="line">        temp=x_scale(:,y==tem);</span><br><span class="line">        %distance(i,j)指第i个样本到第j个样本的距离</span><br><span class="line">        distance=zeros(size(temp));</span><br><span class="line">        for i=1:sum(y==tem)</span><br><span class="line">            distance(i,:)=sum((temp-repmat(temp(:,i),1,sum(y==tem))).^2);</span><br><span class="line">        end</span><br><span class="line">        %找到距离最远的两个样本</span><br><span class="line">        [row,col]=find(distance==max(max(distance)),1);</span><br><span class="line">        %分别找到这个两个样本的所在位置</span><br><span class="line">        temp=find(y==tem);</span><br><span class="line">        row=temp(row);</span><br><span class="line">        col=temp(col);</span><br><span class="line">        %类别数+1</span><br><span class="line">        class_num=class_num+1;</span><br><span class="line">        %令该类别撤销</span><br><span class="line">        y(y==tem)=0;</span><br><span class="line">        %添加两个新类别，一个覆盖原类别，另一个类别为原类别数+1</span><br><span class="line">        y(row)=tem;</span><br><span class="line">        y(col)=class_num;</span><br><span class="line">        continue;</span><br><span class="line">    end</span><br><span class="line">    %如果现有类别数小于等于希望类别数的一半，拆分类内距离最大类别</span><br><span class="line">    if class_num&lt;=hope_class_num/2</span><br><span class="line">        tem=find(distance_class_inner==max(distance_class_inner),1);</span><br><span class="line">        %temp为该类别的样本</span><br><span class="line">        temp=x_scale(:,y==tem);</span><br><span class="line">        %distance(i,j)指第i个样本到第j个样本的距离</span><br><span class="line">        distance=zeros(size(temp));</span><br><span class="line">        for i=1:sum(y==tem)</span><br><span class="line">            distance(i,:)=sum((temp-repmat(temp(:,i),1,sum(y==tem))).^2);</span><br><span class="line">        end</span><br><span class="line">        %找到距离最远的两个样本</span><br><span class="line">        [row,col]=find(distance==max(max(distance)),1);</span><br><span class="line">        %分别找到这个两个样本的所在位置</span><br><span class="line">        temp=find(y==tem);</span><br><span class="line">        row=temp(row);</span><br><span class="line">        col=temp(col);</span><br><span class="line">        %类别数+1</span><br><span class="line">        class_num=class_num+1;</span><br><span class="line">        %令该类别撤销</span><br><span class="line">        y(y==tem)=0;</span><br><span class="line">        %添加两个新类别，一个覆盖原类别，另一个类别为原类别数+1</span><br><span class="line">        y(row)=tem;</span><br><span class="line">        y(col)=class_num;</span><br><span class="line">        continue;</span><br><span class="line">    end</span><br><span class="line">    %如果现有类别数大于等于希望类别数的二倍，合并类间距离最小的两类</span><br><span class="line">    if class_num&gt;=hope_class_num*2</span><br><span class="line">        %找到距离最近的两个类别</span><br><span class="line">        [row,col]=find(distance_between_class==min(min(distance_between_class)),1);</span><br><span class="line">        %类别数-1</span><br><span class="line">        class_num=class_num-1;</span><br><span class="line">        %将col类合并到row类中</span><br><span class="line">        y(y==col)=row;</span><br><span class="line">        %调整类别序号</span><br><span class="line">        y(y&gt;col)=y(y&gt;col)-1;</span><br><span class="line">        continue;</span><br><span class="line">    end</span><br><span class="line">    %添加样本类内间距最大值</span><br><span class="line">    break;</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="ISODATA-display-m"><a href="#ISODATA-display-m" class="headerlink" title="ISODATA_display.m"></a>ISODATA_display.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">function ISODATA_display(x,y,class_center,sample_num,class_num)</span><br><span class="line">color_bar=zeros(class_num,3);</span><br><span class="line">hold on;</span><br><span class="line">for i=1:class_num</span><br><span class="line">    color_bar(i,:)=[rand(1),rand(1),rand(1)];</span><br><span class="line">    %绘制样本中心，用*表示</span><br><span class="line">    plot(class_center(1,i),class_center(2,i),'color',color_bar(i,:),'marker','*')</span><br><span class="line">end</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    %绘制数据集，用o表示</span><br><span class="line">    plot(x(1,i),x(2,i),'color',color_bar(y(i),:),'marker','o');</span><br><span class="line">end</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/isodata.png" alt="ISODATA"></p><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>大数据集时，对噪声数据不敏感</li><li>可以动态调整类别个数和类别中心</li><li>在先验知识不足的情况下有较好的分类能力</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>对初始中心点敏感</li><li>算法复杂，分类速度较慢</li><li>只适合分布呈凸型或者球形的数据集</li><li>对于高维数据，距离的度量并不是很好</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;ISODATA&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>K均值聚类(K-MEANS)</title>
    <link href="https://USTCcoder.github.io/2019/05/03/clustering_KMEANS/"/>
    <id>https://USTCcoder.github.io/2019/05/03/clustering_KMEANS/</id>
    <published>2019-05-03T11:14:05.000Z</published>
    <updated>2019-08-07T15:53:24.367Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">K-Means聚类方法</font></strong></center><p></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  K-Means :随机选取<strong>N个</strong>对象作为初始的聚类中心，然后计算每个对象与各个种子聚类中心之间的距离，把每个对象分配给距离它最近的聚类中心。每分配一个样本，聚类中心会根据聚类中现有的对象被<strong>重新计算</strong>。这个过程将不断重复直到满足某个终止条件。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 求出N个类别的聚类中心a<sub>1</sub>,a<sub>2</sub>, … ,a<sub>N</sub></font></p><script type="math/tex; mode=display">a_i=\frac {1}{\lvert C_i \rvert}\underset{x_i \in C_j}{\sum}{x_i}</script><p>  <font size="4">2. 对于每个样本x<sub>j</sub>，将其标记为距离类别中心a<sub>i</sub>最近的一类</font></p><script type="math/tex; mode=display">x_j \in C_i \ , \ 其中k=\underset{i,a_i \in C_k}{arg \ min}\ d(x_j,a_i)</script><p>  <font size="4">3. 重复步骤1，2直到满足某个终止条件</font><br><img src="/images/MACHINE/kmeans1.png" alt="KMEANS"></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/MACHINE/kmeans9.png" alt="KMEANS"></p><p><br><br></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用数据集可以查看相关文档，<a href="https://ustccoder.github.io/2019/04/25/clustering_Dataset/">数据集(Data Set)</a></font></p><h2 id="KMEANS-main-m"><a href="#KMEANS-main-m" class="headerlink" title="KMEANS_main.m"></a>KMEANS_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">load('..\\cluster_gauss.mat');</span><br><span class="line">%输入x的矩阵</span><br><span class="line">x=data;</span><br><span class="line">randIndex = randperm(size(x,2));</span><br><span class="line">x=x(:,randIndex);</span><br><span class="line">%类别数目，请输入大于1的数</span><br><span class="line">class_num=3;</span><br><span class="line">%样本数</span><br><span class="line">sample_num=size(x,2);</span><br><span class="line">%特征数目</span><br><span class="line">feat_num=size(x,1);</span><br><span class="line">%尺度缩放到0-1</span><br><span class="line">x_scale=zeros(size(x));</span><br><span class="line">for i=1:feat_num</span><br><span class="line">    x_scale(i,:)=(x(i,:)-min(x(i,:)))/(max(x(i,:))-min(x(i,:)));</span><br><span class="line">end</span><br><span class="line">%类别中心位置</span><br><span class="line">loc_center=zeros(feat_num,class_num);</span><br><span class="line">%如果类别数大于样本数或者类别数小于1，则无法分类</span><br><span class="line">if class_num&gt;sample_num||class_num&lt;1</span><br><span class="line">    disp('ERROR!')</span><br><span class="line">else</span><br><span class="line">    %如果类别数等于1，则所有的样本都属于该类别,聚类中心为所有样本的中点</span><br><span class="line">    if class_num==1</span><br><span class="line">        y=ones(1,sample_num);</span><br><span class="line">        loc_center(:,1)=sum(x,2)/sample_num;</span><br><span class="line">        k=0;</span><br><span class="line">    else</span><br><span class="line">        %取前class_num个样本作为初始类别</span><br><span class="line">        loc_center=x_scale(:,1:class_num);</span><br><span class="line">        %ISO聚类法</span><br><span class="line">        [y,loc_center,k]=KMEANS_classify(x_scale,loc_center,sample_num,class_num);</span><br><span class="line">        %将缩放后的聚类中心复原</span><br><span class="line">        for i=1:feat_num</span><br><span class="line">            loc_center(i,:)=loc_center(i,:)*(max(x(i,:))-min(x(i,:)))+min(x(i,:));</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    if k&gt;=1000</span><br><span class="line">        disp('Incorrect Classify')</span><br><span class="line">    else</span><br><span class="line">        %如果数据的特征是二维的，可以绘图表示</span><br><span class="line">        if feat_num==2</span><br><span class="line">            KMEANS_display(x,y,loc_center,sample_num,class_num)</span><br><span class="line">        else</span><br><span class="line">            disp('The Feature Is Not Two-Dimensional');</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="KMEANS-classify-m"><a href="#KMEANS-classify-m" class="headerlink" title="KMEANS_classify.m"></a>KMEANS_classify.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">function [y,loc_center,k]=KMEANS_classify(x_scale,loc_center,sample_num,class_num)</span><br><span class="line">%设置迭代次数</span><br><span class="line">k=0;</span><br><span class="line">while 1</span><br><span class="line">    %初始化最新的分类中心</span><br><span class="line">    loc_center_new=zeros(size(loc_center));</span><br><span class="line">    distance=zeros(class_num,sample_num);</span><br><span class="line">    %distance为每一个样本到每一类的距离</span><br><span class="line">    for i=1:class_num</span><br><span class="line">        distance(i,:)=sum((x_scale-repmat(loc_center(:,i),1,sample_num)).^2);</span><br><span class="line">    end</span><br><span class="line">    %求出每个样本到哪一类最近</span><br><span class="line">    [~,y]=min(distance);</span><br><span class="line">    %更新分类中心</span><br><span class="line">    for i=1:class_num</span><br><span class="line">        loc_center_new(:,i)=sum(x_scale(:,y==i),2)/sum(y==i);</span><br><span class="line">    end</span><br><span class="line">    %如果分类中心和上一次分类中心相等则分类完毕</span><br><span class="line">    if isequal(loc_center_new,loc_center)</span><br><span class="line">        break;</span><br><span class="line">    %否则继续分类</span><br><span class="line">    else</span><br><span class="line">        loc_center=loc_center_new;</span><br><span class="line">        k=k+1;</span><br><span class="line">        %如果分类次数达到1000仍然没有结束，则强制分类结束</span><br><span class="line">        if k&gt;=1000</span><br><span class="line">            break;</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="KMEANS-display-m"><a href="#KMEANS-display-m" class="headerlink" title="KMEANS_display.m"></a>KMEANS_display.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">function KMEANS_display(x,y,loc_center,sample_num,class_num)</span><br><span class="line">hold on;</span><br><span class="line">color_bar=zeros(class_num,3);</span><br><span class="line">%画出每一类的聚类中心，用*表示</span><br><span class="line">for i=1:class_num</span><br><span class="line">    color_bar(i,:)=[rand(1),rand(1),rand(1)];</span><br><span class="line">    plot(loc_center(1,i),loc_center(2,i),'color',color_bar(i,:),'marker','*')</span><br><span class="line">end</span><br><span class="line">%画出每一类的样本数据，用o表示</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    plot(x(1,i),x(2,i),'color',color_bar(y(i),:),'marker','o');</span><br><span class="line">end</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/kmeans.png" alt="KMEANS"></p><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>算法简单，容易理解</li><li>大数据集时，对噪声数据不敏感</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>对初始中心点敏感</li><li>需要在测试前知道类别的个数</li><li>只适合分布呈凸型或者球形的数据集</li><li>对于高维数据，距离的度量并不是很好</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;K-Means&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>分裂的层次聚类(DIANA)</title>
    <link href="https://USTCcoder.github.io/2019/05/02/clustering_DIANA/"/>
    <id>https://USTCcoder.github.io/2019/05/02/clustering_DIANA/</id>
    <published>2019-05-02T11:38:26.000Z</published>
    <updated>2019-08-07T15:52:59.800Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">分裂的层次聚类方法</font></strong></center><p></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  DIANA(Divisive Analysis):采用<strong>自顶向下</strong>的策略，最初将所有对象置于一个类中，然后根据某些准则将这些类别<strong>逐渐细分</strong>。细分过程反复进行直到类别达到预期的数目。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 将所有样本都作为同一类</font></p><p>  <font size="4">2. 分裂所有类别中到该类中心距离最大的样本，将其单独作为一类，按照最近邻分类，直到满足某个终止条件</font></p><script type="math/tex; mode=display">d_{max}=\underset{C_i \subseteq C}{max} \ (\underset{x_i \in C_i}{max} \ {d(x_i,\overline C_i)}) \ , \ 其中\overline {C_i}=\frac {1}{\lvert C_i \rvert}\underset{x_i \in C_i}{\sum}{x_i}</script><p><img src="/images/MACHINE/diana1.png" alt="DIANA"></p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/MACHINE/diana9.png" alt="DIANA"></p><p><br><br></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用数据集可以查看相关文档，<a href="https://ustccoder.github.io/2019/04/25/clustering_Dataset/">数据集(Data Set)</a></font></p><h2 id="DIANA-main-m"><a href="#DIANA-main-m" class="headerlink" title="DIANA_main.m"></a>DIANA_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">load('..\\cluster_gauss.mat');</span><br><span class="line">%输入x的矩阵</span><br><span class="line">x=data;</span><br><span class="line">randIndex = randperm(size(x,2));</span><br><span class="line">x=x(:,randIndex);</span><br><span class="line">%希望划分的类别数</span><br><span class="line">class_num=3;</span><br><span class="line">%样本数</span><br><span class="line">sample_num=size(x,2);</span><br><span class="line">%特征数目</span><br><span class="line">feat_num=size(x,1);</span><br><span class="line">%尺度缩放到0-1</span><br><span class="line">x_scale=zeros(size(x));</span><br><span class="line">for i=1:feat_num</span><br><span class="line">    x_scale(i,:)=(x(i,:)-min(x(i,:)))/(max(x(i,:))-min(x(i,:)));</span><br><span class="line">end</span><br><span class="line">[y,class_center]=DIANA_classify(x_scale,sample_num,class_num);</span><br><span class="line">%样本中心尺度复原</span><br><span class="line">for i=1:feat_num</span><br><span class="line">    class_center(i,:)=(max(x(i,:))-min(x(i,:)))*class_center(i,:)+min(x(i,:));</span><br><span class="line">end</span><br><span class="line">%如果数据的特征是二维的，可以绘图表示</span><br><span class="line">if feat_num==2</span><br><span class="line">    DIANA_display(x,y,class_center,sample_num,class_num);</span><br><span class="line">else</span><br><span class="line">    disp('The Feature Is Not Two-Dimensional');</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="DIANA-classify-m"><a href="#DIANA-classify-m" class="headerlink" title="DIANA_classify.m"></a>DIANA_classify.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">function [y,class_center]=DIANA_classify(x_scale,sample_num,class_num)</span><br><span class="line">if class_num==1</span><br><span class="line">    class_center=sum(x_scale,2)/sample_num;</span><br><span class="line">    y=ones(1,sample_num);</span><br><span class="line">else</span><br><span class="line">    %给每一个样本都视为0类</span><br><span class="line">    y=zeros(1,sample_num);</span><br><span class="line">    distance=zeros(sample_num);</span><br><span class="line">    for i=1:sample_num</span><br><span class="line">        distance(i,:)=sum((x_scale-repmat(x_scale(:,i),1,sample_num)).^2);</span><br><span class="line">    end</span><br><span class="line">    %找到距离最远的两个样本</span><br><span class="line">    [row,col]=find(distance==max(max(distance)),1);</span><br><span class="line">    %将row分为第一类,col分为第二类</span><br><span class="line">    y(row)=1;</span><br><span class="line">    y(col)=2;</span><br><span class="line">    %设置第一类和第二类的中心</span><br><span class="line">    class_center(:,1)=x_scale(:,row);</span><br><span class="line">    class_center(:,2)=x_scale(:,col);</span><br><span class="line">    %目前的类别数</span><br><span class="line">    class_num_temp=2;</span><br><span class="line">    distance_min=zeros(1,sample_num);</span><br><span class="line">    while class_num_temp~=class_num</span><br><span class="line">        for i=1:sample_num</span><br><span class="line">            %求每个样本到每一类的距离</span><br><span class="line">            distance=sum((class_center-repmat(x_scale(:,i),1,class_num_temp)).^2);</span><br><span class="line">            %求出每个样本到每一类距离最小值</span><br><span class="line">            distance_min(i)=distance(find(distance==min(distance),1));</span><br><span class="line">        end</span><br><span class="line">        %找到距离最大值作为一类</span><br><span class="line">        temp=find(distance_min==max(distance_min),1);</span><br><span class="line">        %类别数+1</span><br><span class="line">        class_num_temp=class_num_temp+1;</span><br><span class="line">        %修改类别信息</span><br><span class="line">        y(temp)=class_num_temp;</span><br><span class="line">        for i=1:class_num_temp</span><br><span class="line">            %更新类别的中心</span><br><span class="line">            class_center(:,i)=sum(x_scale(:,y==i),2)/sum(y==i);</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    %采用最近邻进行分类</span><br><span class="line">    for i=1:sample_num</span><br><span class="line">        distance=sum((class_center-repmat(x_scale(:,i),1,class_num_temp)).^2);</span><br><span class="line">        y(i)=find(distance==min(distance),1);</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="DIANA-display-m"><a href="#DIANA-display-m" class="headerlink" title="DIANA_display.m"></a>DIANA_display.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">function DIANA_display(x,y,class_center,sample_num,class_num)</span><br><span class="line">color_bar=zeros(class_num,3);</span><br><span class="line">hold on;</span><br><span class="line">for i=1:class_num</span><br><span class="line">    color_bar(i,:)=[rand(1),rand(1),rand(1)];</span><br><span class="line">    %绘制样本中心，用*表示</span><br><span class="line">    plot(class_center(1,i),class_center(2,i),'color',color_bar(i,:),'marker','*')</span><br><span class="line">end</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    %绘制数据集，用o表示</span><br><span class="line">    plot(x(1,i),x(2,i),'color',color_bar(y(i),:),'marker','o');</span><br><span class="line">end</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/diana.png" alt="DIANA"></p><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>算法简单，容易理解</li><li>不依赖初始值的选择</li><li>对于类别较少的训练集分类较快</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>对噪声数据敏感</li><li>分裂操作不能撤销</li><li>需要在测试前知道类别的个数</li><li>对于类别较多的训练集分类较慢</li><li>只适合分布呈凸型或者球形的数据集</li><li>对于高维数据，距离的度量并不是很好</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;DIANA&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>凝聚的层次聚类(AGNES)</title>
    <link href="https://USTCcoder.github.io/2019/05/01/clustering_AGNES/"/>
    <id>https://USTCcoder.github.io/2019/05/01/clustering_AGNES/</id>
    <published>2019-05-01T11:29:40.000Z</published>
    <updated>2019-08-07T15:51:48.176Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">凝聚的层次聚类方法</font></strong></center><p></p><h1 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a><font size="5" color="red">原理解读</font></h1><p>  AGNES(Agglomerative Nesting):采用<strong>自底向上</strong>的策略，最初将每个对象作为一个类，然后根据某些准则将这些类别<strong>逐一合并</strong>。合并的过程反复进行直到类别达到预期的数目。<br><a id="more"></a></p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a><font size="5" color="red">核心思想</font></h1><p>  <font size="4">1. 将每一个样本都单独作为一类</font></p><p>  <font size="4">2. 合并两类(多种定义方法)，直到满足某个终止条件</font></p><ul><li><p>最小距离：将两个类别之间最近的两个样本之间的距离作为两个类别之间的距离</p><script type="math/tex; mode=display">d_{min}=\underset{x_i \in C_i,x_j \in C_j}{min}d(x_i,x_j)</script></li><li><p>最大距离：将两个类别之间最远的两个样本之间的距离作为两个类别之间的距离</p><script type="math/tex; mode=display">d_{max}=\underset{x_i \in C_i,x_j \in C_j}{max}d(x_i-x_j)</script></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/MACHINE/agnes1.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/MACHINE/agnes2.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><ul><li><p>均值距离：将两个类别中样本的平均值之间的距离作为两个类别之间的距离</p><script type="math/tex; mode=display">d_{mean}=d(\overline {C_i}- \overline {C_j}) \ , \ 其中\overline {C_i}=\frac {1}{\lvert C_i \rvert}\underset{x_i \in C_i}{\sum}{x_i}</script></li><li><p>平均距离：将两个类别中样本间两两距离的平均值作为两个类别之间的距离</p><script type="math/tex; mode=display">d_{avg}=\frac {1}{\lvert C_i \rvert \lvert C_j \rvert}\underset{x_i \in C_i}{\sum}\underset{x_j \in C_j}{\sum}d(x_i-x_j)</script></li></ul><p></p><center><div style="float:left;margin-left:50px"><img src="/images/MACHINE/agnes3.png" width="200" height="260"></div></center><p></p><div style="float:right;margin-right:50px"><img src="/images/MACHINE/agnes4.png" width="200" height="260"></div><div style="float:none;clear:both;"></div><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><font size="5" color="red">算法流程</font></h1><p><img src="/images/MACHINE/agnes9.png" alt="AGNES"></p><p><br><br></p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><p><font size="4">代码中所用数据集可以查看相关文档，<a href="https://ustccoder.github.io/2019/04/25/clustering_Dataset/">数据集(Data Set)</a></font></p><h2 id="AGNES-main-m"><a href="#AGNES-main-m" class="headerlink" title="AGNES_main.m"></a>AGNES_main.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">load('..\\cluster_gauss.mat');</span><br><span class="line">%输入x的矩阵</span><br><span class="line">x=data;</span><br><span class="line">randIndex = randperm(size(x,2));</span><br><span class="line">x=x(:,randIndex);</span><br><span class="line">%希望划分的类别数</span><br><span class="line">class_num=3;</span><br><span class="line">%样本数</span><br><span class="line">sample_num=size(x,2);</span><br><span class="line">%特征数目</span><br><span class="line">feat_num=size(x,1);</span><br><span class="line">%尺度缩放到0-1</span><br><span class="line">x_scale=zeros(size(x));</span><br><span class="line">for i=1:feat_num</span><br><span class="line">    x_scale(i,:)=(x(i,:)-min(x(i,:)))/(max(x(i,:))-min(x(i,:)));</span><br><span class="line">end</span><br><span class="line">[y,class_center]=AGNES_classify(x_scale,sample_num,class_num);</span><br><span class="line">%样本中心尺度复原</span><br><span class="line">for i=1:feat_num</span><br><span class="line">    class_center(i,:)=(max(x(i,:))-min(x(i,:)))*class_center(i,:)+min(x(i,:));</span><br><span class="line">end</span><br><span class="line">%如果数据的特征是二维的，可以绘图表示</span><br><span class="line">if feat_num==2</span><br><span class="line">    AGNES_display(x,y,class_center,sample_num,class_num);</span><br><span class="line">else</span><br><span class="line">    disp('The Feature Is Not Two-Dimensional');</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="AGNES-classify-m"><a href="#AGNES-classify-m" class="headerlink" title="AGNES_classify.m"></a>AGNES_classify.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">function [y,class_center]=AGNES_classify(x_scale,sample_num,class_num)</span><br><span class="line">%给每一个样本分配一个初始类别</span><br><span class="line">y=1:sample_num;</span><br><span class="line">%当前的类别数</span><br><span class="line">class_num_temp=sample_num;</span><br><span class="line">%初始化当前每一类的中心</span><br><span class="line">class_center=x_scale;</span><br><span class="line">while class_num_temp~=class_num</span><br><span class="line">    %初始化类别中心距</span><br><span class="line">    center_distance=zeros(class_num_temp);</span><br><span class="line">    for i=1:class_num_temp</span><br><span class="line">        %计算类别中心距</span><br><span class="line">        center_distance(i,:)=sum((class_center-repmat(class_center(:,i),1,class_num_temp)).^2);</span><br><span class="line">        center_distance(i,i)=inf;</span><br><span class="line">    end</span><br><span class="line">    %从中心距中找到最小值</span><br><span class="line">    [row,col]=find(center_distance==min(min(center_distance)),1);</span><br><span class="line">    %将两类合并</span><br><span class="line">    y(y==col)=row;</span><br><span class="line">    %更新类别，从第1类连续分类</span><br><span class="line">    y(y&gt;col)=y(y&gt;col)-1;</span><br><span class="line">    %类别数-1</span><br><span class="line">    class_num_temp=class_num_temp-1;</span><br><span class="line">    %初始化样本中心</span><br><span class="line">    class_center=zeros(2,class_num_temp);</span><br><span class="line">    for i=1:class_num_temp</span><br><span class="line">        %计算当前每一类的样本中心</span><br><span class="line">        class_center(:,i)=sum(x_scale(:,y==i),2)/sum(y==i);</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure><h2 id="AGNES-display-m"><a href="#AGNES-display-m" class="headerlink" title="AGNES_display.m"></a>AGNES_display.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">function AGNES_display(x,y,class_center,sample_num,class_num)</span><br><span class="line">color_bar=zeros(class_num,3);</span><br><span class="line">hold on;</span><br><span class="line">for i=1:class_num</span><br><span class="line">    color_bar(i,:)=[rand(1),rand(1),rand(1)];</span><br><span class="line">    %绘制样本中心，用*表示</span><br><span class="line">    plot(class_center(1,i),class_center(2,i),'color',color_bar(i,:),'marker','*')</span><br><span class="line">end</span><br><span class="line">for i=1:sample_num</span><br><span class="line">    %绘制数据集，用o表示</span><br><span class="line">    plot(x(1,i),x(2,i),'color',color_bar(y(i),:),'marker','o');</span><br><span class="line">end</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p><br><br></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><font size="5" color="red">实验结果</font></h1><p><img src="/images/MACHINE/agnes.png" alt="AGNES"></p><h1 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a><font size="5" color="red">性能比较</font></h1><ul><li><font size="4"><strong>优点：</strong><ul><li>对噪声数据不敏感</li><li>算法简单，容易理解</li><li>不依赖初始值的选择</li><li>对于类别较多的训练集分类较快</li></ul></font></li><li><font size="4"><strong>缺点：</strong><ul><li>合并操作不能撤销</li><li>需要在测试前知道类别的个数</li><li>对于类别较少的训练集分类较慢</li><li>只适合分布呈凸型或者球形的数据集</li><li>对于高维数据，距离的度量并不是很好</li></ul></font></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;AGNES&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>数据集(Data Set)</title>
    <link href="https://USTCcoder.github.io/2019/04/25/clustering_Dataset/"/>
    <id>https://USTCcoder.github.io/2019/04/25/clustering_Dataset/</id>
    <published>2019-04-25T10:38:53.000Z</published>
    <updated>2019-08-07T15:52:18.752Z</updated>
    
    <content type="html"><![CDATA[<p><strong><font size="5" color="gray"></font></strong></p><center><strong><font size="5" color="gray">数据集说明</font></strong></center><p></p><h1 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a><font size="5" color="red">原理介绍</font></h1><p>  Data Set:对于机器学习领域来说，数据集的选择是<strong>至关重要</strong>的，一个数据集的好坏往往可以<strong>直接决定聚类结果</strong>，通常一个算法<strong>很难</strong>适用于所有的数据集。因此我们需要设计各种数据集，并且<strong>分析哪一种数据类型适合用哪一种算法</strong>，只有这样，在今后的使用中才能得心应手。<a id="more"></a>考虑到数据集的适应性，设计了以下五种不同的数据集，包括<strong>水平竖直型</strong>数据，<strong>斜线型</strong>数据，<strong>圆形</strong>数据，<strong>高斯型</strong>数据和<strong>混合型</strong>数据。</p><h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a><font size="5" color="red">代码实战</font></h1><h2 id="line-data-m"><a href="#line-data-m" class="headerlink" title="line_data.m"></a>line_data.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">hold on;</span><br><span class="line">axis equal;</span><br><span class="line">%线的长度 </span><br><span class="line">long=[10,10,10];</span><br><span class="line">%线的宽度</span><br><span class="line">wide=[1,1,1];</span><br><span class="line">%线的起始位置</span><br><span class="line">x_0=[0,0,0];</span><br><span class="line">y_0=[2,5,8];</span><br><span class="line">%每一条线上元素的个数</span><br><span class="line">num=[500,500,500];</span><br><span class="line">data_temp=zeros(2,sum(num));</span><br><span class="line">for i=1:length(long)</span><br><span class="line">    if i==1</span><br><span class="line">        data_temp(:,1:num(i))=[rand(1,num(i))*long(i)+x_0(i);rand(1,num(i))*wide(i)+y_0(i)];</span><br><span class="line">    else</span><br><span class="line">        data_temp(:,sum(num(1:i-1))+1:sum(num(1:i)))=[rand(1,sum(num(1:i))-sum(num(1:i-1)))*long(i)+x_0(i);rand(1,sum(num(1:i))-sum(num(1:i-1)))*wide(i)+y_0(i)];</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">%随机打乱顺序</span><br><span class="line">randIndex = randperm(size(data_temp,2));</span><br><span class="line">data=data_temp(:,randIndex);</span><br><span class="line">plot(data(1,:),data(2,:),'o');</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/MACHINE/cluster_line.png" alt="line"></p><h2 id="slash-data-m"><a href="#slash-data-m" class="headerlink" title="slash_data.m"></a>slash_data.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">hold on;</span><br><span class="line">axis equal;</span><br><span class="line">%x的起始和终止位置</span><br><span class="line">begend=[0,0,1,6;...</span><br><span class="line">    10,10,5,10];</span><br><span class="line">%斜率和截距</span><br><span class="line">kb=[1,1,-5,-5;...</span><br><span class="line">    -2,7,20,50];</span><br><span class="line">data_temp=[];</span><br><span class="line">for i=1:size(begend,2)</span><br><span class="line">    x=begend(1,i):0.1:begend(2,i);</span><br><span class="line">    data_temp=[data_temp,[x+rand(1,length(x))-0.5;kb(1,i)*x+kb(2,i)+rand(1,length(x))-0.5]];</span><br><span class="line">end</span><br><span class="line">%随机打乱顺序</span><br><span class="line">randIndex = randperm(size(data_temp,2));</span><br><span class="line">data=data_temp(:,randIndex);</span><br><span class="line">plot(data(1,:),data(2,:),'o');</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/MACHINE/cluster_slash.png" alt="slash"></p><h2 id="gauss-data-m"><a href="#gauss-data-m" class="headerlink" title="gauss_data.m"></a>gauss_data.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">hold on;</span><br><span class="line">axis equal;</span><br><span class="line">%簇个数</span><br><span class="line">num=3;</span><br><span class="line">%每一类的个数</span><br><span class="line">number=300;</span><br><span class="line">%u和sigma</span><br><span class="line">data_temp=zeros(2,num*number);</span><br><span class="line">usigma_x=[0,2,6;</span><br><span class="line">    1,1,1];</span><br><span class="line">usigma_y=[0,6,2;</span><br><span class="line">    1,1,1];</span><br><span class="line">for i=1:num</span><br><span class="line">    data_temp(:,(i-1)*number+1:i*number)=[normrnd(usigma_x(1,i),usigma_x(2,i),1,number);normrnd(usigma_y(1,i),usigma_y(2,i),1,number)];</span><br><span class="line">end</span><br><span class="line">%随机打乱顺序</span><br><span class="line">randIndex = randperm(size(data_temp,2));</span><br><span class="line">data=data_temp(:,randIndex);</span><br><span class="line">plot(data(1,:),data(2,:),'o');</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/MACHINE/cluster_gauss.png" alt="gauss"></p><h2 id="cicle-data-m"><a href="#cicle-data-m" class="headerlink" title="cicle_data.m"></a>cicle_data.m</h2><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">hold on;</span><br><span class="line">axis equal;</span><br><span class="line">theta = 0:0.05:2*pi;</span><br><span class="line">x=cos(theta);</span><br><span class="line">y=sin(theta);</span><br><span class="line">%椭圆方程(x+x0)^2/a^2+(y+y0)^2/b^2=1</span><br><span class="line">ab=[3,4,6,10;...</span><br><span class="line">    3,4,6,10];</span><br><span class="line">xy_0=[0,0,0,0;...</span><br><span class="line">    0,0,0,0];</span><br><span class="line">data_temp=zeros(2,length(theta)*size(ab,2));</span><br><span class="line">for i=1:size(ab,2)</span><br><span class="line">    data_temp(:,(i-1)*length(theta)+1:i*length(theta))=([x;y].*repmat(ab(:,i),1,length(theta)))+repmat(xy_0(:,i),1,length(theta));</span><br><span class="line">end</span><br><span class="line">%随机打乱顺序</span><br><span class="line">randIndex = randperm(size(data_temp,2));</span><br><span class="line">data=data_temp(:,randIndex);</span><br><span class="line">plot(data(1,:),data(2,:),'o');</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/MACHINE/cluster_cicle.png" alt="cicle"></p><h2 id="mixture-data-m"><a href="#mixture-data-m" class="headerlink" title="mixture_data.m"></a>mixture_data.m</h2><p><font size="4">由上面的四种数据集组合之后可以形成混合数据集。</font><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">clear;clc;close all;</span><br><span class="line">hold on;</span><br><span class="line">axis equal;</span><br><span class="line">%x1的起始和终止位置</span><br><span class="line">x_1=2:0.1:6;</span><br><span class="line">%x1的斜率和截距</span><br><span class="line">kb_1=[1;4];</span><br><span class="line">data_1=[x_1+rand(1,length(x_1))-0.5;kb_1(1)*x_1+kb_1(2)+rand(1,length(x_1))-0.5];</span><br><span class="line">%x2的起始和终止位置</span><br><span class="line">x_2=1:0.05:7;</span><br><span class="line">%x2的斜率和截距</span><br><span class="line">kb_2=[-1;12];</span><br><span class="line">data_2=[x_2+rand(1,length(x_2))-0.5;kb_2(1)*x_2+kb_2(2)+rand(1,length(x_2))-0.5];</span><br><span class="line">%产生高斯数据集</span><br><span class="line">data_3=normrnd(12,1.5,2,200);</span><br><span class="line">%线的长度 </span><br><span class="line">long=[5,1];</span><br><span class="line">%线的宽度</span><br><span class="line">wide=[1,5];</span><br><span class="line">%线的起始位置</span><br><span class="line">x_0=[6,11];</span><br><span class="line">y_0=[1,1];</span><br><span class="line">%每一条线上元素的个数</span><br><span class="line">num=[100,100];</span><br><span class="line">data_4=zeros(2,sum(num));</span><br><span class="line">for i=1:length(long)</span><br><span class="line">    if i==1</span><br><span class="line">        data_4(:,1:num(i))=[rand(1,num(i))*long(i)+x_0(i);rand(1,num(i))*wide(i)+y_0(i)];</span><br><span class="line">    else</span><br><span class="line">        data_4(:,sum(num(1:i-1))+1:sum(num(1:i)))=[rand(1,sum(num(1:i))-sum(num(1:i-1)))*long(i)+x_0(i);rand(1,sum(num(1:i))-sum(num(1:i-1)))*wide(i)+y_0(i)];</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">%产生噪声点</span><br><span class="line">data_5=rand(2,38)*16;</span><br><span class="line">data_temp=[data_1,data_2,data_3,data_4,data_5];</span><br><span class="line">%随机打乱顺序</span><br><span class="line">randIndex = randperm(size(data_temp,2));</span><br><span class="line">data=data_temp(:,randIndex);</span><br><span class="line">plot(data(1,:),data(2,:),'o');</span><br><span class="line">hold off;</span><br></pre></td></tr></tbody></table></figure><p></p><p><img src="/images/MACHINE/cluster_mixture.png" alt="mixture"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;Data Set&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="https://USTCcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Turing Reward in 2018</title>
    <link href="https://USTCcoder.github.io/2018/03/30/TuringReward-2018/"/>
    <id>https://USTCcoder.github.io/2018/03/30/TuringReward-2018/</id>
    <published>2018-03-30T11:29:40.000Z</published>
    <updated>2020-06-12T11:29:43.255Z</updated>
    
    <content type="html"><![CDATA[<p>  2019 年 3 月 27 日，ACM 宣布，深度学习三位大牛 Yoshua Bengio、Yann LeCun、Geoffrey Hinton 因“在概念和工程方面使深度神经网络成为计算的关键组成部分的突破”获得了 2018 年的图灵奖。近年来，深度学习方法一直是计算机视觉、语音识别、自然语言处理和机器人技术以及其他应用中惊人突破的原因。<a id="more"></a>在 ACM 的公告中是这样写道的：虽然在 20 世纪 80 年代引入了人工神经网络作为帮助计算机识别模式和模拟人类智能的工具，但到了 21 世纪初，LeCun、Hinton 和 Bengio 仍坚持这种方法的小团体。虽然他们重新点燃人工智能社区对神经网络兴趣的努力在最初曾遭到怀疑，但其想法引发了重大的技术进步，其方法现在已成为该领域的主导范例。此前他们在深度学习领域的地位早已是无人不知，尽管三人走向了不同的方向，但他们仍然是多年的合作伙伴和挚友。让我们先来看看三位所作出的主要贡献：<br><strong>杰弗里·辛顿(Geoffery Hinton)</strong><br><img src="/images/TURING/hinton.jpeg" alt="Geoffery Hinton"><br>  <strong>反向传播(Back Propagation)</strong>：在 1986 年与 David Rumelhart 和 Ronald Williams 共同撰写的 “Learning Internal Representations by Error Propagation” 一文中，Hinton 证明了反向传播算法允许神经网络发现自己的数据内部表示，这使得使用神经网络成为可能网络解决以前被认为超出其范围的问题。如今，反向传播算法是大多数神经网络的标准。 </p><p>  <strong>玻尔兹曼机(Boltzmann Machines)</strong>：1983 年，Hinton 与 Terrence Sejnowski 一起，发明了玻尔兹曼机，这是第一个能够学习不属于输入或输出的神经元内部表示的神经网络之一。 </p><p>  <strong>对卷积神经网络的改进(Improvement of Convolutional Neural Network)</strong>：2012 年，Hinton 和他的学生 Alex Krizhevsky 以及 Ilya Sutskever 通过 Rectified Linear Neurons 和 Dropout Regularization 改进了卷积神经网络，并在著名的 ImageNet 评测中将对象识别的错误率减半，在计算机视觉领域掀起一场革命。 </p><p><strong>约书亚·本吉奥(Yoshua Bengio)</strong><br><img src="/images/TURING/bengio.jpeg" alt="Yoshua Bengio"><br>  <strong>序列的概率模型(Probabilistic models of sequences)</strong>：在 20 世纪 90 年代，Bengio 将神经网络与序列的概率模型相结合，例如隐马尔可夫模型。这些想法被纳入 AT＆T / NCR 用于阅读手写支票中，被认为是 20 世纪 90 年代神经网络研究的巅峰之作。现代深度学习语音识别系统也是这些概念的扩展。 </p><p>  <strong>高维词向量嵌入和注意力(High-dimensional word embeddings and attention)</strong>：2000 年，Bengio 撰写了具有里程碑意义的论文“A Neural Probabilistic Language Model”，它引入了高维词向量作为词义的表示。Bengio 的见解对自然语言处理任务产生了巨大而持久的影响，包括语言翻译、问答和视觉问答。他的团队还引入了注意力机制，这种机制促使了机器翻译的突破，并构成了深度学习的序列处理的关键组成部分。 </p><p>  <strong>生成式对抗网络(Generative Adversarial Networks)</strong>：自 2010 年以来，Bengio 关于生成性深度学习的论文，特别是与 Ian Goodfellow 共同开发的生成性对抗网络（GAN），引发了计算机视觉和计算机图形学的革命。 </p><p><strong>杨立昆(Yann LeCun)</strong><br><img src="/images/TURING/lecun.jpeg" alt="Yann LeCun"><br>  <strong>卷积神经网络(Convolutional Neural Networks)</strong>：在 20 世纪 80 年代，LeCun 开发了卷积神经网络，现已成为该领域的基本理论基础。除了其他优点之外，它还具有使深度学习更有效的必要性。在 20 世纪 80 年代后期，多伦多大学和贝尔实验室工作期间，LeCun 是第一个在手写数字图像上训练卷积神经网络系统的人。如今，卷积神经网络是计算机视觉以及语音识别、语音合成、图像合成和自然语言处理的行业标准。它们用于各种应用，包括自动驾驶、医学图像分析、语音激活助手和信息过滤。 </p><p>  <strong>改进反向传播算法(Improved Back Propagation Algorithms)</strong>：LeCun 提出了一个早期的反向传播算法 backprop，并根据变分原理对其进行了简洁的推导。他的工作让加快了反向传播算，包括描述两种加速学习时间的简单方法。 </p><p>  <strong>拓宽神经网络的范围(Widening the Range of Neural Networks)</strong>：LeCun 还将神经网络作为可以完成更为广泛任务的计算模型，其早期工作现已成为 AI 的基础概念。例如，在图像识别领域，他研究了如何在神经网络中学习分层特征表示，这个理念现在通常用于许多识别任务中。与 LéonBottou 一起，他还提出了学习系统可以构建为复杂的模块网络，其中通过自动区分来执行反向传播，目前在每个现代深度学习软件中得到使用。他们还提出了可以操作结构化数据的深度学习架构，如图形。 在《连线》杂志的报道中，Geoffery Hinton 被问及获得图领奖的意义时，他表示十分惊讶，“我猜神经网络现在是受人尊敬的计算机科学”，因为在他看来图灵将是计算机科学中最值得尊敬的学科了。 据了解，图灵奖由 ACM 于 1966 年设置，每年颁发一次，设立目的之一是纪念著名的计算机科学先驱艾伦 • 图灵（Alan Turing），他在 20 世纪 30 年代、40 年代和 50 年代奠定了计算和人工智能的早期基础。 图灵奖是计算机科学领域的最高奖。获奖者必须在计算机领域具有持久重大的先进性技术贡献。人工智能领域的先驱马文 • 明斯基（Marvin Lee Minsky）、约翰 • 麦卡锡（John McCarthy）、艾伦 • 纽厄尔（Allen Newell）和司马贺（Herbert Alexander Simon）等人都曾经获奖。华人科学家姚期智 2000 年因为伪随机数生成等计算领域的重要贡献获奖。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;DEEP LEARNING&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="Computer Science" scheme="https://USTCcoder.github.io/categories/Computer-Science/"/>
    
      <category term="Turing Reward" scheme="https://USTCcoder.github.io/categories/Computer-Science/Turing-Reward/"/>
    
    
  </entry>
  
  <entry>
    <title>Turing Reward in 2017</title>
    <link href="https://USTCcoder.github.io/2018/03/21/TuringReward-2017/"/>
    <id>https://USTCcoder.github.io/2018/03/21/TuringReward-2017/</id>
    <published>2018-03-21T11:29:40.000Z</published>
    <updated>2020-06-12T08:46:12.249Z</updated>
    
    <content type="html"><![CDATA[<p>  2018 年 3 月 21 日，美国计算机协会（ACM）将2017年图灵奖授予斯坦福大学前校长约翰·轩尼诗（John L. Hennessy）和加州大学伯克利分校退休教授大卫·帕特森（David A. Patterson），以表彰他们开创了一种系统的、定量的方法来设计和评价计算机体系结构，并对RISC微处理器行业产生了持久的影响。</p><a id="more"></a><p><img src="/images/TURING/hennessyandpatterson.jpeg" alt="Group photo"></p><p>  降低处理器复杂性的概念架构研究可以追溯到1960年，也就是由IBM资助的801项目，由轩尼诗和帕特森负责。斯坦福大学和加州大学伯克利分校都在大力研究RISC架构的可行性方法,推广其概念,并介绍给学术界和产业界。RISC方法不同于当时流行的复杂指令集计算机(CISC)，因为它只需要一组简单和通用的指令(计算机必须执行的功能)，需要的晶体管数量比复杂指令集少，并且减少了计算机必须执行的工作量。</p><p>  帕特森的伯克利团队创造了RISC这个词，并在1982年建立并演示了他们的RISC-1型处理器。RISC-1型处理器采用了44000个晶体管中，其性能要优于传统的CISC设计，后者的实现往往需要100,000个晶体管。轩尼诗于1984年联合创立了MIPS电脑系统公司，将斯坦福团队的工作市场化。后来，伯克利团队的研究成果通过Sun Microsystems公司的SPARC微处理器架构商业化。</p><p>  尽管许多计算机架构师最初对RISC持怀疑态度，但MIPS和SPARC市场化的成功，RISC设计较低的生产成本，以及更多的研究进展，使RISC得到了业界的广泛接纳。到20世纪90年代中期，RISC微处理器已经在整个领域占据主导地位。</p><p><strong>约翰·轩尼诗(John L. Hennessy)</strong></p><p><img src="/images/TURING/hennessy.jpeg" alt="John L. Hennessy"></p><p>  约翰·轩尼诗，为 MIPS 科技公司创始人，第十任斯坦福大学校长、Alphabet公司董事长。</p><p>  Hennessy出生于1953年。</p><p>  1973年，他从维拉诺瓦大学获取电机工程学士学位。</p><p>  1975年以及1977年，分别从纽约石溪大学获取计算机科学硕士及博士学位。</p><p>  1977年成为斯坦福大学的教师。</p><p>  1981年，他开始进行MIPS项目，并研究RISC处理器。</p><p>  1984年，他利用年度休假的时间创建了 MIPS Computer Systems Inc.，将他研究开发的技术进行商业化。</p><p>  1987年，他成为电气工程和计算机科学的 Willard 和 Inez Kerr Bell 教授。</p><p>  1989年到1993年，Hennessy担任了斯坦福大学计算机系统实验室主任。</p><p>  1994年到1996年，他曾担任斯坦福大学计算机科学系主任。</p><p>  1996年到1999年，他担任斯坦福大学工程学院院长。</p><p>  1999年，斯坦福大学校长格哈德·卡斯帕（Gerhard Casper）任命Hennessy接任斯坦福大学教务长。</p><p>  随后 2000年卡斯帕卸任后，斯坦福董事会任命Hennessy接替卡斯帕出任校长一职，并一直延续到 2016年。在这段时间内斯坦福完成了从一个地区性教育机构到世界顶级大学的蜕变，斯坦福外围的硅谷也成为了世界创新的引擎，而Hennessy教授则成为公认的「硅谷教父」。</p><p>  此外值得注意的是Hennessy从2004年起便加入了Google（后来的Alphabet公司）的董事会，并于2007年担任独立董事。</p><p>  在 2018年 2 月，伴随着 Alphabet 公司（Google 的母公司）发布 2017年财报，还同时宣布 66 岁的Hennessy为 Alphabet 的第三任董事长。（雷锋网曾经有详细报道：全年营收破千亿美元的 Google，迎来了 20年来最大的人事变动）</p><p>  在研究方面，Hennessy与Patterson共同为RISC微处理器创建了一个系统的量化方法。同时他们编写的《计算机体系结构（量化研究方法）》（Computer Architecture: A Quantitative Approach），从1990年以来一直被广泛用作研究生的权威教材，另一方面，Hennessy将 Donald Knuth 的 MIX 处理器更新为 MMIX 做出了贡献。</p><p>  2004年，他1989年合作的一篇关于高性能缓存层次结构的论文获得了计算机械协会 SIGARCH ISCA 的影响论文奖。</p><p>  2009年，Hennessy再次获得该奖，这次是他在1994年合作的有关斯坦福 FLASH 多处理器的论文。</p><p><strong>大卫·帕特森(David A. Patterson)</strong></p><p><img src="/images/TURING/patterson.jpeg" alt="David A. Patterson"></p><p>  Patterson 出生于1947年，</p><p>  1969年从加州大学洛杉矶分校获数学学士学位。</p><p>  1970年和1976年，从加州大学洛杉矶分校分别获得计算机硕士和博士学位。</p><p>  1976年，博士毕业后，加入加州大学伯克利分校计算机系。</p><p>  1994年，当选美国计算机协会会士（ACM Fellow）。</p><p>  2004年至 2006年，任美国计算机协会主席。</p><p>  2016年，DavidPatterson教授宣布从加州大学伯克利分校退休，学校给他举办了一个退休典礼，纪念他在计算机架构方面的 40年学术生涯。一年之后，教授公开宣布自己加入谷歌 TPU 团队，谷歌的 TPU 论文中也有他的署名。（雷锋网详细报道：David Patterson教授公开宣布加入谷歌TPU团队，好戏才刚刚开场）</p><p>  Patterson教授在伯克利大学带领团队长期进行RISC的研究，对全世界RISC处理器的研发和相关应用做出了巨大贡献；他在 2003年到 2005年间是美国总统信息技术咨询委员会成员，2004 到 2006年间任国际计算机学会主席；他还是磁盘阵列 RAID 的研发者之一。</p><p>  威斯康星大学麦迪逊分校计算机系的主任Mark Hill认为，Patterson教授在计算机架构方面是「20世纪后50年里最杰出的几个人之一」。他同时还表示，Patterson教授与Hennessy教授合著的那本计算机架构书是这个领域近25年来最有影响力的教科书。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;table&gt;&lt;tr&gt;&lt;td bgcolor=grey&gt;&lt;font size=5 face=&quot;STXihei&quot; color=white&gt;&lt;center&gt;RISC Microprocessors&lt;/center&gt;&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="Computer Science" scheme="https://USTCcoder.github.io/categories/Computer-Science/"/>
    
      <category term="Turing Reward" scheme="https://USTCcoder.github.io/categories/Computer-Science/Turing-Reward/"/>
    
    
  </entry>
  
</feed>
